{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import unicode_literals  \n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as linear_model\n",
    "\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "import influence.experiments as experiments\n",
    "from influence.nlprocessor import NLProcessor\n",
    "from influence.binaryLogisticRegressionWithLBFGS import BinaryLogisticRegressionWithLBFGS\n",
    "from load_spam import load_spam\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "os.chdir('/Users/mr54725/Documents/repos/INF/')\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets = load_spam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "input_dim = data_sets.train.x.shape[1]\n",
    "weight_decay = 0.0001\n",
    "# weight_decay = 1000 / len(lr_data_sets.train.labels)\n",
    "batch_size = 100\n",
    "initial_learning_rate = 0.001 \n",
    "keep_probs = None\n",
    "decay_epochs = [1000, 10000]\n",
    "max_lbfgs_iter = 1000\n",
    "\n",
    "tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 5338\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mr54725/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [58] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.026312243\n",
      "Train loss (w/o reg) on all data: 0.012582187\n",
      "Test loss (w/o reg) on all data: 0.23130246\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9468599033816425\n",
      "Norm of the mean of gradients: 3.6910953e-07\n",
      "Norm of the params: 16.571096\n"
     ]
    }
   ],
   "source": [
    "tf_model = BinaryLogisticRegressionWithLBFGS(\n",
    "    input_dim=input_dim,\n",
    "    weight_decay=weight_decay,\n",
    "    max_lbfgs_iter=max_lbfgs_iter,\n",
    "    num_classes=num_classes, \n",
    "    batch_size=batch_size,\n",
    "    data_sets=data_sets,\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    keep_probs=keep_probs,\n",
    "    decay_epochs=decay_epochs,\n",
    "    mini_batch=False,\n",
    "    train_dir='output',\n",
    "    log_dir='log',\n",
    "    model_name='spam_logreg')\n",
    "\n",
    "tf_model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.copy(tf_model.data_sets.train.x)\n",
    "Y_train = np.copy(tf_model.data_sets.train.labels)\n",
    "X_test = np.copy(tf_model.data_sets.test.x)\n",
    "Y_test = np.copy(tf_model.data_sets.test.labels) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train_examples = Y_train.shape[0] \n",
    "num_flip_vals = 6\n",
    "num_check_vals = 6\n",
    "num_random_seeds = 40\n",
    "\n",
    "dims = (num_flip_vals, num_check_vals, num_random_seeds, 3)\n",
    "fixed_influence_loo_results = np.zeros(dims)\n",
    "fixed_loss_results = np.zeros(dims)\n",
    "fixed_random_results = np.zeros(dims)\n",
    "\n",
    "flipped_results = np.zeros((num_flip_vals, num_random_seeds, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_results = tf_model.sess.run(\n",
    "    [tf_model.loss_no_reg, tf_model.accuracy_op], \n",
    "    feed_dict=tf_model.all_test_feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig loss: 0.23130. Accuracy: 0.947\n",
      "Using normal model\n",
      "LBFGS training took [292] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.09586017\n",
      "Train loss (w/o reg) on all data: 0.06288983\n",
      "Test loss (w/o reg) on all data: 0.68472266\n",
      "Train acc on all data:  0.9871887841430989\n",
      "Test acc on all data:   0.8859903381642512\n",
      "Norm of the mean of gradients: 1.050105e-05\n",
      "Norm of the params: 25.678919\n",
      "Flipped loss: 0.68472. Accuracy: 0.886\n",
      "### Flips: 206, rs: 0, checks: 206\n",
      "Using normal model\n",
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.047704037\n",
      "Train loss (w/o reg) on all data: 0.0292461\n",
      "Test loss (w/o reg) on all data: 0.4380329\n",
      "Train acc on all data:  0.9961324631375392\n",
      "Test acc on all data:   0.9033816425120773\n",
      "Norm of the mean of gradients: 1.3596321e-05\n",
      "Norm of the params: 19.213503\n",
      "     Influence (LOO): fixed 137 labels. Loss 0.43803. Accuracy 0.903.\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.04579232\n",
      "Train loss (w/o reg) on all data: 0.022352898\n",
      "Test loss (w/o reg) on all data: 0.33680317\n",
      "Train acc on all data:  0.9995165578921924\n",
      "Test acc on all data:   0.9178743961352657\n",
      "Norm of the mean of gradients: 1.607386e-05\n",
      "Norm of the params: 21.651524\n",
      "                Loss: fixed 104 labels. Loss 0.33680. Accuracy 0.918.\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.09369269\n",
      "Train loss (w/o reg) on all data: 0.061043937\n",
      "Test loss (w/o reg) on all data: 0.71600753\n",
      "Train acc on all data:  0.988155668358714\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 1.38751375e-05\n",
      "Norm of the params: 25.553377\n",
      "              Random: fixed   8 labels. Loss 0.71601. Accuracy 0.889.\n",
      "### Flips: 206, rs: 0, checks: 412\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.03969448\n",
      "Train loss (w/o reg) on all data: 0.024069615\n",
      "Test loss (w/o reg) on all data: 0.3071615\n",
      "Train acc on all data:  0.9968576262992507\n",
      "Test acc on all data:   0.9371980676328503\n",
      "Norm of the mean of gradients: 5.4857896e-06\n",
      "Norm of the params: 17.677593\n",
      "     Influence (LOO): fixed 173 labels. Loss 0.30716. Accuracy 0.937.\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.038408004\n",
      "Train loss (w/o reg) on all data: 0.018416954\n",
      "Test loss (w/o reg) on all data: 0.34190613\n",
      "Train acc on all data:  0.9997582789460963\n",
      "Test acc on all data:   0.9342995169082126\n",
      "Norm of the mean of gradients: 4.2063543e-06\n",
      "Norm of the params: 19.99553\n",
      "                Loss: fixed 137 labels. Loss 0.34191. Accuracy 0.934.\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.0907093\n",
      "Train loss (w/o reg) on all data: 0.05877675\n",
      "Test loss (w/o reg) on all data: 0.63578856\n",
      "Train acc on all data:  0.9888808315204254\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 5.562285e-06\n",
      "Norm of the params: 25.271551\n",
      "              Random: fixed  20 labels. Loss 0.63579. Accuracy 0.889.\n",
      "### Flips: 206, rs: 0, checks: 618\n",
      "Using normal model\n",
      "LBFGS training took [82] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.037068676\n",
      "Train loss (w/o reg) on all data: 0.022464292\n",
      "Test loss (w/o reg) on all data: 0.3054017\n",
      "Train acc on all data:  0.9968576262992507\n",
      "Test acc on all data:   0.9371980676328503\n",
      "Norm of the mean of gradients: 6.0193584e-06\n",
      "Norm of the params: 17.090576\n",
      "     Influence (LOO): fixed 183 labels. Loss 0.30540. Accuracy 0.937.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.03455723\n",
      "Train loss (w/o reg) on all data: 0.016514437\n",
      "Test loss (w/o reg) on all data: 0.31702152\n",
      "Train acc on all data:  0.9995165578921924\n",
      "Test acc on all data:   0.9371980676328503\n",
      "Norm of the mean of gradients: 3.322044e-06\n",
      "Norm of the params: 18.996204\n",
      "                Loss: fixed 162 labels. Loss 0.31702. Accuracy 0.937.\n",
      "Using normal model\n",
      "LBFGS training took [309] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.085943274\n",
      "Train loss (w/o reg) on all data: 0.054798856\n",
      "Test loss (w/o reg) on all data: 0.591473\n",
      "Train acc on all data:  0.989364273628233\n",
      "Test acc on all data:   0.9033816425120773\n",
      "Norm of the mean of gradients: 6.577449e-06\n",
      "Norm of the params: 24.957735\n",
      "              Random: fixed  30 labels. Loss 0.59147. Accuracy 0.903.\n",
      "### Flips: 206, rs: 0, checks: 824\n",
      "Using normal model\n",
      "LBFGS training took [88] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.035493486\n",
      "Train loss (w/o reg) on all data: 0.020926584\n",
      "Test loss (w/o reg) on all data: 0.29141897\n",
      "Train acc on all data:  0.9970993473531544\n",
      "Test acc on all data:   0.9381642512077295\n",
      "Norm of the mean of gradients: 7.63666e-07\n",
      "Norm of the params: 17.06863\n",
      "     Influence (LOO): fixed 186 labels. Loss 0.29142. Accuracy 0.938.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.031337176\n",
      "Train loss (w/o reg) on all data: 0.015027762\n",
      "Test loss (w/o reg) on all data: 0.30537072\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.936231884057971\n",
      "Norm of the mean of gradients: 2.9272212e-05\n",
      "Norm of the params: 18.060686\n",
      "                Loss: fixed 173 labels. Loss 0.30537. Accuracy 0.936.\n",
      "Using normal model\n",
      "LBFGS training took [313] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.0827199\n",
      "Train loss (w/o reg) on all data: 0.052195903\n",
      "Test loss (w/o reg) on all data: 0.54684716\n",
      "Train acc on all data:  0.990572878897752\n",
      "Test acc on all data:   0.9091787439613527\n",
      "Norm of the mean of gradients: 2.140194e-05\n",
      "Norm of the params: 24.707895\n",
      "              Random: fixed  39 labels. Loss 0.54685. Accuracy 0.909.\n",
      "### Flips: 206, rs: 0, checks: 1030\n",
      "Using normal model\n",
      "LBFGS training took [95] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.033673786\n",
      "Train loss (w/o reg) on all data: 0.01940171\n",
      "Test loss (w/o reg) on all data: 0.28216505\n",
      "Train acc on all data:  0.9973410684070583\n",
      "Test acc on all data:   0.9439613526570049\n",
      "Norm of the mean of gradients: 6.771426e-07\n",
      "Norm of the params: 16.895012\n",
      "     Influence (LOO): fixed 192 labels. Loss 0.28217. Accuracy 0.944.\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.030123204\n",
      "Train loss (w/o reg) on all data: 0.014422308\n",
      "Test loss (w/o reg) on all data: 0.29602388\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9400966183574879\n",
      "Norm of the mean of gradients: 2.9544967e-06\n",
      "Norm of the params: 17.720552\n",
      "                Loss: fixed 179 labels. Loss 0.29602. Accuracy 0.940.\n",
      "Using normal model\n",
      "LBFGS training took [301] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08012903\n",
      "Train loss (w/o reg) on all data: 0.05007428\n",
      "Test loss (w/o reg) on all data: 0.5402758\n",
      "Train acc on all data:  0.991781484167271\n",
      "Test acc on all data:   0.9072463768115943\n",
      "Norm of the mean of gradients: 8.787398e-06\n",
      "Norm of the params: 24.517235\n",
      "              Random: fixed  47 labels. Loss 0.54028. Accuracy 0.907.\n",
      "### Flips: 206, rs: 0, checks: 1236\n",
      "Using normal model\n",
      "LBFGS training took [85] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.03085891\n",
      "Train loss (w/o reg) on all data: 0.016638212\n",
      "Test loss (w/o reg) on all data: 0.2668073\n",
      "Train acc on all data:  0.9980662315687696\n",
      "Test acc on all data:   0.9458937198067633\n",
      "Norm of the mean of gradients: 1.6633259e-06\n",
      "Norm of the params: 16.864578\n",
      "     Influence (LOO): fixed 195 labels. Loss 0.26681. Accuracy 0.946.\n",
      "Using normal model\n",
      "LBFGS training took [199] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.029834228\n",
      "Train loss (w/o reg) on all data: 0.014295562\n",
      "Test loss (w/o reg) on all data: 0.29285932\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9410628019323671\n",
      "Norm of the mean of gradients: 2.729463e-06\n",
      "Norm of the params: 17.628763\n",
      "                Loss: fixed 183 labels. Loss 0.29286. Accuracy 0.941.\n",
      "Using normal model\n",
      "LBFGS training took [282] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.07721685\n",
      "Train loss (w/o reg) on all data: 0.047853\n",
      "Test loss (w/o reg) on all data: 0.5646801\n",
      "Train acc on all data:  0.9922649262750786\n",
      "Test acc on all data:   0.9033816425120773\n",
      "Norm of the mean of gradients: 1.8168856e-05\n",
      "Norm of the params: 24.2338\n",
      "              Random: fixed  59 labels. Loss 0.56468. Accuracy 0.903.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [335] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.1006\n",
      "Train loss (w/o reg) on all data: 0.067647725\n",
      "Test loss (w/o reg) on all data: 0.4609756\n",
      "Train acc on all data:  0.9859801788735799\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 5.9097115e-06\n",
      "Norm of the params: 25.67188\n",
      "Flipped loss: 0.46098. Accuracy: 0.889\n",
      "### Flips: 206, rs: 1, checks: 206\n",
      "Using normal model\n",
      "LBFGS training took [146] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.051550373\n",
      "Train loss (w/o reg) on all data: 0.033563662\n",
      "Test loss (w/o reg) on all data: 0.3041288\n",
      "Train acc on all data:  0.9937152525985014\n",
      "Test acc on all data:   0.9265700483091788\n",
      "Norm of the mean of gradients: 3.9909755e-06\n",
      "Norm of the params: 18.96666\n",
      "     Influence (LOO): fixed 136 labels. Loss 0.30413. Accuracy 0.927.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.04381682\n",
      "Train loss (w/o reg) on all data: 0.021597132\n",
      "Test loss (w/o reg) on all data: 0.34238857\n",
      "Train acc on all data:  0.9987913947304811\n",
      "Test acc on all data:   0.9246376811594202\n",
      "Norm of the mean of gradients: 6.5925774e-06\n",
      "Norm of the params: 21.080652\n",
      "                Loss: fixed 106 labels. Loss 0.34239. Accuracy 0.925.\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.09821102\n",
      "Train loss (w/o reg) on all data: 0.06579315\n",
      "Test loss (w/o reg) on all data: 0.46363437\n",
      "Train acc on all data:  0.9869470630891951\n",
      "Test acc on all data:   0.893719806763285\n",
      "Norm of the mean of gradients: 1.08512e-05\n",
      "Norm of the params: 25.462864\n",
      "              Random: fixed  12 labels. Loss 0.46363. Accuracy 0.894.\n",
      "### Flips: 206, rs: 1, checks: 412\n",
      "Using normal model\n",
      "LBFGS training took [83] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.044697188\n",
      "Train loss (w/o reg) on all data: 0.029828582\n",
      "Test loss (w/o reg) on all data: 0.25831282\n",
      "Train acc on all data:  0.994198694706309\n",
      "Test acc on all data:   0.9400966183574879\n",
      "Norm of the mean of gradients: 7.964514e-07\n",
      "Norm of the params: 17.244482\n",
      "     Influence (LOO): fixed 172 labels. Loss 0.25831. Accuracy 0.940.\n",
      "Using normal model\n",
      "LBFGS training took [199] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.036506236\n",
      "Train loss (w/o reg) on all data: 0.017547652\n",
      "Test loss (w/o reg) on all data: 0.2917993\n",
      "Train acc on all data:  0.9995165578921924\n",
      "Test acc on all data:   0.9381642512077295\n",
      "Norm of the mean of gradients: 8.1357275e-06\n",
      "Norm of the params: 19.472332\n",
      "                Loss: fixed 143 labels. Loss 0.29180. Accuracy 0.938.\n",
      "Using normal model\n",
      "LBFGS training took [319] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.09246042\n",
      "Train loss (w/o reg) on all data: 0.060850028\n",
      "Test loss (w/o reg) on all data: 0.4794573\n",
      "Train acc on all data:  0.9888808315204254\n",
      "Test acc on all data:   0.9014492753623189\n",
      "Norm of the mean of gradients: 1.6074686e-05\n",
      "Norm of the params: 25.143744\n",
      "              Random: fixed  24 labels. Loss 0.47946. Accuracy 0.901.\n",
      "### Flips: 206, rs: 1, checks: 618\n",
      "Using normal model\n",
      "LBFGS training took [80] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.041851573\n",
      "Train loss (w/o reg) on all data: 0.02757885\n",
      "Test loss (w/o reg) on all data: 0.26763964\n",
      "Train acc on all data:  0.9949238578680203\n",
      "Test acc on all data:   0.9410628019323671\n",
      "Norm of the mean of gradients: 1.4319547e-06\n",
      "Norm of the params: 16.895401\n",
      "     Influence (LOO): fixed 180 labels. Loss 0.26764. Accuracy 0.941.\n",
      "Using normal model\n",
      "LBFGS training took [172] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.033152442\n",
      "Train loss (w/o reg) on all data: 0.01591182\n",
      "Test loss (w/o reg) on all data: 0.27440846\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9391304347826087\n",
      "Norm of the mean of gradients: 2.5294066e-06\n",
      "Norm of the params: 18.569126\n",
      "                Loss: fixed 159 labels. Loss 0.27441. Accuracy 0.939.\n",
      "Using normal model\n",
      "LBFGS training took [307] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08913933\n",
      "Train loss (w/o reg) on all data: 0.058544002\n",
      "Test loss (w/o reg) on all data: 0.4249753\n",
      "Train acc on all data:  0.9891225525743292\n",
      "Test acc on all data:   0.9033816425120773\n",
      "Norm of the mean of gradients: 8.159442e-06\n",
      "Norm of the params: 24.736742\n",
      "              Random: fixed  35 labels. Loss 0.42498. Accuracy 0.903.\n",
      "### Flips: 206, rs: 1, checks: 824\n",
      "Using normal model\n",
      "LBFGS training took [80] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.03908828\n",
      "Train loss (w/o reg) on all data: 0.024748918\n",
      "Test loss (w/o reg) on all data: 0.25684875\n",
      "Train acc on all data:  0.9958907420836355\n",
      "Test acc on all data:   0.9449275362318841\n",
      "Norm of the mean of gradients: 7.645393e-07\n",
      "Norm of the params: 16.934793\n",
      "     Influence (LOO): fixed 186 labels. Loss 0.25685. Accuracy 0.945.\n",
      "Using normal model\n",
      "LBFGS training took [122] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.029920042\n",
      "Train loss (w/o reg) on all data: 0.014346259\n",
      "Test loss (w/o reg) on all data: 0.26195464\n",
      "Train acc on all data:  0.9990331157843848\n",
      "Test acc on all data:   0.9449275362318841\n",
      "Norm of the mean of gradients: 1.9693293e-06\n",
      "Norm of the params: 17.648676\n",
      "                Loss: fixed 178 labels. Loss 0.26195. Accuracy 0.945.\n",
      "Using normal model\n",
      "LBFGS training took [292] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.087157294\n",
      "Train loss (w/o reg) on all data: 0.057108536\n",
      "Test loss (w/o reg) on all data: 0.40747115\n",
      "Train acc on all data:  0.9886391104665216\n",
      "Test acc on all data:   0.9082125603864735\n",
      "Norm of the mean of gradients: 4.146107e-06\n",
      "Norm of the params: 24.514803\n",
      "              Random: fixed  47 labels. Loss 0.40747. Accuracy 0.908.\n",
      "### Flips: 206, rs: 1, checks: 1030\n",
      "Using normal model\n",
      "LBFGS training took [74] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.037830964\n",
      "Train loss (w/o reg) on all data: 0.023437114\n",
      "Test loss (w/o reg) on all data: 0.25944966\n",
      "Train acc on all data:  0.9961324631375392\n",
      "Test acc on all data:   0.9429951690821256\n",
      "Norm of the mean of gradients: 5.17867e-07\n",
      "Norm of the params: 16.966942\n",
      "     Influence (LOO): fixed 190 labels. Loss 0.25945. Accuracy 0.943.\n",
      "Using normal model\n",
      "LBFGS training took [101] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.029132532\n",
      "Train loss (w/o reg) on all data: 0.013977691\n",
      "Test loss (w/o reg) on all data: 0.25896326\n",
      "Train acc on all data:  0.9990331157843848\n",
      "Test acc on all data:   0.9420289855072463\n",
      "Norm of the mean of gradients: 6.604408e-07\n",
      "Norm of the params: 17.409676\n",
      "                Loss: fixed 183 labels. Loss 0.25896. Accuracy 0.942.\n",
      "Using normal model\n",
      "LBFGS training took [286] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08435056\n",
      "Train loss (w/o reg) on all data: 0.05514385\n",
      "Test loss (w/o reg) on all data: 0.4029405\n",
      "Train acc on all data:  0.9898477157360406\n",
      "Test acc on all data:   0.9178743961352657\n",
      "Norm of the mean of gradients: 6.2771073e-06\n",
      "Norm of the params: 24.168875\n",
      "              Random: fixed  57 labels. Loss 0.40294. Accuracy 0.918.\n",
      "### Flips: 206, rs: 1, checks: 1236\n",
      "Using normal model\n",
      "LBFGS training took [68] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.035846453\n",
      "Train loss (w/o reg) on all data: 0.021547994\n",
      "Test loss (w/o reg) on all data: 0.26525116\n",
      "Train acc on all data:  0.9968576262992507\n",
      "Test acc on all data:   0.9449275362318841\n",
      "Norm of the mean of gradients: 1.8740122e-06\n",
      "Norm of the params: 16.910627\n",
      "     Influence (LOO): fixed 193 labels. Loss 0.26525. Accuracy 0.945.\n",
      "Using normal model\n",
      "LBFGS training took [97] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.028474577\n",
      "Train loss (w/o reg) on all data: 0.013629575\n",
      "Test loss (w/o reg) on all data: 0.24317347\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9410628019323671\n",
      "Norm of the mean of gradients: 7.7374483e-07\n",
      "Norm of the params: 17.23079\n",
      "                Loss: fixed 189 labels. Loss 0.24317. Accuracy 0.941.\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.0819349\n",
      "Train loss (w/o reg) on all data: 0.053416874\n",
      "Test loss (w/o reg) on all data: 0.4107175\n",
      "Train acc on all data:  0.9903311578438482\n",
      "Test acc on all data:   0.9178743961352657\n",
      "Norm of the mean of gradients: 8.397284e-06\n",
      "Norm of the params: 23.882223\n",
      "              Random: fixed  69 labels. Loss 0.41072. Accuracy 0.918.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.09634335\n",
      "Train loss (w/o reg) on all data: 0.06318563\n",
      "Test loss (w/o reg) on all data: 0.6646311\n",
      "Train acc on all data:  0.9886391104665216\n",
      "Test acc on all data:   0.8347826086956521\n",
      "Norm of the mean of gradients: 2.9230016e-05\n",
      "Norm of the params: 25.751791\n",
      "Flipped loss: 0.66463. Accuracy: 0.835\n",
      "### Flips: 206, rs: 2, checks: 206\n",
      "Using normal model\n",
      "LBFGS training took [83] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.045698404\n",
      "Train loss (w/o reg) on all data: 0.029091261\n",
      "Test loss (w/o reg) on all data: 0.2675844\n",
      "Train acc on all data:  0.9944404157602127\n",
      "Test acc on all data:   0.9429951690821256\n",
      "Norm of the mean of gradients: 8.247417e-07\n",
      "Norm of the params: 18.22479\n",
      "     Influence (LOO): fixed 147 labels. Loss 0.26758. Accuracy 0.943.\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.047422707\n",
      "Train loss (w/o reg) on all data: 0.023423987\n",
      "Test loss (w/o reg) on all data: 0.3667804\n",
      "Train acc on all data:  0.9987913947304811\n",
      "Test acc on all data:   0.9178743961352657\n",
      "Norm of the mean of gradients: 7.110982e-06\n",
      "Norm of the params: 21.90832\n",
      "                Loss: fixed  95 labels. Loss 0.36678. Accuracy 0.918.\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.092120245\n",
      "Train loss (w/o reg) on all data: 0.060164426\n",
      "Test loss (w/o reg) on all data: 0.5706241\n",
      "Train acc on all data:  0.989364273628233\n",
      "Test acc on all data:   0.8724637681159421\n",
      "Norm of the mean of gradients: 8.402699e-06\n",
      "Norm of the params: 25.280752\n",
      "              Random: fixed  11 labels. Loss 0.57062. Accuracy 0.872.\n",
      "### Flips: 206, rs: 2, checks: 412\n",
      "Using normal model\n",
      "LBFGS training took [66] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.03940923\n",
      "Train loss (w/o reg) on all data: 0.024531875\n",
      "Test loss (w/o reg) on all data: 0.26983434\n",
      "Train acc on all data:  0.9949238578680203\n",
      "Test acc on all data:   0.9391304347826087\n",
      "Norm of the mean of gradients: 3.5997667e-07\n",
      "Norm of the params: 17.249557\n",
      "     Influence (LOO): fixed 175 labels. Loss 0.26983. Accuracy 0.939.\n",
      "Using normal model\n",
      "LBFGS training took [138] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.035851534\n",
      "Train loss (w/o reg) on all data: 0.01729711\n",
      "Test loss (w/o reg) on all data: 0.3046504\n",
      "Train acc on all data:  0.9990331157843848\n",
      "Test acc on all data:   0.927536231884058\n",
      "Norm of the mean of gradients: 2.4895064e-06\n",
      "Norm of the params: 19.263664\n",
      "                Loss: fixed 143 labels. Loss 0.30465. Accuracy 0.928.\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.09019634\n",
      "Train loss (w/o reg) on all data: 0.058547802\n",
      "Test loss (w/o reg) on all data: 0.5434704\n",
      "Train acc on all data:  0.989364273628233\n",
      "Test acc on all data:   0.8695652173913043\n",
      "Norm of the mean of gradients: 2.3752988e-05\n",
      "Norm of the params: 25.15891\n",
      "              Random: fixed  18 labels. Loss 0.54347. Accuracy 0.870.\n",
      "### Flips: 206, rs: 2, checks: 618\n",
      "Using normal model\n",
      "LBFGS training took [66] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.037714653\n",
      "Train loss (w/o reg) on all data: 0.023212757\n",
      "Test loss (w/o reg) on all data: 0.2563545\n",
      "Train acc on all data:  0.9961324631375392\n",
      "Test acc on all data:   0.9410628019323671\n",
      "Norm of the mean of gradients: 4.5930605e-07\n",
      "Norm of the params: 17.030502\n",
      "     Influence (LOO): fixed 184 labels. Loss 0.25635. Accuracy 0.941.\n",
      "Using normal model\n",
      "LBFGS training took [130] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.032613494\n",
      "Train loss (w/o reg) on all data: 0.015815815\n",
      "Test loss (w/o reg) on all data: 0.27666438\n",
      "Train acc on all data:  0.9987913947304811\n",
      "Test acc on all data:   0.9371980676328503\n",
      "Norm of the mean of gradients: 3.594218e-06\n",
      "Norm of the params: 18.32904\n",
      "                Loss: fixed 164 labels. Loss 0.27666. Accuracy 0.937.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08787556\n",
      "Train loss (w/o reg) on all data: 0.05682001\n",
      "Test loss (w/o reg) on all data: 0.54665834\n",
      "Train acc on all data:  0.9900894367899444\n",
      "Test acc on all data:   0.8705314009661835\n",
      "Norm of the mean of gradients: 7.781435e-06\n",
      "Norm of the params: 24.922106\n",
      "              Random: fixed  26 labels. Loss 0.54666. Accuracy 0.871.\n",
      "### Flips: 206, rs: 2, checks: 824\n",
      "Using normal model\n",
      "LBFGS training took [62] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.034945153\n",
      "Train loss (w/o reg) on all data: 0.020431718\n",
      "Test loss (w/o reg) on all data: 0.25698367\n",
      "Train acc on all data:  0.9966159052453468\n",
      "Test acc on all data:   0.9439613526570049\n",
      "Norm of the mean of gradients: 1.7540299e-06\n",
      "Norm of the params: 17.037277\n",
      "     Influence (LOO): fixed 188 labels. Loss 0.25698. Accuracy 0.944.\n",
      "Using normal model\n",
      "LBFGS training took [104] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.029405097\n",
      "Train loss (w/o reg) on all data: 0.014105469\n",
      "Test loss (w/o reg) on all data: 0.255467\n",
      "Train acc on all data:  0.9990331157843848\n",
      "Test acc on all data:   0.9420289855072463\n",
      "Norm of the mean of gradients: 1.4401236e-06\n",
      "Norm of the params: 17.492647\n",
      "                Loss: fixed 177 labels. Loss 0.25547. Accuracy 0.942.\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.0833451\n",
      "Train loss (w/o reg) on all data: 0.053827893\n",
      "Test loss (w/o reg) on all data: 0.55208695\n",
      "Train acc on all data:  0.9903311578438482\n",
      "Test acc on all data:   0.8714975845410629\n",
      "Norm of the mean of gradients: 4.617509e-06\n",
      "Norm of the params: 24.296999\n",
      "              Random: fixed  42 labels. Loss 0.55209. Accuracy 0.871.\n",
      "### Flips: 206, rs: 2, checks: 1030\n",
      "Using normal model\n",
      "LBFGS training took [67] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.03386192\n",
      "Train loss (w/o reg) on all data: 0.019900216\n",
      "Test loss (w/o reg) on all data: 0.25176984\n",
      "Train acc on all data:  0.9968576262992507\n",
      "Test acc on all data:   0.9439613526570049\n",
      "Norm of the mean of gradients: 1.1662084e-06\n",
      "Norm of the params: 16.710302\n",
      "     Influence (LOO): fixed 192 labels. Loss 0.25177. Accuracy 0.944.\n",
      "Using normal model\n",
      "LBFGS training took [95] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.028542332\n",
      "Train loss (w/o reg) on all data: 0.013711782\n",
      "Test loss (w/o reg) on all data: 0.26932096\n",
      "Train acc on all data:  0.9987913947304811\n",
      "Test acc on all data:   0.9400966183574879\n",
      "Norm of the mean of gradients: 5.982701e-07\n",
      "Norm of the params: 17.222403\n",
      "                Loss: fixed 186 labels. Loss 0.26932. Accuracy 0.940.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08081129\n",
      "Train loss (w/o reg) on all data: 0.0518768\n",
      "Test loss (w/o reg) on all data: 0.5327804\n",
      "Train acc on all data:  0.9912980420594634\n",
      "Test acc on all data:   0.881159420289855\n",
      "Norm of the mean of gradients: 1.3981112e-05\n",
      "Norm of the params: 24.055973\n",
      "              Random: fixed  52 labels. Loss 0.53278. Accuracy 0.881.\n",
      "### Flips: 206, rs: 2, checks: 1236\n",
      "Using normal model\n",
      "LBFGS training took [63] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.032487147\n",
      "Train loss (w/o reg) on all data: 0.018651819\n",
      "Test loss (w/o reg) on all data: 0.23493835\n",
      "Train acc on all data:  0.9973410684070583\n",
      "Test acc on all data:   0.9468599033816425\n",
      "Norm of the mean of gradients: 3.509569e-07\n",
      "Norm of the params: 16.634502\n",
      "     Influence (LOO): fixed 195 labels. Loss 0.23494. Accuracy 0.947.\n",
      "Using normal model\n",
      "LBFGS training took [77] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.027780771\n",
      "Train loss (w/o reg) on all data: 0.013338073\n",
      "Test loss (w/o reg) on all data: 0.2694794\n",
      "Train acc on all data:  0.9990331157843848\n",
      "Test acc on all data:   0.9400966183574879\n",
      "Norm of the mean of gradients: 5.0029e-07\n",
      "Norm of the params: 16.995705\n",
      "                Loss: fixed 189 labels. Loss 0.26948. Accuracy 0.940.\n",
      "Using normal model\n",
      "LBFGS training took [159] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.07532245\n",
      "Train loss (w/o reg) on all data: 0.047599845\n",
      "Test loss (w/o reg) on all data: 0.48821062\n",
      "Train acc on all data:  0.9922649262750786\n",
      "Test acc on all data:   0.8879227053140096\n",
      "Norm of the mean of gradients: 6.4613714e-06\n",
      "Norm of the params: 23.54681\n",
      "              Random: fixed  68 labels. Loss 0.48821. Accuracy 0.888.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [358] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.103110366\n",
      "Train loss (w/o reg) on all data: 0.07178589\n",
      "Test loss (w/o reg) on all data: 0.45149013\n",
      "Train acc on all data:  0.9857384578196761\n",
      "Test acc on all data:   0.9043478260869565\n",
      "Norm of the mean of gradients: 9.181642e-06\n",
      "Norm of the params: 25.029772\n",
      "Flipped loss: 0.45149. Accuracy: 0.904\n",
      "### Flips: 206, rs: 3, checks: 206\n",
      "Using normal model\n",
      "LBFGS training took [86] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.052776873\n",
      "Train loss (w/o reg) on all data: 0.037053186\n",
      "Test loss (w/o reg) on all data: 0.28429523\n",
      "Train acc on all data:  0.9915397631133672\n",
      "Test acc on all data:   0.9400966183574879\n",
      "Norm of the mean of gradients: 7.217853e-07\n",
      "Norm of the params: 17.733406\n",
      "     Influence (LOO): fixed 141 labels. Loss 0.28430. Accuracy 0.940.\n",
      "Using normal model\n",
      "LBFGS training took [264] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.045513213\n",
      "Train loss (w/o reg) on all data: 0.022147559\n",
      "Test loss (w/o reg) on all data: 0.33595976\n",
      "Train acc on all data:  0.9995165578921924\n",
      "Test acc on all data:   0.9256038647342996\n",
      "Norm of the mean of gradients: 6.715954e-06\n",
      "Norm of the params: 21.61743\n",
      "                Loss: fixed 102 labels. Loss 0.33596. Accuracy 0.926.\n",
      "Using normal model\n",
      "LBFGS training took [310] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.10064699\n",
      "Train loss (w/o reg) on all data: 0.06984227\n",
      "Test loss (w/o reg) on all data: 0.47078934\n",
      "Train acc on all data:  0.9859801788735799\n",
      "Test acc on all data:   0.9062801932367149\n",
      "Norm of the mean of gradients: 2.4002433e-05\n",
      "Norm of the params: 24.82125\n",
      "              Random: fixed  11 labels. Loss 0.47079. Accuracy 0.906.\n",
      "### Flips: 206, rs: 3, checks: 412\n",
      "Using normal model\n",
      "LBFGS training took [83] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.049709834\n",
      "Train loss (w/o reg) on all data: 0.03513778\n",
      "Test loss (w/o reg) on all data: 0.28526053\n",
      "Train acc on all data:  0.9915397631133672\n",
      "Test acc on all data:   0.9429951690821256\n",
      "Norm of the mean of gradients: 3.3751226e-06\n",
      "Norm of the params: 17.071651\n",
      "     Influence (LOO): fixed 160 labels. Loss 0.28526. Accuracy 0.943.\n",
      "Using normal model\n",
      "LBFGS training took [175] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.034340754\n",
      "Train loss (w/o reg) on all data: 0.01653091\n",
      "Test loss (w/o reg) on all data: 0.293383\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9304347826086956\n",
      "Norm of the mean of gradients: 6.057177e-06\n",
      "Norm of the params: 18.873178\n",
      "                Loss: fixed 143 labels. Loss 0.29338. Accuracy 0.930.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.09666942\n",
      "Train loss (w/o reg) on all data: 0.06686183\n",
      "Test loss (w/o reg) on all data: 0.44738242\n",
      "Train acc on all data:  0.9867053420352913\n",
      "Test acc on all data:   0.9062801932367149\n",
      "Norm of the mean of gradients: 4.524338e-05\n",
      "Norm of the params: 24.416218\n",
      "              Random: fixed  21 labels. Loss 0.44738. Accuracy 0.906.\n",
      "### Flips: 206, rs: 3, checks: 618\n",
      "Using normal model\n",
      "LBFGS training took [74] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.04796801\n",
      "Train loss (w/o reg) on all data: 0.03361244\n",
      "Test loss (w/o reg) on all data: 0.28930128\n",
      "Train acc on all data:  0.9922649262750786\n",
      "Test acc on all data:   0.9468599033816425\n",
      "Norm of the mean of gradients: 7.240813e-07\n",
      "Norm of the params: 16.944365\n",
      "     Influence (LOO): fixed 167 labels. Loss 0.28930. Accuracy 0.947.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.030481534\n",
      "Train loss (w/o reg) on all data: 0.0145017225\n",
      "Test loss (w/o reg) on all data: 0.24574992\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9439613526570049\n",
      "Norm of the mean of gradients: 1.346329e-06\n",
      "Norm of the params: 17.877256\n",
      "                Loss: fixed 161 labels. Loss 0.24575. Accuracy 0.944.\n",
      "Using normal model\n",
      "LBFGS training took [285] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.0939928\n",
      "Train loss (w/o reg) on all data: 0.06482941\n",
      "Test loss (w/o reg) on all data: 0.4130973\n",
      "Train acc on all data:  0.9874305051970027\n",
      "Test acc on all data:   0.9101449275362319\n",
      "Norm of the mean of gradients: 4.0197097e-05\n",
      "Norm of the params: 24.15094\n",
      "              Random: fixed  35 labels. Loss 0.41310. Accuracy 0.910.\n",
      "### Flips: 206, rs: 3, checks: 824\n",
      "Using normal model\n",
      "LBFGS training took [77] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.044519786\n",
      "Train loss (w/o reg) on all data: 0.030391388\n",
      "Test loss (w/o reg) on all data: 0.28320974\n",
      "Train acc on all data:  0.9934735315445975\n",
      "Test acc on all data:   0.9420289855072463\n",
      "Norm of the mean of gradients: 6.0922054e-07\n",
      "Norm of the params: 16.809757\n",
      "     Influence (LOO): fixed 178 labels. Loss 0.28321. Accuracy 0.942.\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.028967097\n",
      "Train loss (w/o reg) on all data: 0.013787051\n",
      "Test loss (w/o reg) on all data: 0.234811\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9429951690821256\n",
      "Norm of the mean of gradients: 2.4026663e-06\n",
      "Norm of the params: 17.424156\n",
      "                Loss: fixed 169 labels. Loss 0.23481. Accuracy 0.943.\n",
      "Using normal model\n",
      "LBFGS training took [322] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.09173815\n",
      "Train loss (w/o reg) on all data: 0.06306806\n",
      "Test loss (w/o reg) on all data: 0.4199593\n",
      "Train acc on all data:  0.988155668358714\n",
      "Test acc on all data:   0.9082125603864735\n",
      "Norm of the mean of gradients: 6.6587504e-06\n",
      "Norm of the params: 23.945814\n",
      "              Random: fixed  42 labels. Loss 0.41996. Accuracy 0.908.\n",
      "### Flips: 206, rs: 3, checks: 1030\n",
      "Using normal model\n",
      "LBFGS training took [73] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.043651544\n",
      "Train loss (w/o reg) on all data: 0.029504549\n",
      "Test loss (w/o reg) on all data: 0.28316432\n",
      "Train acc on all data:  0.9937152525985014\n",
      "Test acc on all data:   0.9420289855072463\n",
      "Norm of the mean of gradients: 7.6566243e-07\n",
      "Norm of the params: 16.82082\n",
      "     Influence (LOO): fixed 181 labels. Loss 0.28316. Accuracy 0.942.\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.028047826\n",
      "Train loss (w/o reg) on all data: 0.013350924\n",
      "Test loss (w/o reg) on all data: 0.23809388\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9468599033816425\n",
      "Norm of the mean of gradients: 2.3957816e-06\n",
      "Norm of the params: 17.14463\n",
      "                Loss: fixed 179 labels. Loss 0.23809. Accuracy 0.947.\n",
      "Using normal model\n",
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.087536514\n",
      "Train loss (w/o reg) on all data: 0.059476256\n",
      "Test loss (w/o reg) on all data: 0.4240608\n",
      "Train acc on all data:  0.9883973894126178\n",
      "Test acc on all data:   0.9111111111111111\n",
      "Norm of the mean of gradients: 1.9031242e-05\n",
      "Norm of the params: 23.689772\n",
      "              Random: fixed  51 labels. Loss 0.42406. Accuracy 0.911.\n",
      "### Flips: 206, rs: 3, checks: 1236\n",
      "Using normal model\n",
      "LBFGS training took [70] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.037781008\n",
      "Train loss (w/o reg) on all data: 0.023683885\n",
      "Test loss (w/o reg) on all data: 0.24010025\n",
      "Train acc on all data:  0.9958907420836355\n",
      "Test acc on all data:   0.9458937198067633\n",
      "Norm of the mean of gradients: 8.537557e-07\n",
      "Norm of the params: 16.79114\n",
      "     Influence (LOO): fixed 190 labels. Loss 0.24010. Accuracy 0.946.\n",
      "Using normal model\n",
      "LBFGS training took [93] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.02740735\n",
      "Train loss (w/o reg) on all data: 0.013031183\n",
      "Test loss (w/o reg) on all data: 0.23620039\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9468599033816425\n",
      "Norm of the mean of gradients: 2.2614142e-06\n",
      "Norm of the params: 16.95652\n",
      "                Loss: fixed 187 labels. Loss 0.23620. Accuracy 0.947.\n",
      "Using normal model\n",
      "LBFGS training took [269] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.084354445\n",
      "Train loss (w/o reg) on all data: 0.05707213\n",
      "Test loss (w/o reg) on all data: 0.39761305\n",
      "Train acc on all data:  0.9896059946821368\n",
      "Test acc on all data:   0.9120772946859903\n",
      "Norm of the mean of gradients: 2.857594e-05\n",
      "Norm of the params: 23.359076\n",
      "              Random: fixed  60 labels. Loss 0.39761. Accuracy 0.912.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.101434976\n",
      "Train loss (w/o reg) on all data: 0.06869209\n",
      "Test loss (w/o reg) on all data: 0.4564158\n",
      "Train acc on all data:  0.9845298525501571\n",
      "Test acc on all data:   0.8917874396135266\n",
      "Norm of the mean of gradients: 3.8492594e-06\n",
      "Norm of the params: 25.590195\n",
      "Flipped loss: 0.45642. Accuracy: 0.892\n",
      "### Flips: 206, rs: 4, checks: 206\n",
      "Using normal model\n",
      "LBFGS training took [94] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.04908371\n",
      "Train loss (w/o reg) on all data: 0.031389955\n",
      "Test loss (w/o reg) on all data: 0.42432097\n",
      "Train acc on all data:  0.9949238578680203\n",
      "Test acc on all data:   0.9304347826086956\n",
      "Norm of the mean of gradients: 8.4701395e-07\n",
      "Norm of the params: 18.811571\n",
      "     Influence (LOO): fixed 140 labels. Loss 0.42432. Accuracy 0.930.\n",
      "Using normal model\n",
      "LBFGS training took [141] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.047416523\n",
      "Train loss (w/o reg) on all data: 0.023386938\n",
      "Test loss (w/o reg) on all data: 0.42048204\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9188405797101449\n",
      "Norm of the mean of gradients: 1.42954905e-05\n",
      "Norm of the params: 21.922401\n",
      "                Loss: fixed 105 labels. Loss 0.42048. Accuracy 0.919.\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.100087866\n",
      "Train loss (w/o reg) on all data: 0.06776905\n",
      "Test loss (w/o reg) on all data: 0.4366659\n",
      "Train acc on all data:  0.9845298525501571\n",
      "Test acc on all data:   0.8995169082125604\n",
      "Norm of the mean of gradients: 3.822504e-06\n",
      "Norm of the params: 25.423931\n",
      "              Random: fixed   6 labels. Loss 0.43667. Accuracy 0.900.\n",
      "### Flips: 206, rs: 4, checks: 412\n",
      "Using normal model\n",
      "LBFGS training took [95] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.043716207\n",
      "Train loss (w/o reg) on all data: 0.028160755\n",
      "Test loss (w/o reg) on all data: 0.40160093\n",
      "Train acc on all data:  0.9951655789219241\n",
      "Test acc on all data:   0.9400966183574879\n",
      "Norm of the mean of gradients: 1.5742714e-06\n",
      "Norm of the params: 17.638283\n",
      "     Influence (LOO): fixed 172 labels. Loss 0.40160. Accuracy 0.940.\n",
      "Using normal model\n",
      "LBFGS training took [167] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.036175437\n",
      "Train loss (w/o reg) on all data: 0.01732236\n",
      "Test loss (w/o reg) on all data: 0.41367027\n",
      "Train acc on all data:  0.9990331157843848\n",
      "Test acc on all data:   0.9314009661835749\n",
      "Norm of the mean of gradients: 1.4912345e-06\n",
      "Norm of the params: 19.418076\n",
      "                Loss: fixed 150 labels. Loss 0.41367. Accuracy 0.931.\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.09696674\n",
      "Train loss (w/o reg) on all data: 0.065549955\n",
      "Test loss (w/o reg) on all data: 0.42870116\n",
      "Train acc on all data:  0.9854967367657723\n",
      "Test acc on all data:   0.8995169082125604\n",
      "Norm of the mean of gradients: 8.73724e-06\n",
      "Norm of the params: 25.066626\n",
      "              Random: fixed  17 labels. Loss 0.42870. Accuracy 0.900.\n",
      "### Flips: 206, rs: 4, checks: 618\n",
      "Using normal model\n",
      "LBFGS training took [104] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.041133314\n",
      "Train loss (w/o reg) on all data: 0.02605421\n",
      "Test loss (w/o reg) on all data: 0.41863617\n",
      "Train acc on all data:  0.9958907420836355\n",
      "Test acc on all data:   0.9352657004830918\n",
      "Norm of the mean of gradients: 3.4234e-06\n",
      "Norm of the params: 17.366117\n",
      "     Influence (LOO): fixed 182 labels. Loss 0.41864. Accuracy 0.935.\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.034267943\n",
      "Train loss (w/o reg) on all data: 0.016383754\n",
      "Test loss (w/o reg) on all data: 0.4153665\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9342995169082126\n",
      "Norm of the mean of gradients: 4.9221626e-06\n",
      "Norm of the params: 18.912529\n",
      "                Loss: fixed 167 labels. Loss 0.41537. Accuracy 0.934.\n",
      "Using normal model\n",
      "LBFGS training took [187] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.09279521\n",
      "Train loss (w/o reg) on all data: 0.062402412\n",
      "Test loss (w/o reg) on all data: 0.42249304\n",
      "Train acc on all data:  0.9857384578196761\n",
      "Test acc on all data:   0.8985507246376812\n",
      "Norm of the mean of gradients: 8.178659e-06\n",
      "Norm of the params: 24.654732\n",
      "              Random: fixed  27 labels. Loss 0.42249. Accuracy 0.899.\n",
      "### Flips: 206, rs: 4, checks: 824\n",
      "Using normal model\n",
      "LBFGS training took [103] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.038255565\n",
      "Train loss (w/o reg) on all data: 0.023202907\n",
      "Test loss (w/o reg) on all data: 0.3838755\n",
      "Train acc on all data:  0.9966159052453468\n",
      "Test acc on all data:   0.9371980676328503\n",
      "Norm of the mean of gradients: 1.6494978e-06\n",
      "Norm of the params: 17.350883\n",
      "     Influence (LOO): fixed 186 labels. Loss 0.38388. Accuracy 0.937.\n",
      "Using normal model\n",
      "LBFGS training took [147] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.031472377\n",
      "Train loss (w/o reg) on all data: 0.014963893\n",
      "Test loss (w/o reg) on all data: 0.35157117\n",
      "Train acc on all data:  0.9995165578921924\n",
      "Test acc on all data:   0.9420289855072463\n",
      "Norm of the mean of gradients: 3.5617336e-06\n",
      "Norm of the params: 18.170572\n",
      "                Loss: fixed 181 labels. Loss 0.35157. Accuracy 0.942.\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.091020755\n",
      "Train loss (w/o reg) on all data: 0.061367895\n",
      "Test loss (w/o reg) on all data: 0.4110787\n",
      "Train acc on all data:  0.9867053420352913\n",
      "Test acc on all data:   0.8995169082125604\n",
      "Norm of the mean of gradients: 5.4329225e-06\n",
      "Norm of the params: 24.352768\n",
      "              Random: fixed  36 labels. Loss 0.41108. Accuracy 0.900.\n",
      "### Flips: 206, rs: 4, checks: 1030\n",
      "Using normal model\n",
      "LBFGS training took [71] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.03488619\n",
      "Train loss (w/o reg) on all data: 0.020596793\n",
      "Test loss (w/o reg) on all data: 0.33049378\n",
      "Train acc on all data:  0.9973410684070583\n",
      "Test acc on all data:   0.9400966183574879\n",
      "Norm of the mean of gradients: 4.0367837e-07\n",
      "Norm of the params: 16.905266\n",
      "     Influence (LOO): fixed 192 labels. Loss 0.33049. Accuracy 0.940.\n",
      "Using normal model\n",
      "LBFGS training took [144] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.030635053\n",
      "Train loss (w/o reg) on all data: 0.014577309\n",
      "Test loss (w/o reg) on all data: 0.33008\n",
      "Train acc on all data:  0.9995165578921924\n",
      "Test acc on all data:   0.9439613526570049\n",
      "Norm of the mean of gradients: 1.4097094e-06\n",
      "Norm of the params: 17.920801\n",
      "                Loss: fixed 188 labels. Loss 0.33008. Accuracy 0.944.\n",
      "Using normal model\n",
      "LBFGS training took [155] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08803893\n",
      "Train loss (w/o reg) on all data: 0.058676653\n",
      "Test loss (w/o reg) on all data: 0.40984628\n",
      "Train acc on all data:  0.9871887841430989\n",
      "Test acc on all data:   0.9033816425120773\n",
      "Norm of the mean of gradients: 4.189737e-06\n",
      "Norm of the params: 24.233149\n",
      "              Random: fixed  46 labels. Loss 0.40985. Accuracy 0.903.\n",
      "### Flips: 206, rs: 4, checks: 1236\n",
      "Using normal model\n",
      "LBFGS training took [59] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.030369949\n",
      "Train loss (w/o reg) on all data: 0.016932167\n",
      "Test loss (w/o reg) on all data: 0.27120128\n",
      "Train acc on all data:  0.9980662315687696\n",
      "Test acc on all data:   0.9449275362318841\n",
      "Norm of the mean of gradients: 1.3421902e-06\n",
      "Norm of the params: 16.39377\n",
      "     Influence (LOO): fixed 198 labels. Loss 0.27120. Accuracy 0.945.\n",
      "Using normal model\n",
      "LBFGS training took [133] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.030411215\n",
      "Train loss (w/o reg) on all data: 0.014530493\n",
      "Test loss (w/o reg) on all data: 0.311052\n",
      "Train acc on all data:  0.9995165578921924\n",
      "Test acc on all data:   0.9420289855072463\n",
      "Norm of the mean of gradients: 1.9062642e-06\n",
      "Norm of the params: 17.821743\n",
      "                Loss: fixed 192 labels. Loss 0.31105. Accuracy 0.942.\n",
      "Using normal model\n",
      "LBFGS training took [149] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08149125\n",
      "Train loss (w/o reg) on all data: 0.053512815\n",
      "Test loss (w/o reg) on all data: 0.37709105\n",
      "Train acc on all data:  0.9888808315204254\n",
      "Test acc on all data:   0.9024154589371981\n",
      "Norm of the mean of gradients: 8.53587e-06\n",
      "Norm of the params: 23.655207\n",
      "              Random: fixed  60 labels. Loss 0.37709. Accuracy 0.902.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [466] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.10309997\n",
      "Train loss (w/o reg) on all data: 0.07025405\n",
      "Test loss (w/o reg) on all data: 0.4747482\n",
      "Train acc on all data:  0.9842881314962533\n",
      "Test acc on all data:   0.9043478260869565\n",
      "Norm of the mean of gradients: 2.0204878e-05\n",
      "Norm of the params: 25.630426\n",
      "Flipped loss: 0.47475. Accuracy: 0.904\n",
      "### Flips: 206, rs: 5, checks: 206\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.047713898\n",
      "Train loss (w/o reg) on all data: 0.030043164\n",
      "Test loss (w/o reg) on all data: 0.33400667\n",
      "Train acc on all data:  0.9944404157602127\n",
      "Test acc on all data:   0.9314009661835749\n",
      "Norm of the mean of gradients: 5.9223285e-06\n",
      "Norm of the params: 18.799334\n",
      "     Influence (LOO): fixed 149 labels. Loss 0.33401. Accuracy 0.931.\n",
      "Using normal model\n",
      "LBFGS training took [282] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.04926931\n",
      "Train loss (w/o reg) on all data: 0.02461936\n",
      "Test loss (w/o reg) on all data: 0.3842506\n",
      "Train acc on all data:  0.9987913947304811\n",
      "Test acc on all data:   0.9140096618357488\n",
      "Norm of the mean of gradients: 2.801737e-06\n",
      "Norm of the params: 22.203583\n",
      "                Loss: fixed 102 labels. Loss 0.38425. Accuracy 0.914.\n",
      "Using normal model\n",
      "LBFGS training took [414] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.09869541\n",
      "Train loss (w/o reg) on all data: 0.06641821\n",
      "Test loss (w/o reg) on all data: 0.4979967\n",
      "Train acc on all data:  0.9852550157118685\n",
      "Test acc on all data:   0.8995169082125604\n",
      "Norm of the mean of gradients: 3.107589e-05\n",
      "Norm of the params: 25.407558\n",
      "              Random: fixed  13 labels. Loss 0.49800. Accuracy 0.900.\n",
      "### Flips: 206, rs: 5, checks: 412\n",
      "Using normal model\n",
      "LBFGS training took [145] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.042422142\n",
      "Train loss (w/o reg) on all data: 0.026612202\n",
      "Test loss (w/o reg) on all data: 0.28768182\n",
      "Train acc on all data:  0.9949238578680203\n",
      "Test acc on all data:   0.9400966183574879\n",
      "Norm of the mean of gradients: 5.591077e-06\n",
      "Norm of the params: 17.78198\n",
      "     Influence (LOO): fixed 176 labels. Loss 0.28768. Accuracy 0.940.\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.039579175\n",
      "Train loss (w/o reg) on all data: 0.019239206\n",
      "Test loss (w/o reg) on all data: 0.30435377\n",
      "Train acc on all data:  0.9990331157843848\n",
      "Test acc on all data:   0.9227053140096618\n",
      "Norm of the mean of gradients: 9.086616e-06\n",
      "Norm of the params: 20.169266\n",
      "                Loss: fixed 141 labels. Loss 0.30435. Accuracy 0.923.\n",
      "Using normal model\n",
      "LBFGS training took [395] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.09504189\n",
      "Train loss (w/o reg) on all data: 0.063497595\n",
      "Test loss (w/o reg) on all data: 0.51961416\n",
      "Train acc on all data:  0.9879139473048103\n",
      "Test acc on all data:   0.8946859903381642\n",
      "Norm of the mean of gradients: 4.8187034e-05\n",
      "Norm of the params: 25.117443\n",
      "              Random: fixed  25 labels. Loss 0.51961. Accuracy 0.895.\n",
      "### Flips: 206, rs: 5, checks: 618\n",
      "Using normal model\n",
      "LBFGS training took [75] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.03981501\n",
      "Train loss (w/o reg) on all data: 0.02501191\n",
      "Test loss (w/o reg) on all data: 0.28159693\n",
      "Train acc on all data:  0.9951655789219241\n",
      "Test acc on all data:   0.9468599033816425\n",
      "Norm of the mean of gradients: 5.890215e-07\n",
      "Norm of the params: 17.206453\n",
      "     Influence (LOO): fixed 184 labels. Loss 0.28160. Accuracy 0.947.\n",
      "Using normal model\n",
      "LBFGS training took [193] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.034841463\n",
      "Train loss (w/o reg) on all data: 0.016807733\n",
      "Test loss (w/o reg) on all data: 0.27354932\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9371980676328503\n",
      "Norm of the mean of gradients: 2.014604e-06\n",
      "Norm of the params: 18.991432\n",
      "                Loss: fixed 159 labels. Loss 0.27355. Accuracy 0.937.\n",
      "Using normal model\n",
      "LBFGS training took [419] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.09095578\n",
      "Train loss (w/o reg) on all data: 0.060188223\n",
      "Test loss (w/o reg) on all data: 0.54210114\n",
      "Train acc on all data:  0.9883973894126178\n",
      "Test acc on all data:   0.9043478260869565\n",
      "Norm of the mean of gradients: 3.8176495e-05\n",
      "Norm of the params: 24.80627\n",
      "              Random: fixed  37 labels. Loss 0.54210. Accuracy 0.904.\n",
      "### Flips: 206, rs: 5, checks: 824\n",
      "Using normal model\n",
      "LBFGS training took [77] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.03924071\n",
      "Train loss (w/o reg) on all data: 0.024344746\n",
      "Test loss (w/o reg) on all data: 0.27928907\n",
      "Train acc on all data:  0.9954072999758279\n",
      "Test acc on all data:   0.9478260869565217\n",
      "Norm of the mean of gradients: 7.488971e-07\n",
      "Norm of the params: 17.260345\n",
      "     Influence (LOO): fixed 186 labels. Loss 0.27929. Accuracy 0.948.\n",
      "Using normal model\n",
      "LBFGS training took [168] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.031252563\n",
      "Train loss (w/o reg) on all data: 0.014921752\n",
      "Test loss (w/o reg) on all data: 0.26880077\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9400966183574879\n",
      "Norm of the mean of gradients: 5.024052e-06\n",
      "Norm of the params: 18.07253\n",
      "                Loss: fixed 170 labels. Loss 0.26880. Accuracy 0.940.\n",
      "Using normal model\n",
      "LBFGS training took [410] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08802373\n",
      "Train loss (w/o reg) on all data: 0.057884455\n",
      "Test loss (w/o reg) on all data: 0.5404564\n",
      "Train acc on all data:  0.9888808315204254\n",
      "Test acc on all data:   0.9053140096618357\n",
      "Norm of the mean of gradients: 3.520817e-05\n",
      "Norm of the params: 24.551693\n",
      "              Random: fixed  47 labels. Loss 0.54046. Accuracy 0.905.\n",
      "### Flips: 206, rs: 5, checks: 1030\n",
      "Using normal model\n",
      "LBFGS training took [75] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.037577406\n",
      "Train loss (w/o reg) on all data: 0.023097241\n",
      "Test loss (w/o reg) on all data: 0.2731153\n",
      "Train acc on all data:  0.9956490210297317\n",
      "Test acc on all data:   0.9439613526570049\n",
      "Norm of the mean of gradients: 2.0196042e-06\n",
      "Norm of the params: 17.017733\n",
      "     Influence (LOO): fixed 189 labels. Loss 0.27312. Accuracy 0.944.\n",
      "Using normal model\n",
      "LBFGS training took [118] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.029812327\n",
      "Train loss (w/o reg) on all data: 0.014206301\n",
      "Test loss (w/o reg) on all data: 0.2658613\n",
      "Train acc on all data:  0.9990331157843848\n",
      "Test acc on all data:   0.9400966183574879\n",
      "Norm of the mean of gradients: 1.8171351e-06\n",
      "Norm of the params: 17.666933\n",
      "                Loss: fixed 178 labels. Loss 0.26586. Accuracy 0.940.\n",
      "Using normal model\n",
      "LBFGS training took [396] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08314044\n",
      "Train loss (w/o reg) on all data: 0.05411194\n",
      "Test loss (w/o reg) on all data: 0.47267255\n",
      "Train acc on all data:  0.9900894367899444\n",
      "Test acc on all data:   0.9053140096618357\n",
      "Norm of the mean of gradients: 2.365032e-05\n",
      "Norm of the params: 24.095018\n",
      "              Random: fixed  59 labels. Loss 0.47267. Accuracy 0.905.\n",
      "### Flips: 206, rs: 5, checks: 1236\n",
      "Using normal model\n",
      "LBFGS training took [72] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.03524876\n",
      "Train loss (w/o reg) on all data: 0.020921681\n",
      "Test loss (w/o reg) on all data: 0.26747409\n",
      "Train acc on all data:  0.9966159052453468\n",
      "Test acc on all data:   0.9429951690821256\n",
      "Norm of the mean of gradients: 6.3491007e-07\n",
      "Norm of the params: 16.927544\n",
      "     Influence (LOO): fixed 193 labels. Loss 0.26747. Accuracy 0.943.\n",
      "Using normal model\n",
      "LBFGS training took [84] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.028232666\n",
      "Train loss (w/o reg) on all data: 0.013477419\n",
      "Test loss (w/o reg) on all data: 0.24695529\n",
      "Train acc on all data:  0.9990331157843848\n",
      "Test acc on all data:   0.9487922705314009\n",
      "Norm of the mean of gradients: 1.9464385e-06\n",
      "Norm of the params: 17.178621\n",
      "                Loss: fixed 188 labels. Loss 0.24696. Accuracy 0.949.\n",
      "Using normal model\n",
      "LBFGS training took [411] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08181134\n",
      "Train loss (w/o reg) on all data: 0.052991044\n",
      "Test loss (w/o reg) on all data: 0.48777184\n",
      "Train acc on all data:  0.9898477157360406\n",
      "Test acc on all data:   0.9091787439613527\n",
      "Norm of the mean of gradients: 1.7020726e-05\n",
      "Norm of the params: 24.008455\n",
      "              Random: fixed  63 labels. Loss 0.48777. Accuracy 0.909.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [339] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.095991634\n",
      "Train loss (w/o reg) on all data: 0.064387836\n",
      "Test loss (w/o reg) on all data: 0.398705\n",
      "Train acc on all data:  0.9879139473048103\n",
      "Test acc on all data:   0.893719806763285\n",
      "Norm of the mean of gradients: 7.69986e-06\n",
      "Norm of the params: 25.14112\n",
      "Flipped loss: 0.39871. Accuracy: 0.894\n",
      "### Flips: 206, rs: 6, checks: 206\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.05362428\n",
      "Train loss (w/o reg) on all data: 0.03515107\n",
      "Test loss (w/o reg) on all data: 0.30551088\n",
      "Train acc on all data:  0.9932318104906938\n",
      "Test acc on all data:   0.9323671497584541\n",
      "Norm of the mean of gradients: 2.073345e-06\n",
      "Norm of the params: 19.22145\n",
      "     Influence (LOO): fixed 135 labels. Loss 0.30551. Accuracy 0.932.\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.04191898\n",
      "Train loss (w/o reg) on all data: 0.0204144\n",
      "Test loss (w/o reg) on all data: 0.3086267\n",
      "Train acc on all data:  0.9990331157843848\n",
      "Test acc on all data:   0.9265700483091788\n",
      "Norm of the mean of gradients: 9.384602e-06\n",
      "Norm of the params: 20.738651\n",
      "                Loss: fixed 113 labels. Loss 0.30863. Accuracy 0.927.\n",
      "Using normal model\n",
      "LBFGS training took [323] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.09348784\n",
      "Train loss (w/o reg) on all data: 0.062868506\n",
      "Test loss (w/o reg) on all data: 0.4005743\n",
      "Train acc on all data:  0.988155668358714\n",
      "Test acc on all data:   0.8985507246376812\n",
      "Norm of the mean of gradients: 6.004042e-06\n",
      "Norm of the params: 24.74644\n",
      "              Random: fixed  12 labels. Loss 0.40057. Accuracy 0.899.\n",
      "### Flips: 206, rs: 6, checks: 412\n",
      "Using normal model\n",
      "LBFGS training took [142] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.049496524\n",
      "Train loss (w/o reg) on all data: 0.032630406\n",
      "Test loss (w/o reg) on all data: 0.2972029\n",
      "Train acc on all data:  0.994198694706309\n",
      "Test acc on all data:   0.9400966183574879\n",
      "Norm of the mean of gradients: 6.415257e-06\n",
      "Norm of the params: 18.36634\n",
      "     Influence (LOO): fixed 160 labels. Loss 0.29720. Accuracy 0.940.\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.03697265\n",
      "Train loss (w/o reg) on all data: 0.01771497\n",
      "Test loss (w/o reg) on all data: 0.2780024\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9342995169082126\n",
      "Norm of the mean of gradients: 9.936064e-06\n",
      "Norm of the params: 19.625334\n",
      "                Loss: fixed 141 labels. Loss 0.27800. Accuracy 0.934.\n",
      "Using normal model\n",
      "LBFGS training took [330] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.09162642\n",
      "Train loss (w/o reg) on all data: 0.061804518\n",
      "Test loss (w/o reg) on all data: 0.41266984\n",
      "Train acc on all data:  0.9879139473048103\n",
      "Test acc on all data:   0.9033816425120773\n",
      "Norm of the mean of gradients: 8.358789e-06\n",
      "Norm of the params: 24.422083\n",
      "              Random: fixed  21 labels. Loss 0.41267. Accuracy 0.903.\n",
      "### Flips: 206, rs: 6, checks: 618\n",
      "Using normal model\n",
      "LBFGS training took [119] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.04643491\n",
      "Train loss (w/o reg) on all data: 0.030431837\n",
      "Test loss (w/o reg) on all data: 0.28975043\n",
      "Train acc on all data:  0.9946821368141165\n",
      "Test acc on all data:   0.9420289855072463\n",
      "Norm of the mean of gradients: 5.3233302e-06\n",
      "Norm of the params: 17.890266\n",
      "     Influence (LOO): fixed 174 labels. Loss 0.28975. Accuracy 0.942.\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.033633463\n",
      "Train loss (w/o reg) on all data: 0.01604795\n",
      "Test loss (w/o reg) on all data: 0.27946135\n",
      "Train acc on all data:  0.9995165578921924\n",
      "Test acc on all data:   0.9391304347826087\n",
      "Norm of the mean of gradients: 1.3402925e-05\n",
      "Norm of the params: 18.753937\n",
      "                Loss: fixed 163 labels. Loss 0.27946. Accuracy 0.939.\n",
      "Using normal model\n",
      "LBFGS training took [303] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08690612\n",
      "Train loss (w/o reg) on all data: 0.057758126\n",
      "Test loss (w/o reg) on all data: 0.42847198\n",
      "Train acc on all data:  0.9891225525743292\n",
      "Test acc on all data:   0.9014492753623189\n",
      "Norm of the mean of gradients: 1.4660717e-05\n",
      "Norm of the params: 24.144567\n",
      "              Random: fixed  33 labels. Loss 0.42847. Accuracy 0.901.\n",
      "### Flips: 206, rs: 6, checks: 824\n",
      "Using normal model\n",
      "LBFGS training took [121] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.044143766\n",
      "Train loss (w/o reg) on all data: 0.028461318\n",
      "Test loss (w/o reg) on all data: 0.29111603\n",
      "Train acc on all data:  0.9951655789219241\n",
      "Test acc on all data:   0.9429951690821256\n",
      "Norm of the mean of gradients: 2.7514252e-06\n",
      "Norm of the params: 17.710136\n",
      "     Influence (LOO): fixed 180 labels. Loss 0.29112. Accuracy 0.943.\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.030943416\n",
      "Train loss (w/o reg) on all data: 0.014762577\n",
      "Test loss (w/o reg) on all data: 0.27776283\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9371980676328503\n",
      "Norm of the mean of gradients: 1.1276126e-06\n",
      "Norm of the params: 17.989353\n",
      "                Loss: fixed 176 labels. Loss 0.27776. Accuracy 0.937.\n",
      "Using normal model\n",
      "LBFGS training took [300] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08442954\n",
      "Train loss (w/o reg) on all data: 0.05594384\n",
      "Test loss (w/o reg) on all data: 0.4312535\n",
      "Train acc on all data:  0.989364273628233\n",
      "Test acc on all data:   0.9091787439613527\n",
      "Norm of the mean of gradients: 8.1279295e-06\n",
      "Norm of the params: 23.86868\n",
      "              Random: fixed  43 labels. Loss 0.43125. Accuracy 0.909.\n",
      "### Flips: 206, rs: 6, checks: 1030\n",
      "Using normal model\n",
      "LBFGS training took [98] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.04249543\n",
      "Train loss (w/o reg) on all data: 0.027482811\n",
      "Test loss (w/o reg) on all data: 0.28384274\n",
      "Train acc on all data:  0.9951655789219241\n",
      "Test acc on all data:   0.9410628019323671\n",
      "Norm of the mean of gradients: 3.6686758e-06\n",
      "Norm of the params: 17.32779\n",
      "     Influence (LOO): fixed 184 labels. Loss 0.28384. Accuracy 0.941.\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.030236023\n",
      "Train loss (w/o reg) on all data: 0.014389919\n",
      "Test loss (w/o reg) on all data: 0.27481294\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9410628019323671\n",
      "Norm of the mean of gradients: 3.118419e-06\n",
      "Norm of the params: 17.802305\n",
      "                Loss: fixed 181 labels. Loss 0.27481. Accuracy 0.941.\n",
      "Using normal model\n",
      "LBFGS training took [318] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08159533\n",
      "Train loss (w/o reg) on all data: 0.05342185\n",
      "Test loss (w/o reg) on all data: 0.43113154\n",
      "Train acc on all data:  0.9903311578438482\n",
      "Test acc on all data:   0.9111111111111111\n",
      "Norm of the mean of gradients: 6.1901924e-06\n",
      "Norm of the params: 23.737516\n",
      "              Random: fixed  51 labels. Loss 0.43113. Accuracy 0.911.\n",
      "### Flips: 206, rs: 6, checks: 1236\n",
      "Using normal model\n",
      "LBFGS training took [83] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.04229622\n",
      "Train loss (w/o reg) on all data: 0.027541405\n",
      "Test loss (w/o reg) on all data: 0.27887124\n",
      "Train acc on all data:  0.9949238578680203\n",
      "Test acc on all data:   0.9420289855072463\n",
      "Norm of the mean of gradients: 7.004549e-07\n",
      "Norm of the params: 17.178371\n",
      "     Influence (LOO): fixed 186 labels. Loss 0.27887. Accuracy 0.942.\n",
      "Using normal model\n",
      "LBFGS training took [146] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.028931431\n",
      "Train loss (w/o reg) on all data: 0.013795937\n",
      "Test loss (w/o reg) on all data: 0.2812357\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9400966183574879\n",
      "Norm of the mean of gradients: 1.3435658e-06\n",
      "Norm of the params: 17.398563\n",
      "                Loss: fixed 188 labels. Loss 0.28124. Accuracy 0.940.\n",
      "Using normal model\n",
      "LBFGS training took [299] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.07680682\n",
      "Train loss (w/o reg) on all data: 0.049859747\n",
      "Test loss (w/o reg) on all data: 0.4292367\n",
      "Train acc on all data:  0.9915397631133672\n",
      "Test acc on all data:   0.9033816425120773\n",
      "Norm of the mean of gradients: 4.7303233e-06\n",
      "Norm of the params: 23.215113\n",
      "              Random: fixed  65 labels. Loss 0.42924. Accuracy 0.903.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.09371601\n",
      "Train loss (w/o reg) on all data: 0.062422898\n",
      "Test loss (w/o reg) on all data: 0.5044837\n",
      "Train acc on all data:  0.9871887841430989\n",
      "Test acc on all data:   0.8908212560386474\n",
      "Norm of the mean of gradients: 3.5728838e-06\n",
      "Norm of the params: 25.017237\n",
      "Flipped loss: 0.50448. Accuracy: 0.891\n",
      "### Flips: 206, rs: 7, checks: 206\n",
      "Using normal model\n",
      "LBFGS training took [127] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.046387814\n",
      "Train loss (w/o reg) on all data: 0.028511904\n",
      "Test loss (w/o reg) on all data: 0.33122632\n",
      "Train acc on all data:  0.9951655789219241\n",
      "Test acc on all data:   0.9246376811594202\n",
      "Norm of the mean of gradients: 1.895214e-06\n",
      "Norm of the params: 18.908152\n",
      "     Influence (LOO): fixed 137 labels. Loss 0.33123. Accuracy 0.925.\n",
      "Using normal model\n",
      "LBFGS training took [202] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.047757752\n",
      "Train loss (w/o reg) on all data: 0.023653423\n",
      "Test loss (w/o reg) on all data: 0.40181375\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9188405797101449\n",
      "Norm of the mean of gradients: 1.7863622e-06\n",
      "Norm of the params: 21.95647\n",
      "                Loss: fixed  95 labels. Loss 0.40181. Accuracy 0.919.\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.09075612\n",
      "Train loss (w/o reg) on all data: 0.0602488\n",
      "Test loss (w/o reg) on all data: 0.50726515\n",
      "Train acc on all data:  0.9879139473048103\n",
      "Test acc on all data:   0.8917874396135266\n",
      "Norm of the mean of gradients: 1.6843947e-05\n",
      "Norm of the params: 24.701143\n",
      "              Random: fixed  15 labels. Loss 0.50727. Accuracy 0.892.\n",
      "### Flips: 206, rs: 7, checks: 412\n",
      "Using normal model\n",
      "LBFGS training took [119] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.03903405\n",
      "Train loss (w/o reg) on all data: 0.023592548\n",
      "Test loss (w/o reg) on all data: 0.27318794\n",
      "Train acc on all data:  0.9961324631375392\n",
      "Test acc on all data:   0.9400966183574879\n",
      "Norm of the mean of gradients: 1.9233782e-06\n",
      "Norm of the params: 17.573565\n",
      "     Influence (LOO): fixed 175 labels. Loss 0.27319. Accuracy 0.940.\n",
      "Using normal model\n",
      "LBFGS training took [155] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.035772912\n",
      "Train loss (w/o reg) on all data: 0.017200392\n",
      "Test loss (w/o reg) on all data: 0.34851858\n",
      "Train acc on all data:  0.9990331157843848\n",
      "Test acc on all data:   0.9333333333333333\n",
      "Norm of the mean of gradients: 1.6054412e-06\n",
      "Norm of the params: 19.273048\n",
      "                Loss: fixed 144 labels. Loss 0.34852. Accuracy 0.933.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.084991656\n",
      "Train loss (w/o reg) on all data: 0.054732375\n",
      "Test loss (w/o reg) on all data: 0.49343464\n",
      "Train acc on all data:  0.989364273628233\n",
      "Test acc on all data:   0.8927536231884058\n",
      "Norm of the mean of gradients: 2.2645436e-05\n",
      "Norm of the params: 24.600521\n",
      "              Random: fixed  28 labels. Loss 0.49343. Accuracy 0.893.\n",
      "### Flips: 206, rs: 7, checks: 618\n",
      "Using normal model\n",
      "LBFGS training took [111] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.036517266\n",
      "Train loss (w/o reg) on all data: 0.021766216\n",
      "Test loss (w/o reg) on all data: 0.26074323\n",
      "Train acc on all data:  0.9966159052453468\n",
      "Test acc on all data:   0.9429951690821256\n",
      "Norm of the mean of gradients: 6.891731e-07\n",
      "Norm of the params: 17.176178\n",
      "     Influence (LOO): fixed 184 labels. Loss 0.26074. Accuracy 0.943.\n",
      "Using normal model\n",
      "LBFGS training took [154] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.03180514\n",
      "Train loss (w/o reg) on all data: 0.01508352\n",
      "Test loss (w/o reg) on all data: 0.28563628\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9429951690821256\n",
      "Norm of the mean of gradients: 2.068653e-06\n",
      "Norm of the params: 18.287495\n",
      "                Loss: fixed 163 labels. Loss 0.28564. Accuracy 0.943.\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.082707494\n",
      "Train loss (w/o reg) on all data: 0.053350642\n",
      "Test loss (w/o reg) on all data: 0.44947097\n",
      "Train acc on all data:  0.9896059946821368\n",
      "Test acc on all data:   0.9014492753623189\n",
      "Norm of the mean of gradients: 8.869244e-06\n",
      "Norm of the params: 24.23091\n",
      "              Random: fixed  40 labels. Loss 0.44947. Accuracy 0.901.\n",
      "### Flips: 206, rs: 7, checks: 824\n",
      "Using normal model\n",
      "LBFGS training took [98] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.03565877\n",
      "Train loss (w/o reg) on all data: 0.021131216\n",
      "Test loss (w/o reg) on all data: 0.25692853\n",
      "Train acc on all data:  0.9968576262992507\n",
      "Test acc on all data:   0.9391304347826087\n",
      "Norm of the mean of gradients: 6.176821e-07\n",
      "Norm of the params: 17.045563\n",
      "     Influence (LOO): fixed 189 labels. Loss 0.25693. Accuracy 0.939.\n",
      "Using normal model\n",
      "LBFGS training took [103] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.028766854\n",
      "Train loss (w/o reg) on all data: 0.013673198\n",
      "Test loss (w/o reg) on all data: 0.26712897\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9458937198067633\n",
      "Norm of the mean of gradients: 7.075635e-07\n",
      "Norm of the params: 17.3745\n",
      "                Loss: fixed 180 labels. Loss 0.26713. Accuracy 0.946.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.07916371\n",
      "Train loss (w/o reg) on all data: 0.05022079\n",
      "Test loss (w/o reg) on all data: 0.4311712\n",
      "Train acc on all data:  0.9908145999516558\n",
      "Test acc on all data:   0.9033816425120773\n",
      "Norm of the mean of gradients: 3.2322766e-06\n",
      "Norm of the params: 24.059477\n",
      "              Random: fixed  53 labels. Loss 0.43117. Accuracy 0.903.\n",
      "### Flips: 206, rs: 7, checks: 1030\n",
      "Using normal model\n",
      "LBFGS training took [112] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.034530483\n",
      "Train loss (w/o reg) on all data: 0.020487005\n",
      "Test loss (w/o reg) on all data: 0.2464268\n",
      "Train acc on all data:  0.9970993473531544\n",
      "Test acc on all data:   0.9420289855072463\n",
      "Norm of the mean of gradients: 1.2050547e-06\n",
      "Norm of the params: 16.759165\n",
      "     Influence (LOO): fixed 194 labels. Loss 0.24643. Accuracy 0.942.\n",
      "Using normal model\n",
      "LBFGS training took [89] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.028284818\n",
      "Train loss (w/o reg) on all data: 0.01344801\n",
      "Test loss (w/o reg) on all data: 0.25314748\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9458937198067633\n",
      "Norm of the mean of gradients: 1.3332518e-06\n",
      "Norm of the params: 17.22603\n",
      "                Loss: fixed 185 labels. Loss 0.25315. Accuracy 0.946.\n",
      "Using normal model\n",
      "LBFGS training took [202] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.07496968\n",
      "Train loss (w/o reg) on all data: 0.046971094\n",
      "Test loss (w/o reg) on all data: 0.4448625\n",
      "Train acc on all data:  0.991781484167271\n",
      "Test acc on all data:   0.9082125603864735\n",
      "Norm of the mean of gradients: 9.099974e-06\n",
      "Norm of the params: 23.663723\n",
      "              Random: fixed  66 labels. Loss 0.44486. Accuracy 0.908.\n",
      "### Flips: 206, rs: 7, checks: 1236\n",
      "Using normal model\n",
      "LBFGS training took [86] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.03411385\n",
      "Train loss (w/o reg) on all data: 0.020291116\n",
      "Test loss (w/o reg) on all data: 0.2500498\n",
      "Train acc on all data:  0.9970993473531544\n",
      "Test acc on all data:   0.9429951690821256\n",
      "Norm of the mean of gradients: 8.792464e-07\n",
      "Norm of the params: 16.626926\n",
      "     Influence (LOO): fixed 196 labels. Loss 0.25005. Accuracy 0.943.\n",
      "Using normal model\n",
      "LBFGS training took [104] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.027875036\n",
      "Train loss (w/o reg) on all data: 0.013262091\n",
      "Test loss (w/o reg) on all data: 0.25384474\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9458937198067633\n",
      "Norm of the mean of gradients: 4.8421903e-06\n",
      "Norm of the params: 17.095583\n",
      "                Loss: fixed 190 labels. Loss 0.25384. Accuracy 0.946.\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.07135672\n",
      "Train loss (w/o reg) on all data: 0.043781076\n",
      "Test loss (w/o reg) on all data: 0.45732212\n",
      "Train acc on all data:  0.9929900894367899\n",
      "Test acc on all data:   0.9072463768115943\n",
      "Norm of the mean of gradients: 6.6030916e-06\n",
      "Norm of the params: 23.484312\n",
      "              Random: fixed  77 labels. Loss 0.45732. Accuracy 0.907.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [331] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.10125911\n",
      "Train loss (w/o reg) on all data: 0.06871382\n",
      "Test loss (w/o reg) on all data: 0.4055952\n",
      "Train acc on all data:  0.9857384578196761\n",
      "Test acc on all data:   0.9178743961352657\n",
      "Norm of the mean of gradients: 6.0930747e-06\n",
      "Norm of the params: 25.51286\n",
      "Flipped loss: 0.40560. Accuracy: 0.918\n",
      "### Flips: 206, rs: 8, checks: 206\n",
      "Using normal model\n",
      "LBFGS training took [139] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.052802756\n",
      "Train loss (w/o reg) on all data: 0.03453162\n",
      "Test loss (w/o reg) on all data: 0.3205517\n",
      "Train acc on all data:  0.9939569736524051\n",
      "Test acc on all data:   0.9371980676328503\n",
      "Norm of the mean of gradients: 4.630209e-06\n",
      "Norm of the params: 19.116037\n",
      "     Influence (LOO): fixed 131 labels. Loss 0.32055. Accuracy 0.937.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.048354283\n",
      "Train loss (w/o reg) on all data: 0.024152365\n",
      "Test loss (w/o reg) on all data: 0.25608394\n",
      "Train acc on all data:  0.9987913947304811\n",
      "Test acc on all data:   0.927536231884058\n",
      "Norm of the mean of gradients: 1.6741145e-05\n",
      "Norm of the params: 22.000872\n",
      "                Loss: fixed  97 labels. Loss 0.25608. Accuracy 0.928.\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.097368345\n",
      "Train loss (w/o reg) on all data: 0.06552352\n",
      "Test loss (w/o reg) on all data: 0.3961964\n",
      "Train acc on all data:  0.9869470630891951\n",
      "Test acc on all data:   0.9178743961352657\n",
      "Norm of the mean of gradients: 5.9709605e-06\n",
      "Norm of the params: 25.236814\n",
      "              Random: fixed  10 labels. Loss 0.39620. Accuracy 0.918.\n",
      "### Flips: 206, rs: 8, checks: 412\n",
      "Using normal model\n",
      "LBFGS training took [87] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.04460021\n",
      "Train loss (w/o reg) on all data: 0.030023197\n",
      "Test loss (w/o reg) on all data: 0.27832383\n",
      "Train acc on all data:  0.9939569736524051\n",
      "Test acc on all data:   0.9400966183574879\n",
      "Norm of the mean of gradients: 7.414154e-07\n",
      "Norm of the params: 17.074553\n",
      "     Influence (LOO): fixed 170 labels. Loss 0.27832. Accuracy 0.940.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.038182847\n",
      "Train loss (w/o reg) on all data: 0.018549383\n",
      "Test loss (w/o reg) on all data: 0.22900221\n",
      "Train acc on all data:  0.9990331157843848\n",
      "Test acc on all data:   0.9468599033816425\n",
      "Norm of the mean of gradients: 1.4159715e-05\n",
      "Norm of the params: 19.815882\n",
      "                Loss: fixed 140 labels. Loss 0.22900. Accuracy 0.947.\n",
      "Using normal model\n",
      "LBFGS training took [312] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.09171337\n",
      "Train loss (w/o reg) on all data: 0.06080127\n",
      "Test loss (w/o reg) on all data: 0.40263194\n",
      "Train acc on all data:  0.9886391104665216\n",
      "Test acc on all data:   0.9246376811594202\n",
      "Norm of the mean of gradients: 2.239337e-05\n",
      "Norm of the params: 24.86448\n",
      "              Random: fixed  21 labels. Loss 0.40263. Accuracy 0.925.\n",
      "### Flips: 206, rs: 8, checks: 618\n",
      "Using normal model\n",
      "LBFGS training took [92] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.042834345\n",
      "Train loss (w/o reg) on all data: 0.028300444\n",
      "Test loss (w/o reg) on all data: 0.26024902\n",
      "Train acc on all data:  0.9949238578680203\n",
      "Test acc on all data:   0.9458937198067633\n",
      "Norm of the mean of gradients: 5.8798565e-07\n",
      "Norm of the params: 17.049284\n",
      "     Influence (LOO): fixed 177 labels. Loss 0.26025. Accuracy 0.946.\n",
      "Using normal model\n",
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.033542745\n",
      "Train loss (w/o reg) on all data: 0.01611135\n",
      "Test loss (w/o reg) on all data: 0.2602357\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9497584541062802\n",
      "Norm of the mean of gradients: 3.8758253e-06\n",
      "Norm of the params: 18.671577\n",
      "                Loss: fixed 164 labels. Loss 0.26024. Accuracy 0.950.\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08847281\n",
      "Train loss (w/o reg) on all data: 0.058223087\n",
      "Test loss (w/o reg) on all data: 0.39624396\n",
      "Train acc on all data:  0.9896059946821368\n",
      "Test acc on all data:   0.9198067632850242\n",
      "Norm of the mean of gradients: 1.6946202e-05\n",
      "Norm of the params: 24.596638\n",
      "              Random: fixed  32 labels. Loss 0.39624. Accuracy 0.920.\n",
      "### Flips: 206, rs: 8, checks: 824\n",
      "Using normal model\n",
      "LBFGS training took [83] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.041423645\n",
      "Train loss (w/o reg) on all data: 0.02686307\n",
      "Test loss (w/o reg) on all data: 0.27044392\n",
      "Train acc on all data:  0.9954072999758279\n",
      "Test acc on all data:   0.9478260869565217\n",
      "Norm of the mean of gradients: 7.4320906e-07\n",
      "Norm of the params: 17.064926\n",
      "     Influence (LOO): fixed 183 labels. Loss 0.27044. Accuracy 0.948.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.031361084\n",
      "Train loss (w/o reg) on all data: 0.015013491\n",
      "Test loss (w/o reg) on all data: 0.2806025\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9391304347826087\n",
      "Norm of the mean of gradients: 1.7024163e-06\n",
      "Norm of the params: 18.08181\n",
      "                Loss: fixed 174 labels. Loss 0.28060. Accuracy 0.939.\n",
      "Using normal model\n",
      "LBFGS training took [283] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08541416\n",
      "Train loss (w/o reg) on all data: 0.056139067\n",
      "Test loss (w/o reg) on all data: 0.38322702\n",
      "Train acc on all data:  0.9891225525743292\n",
      "Test acc on all data:   0.927536231884058\n",
      "Norm of the mean of gradients: 2.0000982e-05\n",
      "Norm of the params: 24.197145\n",
      "              Random: fixed  42 labels. Loss 0.38323. Accuracy 0.928.\n",
      "### Flips: 206, rs: 8, checks: 1030\n",
      "Using normal model\n",
      "LBFGS training took [78] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.03972263\n",
      "Train loss (w/o reg) on all data: 0.025429118\n",
      "Test loss (w/o reg) on all data: 0.26132584\n",
      "Train acc on all data:  0.9958907420836355\n",
      "Test acc on all data:   0.9487922705314009\n",
      "Norm of the mean of gradients: 1.977567e-06\n",
      "Norm of the params: 16.907698\n",
      "     Influence (LOO): fixed 188 labels. Loss 0.26133. Accuracy 0.949.\n",
      "Using normal model\n",
      "LBFGS training took [186] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.029522542\n",
      "Train loss (w/o reg) on all data: 0.014106033\n",
      "Test loss (w/o reg) on all data: 0.24594395\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9487922705314009\n",
      "Norm of the mean of gradients: 3.975942e-06\n",
      "Norm of the params: 17.559336\n",
      "                Loss: fixed 184 labels. Loss 0.24594. Accuracy 0.949.\n",
      "Using normal model\n",
      "LBFGS training took [286] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08453146\n",
      "Train loss (w/o reg) on all data: 0.055580612\n",
      "Test loss (w/o reg) on all data: 0.39383218\n",
      "Train acc on all data:  0.9898477157360406\n",
      "Test acc on all data:   0.9304347826086956\n",
      "Norm of the mean of gradients: 1.2892917e-05\n",
      "Norm of the params: 24.062775\n",
      "              Random: fixed  48 labels. Loss 0.39383. Accuracy 0.930.\n",
      "### Flips: 206, rs: 8, checks: 1236\n",
      "Using normal model\n",
      "LBFGS training took [73] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.032584265\n",
      "Train loss (w/o reg) on all data: 0.018287938\n",
      "Test loss (w/o reg) on all data: 0.24031082\n",
      "Train acc on all data:  0.9978245105148659\n",
      "Test acc on all data:   0.9478260869565217\n",
      "Norm of the mean of gradients: 9.1037197e-07\n",
      "Norm of the params: 16.90936\n",
      "     Influence (LOO): fixed 197 labels. Loss 0.24031. Accuracy 0.948.\n",
      "Using normal model\n",
      "LBFGS training took [150] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.028073773\n",
      "Train loss (w/o reg) on all data: 0.013382292\n",
      "Test loss (w/o reg) on all data: 0.24996565\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9545893719806763\n",
      "Norm of the mean of gradients: 7.3148353e-06\n",
      "Norm of the params: 17.14146\n",
      "                Loss: fixed 188 labels. Loss 0.24997. Accuracy 0.955.\n",
      "Using normal model\n",
      "LBFGS training took [269] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.07915054\n",
      "Train loss (w/o reg) on all data: 0.05043875\n",
      "Test loss (w/o reg) on all data: 0.3816694\n",
      "Train acc on all data:  0.9908145999516558\n",
      "Test acc on all data:   0.9256038647342996\n",
      "Norm of the mean of gradients: 5.037489e-06\n",
      "Norm of the params: 23.96322\n",
      "              Random: fixed  61 labels. Loss 0.38167. Accuracy 0.926.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.091401234\n",
      "Train loss (w/o reg) on all data: 0.06021926\n",
      "Test loss (w/o reg) on all data: 0.4287567\n",
      "Train acc on all data:  0.9874305051970027\n",
      "Test acc on all data:   0.8801932367149758\n",
      "Norm of the mean of gradients: 9.130379e-06\n",
      "Norm of the params: 24.97278\n",
      "Flipped loss: 0.42876. Accuracy: 0.880\n",
      "### Flips: 206, rs: 9, checks: 206\n",
      "Using normal model\n",
      "LBFGS training took [75] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.048527867\n",
      "Train loss (w/o reg) on all data: 0.031607393\n",
      "Test loss (w/o reg) on all data: 0.2831903\n",
      "Train acc on all data:  0.994198694706309\n",
      "Test acc on all data:   0.9429951690821256\n",
      "Norm of the mean of gradients: 1.3907706e-06\n",
      "Norm of the params: 18.395908\n",
      "     Influence (LOO): fixed 137 labels. Loss 0.28319. Accuracy 0.943.\n",
      "Using normal model\n",
      "LBFGS training took [190] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.044840794\n",
      "Train loss (w/o reg) on all data: 0.022018649\n",
      "Test loss (w/o reg) on all data: 0.30062127\n",
      "Train acc on all data:  0.9990331157843848\n",
      "Test acc on all data:   0.927536231884058\n",
      "Norm of the mean of gradients: 3.766285e-06\n",
      "Norm of the params: 21.36453\n",
      "                Loss: fixed  99 labels. Loss 0.30062. Accuracy 0.928.\n",
      "Using normal model\n",
      "LBFGS training took [258] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08941215\n",
      "Train loss (w/o reg) on all data: 0.058631726\n",
      "Test loss (w/o reg) on all data: 0.42454267\n",
      "Train acc on all data:  0.9886391104665216\n",
      "Test acc on all data:   0.8821256038647343\n",
      "Norm of the mean of gradients: 4.6814844e-06\n",
      "Norm of the params: 24.81146\n",
      "              Random: fixed   6 labels. Loss 0.42454. Accuracy 0.882.\n",
      "### Flips: 206, rs: 9, checks: 412\n",
      "Using normal model\n",
      "LBFGS training took [71] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.042773236\n",
      "Train loss (w/o reg) on all data: 0.027963776\n",
      "Test loss (w/o reg) on all data: 0.26938224\n",
      "Train acc on all data:  0.9946821368141165\n",
      "Test acc on all data:   0.9487922705314009\n",
      "Norm of the mean of gradients: 5.953395e-07\n",
      "Norm of the params: 17.21015\n",
      "     Influence (LOO): fixed 170 labels. Loss 0.26938. Accuracy 0.949.\n",
      "Using normal model\n",
      "LBFGS training took [146] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.034723464\n",
      "Train loss (w/o reg) on all data: 0.016582789\n",
      "Test loss (w/o reg) on all data: 0.2793475\n",
      "Train acc on all data:  0.9997582789460963\n",
      "Test acc on all data:   0.9391304347826087\n",
      "Norm of the mean of gradients: 2.1321375e-06\n",
      "Norm of the params: 19.047663\n",
      "                Loss: fixed 138 labels. Loss 0.27935. Accuracy 0.939.\n",
      "Using normal model\n",
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08721228\n",
      "Train loss (w/o reg) on all data: 0.056800157\n",
      "Test loss (w/o reg) on all data: 0.41359803\n",
      "Train acc on all data:  0.9896059946821368\n",
      "Test acc on all data:   0.885024154589372\n",
      "Norm of the mean of gradients: 2.8846032e-05\n",
      "Norm of the params: 24.662575\n",
      "              Random: fixed  18 labels. Loss 0.41360. Accuracy 0.885.\n",
      "### Flips: 206, rs: 9, checks: 618\n",
      "Using normal model\n",
      "LBFGS training took [70] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.040978476\n",
      "Train loss (w/o reg) on all data: 0.026686903\n",
      "Test loss (w/o reg) on all data: 0.27129275\n",
      "Train acc on all data:  0.9949238578680203\n",
      "Test acc on all data:   0.9439613526570049\n",
      "Norm of the mean of gradients: 5.3838943e-07\n",
      "Norm of the params: 16.906553\n",
      "     Influence (LOO): fixed 182 labels. Loss 0.27129. Accuracy 0.944.\n",
      "Using normal model\n",
      "LBFGS training took [137] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.03151044\n",
      "Train loss (w/o reg) on all data: 0.014982096\n",
      "Test loss (w/o reg) on all data: 0.2598375\n",
      "Train acc on all data:  0.9995165578921924\n",
      "Test acc on all data:   0.9468599033816425\n",
      "Norm of the mean of gradients: 1.4495888e-06\n",
      "Norm of the params: 18.181505\n",
      "                Loss: fixed 156 labels. Loss 0.25984. Accuracy 0.947.\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08521067\n",
      "Train loss (w/o reg) on all data: 0.055393077\n",
      "Test loss (w/o reg) on all data: 0.41111675\n",
      "Train acc on all data:  0.9903311578438482\n",
      "Test acc on all data:   0.8908212560386474\n",
      "Norm of the mean of gradients: 5.218875e-06\n",
      "Norm of the params: 24.420317\n",
      "              Random: fixed  28 labels. Loss 0.41112. Accuracy 0.891.\n",
      "### Flips: 206, rs: 9, checks: 824\n",
      "Using normal model\n",
      "LBFGS training took [67] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.03859879\n",
      "Train loss (w/o reg) on all data: 0.024435496\n",
      "Test loss (w/o reg) on all data: 0.27738136\n",
      "Train acc on all data:  0.9958907420836355\n",
      "Test acc on all data:   0.9420289855072463\n",
      "Norm of the mean of gradients: 5.1213755e-07\n",
      "Norm of the params: 16.83051\n",
      "     Influence (LOO): fixed 188 labels. Loss 0.27738. Accuracy 0.942.\n",
      "Using normal model\n",
      "LBFGS training took [84] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.02866136\n",
      "Train loss (w/o reg) on all data: 0.013639899\n",
      "Test loss (w/o reg) on all data: 0.24302742\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9468599033816425\n",
      "Norm of the mean of gradients: 5.4356946e-07\n",
      "Norm of the params: 17.332891\n",
      "                Loss: fixed 174 labels. Loss 0.24303. Accuracy 0.947.\n",
      "Using normal model\n",
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08342649\n",
      "Train loss (w/o reg) on all data: 0.054044332\n",
      "Test loss (w/o reg) on all data: 0.43121698\n",
      "Train acc on all data:  0.9900894367899444\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 4.363677e-06\n",
      "Norm of the params: 24.241352\n",
      "              Random: fixed  34 labels. Loss 0.43122. Accuracy 0.889.\n",
      "### Flips: 206, rs: 9, checks: 1030\n",
      "Using normal model\n",
      "LBFGS training took [67] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.033999786\n",
      "Train loss (w/o reg) on all data: 0.02014582\n",
      "Test loss (w/o reg) on all data: 0.24955612\n",
      "Train acc on all data:  0.9970993473531544\n",
      "Test acc on all data:   0.9458937198067633\n",
      "Norm of the mean of gradients: 7.0521986e-07\n",
      "Norm of the params: 16.645702\n",
      "     Influence (LOO): fixed 196 labels. Loss 0.24956. Accuracy 0.946.\n",
      "Using normal model\n",
      "LBFGS training took [80] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.027528038\n",
      "Train loss (w/o reg) on all data: 0.013098895\n",
      "Test loss (w/o reg) on all data: 0.23480016\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9468599033816425\n",
      "Norm of the mean of gradients: 5.9897707e-07\n",
      "Norm of the params: 16.98773\n",
      "                Loss: fixed 182 labels. Loss 0.23480. Accuracy 0.947.\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08004871\n",
      "Train loss (w/o reg) on all data: 0.05136076\n",
      "Test loss (w/o reg) on all data: 0.40626317\n",
      "Train acc on all data:  0.9910563210055596\n",
      "Test acc on all data:   0.8966183574879227\n",
      "Norm of the mean of gradients: 4.652501e-06\n",
      "Norm of the params: 23.953272\n",
      "              Random: fixed  43 labels. Loss 0.40626. Accuracy 0.897.\n",
      "### Flips: 206, rs: 9, checks: 1236\n",
      "Using normal model\n",
      "LBFGS training took [64] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.032156724\n",
      "Train loss (w/o reg) on all data: 0.018396907\n",
      "Test loss (w/o reg) on all data: 0.23751287\n",
      "Train acc on all data:  0.997582789460962\n",
      "Test acc on all data:   0.9449275362318841\n",
      "Norm of the mean of gradients: 3.9044284e-07\n",
      "Norm of the params: 16.589045\n",
      "     Influence (LOO): fixed 198 labels. Loss 0.23751. Accuracy 0.945.\n",
      "Using normal model\n",
      "LBFGS training took [79] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.027339974\n",
      "Train loss (w/o reg) on all data: 0.013027879\n",
      "Test loss (w/o reg) on all data: 0.23554678\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9468599033816425\n",
      "Norm of the mean of gradients: 6.03661e-07\n",
      "Norm of the params: 16.918688\n",
      "                Loss: fixed 186 labels. Loss 0.23555. Accuracy 0.947.\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.075501405\n",
      "Train loss (w/o reg) on all data: 0.047593813\n",
      "Test loss (w/o reg) on all data: 0.3936115\n",
      "Train acc on all data:  0.9912980420594634\n",
      "Test acc on all data:   0.8985507246376812\n",
      "Norm of the mean of gradients: 3.9833044e-06\n",
      "Norm of the params: 23.625235\n",
      "              Random: fixed  56 labels. Loss 0.39361. Accuracy 0.899.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.097167164\n",
      "Train loss (w/o reg) on all data: 0.065400995\n",
      "Test loss (w/o reg) on all data: 0.5140105\n",
      "Train acc on all data:  0.9871887841430989\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 4.6448513e-06\n",
      "Norm of the params: 25.205627\n",
      "Flipped loss: 0.51401. Accuracy: 0.889\n",
      "### Flips: 206, rs: 10, checks: 206\n",
      "Using normal model\n",
      "LBFGS training took [143] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.04950361\n",
      "Train loss (w/o reg) on all data: 0.031063959\n",
      "Test loss (w/o reg) on all data: 0.36025295\n",
      "Train acc on all data:  0.9946821368141165\n",
      "Test acc on all data:   0.9294685990338164\n",
      "Norm of the mean of gradients: 4.336577e-06\n",
      "Norm of the params: 19.203987\n",
      "     Influence (LOO): fixed 136 labels. Loss 0.36025. Accuracy 0.929.\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.047787286\n",
      "Train loss (w/o reg) on all data: 0.02371246\n",
      "Test loss (w/o reg) on all data: 0.43139887\n",
      "Train acc on all data:  0.9985496736765772\n",
      "Test acc on all data:   0.9140096618357488\n",
      "Norm of the mean of gradients: 2.7851677e-06\n",
      "Norm of the params: 21.943031\n",
      "                Loss: fixed  91 labels. Loss 0.43140. Accuracy 0.914.\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.09364434\n",
      "Train loss (w/o reg) on all data: 0.06245644\n",
      "Test loss (w/o reg) on all data: 0.5248035\n",
      "Train acc on all data:  0.9876722262509064\n",
      "Test acc on all data:   0.8859903381642512\n",
      "Norm of the mean of gradients: 5.0972217e-06\n",
      "Norm of the params: 24.975155\n",
      "              Random: fixed  13 labels. Loss 0.52480. Accuracy 0.886.\n",
      "### Flips: 206, rs: 10, checks: 412\n",
      "Using normal model\n",
      "LBFGS training took [123] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.042536784\n",
      "Train loss (w/o reg) on all data: 0.026905667\n",
      "Test loss (w/o reg) on all data: 0.28468344\n",
      "Train acc on all data:  0.9949238578680203\n",
      "Test acc on all data:   0.9420289855072463\n",
      "Norm of the mean of gradients: 2.502113e-06\n",
      "Norm of the params: 17.681131\n",
      "     Influence (LOO): fixed 177 labels. Loss 0.28468. Accuracy 0.942.\n",
      "Using normal model\n",
      "LBFGS training took [193] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.038058497\n",
      "Train loss (w/o reg) on all data: 0.018230991\n",
      "Test loss (w/o reg) on all data: 0.37563372\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9304347826086956\n",
      "Norm of the mean of gradients: 3.3561844e-06\n",
      "Norm of the params: 19.913567\n",
      "                Loss: fixed 134 labels. Loss 0.37563. Accuracy 0.930.\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08944846\n",
      "Train loss (w/o reg) on all data: 0.059207175\n",
      "Test loss (w/o reg) on all data: 0.4916208\n",
      "Train acc on all data:  0.9888808315204254\n",
      "Test acc on all data:   0.9014492753623189\n",
      "Norm of the mean of gradients: 4.6045006e-06\n",
      "Norm of the params: 24.593206\n",
      "              Random: fixed  26 labels. Loss 0.49162. Accuracy 0.901.\n",
      "### Flips: 206, rs: 10, checks: 618\n",
      "Using normal model\n",
      "LBFGS training took [129] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.04053759\n",
      "Train loss (w/o reg) on all data: 0.025298864\n",
      "Test loss (w/o reg) on all data: 0.2775623\n",
      "Train acc on all data:  0.9951655789219241\n",
      "Test acc on all data:   0.9410628019323671\n",
      "Norm of the mean of gradients: 1.404123e-06\n",
      "Norm of the params: 17.45779\n",
      "     Influence (LOO): fixed 184 labels. Loss 0.27756. Accuracy 0.941.\n",
      "Using normal model\n",
      "LBFGS training took [177] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.032244414\n",
      "Train loss (w/o reg) on all data: 0.015353552\n",
      "Test loss (w/o reg) on all data: 0.35410506\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.936231884057971\n",
      "Norm of the mean of gradients: 4.9225564e-06\n",
      "Norm of the params: 18.379808\n",
      "                Loss: fixed 160 labels. Loss 0.35411. Accuracy 0.936.\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08720477\n",
      "Train loss (w/o reg) on all data: 0.057680607\n",
      "Test loss (w/o reg) on all data: 0.46634135\n",
      "Train acc on all data:  0.9898477157360406\n",
      "Test acc on all data:   0.9004830917874396\n",
      "Norm of the mean of gradients: 6.8083045e-06\n",
      "Norm of the params: 24.299868\n",
      "              Random: fixed  36 labels. Loss 0.46634. Accuracy 0.900.\n",
      "### Flips: 206, rs: 10, checks: 824\n",
      "Using normal model\n",
      "LBFGS training took [116] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.038179092\n",
      "Train loss (w/o reg) on all data: 0.023501815\n",
      "Test loss (w/o reg) on all data: 0.27920365\n",
      "Train acc on all data:  0.9961324631375392\n",
      "Test acc on all data:   0.9420289855072463\n",
      "Norm of the mean of gradients: 1.2005162e-06\n",
      "Norm of the params: 17.13317\n",
      "     Influence (LOO): fixed 190 labels. Loss 0.27920. Accuracy 0.942.\n",
      "Using normal model\n",
      "LBFGS training took [142] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.03115222\n",
      "Train loss (w/o reg) on all data: 0.01487604\n",
      "Test loss (w/o reg) on all data: 0.3196256\n",
      "Train acc on all data:  0.9992748368382887\n",
      "Test acc on all data:   0.9371980676328503\n",
      "Norm of the mean of gradients: 6.2243234e-06\n",
      "Norm of the params: 18.042274\n",
      "                Loss: fixed 171 labels. Loss 0.31963. Accuracy 0.937.\n",
      "Using normal model\n",
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.08434276\n",
      "Train loss (w/o reg) on all data: 0.055495296\n",
      "Test loss (w/o reg) on all data: 0.46326855\n",
      "Train acc on all data:  0.9900894367899444\n",
      "Test acc on all data:   0.9053140096618357\n",
      "Norm of the mean of gradients: 1.5408712e-05\n",
      "Norm of the params: 24.019772\n",
      "              Random: fixed  49 labels. Loss 0.46327. Accuracy 0.905.\n",
      "### Flips: 206, rs: 10, checks: 1030\n",
      "Using normal model\n"
     ]
    }
   ],
   "source": [
    "print('Orig loss: %.5f. Accuracy: %.3f' % (orig_results[0], orig_results[1]))\n",
    "\n",
    "for flips_idx in range(num_flip_vals):\n",
    "    for random_seed_idx in range(num_random_seeds):\n",
    "        \n",
    "        random_seed = flips_idx * (num_random_seeds * 3) + (random_seed_idx * 2)        \n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "        num_flips = int(num_train_examples / 20) * (flips_idx + 1)    \n",
    "        idx_to_flip = np.random.choice(num_train_examples, size=num_flips, replace=False)\n",
    "        Y_train_flipped = np.copy(Y_train)\n",
    "        Y_train_flipped[idx_to_flip] = 1 - Y_train[idx_to_flip] \n",
    "        \n",
    "        tf_model.update_train_x_y(X_train, Y_train_flipped)\n",
    "        tf_model.train()        \n",
    "        flipped_results[flips_idx, random_seed_idx, 1:] = tf_model.sess.run(\n",
    "            [tf_model.loss_no_reg, tf_model.accuracy_op], \n",
    "            feed_dict=tf_model.all_test_feed_dict)\n",
    "        print('Flipped loss: %.5f. Accuracy: %.3f' % (\n",
    "                flipped_results[flips_idx, random_seed_idx, 1], flipped_results[flips_idx, random_seed_idx, 2]))\n",
    "        \n",
    "        train_losses = tf_model.sess.run(tf_model.indiv_loss_no_reg, feed_dict=tf_model.all_train_feed_dict)\n",
    "        train_loo_influences = tf_model.get_loo_influences()\n",
    "\n",
    "        for checks_idx in range(num_check_vals):\n",
    "            np.random.seed(random_seed + 1)\n",
    "            num_checks = int(num_train_examples / 20) * (checks_idx + 1)\n",
    "\n",
    "            print('### Flips: %s, rs: %s, checks: %s' % (num_flips, random_seed_idx, num_checks))\n",
    "\n",
    "            fixed_influence_loo_results[flips_idx, checks_idx, random_seed_idx, :], \\\n",
    "              fixed_loss_results[flips_idx, checks_idx, random_seed_idx, :], \\\n",
    "              fixed_random_results[flips_idx, checks_idx, random_seed_idx, :] \\\n",
    "              = experiments.test_mislabeled_detection_batch(\n",
    "                tf_model, \n",
    "                X_train, Y_train,\n",
    "                Y_train_flipped,\n",
    "                X_test, Y_test, \n",
    "                train_losses, train_loo_influences,\n",
    "                num_flips, num_checks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flips_idx = 0\n",
    "random_seed_idx=0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
