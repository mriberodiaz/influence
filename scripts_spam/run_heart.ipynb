{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import unicode_literals  \n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as linear_model\n",
    "\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "import influence.experiments as experiments\n",
    "#from influence.nlprocessor import NLProcessor\n",
    "from influence.binaryLogisticRegressionWithLBFGS import BinaryLogisticRegressionWithLBFGS\n",
    "from load_heart import load_heart\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/mr54725/Documents/repos/INF')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mr54725/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "data_sets = load_heart()\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "input_dim = data_sets.train.x.shape[1]\n",
    "weight_decay = 0.0001\n",
    "# weight_decay = 1000 / len(lr_data_sets.train.labels)\n",
    "batch_size = 100\n",
    "initial_learning_rate = 0.001 \n",
    "keep_probs = None\n",
    "decay_epochs = [1000, 10000]\n",
    "max_lbfgs_iter = 10000\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf_model = BinaryLogisticRegressionWithLBFGS(\n",
    "    input_dim=input_dim,\n",
    "    weight_decay=weight_decay,\n",
    "    max_lbfgs_iter=max_lbfgs_iter,\n",
    "    num_classes=num_classes, \n",
    "    batch_size=batch_size,\n",
    "    data_sets=data_sets,\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    keep_probs=keep_probs,\n",
    "    decay_epochs=decay_epochs,\n",
    "    mini_batch=False,\n",
    "    train_dir='output',\n",
    "    log_dir='log',\n",
    "    model_name='heart3_logreg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [416] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3592314\n",
      "Train loss (w/o reg) on all data: 0.3589113\n",
      "Test loss (w/o reg) on all data: 0.40642902\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00076461677\n",
      "Norm of the params: 2.5301914\n",
      "Orig loss: 0.40643. Accuracy: 0.800\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4286149\n",
      "Train loss (w/o reg) on all data: 0.42844188\n",
      "Test loss (w/o reg) on all data: 0.4055303\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010028951\n",
      "Norm of the params: 1.8602495\n",
      "Flipped loss: 0.40553. Accuracy: 0.800\n",
      "### Flips: 10, rs: 0, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39530292\n",
      "Train loss (w/o reg) on all data: 0.39509252\n",
      "Test loss (w/o reg) on all data: 0.40066028\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0035135893\n",
      "Norm of the params: 2.0512986\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.40066. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3653808\n",
      "Train loss (w/o reg) on all data: 0.36508062\n",
      "Test loss (w/o reg) on all data: 0.3950678\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00452323\n",
      "Norm of the params: 2.4501228\n",
      "                Loss: fixed   5 labels. Loss 0.39507. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [291] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40963084\n",
      "Train loss (w/o reg) on all data: 0.40940666\n",
      "Test loss (w/o reg) on all data: 0.4053323\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012607281\n",
      "Norm of the params: 2.1174111\n",
      "              Random: fixed   3 labels. Loss 0.40533. Accuracy 0.800.\n",
      "### Flips: 10, rs: 0, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [279] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35614842\n",
      "Train loss (w/o reg) on all data: 0.3558313\n",
      "Test loss (w/o reg) on all data: 0.3959656\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.005218703\n",
      "Norm of the params: 2.518478\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.39597. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [12] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35614812\n",
      "Train loss (w/o reg) on all data: 0.35583135\n",
      "Test loss (w/o reg) on all data: 0.39579412\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016886496\n",
      "Norm of the params: 2.516984\n",
      "                Loss: fixed   6 labels. Loss 0.39579. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4096323\n",
      "Train loss (w/o reg) on all data: 0.4094092\n",
      "Test loss (w/o reg) on all data: 0.404825\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008653971\n",
      "Norm of the params: 2.1123862\n",
      "              Random: fixed   3 labels. Loss 0.40483. Accuracy 0.800.\n",
      "### Flips: 10, rs: 0, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [370] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35614872\n",
      "Train loss (w/o reg) on all data: 0.35583112\n",
      "Test loss (w/o reg) on all data: 0.3959996\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007265825\n",
      "Norm of the params: 2.5203106\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.39600. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [100] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35295218\n",
      "Train loss (w/o reg) on all data: 0.35263196\n",
      "Test loss (w/o reg) on all data: 0.39889324\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009315843\n",
      "Norm of the params: 2.530698\n",
      "                Loss: fixed   7 labels. Loss 0.39889. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [115] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4096606\n",
      "Train loss (w/o reg) on all data: 0.4094406\n",
      "Test loss (w/o reg) on all data: 0.40496165\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013266864\n",
      "Norm of the params: 2.09756\n",
      "              Random: fixed   3 labels. Loss 0.40496. Accuracy 0.800.\n",
      "### Flips: 10, rs: 0, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35397604\n",
      "Train loss (w/o reg) on all data: 0.35361904\n",
      "Test loss (w/o reg) on all data: 0.40140572\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008248337\n",
      "Norm of the params: 2.672127\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.40141. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [303] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.352898\n",
      "Train loss (w/o reg) on all data: 0.3525909\n",
      "Test loss (w/o reg) on all data: 0.40590155\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015275312\n",
      "Norm of the params: 2.478366\n",
      "                Loss: fixed   8 labels. Loss 0.40590. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40963346\n",
      "Train loss (w/o reg) on all data: 0.4094109\n",
      "Test loss (w/o reg) on all data: 0.4059966\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0062693236\n",
      "Norm of the params: 2.1097927\n",
      "              Random: fixed   3 labels. Loss 0.40600. Accuracy 0.778.\n",
      "### Flips: 10, rs: 0, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35397485\n",
      "Train loss (w/o reg) on all data: 0.35361868\n",
      "Test loss (w/o reg) on all data: 0.4016045\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0028152212\n",
      "Norm of the params: 2.668974\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.40160. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35290357\n",
      "Train loss (w/o reg) on all data: 0.35259765\n",
      "Test loss (w/o reg) on all data: 0.40662333\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.000983874\n",
      "Norm of the params: 2.4735925\n",
      "                Loss: fixed   8 labels. Loss 0.40662. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.409632\n",
      "Train loss (w/o reg) on all data: 0.40940842\n",
      "Test loss (w/o reg) on all data: 0.40442285\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002456908\n",
      "Norm of the params: 2.1146464\n",
      "              Random: fixed   3 labels. Loss 0.40442. Accuracy 0.800.\n",
      "### Flips: 10, rs: 0, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35401052\n",
      "Train loss (w/o reg) on all data: 0.35367146\n",
      "Test loss (w/o reg) on all data: 0.4077052\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00080210326\n",
      "Norm of the params: 2.6040313\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.40771. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35401046\n",
      "Train loss (w/o reg) on all data: 0.3536714\n",
      "Test loss (w/o reg) on all data: 0.40770718\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005212436\n",
      "Norm of the params: 2.6040313\n",
      "                Loss: fixed   9 labels. Loss 0.40771. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [292] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4096311\n",
      "Train loss (w/o reg) on all data: 0.40940636\n",
      "Test loss (w/o reg) on all data: 0.40513527\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00051402464\n",
      "Norm of the params: 2.1201458\n",
      "              Random: fixed   3 labels. Loss 0.40514. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41572013\n",
      "Train loss (w/o reg) on all data: 0.41550335\n",
      "Test loss (w/o reg) on all data: 0.37301695\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0030046795\n",
      "Norm of the params: 2.0822592\n",
      "Flipped loss: 0.37302. Accuracy: 0.844\n",
      "### Flips: 10, rs: 1, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38820213\n",
      "Train loss (w/o reg) on all data: 0.3879434\n",
      "Test loss (w/o reg) on all data: 0.39713487\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008716836\n",
      "Norm of the params: 2.2748306\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.39713. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [177] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36588275\n",
      "Train loss (w/o reg) on all data: 0.36552748\n",
      "Test loss (w/o reg) on all data: 0.3903432\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019147174\n",
      "Norm of the params: 2.6656327\n",
      "                Loss: fixed   3 labels. Loss 0.39034. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4157224\n",
      "Train loss (w/o reg) on all data: 0.41550544\n",
      "Test loss (w/o reg) on all data: 0.3729239\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0009477702\n",
      "Norm of the params: 2.0831344\n",
      "              Random: fixed   0 labels. Loss 0.37292. Accuracy 0.844.\n",
      "### Flips: 10, rs: 1, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [163] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3711528\n",
      "Train loss (w/o reg) on all data: 0.3708474\n",
      "Test loss (w/o reg) on all data: 0.39800406\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00047203197\n",
      "Norm of the params: 2.471319\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.39800. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [157] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.341092\n",
      "Train loss (w/o reg) on all data: 0.34068158\n",
      "Test loss (w/o reg) on all data: 0.4183573\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001095797\n",
      "Norm of the params: 2.8649604\n",
      "                Loss: fixed   6 labels. Loss 0.41836. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3995477\n",
      "Train loss (w/o reg) on all data: 0.39929837\n",
      "Test loss (w/o reg) on all data: 0.37277636\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012326122\n",
      "Norm of the params: 2.2330775\n",
      "              Random: fixed   1 labels. Loss 0.37278. Accuracy 0.822.\n",
      "### Flips: 10, rs: 1, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35077095\n",
      "Train loss (w/o reg) on all data: 0.35037524\n",
      "Test loss (w/o reg) on all data: 0.42122638\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009704026\n",
      "Norm of the params: 2.8132458\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.42123. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [272] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3410891\n",
      "Train loss (w/o reg) on all data: 0.3406788\n",
      "Test loss (w/o reg) on all data: 0.41882706\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00045690007\n",
      "Norm of the params: 2.864571\n",
      "                Loss: fixed   6 labels. Loss 0.41883. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [395] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39954194\n",
      "Train loss (w/o reg) on all data: 0.3992894\n",
      "Test loss (w/o reg) on all data: 0.37228823\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00033530942\n",
      "Norm of the params: 2.2473733\n",
      "              Random: fixed   1 labels. Loss 0.37229. Accuracy 0.822.\n",
      "### Flips: 10, rs: 1, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [316] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34491765\n",
      "Train loss (w/o reg) on all data: 0.3445637\n",
      "Test loss (w/o reg) on all data: 0.42827174\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.007438837\n",
      "Norm of the params: 2.6607249\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.42827. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34108862\n",
      "Train loss (w/o reg) on all data: 0.3406791\n",
      "Test loss (w/o reg) on all data: 0.41892081\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0023536405\n",
      "Norm of the params: 2.8618321\n",
      "                Loss: fixed   6 labels. Loss 0.41892. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39955026\n",
      "Train loss (w/o reg) on all data: 0.39929846\n",
      "Test loss (w/o reg) on all data: 0.37326127\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002369285\n",
      "Norm of the params: 2.2441256\n",
      "              Random: fixed   1 labels. Loss 0.37326. Accuracy 0.822.\n",
      "### Flips: 10, rs: 1, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34491834\n",
      "Train loss (w/o reg) on all data: 0.34456268\n",
      "Test loss (w/o reg) on all data: 0.4282313\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005605923\n",
      "Norm of the params: 2.6670609\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.42823. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [279] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34108883\n",
      "Train loss (w/o reg) on all data: 0.34067896\n",
      "Test loss (w/o reg) on all data: 0.41895497\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00036867635\n",
      "Norm of the params: 2.8630667\n",
      "                Loss: fixed   6 labels. Loss 0.41895. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39321837\n",
      "Train loss (w/o reg) on all data: 0.39296827\n",
      "Test loss (w/o reg) on all data: 0.38734707\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009811311\n",
      "Norm of the params: 2.2365503\n",
      "              Random: fixed   2 labels. Loss 0.38735. Accuracy 0.822.\n",
      "### Flips: 10, rs: 1, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [315] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3449174\n",
      "Train loss (w/o reg) on all data: 0.3445635\n",
      "Test loss (w/o reg) on all data: 0.42854318\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.000867612\n",
      "Norm of the params: 2.6602986\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.42854. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34108907\n",
      "Train loss (w/o reg) on all data: 0.34067824\n",
      "Test loss (w/o reg) on all data: 0.4189965\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010970896\n",
      "Norm of the params: 2.866437\n",
      "                Loss: fixed   6 labels. Loss 0.41900. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [307] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39869106\n",
      "Train loss (w/o reg) on all data: 0.3984496\n",
      "Test loss (w/o reg) on all data: 0.3752804\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015515878\n",
      "Norm of the params: 2.197554\n",
      "              Random: fixed   3 labels. Loss 0.37528. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4186853\n",
      "Train loss (w/o reg) on all data: 0.418476\n",
      "Test loss (w/o reg) on all data: 0.4043874\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0051857117\n",
      "Norm of the params: 2.0460346\n",
      "Flipped loss: 0.40439. Accuracy: 0.822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Flips: 10, rs: 2, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [112] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39500993\n",
      "Train loss (w/o reg) on all data: 0.39480647\n",
      "Test loss (w/o reg) on all data: 0.42425647\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009373978\n",
      "Norm of the params: 2.017247\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.42426. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37371653\n",
      "Train loss (w/o reg) on all data: 0.37345892\n",
      "Test loss (w/o reg) on all data: 0.41516155\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003668882\n",
      "Norm of the params: 2.2698874\n",
      "                Loss: fixed   3 labels. Loss 0.41516. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [159] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4186868\n",
      "Train loss (w/o reg) on all data: 0.41847473\n",
      "Test loss (w/o reg) on all data: 0.40488553\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0060967095\n",
      "Norm of the params: 2.0594964\n",
      "              Random: fixed   0 labels. Loss 0.40489. Accuracy 0.822.\n",
      "### Flips: 10, rs: 2, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3682405\n",
      "Train loss (w/o reg) on all data: 0.36798635\n",
      "Test loss (w/o reg) on all data: 0.4151595\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009626537\n",
      "Norm of the params: 2.254584\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.41516. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [128] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3653749\n",
      "Train loss (w/o reg) on all data: 0.3650875\n",
      "Test loss (w/o reg) on all data: 0.4085527\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014446555\n",
      "Norm of the params: 2.3974593\n",
      "                Loss: fixed   4 labels. Loss 0.40855. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41868395\n",
      "Train loss (w/o reg) on all data: 0.4184732\n",
      "Test loss (w/o reg) on all data: 0.40472603\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0057267975\n",
      "Norm of the params: 2.053028\n",
      "              Random: fixed   0 labels. Loss 0.40473. Accuracy 0.822.\n",
      "### Flips: 10, rs: 2, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3527243\n",
      "Train loss (w/o reg) on all data: 0.35243484\n",
      "Test loss (w/o reg) on all data: 0.42013076\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0045960434\n",
      "Norm of the params: 2.4061422\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.42013. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [12] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35272384\n",
      "Train loss (w/o reg) on all data: 0.35243437\n",
      "Test loss (w/o reg) on all data: 0.41996557\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0009391825\n",
      "Norm of the params: 2.4061365\n",
      "                Loss: fixed   6 labels. Loss 0.41997. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [310] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41868395\n",
      "Train loss (w/o reg) on all data: 0.41847277\n",
      "Test loss (w/o reg) on all data: 0.40458137\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0004996863\n",
      "Norm of the params: 2.0550933\n",
      "              Random: fixed   0 labels. Loss 0.40458. Accuracy 0.822.\n",
      "### Flips: 10, rs: 2, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [161] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35509104\n",
      "Train loss (w/o reg) on all data: 0.35479686\n",
      "Test loss (w/o reg) on all data: 0.4304899\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0075694593\n",
      "Norm of the params: 2.4256277\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.43049. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [94] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3527309\n",
      "Train loss (w/o reg) on all data: 0.35244426\n",
      "Test loss (w/o reg) on all data: 0.4195692\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.003291735\n",
      "Norm of the params: 2.3942623\n",
      "                Loss: fixed   6 labels. Loss 0.41957. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4057467\n",
      "Train loss (w/o reg) on all data: 0.4055136\n",
      "Test loss (w/o reg) on all data: 0.40230477\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015810638\n",
      "Norm of the params: 2.159106\n",
      "              Random: fixed   1 labels. Loss 0.40230. Accuracy 0.822.\n",
      "### Flips: 10, rs: 2, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [98] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35508743\n",
      "Train loss (w/o reg) on all data: 0.3547911\n",
      "Test loss (w/o reg) on all data: 0.43070343\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0004521522\n",
      "Norm of the params: 2.4344928\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.43070. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35272533\n",
      "Train loss (w/o reg) on all data: 0.35243708\n",
      "Test loss (w/o reg) on all data: 0.42007545\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0018394704\n",
      "Norm of the params: 2.400991\n",
      "                Loss: fixed   6 labels. Loss 0.42008. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [135] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40575916\n",
      "Train loss (w/o reg) on all data: 0.40552577\n",
      "Test loss (w/o reg) on all data: 0.40327024\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0025037879\n",
      "Norm of the params: 2.1605396\n",
      "              Random: fixed   1 labels. Loss 0.40327. Accuracy 0.822.\n",
      "### Flips: 10, rs: 2, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [167] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35508668\n",
      "Train loss (w/o reg) on all data: 0.3547896\n",
      "Test loss (w/o reg) on all data: 0.4308858\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.003891927\n",
      "Norm of the params: 2.437544\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.43089. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [144] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35301694\n",
      "Train loss (w/o reg) on all data: 0.35272333\n",
      "Test loss (w/o reg) on all data: 0.40377128\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0027486219\n",
      "Norm of the params: 2.4232805\n",
      "                Loss: fixed   7 labels. Loss 0.40377. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4057475\n",
      "Train loss (w/o reg) on all data: 0.40551394\n",
      "Test loss (w/o reg) on all data: 0.40281856\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0130253695\n",
      "Norm of the params: 2.1613507\n",
      "              Random: fixed   1 labels. Loss 0.40282. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39487997\n",
      "Train loss (w/o reg) on all data: 0.39457256\n",
      "Test loss (w/o reg) on all data: 0.4049446\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00066041626\n",
      "Norm of the params: 2.4795756\n",
      "Flipped loss: 0.40494. Accuracy: 0.822\n",
      "### Flips: 10, rs: 3, checks: 10\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38210738\n",
      "Train loss (w/o reg) on all data: 0.3817993\n",
      "Test loss (w/o reg) on all data: 0.38626808\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013687053\n",
      "Norm of the params: 2.4822307\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.38627. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38565812\n",
      "Train loss (w/o reg) on all data: 0.3853438\n",
      "Test loss (w/o reg) on all data: 0.4166671\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001692349\n",
      "Norm of the params: 2.507343\n",
      "                Loss: fixed   1 labels. Loss 0.41667. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3894392\n",
      "Train loss (w/o reg) on all data: 0.38913798\n",
      "Test loss (w/o reg) on all data: 0.39064848\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001362883\n",
      "Norm of the params: 2.45444\n",
      "              Random: fixed   1 labels. Loss 0.39065. Accuracy 0.822.\n",
      "### Flips: 10, rs: 3, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [126] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3753187\n",
      "Train loss (w/o reg) on all data: 0.37499526\n",
      "Test loss (w/o reg) on all data: 0.40078628\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00049907446\n",
      "Norm of the params: 2.5433877\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.40079. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [140] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3584145\n",
      "Train loss (w/o reg) on all data: 0.3580829\n",
      "Test loss (w/o reg) on all data: 0.43042868\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00058880093\n",
      "Norm of the params: 2.5753148\n",
      "                Loss: fixed   5 labels. Loss 0.43043. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3894381\n",
      "Train loss (w/o reg) on all data: 0.38913947\n",
      "Test loss (w/o reg) on all data: 0.39044046\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010750683\n",
      "Norm of the params: 2.4438818\n",
      "              Random: fixed   1 labels. Loss 0.39044. Accuracy 0.822.\n",
      "### Flips: 10, rs: 3, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [168] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36425138\n",
      "Train loss (w/o reg) on all data: 0.363932\n",
      "Test loss (w/o reg) on all data: 0.40891126\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00071227615\n",
      "Norm of the params: 2.5272865\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.40891. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [124] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35091457\n",
      "Train loss (w/o reg) on all data: 0.35056943\n",
      "Test loss (w/o reg) on all data: 0.4224205\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00085368165\n",
      "Norm of the params: 2.62729\n",
      "                Loss: fixed   7 labels. Loss 0.42242. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3894341\n",
      "Train loss (w/o reg) on all data: 0.3891361\n",
      "Test loss (w/o reg) on all data: 0.3900847\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011829591\n",
      "Norm of the params: 2.4412255\n",
      "              Random: fixed   1 labels. Loss 0.39008. Accuracy 0.800.\n",
      "### Flips: 10, rs: 3, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [167] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36206827\n",
      "Train loss (w/o reg) on all data: 0.361736\n",
      "Test loss (w/o reg) on all data: 0.40766975\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007317674\n",
      "Norm of the params: 2.5778475\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.40767. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [189] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34876975\n",
      "Train loss (w/o reg) on all data: 0.34844124\n",
      "Test loss (w/o reg) on all data: 0.41972834\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007062941\n",
      "Norm of the params: 2.5632493\n",
      "                Loss: fixed   8 labels. Loss 0.41973. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38943455\n",
      "Train loss (w/o reg) on all data: 0.3891377\n",
      "Test loss (w/o reg) on all data: 0.3900788\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0029803861\n",
      "Norm of the params: 2.436638\n",
      "              Random: fixed   1 labels. Loss 0.39008. Accuracy 0.800.\n",
      "### Flips: 10, rs: 3, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [115] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3620701\n",
      "Train loss (w/o reg) on all data: 0.36173806\n",
      "Test loss (w/o reg) on all data: 0.40788087\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0029354414\n",
      "Norm of the params: 2.5770094\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.40788. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34731337\n",
      "Train loss (w/o reg) on all data: 0.34696314\n",
      "Test loss (w/o reg) on all data: 0.40536326\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015537424\n",
      "Norm of the params: 2.6466112\n",
      "                Loss: fixed   9 labels. Loss 0.40536. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [139] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38943526\n",
      "Train loss (w/o reg) on all data: 0.38913825\n",
      "Test loss (w/o reg) on all data: 0.39040828\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007892783\n",
      "Norm of the params: 2.4372308\n",
      "              Random: fixed   1 labels. Loss 0.39041. Accuracy 0.800.\n",
      "### Flips: 10, rs: 3, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [136] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34877387\n",
      "Train loss (w/o reg) on all data: 0.34843776\n",
      "Test loss (w/o reg) on all data: 0.4196894\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007379331\n",
      "Norm of the params: 2.5927389\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41969. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [90] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3473141\n",
      "Train loss (w/o reg) on all data: 0.3469646\n",
      "Test loss (w/o reg) on all data: 0.4051104\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011586419\n",
      "Norm of the params: 2.643809\n",
      "                Loss: fixed   9 labels. Loss 0.40511. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3881832\n",
      "Train loss (w/o reg) on all data: 0.38789865\n",
      "Test loss (w/o reg) on all data: 0.3855195\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0024574103\n",
      "Norm of the params: 2.3856497\n",
      "              Random: fixed   2 labels. Loss 0.38552. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4136765\n",
      "Train loss (w/o reg) on all data: 0.41346183\n",
      "Test loss (w/o reg) on all data: 0.4643409\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00024040003\n",
      "Norm of the params: 2.0719755\n",
      "Flipped loss: 0.46434. Accuracy: 0.778\n",
      "### Flips: 10, rs: 4, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33503765\n",
      "Train loss (w/o reg) on all data: 0.33466008\n",
      "Test loss (w/o reg) on all data: 0.4625904\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0023086185\n",
      "Norm of the params: 2.747998\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.46259. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [151] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34035176\n",
      "Train loss (w/o reg) on all data: 0.33998004\n",
      "Test loss (w/o reg) on all data: 0.46093667\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0035660672\n",
      "Norm of the params: 2.7265773\n",
      "                Loss: fixed   4 labels. Loss 0.46094. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4136907\n",
      "Train loss (w/o reg) on all data: 0.41347903\n",
      "Test loss (w/o reg) on all data: 0.465453\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011048338\n",
      "Norm of the params: 2.0573833\n",
      "              Random: fixed   0 labels. Loss 0.46545. Accuracy 0.778.\n",
      "### Flips: 10, rs: 4, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33503613\n",
      "Train loss (w/o reg) on all data: 0.3346582\n",
      "Test loss (w/o reg) on all data: 0.46266523\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00072214054\n",
      "Norm of the params: 2.7492912\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.46267. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33503613\n",
      "Train loss (w/o reg) on all data: 0.3346582\n",
      "Test loss (w/o reg) on all data: 0.46266043\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0014603101\n",
      "Norm of the params: 2.7492912\n",
      "                Loss: fixed   5 labels. Loss 0.46266. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [291] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41367945\n",
      "Train loss (w/o reg) on all data: 0.41346538\n",
      "Test loss (w/o reg) on all data: 0.4649106\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002775962\n",
      "Norm of the params: 2.0691483\n",
      "              Random: fixed   0 labels. Loss 0.46491. Accuracy 0.778.\n",
      "### Flips: 10, rs: 4, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33503568\n",
      "Train loss (w/o reg) on all data: 0.33465824\n",
      "Test loss (w/o reg) on all data: 0.46242583\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00076004263\n",
      "Norm of the params: 2.747491\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.46243. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [1] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33503565\n",
      "Train loss (w/o reg) on all data: 0.3346582\n",
      "Test loss (w/o reg) on all data: 0.46246406\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0013854669\n",
      "Norm of the params: 2.7474911\n",
      "                Loss: fixed   5 labels. Loss 0.46246. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3771533\n",
      "Train loss (w/o reg) on all data: 0.37689042\n",
      "Test loss (w/o reg) on all data: 0.442961\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00048109016\n",
      "Norm of the params: 2.293022\n",
      "              Random: fixed   4 labels. Loss 0.44296. Accuracy 0.800.\n",
      "### Flips: 10, rs: 4, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33503544\n",
      "Train loss (w/o reg) on all data: 0.33465683\n",
      "Test loss (w/o reg) on all data: 0.46230224\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00050442916\n",
      "Norm of the params: 2.7517817\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.46230. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33503544\n",
      "Train loss (w/o reg) on all data: 0.33465683\n",
      "Test loss (w/o reg) on all data: 0.46230948\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0029668622\n",
      "Norm of the params: 2.7517815\n",
      "                Loss: fixed   5 labels. Loss 0.46231. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37716192\n",
      "Train loss (w/o reg) on all data: 0.37690145\n",
      "Test loss (w/o reg) on all data: 0.44272715\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0034725196\n",
      "Norm of the params: 2.282358\n",
      "              Random: fixed   4 labels. Loss 0.44273. Accuracy 0.800.\n",
      "### Flips: 10, rs: 4, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [158] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33453038\n",
      "Train loss (w/o reg) on all data: 0.33418277\n",
      "Test loss (w/o reg) on all data: 0.46170944\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00095898553\n",
      "Norm of the params: 2.6367729\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.46171. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33453035\n",
      "Train loss (w/o reg) on all data: 0.33418274\n",
      "Test loss (w/o reg) on all data: 0.46171427\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015468798\n",
      "Norm of the params: 2.6367729\n",
      "                Loss: fixed   6 labels. Loss 0.46171. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37715507\n",
      "Train loss (w/o reg) on all data: 0.3768929\n",
      "Test loss (w/o reg) on all data: 0.4427972\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00227878\n",
      "Norm of the params: 2.289804\n",
      "              Random: fixed   4 labels. Loss 0.44280. Accuracy 0.800.\n",
      "### Flips: 10, rs: 4, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [136] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3345355\n",
      "Train loss (w/o reg) on all data: 0.33418545\n",
      "Test loss (w/o reg) on all data: 0.46213412\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002474156\n",
      "Norm of the params: 2.6459253\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.46213. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [94] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3341361\n",
      "Train loss (w/o reg) on all data: 0.33377364\n",
      "Test loss (w/o reg) on all data: 0.44357172\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00025860686\n",
      "Norm of the params: 2.6923916\n",
      "                Loss: fixed   7 labels. Loss 0.44357. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [222] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37715673\n",
      "Train loss (w/o reg) on all data: 0.37689498\n",
      "Test loss (w/o reg) on all data: 0.4428924\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.009568028\n",
      "Norm of the params: 2.2880821\n",
      "              Random: fixed   4 labels. Loss 0.44289. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [345] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3907492\n",
      "Train loss (w/o reg) on all data: 0.39042285\n",
      "Test loss (w/o reg) on all data: 0.37807664\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016510129\n",
      "Norm of the params: 2.5547385\n",
      "Flipped loss: 0.37808. Accuracy: 0.822\n",
      "### Flips: 10, rs: 5, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35198972\n",
      "Train loss (w/o reg) on all data: 0.3514865\n",
      "Test loss (w/o reg) on all data: 0.40085763\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006736022\n",
      "Norm of the params: 3.17242\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.40086. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34120914\n",
      "Train loss (w/o reg) on all data: 0.340718\n",
      "Test loss (w/o reg) on all data: 0.39526966\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012714776\n",
      "Norm of the params: 3.134149\n",
      "                Loss: fixed   3 labels. Loss 0.39527. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39074954\n",
      "Train loss (w/o reg) on all data: 0.39042434\n",
      "Test loss (w/o reg) on all data: 0.37798423\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015548916\n",
      "Norm of the params: 2.550358\n",
      "              Random: fixed   0 labels. Loss 0.37798. Accuracy 0.822.\n",
      "### Flips: 10, rs: 5, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33626258\n",
      "Train loss (w/o reg) on all data: 0.33579546\n",
      "Test loss (w/o reg) on all data: 0.40483195\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017044293\n",
      "Norm of the params: 3.0565236\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.40483. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3362626\n",
      "Train loss (w/o reg) on all data: 0.3357955\n",
      "Test loss (w/o reg) on all data: 0.40483662\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00051347446\n",
      "Norm of the params: 3.0565236\n",
      "                Loss: fixed   4 labels. Loss 0.40484. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39074746\n",
      "Train loss (w/o reg) on all data: 0.39042303\n",
      "Test loss (w/o reg) on all data: 0.37827247\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017544756\n",
      "Norm of the params: 2.547279\n",
      "              Random: fixed   0 labels. Loss 0.37827. Accuracy 0.822.\n",
      "### Flips: 10, rs: 5, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [280] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33620465\n",
      "Train loss (w/o reg) on all data: 0.33575702\n",
      "Test loss (w/o reg) on all data: 0.4076202\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018206078\n",
      "Norm of the params: 2.9921334\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.40762. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [179] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3362616\n",
      "Train loss (w/o reg) on all data: 0.3357937\n",
      "Test loss (w/o reg) on all data: 0.4053628\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009529211\n",
      "Norm of the params: 3.0591037\n",
      "                Loss: fixed   4 labels. Loss 0.40536. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40300196\n",
      "Train loss (w/o reg) on all data: 0.40270817\n",
      "Test loss (w/o reg) on all data: 0.37977412\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019016917\n",
      "Norm of the params: 2.4239929\n",
      "              Random: fixed   1 labels. Loss 0.37977. Accuracy 0.822.\n",
      "### Flips: 10, rs: 5, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33496723\n",
      "Train loss (w/o reg) on all data: 0.33453903\n",
      "Test loss (w/o reg) on all data: 0.41830793\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003043862\n",
      "Norm of the params: 2.9264138\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.41831. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [75] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3362204\n",
      "Train loss (w/o reg) on all data: 0.3357816\n",
      "Test loss (w/o reg) on all data: 0.4073189\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001376314\n",
      "Norm of the params: 2.9624991\n",
      "                Loss: fixed   5 labels. Loss 0.40732. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [148] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4046519\n",
      "Train loss (w/o reg) on all data: 0.40437943\n",
      "Test loss (w/o reg) on all data: 0.3812587\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009887315\n",
      "Norm of the params: 2.3344119\n",
      "              Random: fixed   2 labels. Loss 0.38126. Accuracy 0.822.\n",
      "### Flips: 10, rs: 5, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3349684\n",
      "Train loss (w/o reg) on all data: 0.3345378\n",
      "Test loss (w/o reg) on all data: 0.41832712\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010665193\n",
      "Norm of the params: 2.9345293\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.41833. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [185] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33835682\n",
      "Train loss (w/o reg) on all data: 0.3379406\n",
      "Test loss (w/o reg) on all data: 0.40803126\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017539595\n",
      "Norm of the params: 2.8852313\n",
      "                Loss: fixed   6 labels. Loss 0.40803. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [132] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40465197\n",
      "Train loss (w/o reg) on all data: 0.4043779\n",
      "Test loss (w/o reg) on all data: 0.38103354\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007245951\n",
      "Norm of the params: 2.341153\n",
      "              Random: fixed   2 labels. Loss 0.38103. Accuracy 0.822.\n",
      "### Flips: 10, rs: 5, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33496442\n",
      "Train loss (w/o reg) on all data: 0.3345341\n",
      "Test loss (w/o reg) on all data: 0.4175816\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00067420764\n",
      "Norm of the params: 2.9336283\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.41758. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [84] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3372822\n",
      "Train loss (w/o reg) on all data: 0.33687854\n",
      "Test loss (w/o reg) on all data: 0.41696656\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00044906238\n",
      "Norm of the params: 2.8413455\n",
      "                Loss: fixed   7 labels. Loss 0.41697. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38898847\n",
      "Train loss (w/o reg) on all data: 0.3887379\n",
      "Test loss (w/o reg) on all data: 0.38472065\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00180629\n",
      "Norm of the params: 2.238632\n",
      "              Random: fixed   4 labels. Loss 0.38472. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41679877\n",
      "Train loss (w/o reg) on all data: 0.41648382\n",
      "Test loss (w/o reg) on all data: 0.4466912\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002458621\n",
      "Norm of the params: 2.5098226\n",
      "Flipped loss: 0.44669. Accuracy: 0.778\n",
      "### Flips: 10, rs: 6, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [177] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38061345\n",
      "Train loss (w/o reg) on all data: 0.38027552\n",
      "Test loss (w/o reg) on all data: 0.41297773\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001554148\n",
      "Norm of the params: 2.5997398\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.41298. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [94] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36881366\n",
      "Train loss (w/o reg) on all data: 0.36844954\n",
      "Test loss (w/o reg) on all data: 0.41801372\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010855163\n",
      "Norm of the params: 2.6986334\n",
      "                Loss: fixed   4 labels. Loss 0.41801. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [81] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41681257\n",
      "Train loss (w/o reg) on all data: 0.41650426\n",
      "Test loss (w/o reg) on all data: 0.44578582\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0013806997\n",
      "Norm of the params: 2.483193\n",
      "              Random: fixed   0 labels. Loss 0.44579. Accuracy 0.778.\n",
      "### Flips: 10, rs: 6, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [149] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3688139\n",
      "Train loss (w/o reg) on all data: 0.36844578\n",
      "Test loss (w/o reg) on all data: 0.41761792\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0023672949\n",
      "Norm of the params: 2.7133255\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.41762. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [96] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3643176\n",
      "Train loss (w/o reg) on all data: 0.3639608\n",
      "Test loss (w/o reg) on all data: 0.4323975\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0006065425\n",
      "Norm of the params: 2.6712875\n",
      "                Loss: fixed   5 labels. Loss 0.43240. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41679844\n",
      "Train loss (w/o reg) on all data: 0.41648513\n",
      "Test loss (w/o reg) on all data: 0.44602543\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002781009\n",
      "Norm of the params: 2.5032382\n",
      "              Random: fixed   0 labels. Loss 0.44603. Accuracy 0.778.\n",
      "### Flips: 10, rs: 6, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [152] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3688137\n",
      "Train loss (w/o reg) on all data: 0.3684458\n",
      "Test loss (w/o reg) on all data: 0.41825756\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001238013\n",
      "Norm of the params: 2.71245\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.41826. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [145] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35941905\n",
      "Train loss (w/o reg) on all data: 0.35905924\n",
      "Test loss (w/o reg) on all data: 0.41918898\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0007646123\n",
      "Norm of the params: 2.682557\n",
      "                Loss: fixed   6 labels. Loss 0.41919. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41680232\n",
      "Train loss (w/o reg) on all data: 0.41649014\n",
      "Test loss (w/o reg) on all data: 0.44599342\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0022700387\n",
      "Norm of the params: 2.4987037\n",
      "              Random: fixed   0 labels. Loss 0.44599. Accuracy 0.778.\n",
      "### Flips: 10, rs: 6, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [88] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35942054\n",
      "Train loss (w/o reg) on all data: 0.359056\n",
      "Test loss (w/o reg) on all data: 0.4192543\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00094657566\n",
      "Norm of the params: 2.7001739\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.41925. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35942066\n",
      "Train loss (w/o reg) on all data: 0.35905612\n",
      "Test loss (w/o reg) on all data: 0.41925186\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00088970206\n",
      "Norm of the params: 2.7001739\n",
      "                Loss: fixed   6 labels. Loss 0.41925. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [179] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4033693\n",
      "Train loss (w/o reg) on all data: 0.40303284\n",
      "Test loss (w/o reg) on all data: 0.43211564\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011406683\n",
      "Norm of the params: 2.5941\n",
      "              Random: fixed   1 labels. Loss 0.43212. Accuracy 0.778.\n",
      "### Flips: 10, rs: 6, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35644537\n",
      "Train loss (w/o reg) on all data: 0.35609025\n",
      "Test loss (w/o reg) on all data: 0.4206493\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00045274565\n",
      "Norm of the params: 2.6650312\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.42065. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3564453\n",
      "Train loss (w/o reg) on all data: 0.3560902\n",
      "Test loss (w/o reg) on all data: 0.42064825\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0023394513\n",
      "Norm of the params: 2.6650307\n",
      "                Loss: fixed   7 labels. Loss 0.42065. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40336895\n",
      "Train loss (w/o reg) on all data: 0.40303454\n",
      "Test loss (w/o reg) on all data: 0.4308234\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0006724024\n",
      "Norm of the params: 2.5861738\n",
      "              Random: fixed   1 labels. Loss 0.43082. Accuracy 0.778.\n",
      "### Flips: 10, rs: 6, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3564462\n",
      "Train loss (w/o reg) on all data: 0.35609058\n",
      "Test loss (w/o reg) on all data: 0.42064276\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0025863126\n",
      "Norm of the params: 2.6669364\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.42064. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [64] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3562666\n",
      "Train loss (w/o reg) on all data: 0.35590428\n",
      "Test loss (w/o reg) on all data: 0.41649264\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010386793\n",
      "Norm of the params: 2.691836\n",
      "                Loss: fixed   8 labels. Loss 0.41649. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [193] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40139848\n",
      "Train loss (w/o reg) on all data: 0.4010749\n",
      "Test loss (w/o reg) on all data: 0.4273504\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.003474784\n",
      "Norm of the params: 2.54403\n",
      "              Random: fixed   2 labels. Loss 0.42735. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [167] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47290874\n",
      "Train loss (w/o reg) on all data: 0.472783\n",
      "Test loss (w/o reg) on all data: 0.3957107\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018981358\n",
      "Norm of the params: 1.5858747\n",
      "Flipped loss: 0.39571. Accuracy: 0.800\n",
      "### Flips: 10, rs: 7, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [135] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4159065\n",
      "Train loss (w/o reg) on all data: 0.4157124\n",
      "Test loss (w/o reg) on all data: 0.38999966\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00237875\n",
      "Norm of the params: 1.9703195\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.39000. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [285] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35108793\n",
      "Train loss (w/o reg) on all data: 0.3507544\n",
      "Test loss (w/o reg) on all data: 0.41732737\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015948187\n",
      "Norm of the params: 2.58267\n",
      "                Loss: fixed   8 labels. Loss 0.41733. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47290683\n",
      "Train loss (w/o reg) on all data: 0.47278228\n",
      "Test loss (w/o reg) on all data: 0.39582345\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00095881475\n",
      "Norm of the params: 1.5781816\n",
      "              Random: fixed   0 labels. Loss 0.39582. Accuracy 0.800.\n",
      "### Flips: 10, rs: 7, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3631298\n",
      "Train loss (w/o reg) on all data: 0.36282527\n",
      "Test loss (w/o reg) on all data: 0.39888084\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00063309516\n",
      "Norm of the params: 2.4678235\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.39888. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35108694\n",
      "Train loss (w/o reg) on all data: 0.3507543\n",
      "Test loss (w/o reg) on all data: 0.41783243\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0025671877\n",
      "Norm of the params: 2.5793307\n",
      "                Loss: fixed   8 labels. Loss 0.41783. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [333] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47290117\n",
      "Train loss (w/o reg) on all data: 0.47277635\n",
      "Test loss (w/o reg) on all data: 0.39541045\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00037497818\n",
      "Norm of the params: 1.5800337\n",
      "              Random: fixed   0 labels. Loss 0.39541. Accuracy 0.800.\n",
      "### Flips: 10, rs: 7, checks: 30\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [425] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36312944\n",
      "Train loss (w/o reg) on all data: 0.36282602\n",
      "Test loss (w/o reg) on all data: 0.3987202\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016589767\n",
      "Norm of the params: 2.463458\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.39872. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [161] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35108724\n",
      "Train loss (w/o reg) on all data: 0.35075507\n",
      "Test loss (w/o reg) on all data: 0.4176701\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011395947\n",
      "Norm of the params: 2.577544\n",
      "                Loss: fixed   8 labels. Loss 0.41767. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [272] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46197158\n",
      "Train loss (w/o reg) on all data: 0.46183535\n",
      "Test loss (w/o reg) on all data: 0.39200214\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007097555\n",
      "Norm of the params: 1.6506122\n",
      "              Random: fixed   1 labels. Loss 0.39200. Accuracy 0.822.\n",
      "### Flips: 10, rs: 7, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [290] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36312994\n",
      "Train loss (w/o reg) on all data: 0.36282575\n",
      "Test loss (w/o reg) on all data: 0.39911133\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002474155\n",
      "Norm of the params: 2.466505\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.39911. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [77] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3511107\n",
      "Train loss (w/o reg) on all data: 0.35077924\n",
      "Test loss (w/o reg) on all data: 0.41597465\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010694907\n",
      "Norm of the params: 2.5747128\n",
      "                Loss: fixed   8 labels. Loss 0.41597. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [290] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44460854\n",
      "Train loss (w/o reg) on all data: 0.44445586\n",
      "Test loss (w/o reg) on all data: 0.3862597\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.007456157\n",
      "Norm of the params: 1.7473552\n",
      "              Random: fixed   3 labels. Loss 0.38626. Accuracy 0.822.\n",
      "### Flips: 10, rs: 7, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36313048\n",
      "Train loss (w/o reg) on all data: 0.36282694\n",
      "Test loss (w/o reg) on all data: 0.39898708\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017766793\n",
      "Norm of the params: 2.4638987\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.39899. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34898028\n",
      "Train loss (w/o reg) on all data: 0.34864593\n",
      "Test loss (w/o reg) on all data: 0.41952726\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019774742\n",
      "Norm of the params: 2.5859356\n",
      "                Loss: fixed   9 labels. Loss 0.41953. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42756397\n",
      "Train loss (w/o reg) on all data: 0.42739102\n",
      "Test loss (w/o reg) on all data: 0.3784713\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008884247\n",
      "Norm of the params: 1.8598552\n",
      "              Random: fixed   4 labels. Loss 0.37847. Accuracy 0.822.\n",
      "### Flips: 10, rs: 7, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [412] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3489789\n",
      "Train loss (w/o reg) on all data: 0.34864447\n",
      "Test loss (w/o reg) on all data: 0.42007855\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0037521431\n",
      "Norm of the params: 2.5862665\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.42008. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34897888\n",
      "Train loss (w/o reg) on all data: 0.34864444\n",
      "Test loss (w/o reg) on all data: 0.42008713\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00048838847\n",
      "Norm of the params: 2.5862665\n",
      "                Loss: fixed   9 labels. Loss 0.42009. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [264] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42756298\n",
      "Train loss (w/o reg) on all data: 0.42739022\n",
      "Test loss (w/o reg) on all data: 0.378423\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002067952\n",
      "Norm of the params: 1.8587909\n",
      "              Random: fixed   4 labels. Loss 0.37842. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4040822\n",
      "Train loss (w/o reg) on all data: 0.40386388\n",
      "Test loss (w/o reg) on all data: 0.38680255\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006520842\n",
      "Norm of the params: 2.0895905\n",
      "Flipped loss: 0.38680. Accuracy: 0.822\n",
      "### Flips: 10, rs: 8, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [282] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38447934\n",
      "Train loss (w/o reg) on all data: 0.38421386\n",
      "Test loss (w/o reg) on all data: 0.3711413\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014794687\n",
      "Norm of the params: 2.3042188\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.37114. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [165] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3698321\n",
      "Train loss (w/o reg) on all data: 0.36953044\n",
      "Test loss (w/o reg) on all data: 0.39035624\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014927496\n",
      "Norm of the params: 2.4562352\n",
      "                Loss: fixed   3 labels. Loss 0.39036. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4043296\n",
      "Train loss (w/o reg) on all data: 0.40409556\n",
      "Test loss (w/o reg) on all data: 0.39841157\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0005899523\n",
      "Norm of the params: 2.1634736\n",
      "              Random: fixed   1 labels. Loss 0.39841. Accuracy 0.778.\n",
      "### Flips: 10, rs: 8, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3628784\n",
      "Train loss (w/o reg) on all data: 0.36255088\n",
      "Test loss (w/o reg) on all data: 0.38009185\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00051339326\n",
      "Norm of the params: 2.5594327\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.38009. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35546228\n",
      "Train loss (w/o reg) on all data: 0.35509822\n",
      "Test loss (w/o reg) on all data: 0.40045103\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0067714574\n",
      "Norm of the params: 2.6983802\n",
      "                Loss: fixed   5 labels. Loss 0.40045. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3975797\n",
      "Train loss (w/o reg) on all data: 0.3973084\n",
      "Test loss (w/o reg) on all data: 0.4145024\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0018099973\n",
      "Norm of the params: 2.329347\n",
      "              Random: fixed   2 labels. Loss 0.41450. Accuracy 0.778.\n",
      "### Flips: 10, rs: 8, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [272] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3593676\n",
      "Train loss (w/o reg) on all data: 0.3590444\n",
      "Test loss (w/o reg) on all data: 0.38254017\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0009852054\n",
      "Norm of the params: 2.542498\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.38254. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [159] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34747973\n",
      "Train loss (w/o reg) on all data: 0.3471321\n",
      "Test loss (w/o reg) on all data: 0.39896396\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00053570565\n",
      "Norm of the params: 2.6368868\n",
      "                Loss: fixed   7 labels. Loss 0.39896. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [308] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39757967\n",
      "Train loss (w/o reg) on all data: 0.39730725\n",
      "Test loss (w/o reg) on all data: 0.41461915\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0017183034\n",
      "Norm of the params: 2.334228\n",
      "              Random: fixed   2 labels. Loss 0.41462. Accuracy 0.778.\n",
      "### Flips: 10, rs: 8, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [318] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36145175\n",
      "Train loss (w/o reg) on all data: 0.36112022\n",
      "Test loss (w/o reg) on all data: 0.39377812\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00051131146\n",
      "Norm of the params: 2.5749645\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.39378. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [150] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34747815\n",
      "Train loss (w/o reg) on all data: 0.3471294\n",
      "Test loss (w/o reg) on all data: 0.39997652\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00027746754\n",
      "Norm of the params: 2.6410563\n",
      "                Loss: fixed   7 labels. Loss 0.39998. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [357] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38714957\n",
      "Train loss (w/o reg) on all data: 0.3868573\n",
      "Test loss (w/o reg) on all data: 0.43005332\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00066239596\n",
      "Norm of the params: 2.417736\n",
      "              Random: fixed   3 labels. Loss 0.43005. Accuracy 0.756.\n",
      "### Flips: 10, rs: 8, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [292] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34984788\n",
      "Train loss (w/o reg) on all data: 0.34948695\n",
      "Test loss (w/o reg) on all data: 0.412211\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011832557\n",
      "Norm of the params: 2.6868055\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41221. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34984788\n",
      "Train loss (w/o reg) on all data: 0.34948695\n",
      "Test loss (w/o reg) on all data: 0.41220763\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010373688\n",
      "Norm of the params: 2.6868055\n",
      "                Loss: fixed   8 labels. Loss 0.41221. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3871521\n",
      "Train loss (w/o reg) on all data: 0.38686132\n",
      "Test loss (w/o reg) on all data: 0.4295002\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0023839765\n",
      "Norm of the params: 2.411586\n",
      "              Random: fixed   3 labels. Loss 0.42950. Accuracy 0.756.\n",
      "### Flips: 10, rs: 8, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34960422\n",
      "Train loss (w/o reg) on all data: 0.34925508\n",
      "Test loss (w/o reg) on all data: 0.41898075\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001331255\n",
      "Norm of the params: 2.6425009\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.41898. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34960416\n",
      "Train loss (w/o reg) on all data: 0.34925503\n",
      "Test loss (w/o reg) on all data: 0.41897723\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013583448\n",
      "Norm of the params: 2.6425009\n",
      "                Loss: fixed   9 labels. Loss 0.41898. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [316] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3871507\n",
      "Train loss (w/o reg) on all data: 0.3868578\n",
      "Test loss (w/o reg) on all data: 0.4303924\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0022965283\n",
      "Norm of the params: 2.4203358\n",
      "              Random: fixed   3 labels. Loss 0.43039. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43801665\n",
      "Train loss (w/o reg) on all data: 0.43787095\n",
      "Test loss (w/o reg) on all data: 0.40637696\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0033069525\n",
      "Norm of the params: 1.7069913\n",
      "Flipped loss: 0.40638. Accuracy: 0.800\n",
      "### Flips: 10, rs: 9, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3949508\n",
      "Train loss (w/o reg) on all data: 0.39472702\n",
      "Test loss (w/o reg) on all data: 0.39332592\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0023374828\n",
      "Norm of the params: 2.1156332\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.39333. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [64] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38439512\n",
      "Train loss (w/o reg) on all data: 0.3841638\n",
      "Test loss (w/o reg) on all data: 0.40493914\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.009047543\n",
      "Norm of the params: 2.1509027\n",
      "                Loss: fixed   4 labels. Loss 0.40494. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42910075\n",
      "Train loss (w/o reg) on all data: 0.4289496\n",
      "Test loss (w/o reg) on all data: 0.40257585\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010503223\n",
      "Norm of the params: 1.7386657\n",
      "              Random: fixed   1 labels. Loss 0.40258. Accuracy 0.822.\n",
      "### Flips: 10, rs: 9, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [165] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.388869\n",
      "Train loss (w/o reg) on all data: 0.3886328\n",
      "Test loss (w/o reg) on all data: 0.38272077\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00084634114\n",
      "Norm of the params: 2.1734142\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.38272. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36939603\n",
      "Train loss (w/o reg) on all data: 0.36912593\n",
      "Test loss (w/o reg) on all data: 0.42269504\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00028296132\n",
      "Norm of the params: 2.3242164\n",
      "                Loss: fixed   6 labels. Loss 0.42270. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [136] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42153135\n",
      "Train loss (w/o reg) on all data: 0.42135453\n",
      "Test loss (w/o reg) on all data: 0.40648186\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0008060328\n",
      "Norm of the params: 1.8805313\n",
      "              Random: fixed   2 labels. Loss 0.40648. Accuracy 0.844.\n",
      "### Flips: 10, rs: 9, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [166] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38015565\n",
      "Train loss (w/o reg) on all data: 0.3798793\n",
      "Test loss (w/o reg) on all data: 0.3882509\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0022921963\n",
      "Norm of the params: 2.3509307\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.38825. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [119] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3590715\n",
      "Train loss (w/o reg) on all data: 0.35875878\n",
      "Test loss (w/o reg) on all data: 0.40848625\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0057650083\n",
      "Norm of the params: 2.500857\n",
      "                Loss: fixed   8 labels. Loss 0.40849. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4240887\n",
      "Train loss (w/o reg) on all data: 0.4239072\n",
      "Test loss (w/o reg) on all data: 0.4038918\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0069037774\n",
      "Norm of the params: 1.9052349\n",
      "              Random: fixed   3 labels. Loss 0.40389. Accuracy 0.800.\n",
      "### Flips: 10, rs: 9, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [137] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3691754\n",
      "Train loss (w/o reg) on all data: 0.36888653\n",
      "Test loss (w/o reg) on all data: 0.4014217\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0007807582\n",
      "Norm of the params: 2.4036276\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.40142. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35721296\n",
      "Train loss (w/o reg) on all data: 0.3569051\n",
      "Test loss (w/o reg) on all data: 0.40808168\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018975879\n",
      "Norm of the params: 2.4813473\n",
      "                Loss: fixed   9 labels. Loss 0.40808. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41969866\n",
      "Train loss (w/o reg) on all data: 0.41950798\n",
      "Test loss (w/o reg) on all data: 0.3919708\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008283645\n",
      "Norm of the params: 1.9528904\n",
      "              Random: fixed   4 labels. Loss 0.39197. Accuracy 0.822.\n",
      "### Flips: 10, rs: 9, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3572179\n",
      "Train loss (w/o reg) on all data: 0.3569071\n",
      "Test loss (w/o reg) on all data: 0.40838516\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001463594\n",
      "Norm of the params: 2.4932258\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.40839. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3572179\n",
      "Train loss (w/o reg) on all data: 0.3569071\n",
      "Test loss (w/o reg) on all data: 0.4083892\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010897793\n",
      "Norm of the params: 2.4932258\n",
      "                Loss: fixed   9 labels. Loss 0.40839. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41969952\n",
      "Train loss (w/o reg) on all data: 0.4195085\n",
      "Test loss (w/o reg) on all data: 0.39223006\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.000684681\n",
      "Norm of the params: 1.9546326\n",
      "              Random: fixed   4 labels. Loss 0.39223. Accuracy 0.822.\n",
      "### Flips: 10, rs: 9, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [327] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35721326\n",
      "Train loss (w/o reg) on all data: 0.3569048\n",
      "Test loss (w/o reg) on all data: 0.4075009\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019894682\n",
      "Norm of the params: 2.4837382\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.40750. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35721323\n",
      "Train loss (w/o reg) on all data: 0.35690477\n",
      "Test loss (w/o reg) on all data: 0.40750572\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006689291\n",
      "Norm of the params: 2.4837382\n",
      "                Loss: fixed   9 labels. Loss 0.40751. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4197046\n",
      "Train loss (w/o reg) on all data: 0.41951558\n",
      "Test loss (w/o reg) on all data: 0.39154086\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0030793196\n",
      "Norm of the params: 1.9442449\n",
      "              Random: fixed   4 labels. Loss 0.39154. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4272714\n",
      "Train loss (w/o reg) on all data: 0.42711836\n",
      "Test loss (w/o reg) on all data: 0.3838194\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015185009\n",
      "Norm of the params: 1.7495092\n",
      "Flipped loss: 0.38382. Accuracy: 0.822\n",
      "### Flips: 10, rs: 10, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [201] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38455346\n",
      "Train loss (w/o reg) on all data: 0.38429943\n",
      "Test loss (w/o reg) on all data: 0.3779918\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016045731\n",
      "Norm of the params: 2.2541041\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.37799. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [127] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37697262\n",
      "Train loss (w/o reg) on all data: 0.3766897\n",
      "Test loss (w/o reg) on all data: 0.3906342\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00086454605\n",
      "Norm of the params: 2.37867\n",
      "                Loss: fixed   3 labels. Loss 0.39063. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42727196\n",
      "Train loss (w/o reg) on all data: 0.42711914\n",
      "Test loss (w/o reg) on all data: 0.38412595\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016501141\n",
      "Norm of the params: 1.7482766\n",
      "              Random: fixed   0 labels. Loss 0.38413. Accuracy 0.822.\n",
      "### Flips: 10, rs: 10, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [375] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36812764\n",
      "Train loss (w/o reg) on all data: 0.3678053\n",
      "Test loss (w/o reg) on all data: 0.38561612\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00076005945\n",
      "Norm of the params: 2.5390234\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.38562. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [335] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35290995\n",
      "Train loss (w/o reg) on all data: 0.3525827\n",
      "Test loss (w/o reg) on all data: 0.39937913\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007969353\n",
      "Norm of the params: 2.5583231\n",
      "                Loss: fixed   6 labels. Loss 0.39938. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42912173\n",
      "Train loss (w/o reg) on all data: 0.42897275\n",
      "Test loss (w/o reg) on all data: 0.3910206\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012695375\n",
      "Norm of the params: 1.7261915\n",
      "              Random: fixed   1 labels. Loss 0.39102. Accuracy 0.800.\n",
      "### Flips: 10, rs: 10, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [392] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35367805\n",
      "Train loss (w/o reg) on all data: 0.35333008\n",
      "Test loss (w/o reg) on all data: 0.40982023\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014257439\n",
      "Norm of the params: 2.6380506\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.40982. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [76] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3499177\n",
      "Train loss (w/o reg) on all data: 0.34958482\n",
      "Test loss (w/o reg) on all data: 0.4032169\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018196088\n",
      "Norm of the params: 2.5802405\n",
      "                Loss: fixed   7 labels. Loss 0.40322. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [315] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42322072\n",
      "Train loss (w/o reg) on all data: 0.4230705\n",
      "Test loss (w/o reg) on all data: 0.3949116\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00233346\n",
      "Norm of the params: 1.7333455\n",
      "              Random: fixed   2 labels. Loss 0.39491. Accuracy 0.800.\n",
      "### Flips: 10, rs: 10, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [258] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35367918\n",
      "Train loss (w/o reg) on all data: 0.35333085\n",
      "Test loss (w/o reg) on all data: 0.40949628\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020461474\n",
      "Norm of the params: 2.6393778\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.40950. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [82] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34991732\n",
      "Train loss (w/o reg) on all data: 0.34958452\n",
      "Test loss (w/o reg) on all data: 0.40313703\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0039563407\n",
      "Norm of the params: 2.5799484\n",
      "                Loss: fixed   7 labels. Loss 0.40314. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4176031\n",
      "Train loss (w/o reg) on all data: 0.41745654\n",
      "Test loss (w/o reg) on all data: 0.39060575\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.011238203\n",
      "Norm of the params: 1.712061\n",
      "              Random: fixed   4 labels. Loss 0.39061. Accuracy 0.800.\n",
      "### Flips: 10, rs: 10, checks: 50\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35465547\n",
      "Train loss (w/o reg) on all data: 0.35431266\n",
      "Test loss (w/o reg) on all data: 0.40131748\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0041494393\n",
      "Norm of the params: 2.618444\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.40132. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [135] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34991464\n",
      "Train loss (w/o reg) on all data: 0.34958217\n",
      "Test loss (w/o reg) on all data: 0.4019296\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013311129\n",
      "Norm of the params: 2.5786679\n",
      "                Loss: fixed   7 labels. Loss 0.40193. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41762245\n",
      "Train loss (w/o reg) on all data: 0.4174774\n",
      "Test loss (w/o reg) on all data: 0.39199206\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016523448\n",
      "Norm of the params: 1.7032198\n",
      "              Random: fixed   4 labels. Loss 0.39199. Accuracy 0.800.\n",
      "### Flips: 10, rs: 10, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35465038\n",
      "Train loss (w/o reg) on all data: 0.35430974\n",
      "Test loss (w/o reg) on all data: 0.40152752\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017864929\n",
      "Norm of the params: 2.6101367\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.40153. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [157] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35701385\n",
      "Train loss (w/o reg) on all data: 0.3567084\n",
      "Test loss (w/o reg) on all data: 0.39678577\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008380946\n",
      "Norm of the params: 2.4716575\n",
      "                Loss: fixed   9 labels. Loss 0.39679. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41760138\n",
      "Train loss (w/o reg) on all data: 0.41745567\n",
      "Test loss (w/o reg) on all data: 0.3910095\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0050112107\n",
      "Norm of the params: 1.7070346\n",
      "              Random: fixed   4 labels. Loss 0.39101. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40297502\n",
      "Train loss (w/o reg) on all data: 0.4027407\n",
      "Test loss (w/o reg) on all data: 0.38113725\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0018183112\n",
      "Norm of the params: 2.1649265\n",
      "Flipped loss: 0.38114. Accuracy: 0.778\n",
      "### Flips: 10, rs: 11, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [74] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38480374\n",
      "Train loss (w/o reg) on all data: 0.38455188\n",
      "Test loss (w/o reg) on all data: 0.38750014\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00090361823\n",
      "Norm of the params: 2.2443733\n",
      "     Influence (LOO): fixed   1 labels. Loss 0.38750. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [149] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3602021\n",
      "Train loss (w/o reg) on all data: 0.35989523\n",
      "Test loss (w/o reg) on all data: 0.397891\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0003815526\n",
      "Norm of the params: 2.4774098\n",
      "                Loss: fixed   3 labels. Loss 0.39789. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [189] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39809158\n",
      "Train loss (w/o reg) on all data: 0.39782268\n",
      "Test loss (w/o reg) on all data: 0.37571007\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0039851437\n",
      "Norm of the params: 2.319071\n",
      "              Random: fixed   2 labels. Loss 0.37571. Accuracy 0.800.\n",
      "### Flips: 10, rs: 11, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [113] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37083256\n",
      "Train loss (w/o reg) on all data: 0.37053037\n",
      "Test loss (w/o reg) on all data: 0.38892442\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0031421673\n",
      "Norm of the params: 2.4584055\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.38892. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34657788\n",
      "Train loss (w/o reg) on all data: 0.34628174\n",
      "Test loss (w/o reg) on all data: 0.3935279\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.005491651\n",
      "Norm of the params: 2.4337637\n",
      "                Loss: fixed   5 labels. Loss 0.39353. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [155] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39342922\n",
      "Train loss (w/o reg) on all data: 0.39315802\n",
      "Test loss (w/o reg) on all data: 0.3661261\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010746877\n",
      "Norm of the params: 2.3290145\n",
      "              Random: fixed   3 labels. Loss 0.36613. Accuracy 0.800.\n",
      "### Flips: 10, rs: 11, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36446634\n",
      "Train loss (w/o reg) on all data: 0.3641603\n",
      "Test loss (w/o reg) on all data: 0.39097616\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005133919\n",
      "Norm of the params: 2.4740782\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.39098. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [80] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3441432\n",
      "Train loss (w/o reg) on all data: 0.34384277\n",
      "Test loss (w/o reg) on all data: 0.39746746\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0036847284\n",
      "Norm of the params: 2.4512556\n",
      "                Loss: fixed   6 labels. Loss 0.39747. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [167] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39344463\n",
      "Train loss (w/o reg) on all data: 0.39317352\n",
      "Test loss (w/o reg) on all data: 0.36673462\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0028361622\n",
      "Norm of the params: 2.328622\n",
      "              Random: fixed   3 labels. Loss 0.36673. Accuracy 0.800.\n",
      "### Flips: 10, rs: 11, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [132] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34464788\n",
      "Train loss (w/o reg) on all data: 0.3443389\n",
      "Test loss (w/o reg) on all data: 0.40748838\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0004510541\n",
      "Norm of the params: 2.485892\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.40749. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [123] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34414002\n",
      "Train loss (w/o reg) on all data: 0.34384248\n",
      "Test loss (w/o reg) on all data: 0.39708862\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0019810167\n",
      "Norm of the params: 2.4394634\n",
      "                Loss: fixed   6 labels. Loss 0.39709. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [292] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39437652\n",
      "Train loss (w/o reg) on all data: 0.39409053\n",
      "Test loss (w/o reg) on all data: 0.36518598\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009079283\n",
      "Norm of the params: 2.391553\n",
      "              Random: fixed   4 labels. Loss 0.36519. Accuracy 0.800.\n",
      "### Flips: 10, rs: 11, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34875423\n",
      "Train loss (w/o reg) on all data: 0.34843507\n",
      "Test loss (w/o reg) on all data: 0.41682053\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006270232\n",
      "Norm of the params: 2.52643\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41682. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [208] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34807196\n",
      "Train loss (w/o reg) on all data: 0.34775487\n",
      "Test loss (w/o reg) on all data: 0.40556523\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00069880055\n",
      "Norm of the params: 2.5183654\n",
      "                Loss: fixed   8 labels. Loss 0.40557. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39438015\n",
      "Train loss (w/o reg) on all data: 0.39409408\n",
      "Test loss (w/o reg) on all data: 0.3652985\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007913255\n",
      "Norm of the params: 2.3919435\n",
      "              Random: fixed   4 labels. Loss 0.36530. Accuracy 0.800.\n",
      "### Flips: 10, rs: 11, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [195] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34875542\n",
      "Train loss (w/o reg) on all data: 0.34843633\n",
      "Test loss (w/o reg) on all data: 0.41695213\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001953967\n",
      "Norm of the params: 2.5262494\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41695. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [135] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34807336\n",
      "Train loss (w/o reg) on all data: 0.347755\n",
      "Test loss (w/o reg) on all data: 0.40605682\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002388194\n",
      "Norm of the params: 2.5232599\n",
      "                Loss: fixed   8 labels. Loss 0.40606. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [146] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38472748\n",
      "Train loss (w/o reg) on all data: 0.38443792\n",
      "Test loss (w/o reg) on all data: 0.37117675\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.000919419\n",
      "Norm of the params: 2.4064705\n",
      "              Random: fixed   5 labels. Loss 0.37118. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44114652\n",
      "Train loss (w/o reg) on all data: 0.4409115\n",
      "Test loss (w/o reg) on all data: 0.34605435\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0036901708\n",
      "Norm of the params: 2.1680026\n",
      "Flipped loss: 0.34605. Accuracy: 0.844\n",
      "### Flips: 10, rs: 12, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40141052\n",
      "Train loss (w/o reg) on all data: 0.4010962\n",
      "Test loss (w/o reg) on all data: 0.34402364\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00087509386\n",
      "Norm of the params: 2.5073457\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.34402. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3896795\n",
      "Train loss (w/o reg) on all data: 0.389354\n",
      "Test loss (w/o reg) on all data: 0.3513791\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00049646455\n",
      "Norm of the params: 2.55147\n",
      "                Loss: fixed   4 labels. Loss 0.35138. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4230962\n",
      "Train loss (w/o reg) on all data: 0.4228238\n",
      "Test loss (w/o reg) on all data: 0.34326813\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010637996\n",
      "Norm of the params: 2.334241\n",
      "              Random: fixed   1 labels. Loss 0.34327. Accuracy 0.844.\n",
      "### Flips: 10, rs: 12, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39708745\n",
      "Train loss (w/o reg) on all data: 0.3967823\n",
      "Test loss (w/o reg) on all data: 0.3416889\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00685583\n",
      "Norm of the params: 2.470367\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.34169. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35300848\n",
      "Train loss (w/o reg) on all data: 0.35264203\n",
      "Test loss (w/o reg) on all data: 0.39837533\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00034967286\n",
      "Norm of the params: 2.707214\n",
      "                Loss: fixed   8 labels. Loss 0.39838. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [168] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42309576\n",
      "Train loss (w/o reg) on all data: 0.42282444\n",
      "Test loss (w/o reg) on all data: 0.34319013\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0019661905\n",
      "Norm of the params: 2.329524\n",
      "              Random: fixed   1 labels. Loss 0.34319. Accuracy 0.844.\n",
      "### Flips: 10, rs: 12, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39708832\n",
      "Train loss (w/o reg) on all data: 0.3967819\n",
      "Test loss (w/o reg) on all data: 0.34150723\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0016588636\n",
      "Norm of the params: 2.47559\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.34151. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35300857\n",
      "Train loss (w/o reg) on all data: 0.3526421\n",
      "Test loss (w/o reg) on all data: 0.39848527\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00028721357\n",
      "Norm of the params: 2.707286\n",
      "                Loss: fixed   8 labels. Loss 0.39849. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [280] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4230948\n",
      "Train loss (w/o reg) on all data: 0.42282328\n",
      "Test loss (w/o reg) on all data: 0.34339505\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0048734955\n",
      "Norm of the params: 2.330298\n",
      "              Random: fixed   1 labels. Loss 0.34340. Accuracy 0.844.\n",
      "### Flips: 10, rs: 12, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [318] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37056038\n",
      "Train loss (w/o reg) on all data: 0.37023783\n",
      "Test loss (w/o reg) on all data: 0.3676706\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00065523083\n",
      "Norm of the params: 2.5399103\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.36767. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [100] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35089245\n",
      "Train loss (w/o reg) on all data: 0.35054675\n",
      "Test loss (w/o reg) on all data: 0.39012077\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017328948\n",
      "Norm of the params: 2.629478\n",
      "                Loss: fixed   9 labels. Loss 0.39012. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [293] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4230949\n",
      "Train loss (w/o reg) on all data: 0.42282394\n",
      "Test loss (w/o reg) on all data: 0.34323555\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0008216465\n",
      "Norm of the params: 2.3278716\n",
      "              Random: fixed   1 labels. Loss 0.34324. Accuracy 0.844.\n",
      "### Flips: 10, rs: 12, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [299] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37055627\n",
      "Train loss (w/o reg) on all data: 0.37023488\n",
      "Test loss (w/o reg) on all data: 0.36696324\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00040118667\n",
      "Norm of the params: 2.5353076\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.36696. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [83] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35089377\n",
      "Train loss (w/o reg) on all data: 0.35054836\n",
      "Test loss (w/o reg) on all data: 0.3901231\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0021160033\n",
      "Norm of the params: 2.6283953\n",
      "                Loss: fixed   9 labels. Loss 0.39012. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [338] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4230952\n",
      "Train loss (w/o reg) on all data: 0.42282465\n",
      "Test loss (w/o reg) on all data: 0.343589\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006204746\n",
      "Norm of the params: 2.3261728\n",
      "              Random: fixed   1 labels. Loss 0.34359. Accuracy 0.844.\n",
      "### Flips: 10, rs: 12, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36207792\n",
      "Train loss (w/o reg) on all data: 0.3617526\n",
      "Test loss (w/o reg) on all data: 0.37725097\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012809295\n",
      "Norm of the params: 2.550793\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.37725. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [76] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35089594\n",
      "Train loss (w/o reg) on all data: 0.35055053\n",
      "Test loss (w/o reg) on all data: 0.3903915\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013652458\n",
      "Norm of the params: 2.62831\n",
      "                Loss: fixed   9 labels. Loss 0.39039. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [293] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42084178\n",
      "Train loss (w/o reg) on all data: 0.4205775\n",
      "Test loss (w/o reg) on all data: 0.36434075\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00027267658\n",
      "Norm of the params: 2.2990334\n",
      "              Random: fixed   3 labels. Loss 0.36434. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [379] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46113288\n",
      "Train loss (w/o reg) on all data: 0.46098712\n",
      "Test loss (w/o reg) on all data: 0.38365212\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.012218491\n",
      "Norm of the params: 1.7073323\n",
      "Flipped loss: 0.38365. Accuracy: 0.844\n",
      "### Flips: 10, rs: 13, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [125] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41168216\n",
      "Train loss (w/o reg) on all data: 0.41147912\n",
      "Test loss (w/o reg) on all data: 0.3694159\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014838927\n",
      "Norm of the params: 2.0151434\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.36942. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [123] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37438515\n",
      "Train loss (w/o reg) on all data: 0.37411654\n",
      "Test loss (w/o reg) on all data: 0.38975266\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004247349\n",
      "Norm of the params: 2.3177845\n",
      "                Loss: fixed   6 labels. Loss 0.38975. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46113095\n",
      "Train loss (w/o reg) on all data: 0.4609853\n",
      "Test loss (w/o reg) on all data: 0.3836403\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0008958548\n",
      "Norm of the params: 1.7067266\n",
      "              Random: fixed   0 labels. Loss 0.38364. Accuracy 0.844.\n",
      "### Flips: 10, rs: 13, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [111] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40476763\n",
      "Train loss (w/o reg) on all data: 0.40456778\n",
      "Test loss (w/o reg) on all data: 0.36430386\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.006233215\n",
      "Norm of the params: 1.999259\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.36430. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3650636\n",
      "Train loss (w/o reg) on all data: 0.3647646\n",
      "Test loss (w/o reg) on all data: 0.40474477\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0034110283\n",
      "Norm of the params: 2.4454775\n",
      "                Loss: fixed   7 labels. Loss 0.40474. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [323] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4611315\n",
      "Train loss (w/o reg) on all data: 0.460986\n",
      "Test loss (w/o reg) on all data: 0.38376296\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.003827334\n",
      "Norm of the params: 1.7059664\n",
      "              Random: fixed   0 labels. Loss 0.38376. Accuracy 0.844.\n",
      "### Flips: 10, rs: 13, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37100106\n",
      "Train loss (w/o reg) on all data: 0.37069917\n",
      "Test loss (w/o reg) on all data: 0.3909557\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00024206616\n",
      "Norm of the params: 2.4572618\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.39096. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [150] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3592159\n",
      "Train loss (w/o reg) on all data: 0.35891908\n",
      "Test loss (w/o reg) on all data: 0.40018693\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001798557\n",
      "Norm of the params: 2.4363782\n",
      "                Loss: fixed   8 labels. Loss 0.40019. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45601472\n",
      "Train loss (w/o reg) on all data: 0.4558641\n",
      "Test loss (w/o reg) on all data: 0.38978034\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001402347\n",
      "Norm of the params: 1.7355753\n",
      "              Random: fixed   1 labels. Loss 0.38978. Accuracy 0.844.\n",
      "### Flips: 10, rs: 13, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [117] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37101236\n",
      "Train loss (w/o reg) on all data: 0.37070876\n",
      "Test loss (w/o reg) on all data: 0.390312\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011288391\n",
      "Norm of the params: 2.4641008\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.39031. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35625792\n",
      "Train loss (w/o reg) on all data: 0.3559596\n",
      "Test loss (w/o reg) on all data: 0.40034387\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0022007257\n",
      "Norm of the params: 2.4426775\n",
      "                Loss: fixed   9 labels. Loss 0.40034. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [193] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43897524\n",
      "Train loss (w/o reg) on all data: 0.4388081\n",
      "Test loss (w/o reg) on all data: 0.37908274\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0028275887\n",
      "Norm of the params: 1.8282647\n",
      "              Random: fixed   2 labels. Loss 0.37908. Accuracy 0.844.\n",
      "### Flips: 10, rs: 13, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3710025\n",
      "Train loss (w/o reg) on all data: 0.37069935\n",
      "Test loss (w/o reg) on all data: 0.39081892\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008031252\n",
      "Norm of the params: 2.462289\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.39082. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [168] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35625756\n",
      "Train loss (w/o reg) on all data: 0.35595924\n",
      "Test loss (w/o reg) on all data: 0.40019536\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0004350696\n",
      "Norm of the params: 2.4426749\n",
      "                Loss: fixed   9 labels. Loss 0.40020. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4299936\n",
      "Train loss (w/o reg) on all data: 0.4298191\n",
      "Test loss (w/o reg) on all data: 0.3875305\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0021067655\n",
      "Norm of the params: 1.8681214\n",
      "              Random: fixed   3 labels. Loss 0.38753. Accuracy 0.844.\n",
      "### Flips: 10, rs: 13, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [198] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37100497\n",
      "Train loss (w/o reg) on all data: 0.37070227\n",
      "Test loss (w/o reg) on all data: 0.3902707\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007545603\n",
      "Norm of the params: 2.4604547\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.39027. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35625955\n",
      "Train loss (w/o reg) on all data: 0.3559612\n",
      "Test loss (w/o reg) on all data: 0.4001309\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006791826\n",
      "Norm of the params: 2.442753\n",
      "                Loss: fixed   9 labels. Loss 0.40013. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [201] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42999324\n",
      "Train loss (w/o reg) on all data: 0.42981836\n",
      "Test loss (w/o reg) on all data: 0.38734576\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0044713863\n",
      "Norm of the params: 1.8701631\n",
      "              Random: fixed   3 labels. Loss 0.38735. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42155614\n",
      "Train loss (w/o reg) on all data: 0.42132202\n",
      "Test loss (w/o reg) on all data: 0.39560702\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0030292275\n",
      "Norm of the params: 2.1639426\n",
      "Flipped loss: 0.39561. Accuracy: 0.800\n",
      "### Flips: 10, rs: 14, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3946962\n",
      "Train loss (w/o reg) on all data: 0.39438376\n",
      "Test loss (w/o reg) on all data: 0.38646236\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0030859197\n",
      "Norm of the params: 2.4997425\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.38646. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [120] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3648437\n",
      "Train loss (w/o reg) on all data: 0.36445603\n",
      "Test loss (w/o reg) on all data: 0.40786144\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0052409857\n",
      "Norm of the params: 2.7845113\n",
      "                Loss: fixed   4 labels. Loss 0.40786. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42155683\n",
      "Train loss (w/o reg) on all data: 0.4213232\n",
      "Test loss (w/o reg) on all data: 0.39552143\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0031152752\n",
      "Norm of the params: 2.1615844\n",
      "              Random: fixed   0 labels. Loss 0.39552. Accuracy 0.800.\n",
      "### Flips: 10, rs: 14, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [166] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3909837\n",
      "Train loss (w/o reg) on all data: 0.3906687\n",
      "Test loss (w/o reg) on all data: 0.3974705\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0006954664\n",
      "Norm of the params: 2.5099638\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.39747. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36483943\n",
      "Train loss (w/o reg) on all data: 0.36445203\n",
      "Test loss (w/o reg) on all data: 0.40789965\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010489842\n",
      "Norm of the params: 2.7834735\n",
      "                Loss: fixed   4 labels. Loss 0.40790. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [166] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42155713\n",
      "Train loss (w/o reg) on all data: 0.42132372\n",
      "Test loss (w/o reg) on all data: 0.3955349\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005247345\n",
      "Norm of the params: 2.160608\n",
      "              Random: fixed   0 labels. Loss 0.39553. Accuracy 0.800.\n",
      "### Flips: 10, rs: 14, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36072886\n",
      "Train loss (w/o reg) on all data: 0.36035746\n",
      "Test loss (w/o reg) on all data: 0.4241282\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00055106904\n",
      "Norm of the params: 2.7254555\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.42413. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [125] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36124063\n",
      "Train loss (w/o reg) on all data: 0.3608489\n",
      "Test loss (w/o reg) on all data: 0.42069644\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001736961\n",
      "Norm of the params: 2.7989657\n",
      "                Loss: fixed   5 labels. Loss 0.42070. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [172] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4240647\n",
      "Train loss (w/o reg) on all data: 0.42386794\n",
      "Test loss (w/o reg) on all data: 0.3910627\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014051412\n",
      "Norm of the params: 1.983643\n",
      "              Random: fixed   1 labels. Loss 0.39106. Accuracy 0.822.\n",
      "### Flips: 10, rs: 14, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [371] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3607287\n",
      "Train loss (w/o reg) on all data: 0.36035758\n",
      "Test loss (w/o reg) on all data: 0.4241133\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00076473824\n",
      "Norm of the params: 2.7244365\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.42411. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [158] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3576114\n",
      "Train loss (w/o reg) on all data: 0.3571986\n",
      "Test loss (w/o reg) on all data: 0.40818518\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0019526983\n",
      "Norm of the params: 2.873317\n",
      "                Loss: fixed   7 labels. Loss 0.40819. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42406034\n",
      "Train loss (w/o reg) on all data: 0.4238623\n",
      "Test loss (w/o reg) on all data: 0.39132532\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006647234\n",
      "Norm of the params: 1.9902077\n",
      "              Random: fixed   1 labels. Loss 0.39133. Accuracy 0.822.\n",
      "### Flips: 10, rs: 14, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [286] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3586993\n",
      "Train loss (w/o reg) on all data: 0.35834864\n",
      "Test loss (w/o reg) on all data: 0.41026604\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00083959784\n",
      "Norm of the params: 2.6482234\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41027. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35839817\n",
      "Train loss (w/o reg) on all data: 0.358017\n",
      "Test loss (w/o reg) on all data: 0.4059219\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0027559611\n",
      "Norm of the params: 2.7610466\n",
      "                Loss: fixed   8 labels. Loss 0.40592. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [149] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss (w reg) on all data: 0.40785405\n",
      "Train loss (w/o reg) on all data: 0.40763384\n",
      "Test loss (w/o reg) on all data: 0.39942643\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016839139\n",
      "Norm of the params: 2.0985954\n",
      "              Random: fixed   3 labels. Loss 0.39943. Accuracy 0.822.\n",
      "### Flips: 10, rs: 14, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [269] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35923257\n",
      "Train loss (w/o reg) on all data: 0.35891291\n",
      "Test loss (w/o reg) on all data: 0.40621746\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00054656924\n",
      "Norm of the params: 2.5284493\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.40622. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [149] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35778916\n",
      "Train loss (w/o reg) on all data: 0.35742843\n",
      "Test loss (w/o reg) on all data: 0.4100775\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0037512705\n",
      "Norm of the params: 2.6859894\n",
      "                Loss: fixed   9 labels. Loss 0.41008. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40785232\n",
      "Train loss (w/o reg) on all data: 0.40763173\n",
      "Test loss (w/o reg) on all data: 0.3989889\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00036791366\n",
      "Norm of the params: 2.1004798\n",
      "              Random: fixed   3 labels. Loss 0.39899. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [175] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41025054\n",
      "Train loss (w/o reg) on all data: 0.40998334\n",
      "Test loss (w/o reg) on all data: 0.36615482\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009466199\n",
      "Norm of the params: 2.311797\n",
      "Flipped loss: 0.36615. Accuracy: 0.800\n",
      "### Flips: 10, rs: 15, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [52] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3972729\n",
      "Train loss (w/o reg) on all data: 0.3970011\n",
      "Test loss (w/o reg) on all data: 0.37254316\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00096311566\n",
      "Norm of the params: 2.3316863\n",
      "     Influence (LOO): fixed   1 labels. Loss 0.37254. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36561692\n",
      "Train loss (w/o reg) on all data: 0.36528727\n",
      "Test loss (w/o reg) on all data: 0.39632094\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0028984046\n",
      "Norm of the params: 2.567712\n",
      "                Loss: fixed   4 labels. Loss 0.39632. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [144] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41024932\n",
      "Train loss (w/o reg) on all data: 0.40998366\n",
      "Test loss (w/o reg) on all data: 0.36604407\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011248434\n",
      "Norm of the params: 2.305031\n",
      "              Random: fixed   0 labels. Loss 0.36604. Accuracy 0.800.\n",
      "### Flips: 10, rs: 15, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [85] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3856528\n",
      "Train loss (w/o reg) on all data: 0.38534507\n",
      "Test loss (w/o reg) on all data: 0.3679107\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0003996742\n",
      "Norm of the params: 2.4809186\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.36791. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [110] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3607008\n",
      "Train loss (w/o reg) on all data: 0.36035487\n",
      "Test loss (w/o reg) on all data: 0.40262565\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002688015\n",
      "Norm of the params: 2.6302907\n",
      "                Loss: fixed   5 labels. Loss 0.40263. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.405314\n",
      "Train loss (w/o reg) on all data: 0.40503362\n",
      "Test loss (w/o reg) on all data: 0.37185276\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022256419\n",
      "Norm of the params: 2.367992\n",
      "              Random: fixed   1 labels. Loss 0.37185. Accuracy 0.800.\n",
      "### Flips: 10, rs: 15, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37182224\n",
      "Train loss (w/o reg) on all data: 0.37151006\n",
      "Test loss (w/o reg) on all data: 0.38910595\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005153397\n",
      "Norm of the params: 2.4986808\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.38911. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [68] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35627735\n",
      "Train loss (w/o reg) on all data: 0.35594177\n",
      "Test loss (w/o reg) on all data: 0.4064532\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0021380393\n",
      "Norm of the params: 2.5906885\n",
      "                Loss: fixed   6 labels. Loss 0.40645. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [120] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40121004\n",
      "Train loss (w/o reg) on all data: 0.400936\n",
      "Test loss (w/o reg) on all data: 0.37539384\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010397905\n",
      "Norm of the params: 2.341138\n",
      "              Random: fixed   2 labels. Loss 0.37539. Accuracy 0.800.\n",
      "### Flips: 10, rs: 15, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [137] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35626826\n",
      "Train loss (w/o reg) on all data: 0.3559298\n",
      "Test loss (w/o reg) on all data: 0.4070071\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012205329\n",
      "Norm of the params: 2.6017575\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.40701. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35137627\n",
      "Train loss (w/o reg) on all data: 0.35104445\n",
      "Test loss (w/o reg) on all data: 0.4094679\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009079134\n",
      "Norm of the params: 2.5761135\n",
      "                Loss: fixed   8 labels. Loss 0.40947. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [348] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40461537\n",
      "Train loss (w/o reg) on all data: 0.40433583\n",
      "Test loss (w/o reg) on all data: 0.3700368\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00041749474\n",
      "Norm of the params: 2.364538\n",
      "              Random: fixed   3 labels. Loss 0.37004. Accuracy 0.800.\n",
      "### Flips: 10, rs: 15, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [179] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35137534\n",
      "Train loss (w/o reg) on all data: 0.35104233\n",
      "Test loss (w/o reg) on all data: 0.40991104\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010717222\n",
      "Norm of the params: 2.5807617\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.40991. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [163] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35238737\n",
      "Train loss (w/o reg) on all data: 0.35206848\n",
      "Test loss (w/o reg) on all data: 0.41603744\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010769981\n",
      "Norm of the params: 2.5253916\n",
      "                Loss: fixed   9 labels. Loss 0.41604. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [202] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40461683\n",
      "Train loss (w/o reg) on all data: 0.40433675\n",
      "Test loss (w/o reg) on all data: 0.369894\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0003859632\n",
      "Norm of the params: 2.3668354\n",
      "              Random: fixed   3 labels. Loss 0.36989. Accuracy 0.800.\n",
      "### Flips: 10, rs: 15, checks: 60\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [177] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35239008\n",
      "Train loss (w/o reg) on all data: 0.35207224\n",
      "Test loss (w/o reg) on all data: 0.41625887\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00045512585\n",
      "Norm of the params: 2.5212595\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.41626. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [36] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35238755\n",
      "Train loss (w/o reg) on all data: 0.35206923\n",
      "Test loss (w/o reg) on all data: 0.41586676\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018860854\n",
      "Norm of the params: 2.5231535\n",
      "                Loss: fixed   9 labels. Loss 0.41587. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [163] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.406416\n",
      "Train loss (w/o reg) on all data: 0.4061479\n",
      "Test loss (w/o reg) on all data: 0.37558362\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003235338\n",
      "Norm of the params: 2.315658\n",
      "              Random: fixed   4 labels. Loss 0.37558. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44512424\n",
      "Train loss (w/o reg) on all data: 0.44495636\n",
      "Test loss (w/o reg) on all data: 0.41847664\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00042957935\n",
      "Norm of the params: 1.832282\n",
      "Flipped loss: 0.41848. Accuracy: 0.800\n",
      "### Flips: 10, rs: 16, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35603678\n",
      "Train loss (w/o reg) on all data: 0.3556422\n",
      "Test loss (w/o reg) on all data: 0.40031877\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0030963663\n",
      "Norm of the params: 2.8092208\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.40032. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3776461\n",
      "Train loss (w/o reg) on all data: 0.37733215\n",
      "Test loss (w/o reg) on all data: 0.40585807\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00058828515\n",
      "Norm of the params: 2.5057654\n",
      "                Loss: fixed   4 labels. Loss 0.40586. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [157] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4451246\n",
      "Train loss (w/o reg) on all data: 0.4449578\n",
      "Test loss (w/o reg) on all data: 0.41761398\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0038314895\n",
      "Norm of the params: 1.8264377\n",
      "              Random: fixed   0 labels. Loss 0.41761. Accuracy 0.800.\n",
      "### Flips: 10, rs: 16, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3560316\n",
      "Train loss (w/o reg) on all data: 0.35563523\n",
      "Test loss (w/o reg) on all data: 0.40114993\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006675243\n",
      "Norm of the params: 2.8155243\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.40115. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [152] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36029285\n",
      "Train loss (w/o reg) on all data: 0.3599732\n",
      "Test loss (w/o reg) on all data: 0.41456774\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.008042801\n",
      "Norm of the params: 2.5284643\n",
      "                Loss: fixed   6 labels. Loss 0.41457. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4451225\n",
      "Train loss (w/o reg) on all data: 0.44495466\n",
      "Test loss (w/o reg) on all data: 0.41811922\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005026347\n",
      "Norm of the params: 1.8322245\n",
      "              Random: fixed   0 labels. Loss 0.41812. Accuracy 0.800.\n",
      "### Flips: 10, rs: 16, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [335] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35603198\n",
      "Train loss (w/o reg) on all data: 0.35563448\n",
      "Test loss (w/o reg) on all data: 0.4013486\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0021345564\n",
      "Norm of the params: 2.819564\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.40135. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [144] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34624237\n",
      "Train loss (w/o reg) on all data: 0.3458611\n",
      "Test loss (w/o reg) on all data: 0.41219965\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00072148343\n",
      "Norm of the params: 2.7614095\n",
      "                Loss: fixed   8 labels. Loss 0.41220. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [305] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44512334\n",
      "Train loss (w/o reg) on all data: 0.44495514\n",
      "Test loss (w/o reg) on all data: 0.41801575\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0025589445\n",
      "Norm of the params: 1.8340915\n",
      "              Random: fixed   0 labels. Loss 0.41802. Accuracy 0.800.\n",
      "### Flips: 10, rs: 16, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34966716\n",
      "Train loss (w/o reg) on all data: 0.34927997\n",
      "Test loss (w/o reg) on all data: 0.40503955\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007364437\n",
      "Norm of the params: 2.7827992\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.40504. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [83] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3462404\n",
      "Train loss (w/o reg) on all data: 0.3458588\n",
      "Test loss (w/o reg) on all data: 0.41252294\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.007015083\n",
      "Norm of the params: 2.7626088\n",
      "                Loss: fixed   8 labels. Loss 0.41252. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44513232\n",
      "Train loss (w/o reg) on all data: 0.4449649\n",
      "Test loss (w/o reg) on all data: 0.4185321\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.01182838\n",
      "Norm of the params: 1.8298769\n",
      "              Random: fixed   0 labels. Loss 0.41853. Accuracy 0.800.\n",
      "### Flips: 10, rs: 16, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3496663\n",
      "Train loss (w/o reg) on all data: 0.34928137\n",
      "Test loss (w/o reg) on all data: 0.40513542\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006015254\n",
      "Norm of the params: 2.7746453\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.40514. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [51] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3462474\n",
      "Train loss (w/o reg) on all data: 0.34586647\n",
      "Test loss (w/o reg) on all data: 0.41245002\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019868976\n",
      "Norm of the params: 2.7601812\n",
      "                Loss: fixed   8 labels. Loss 0.41245. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4151811\n",
      "Train loss (w/o reg) on all data: 0.41493487\n",
      "Test loss (w/o reg) on all data: 0.41322738\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018131372\n",
      "Norm of the params: 2.219149\n",
      "              Random: fixed   2 labels. Loss 0.41323. Accuracy 0.800.\n",
      "### Flips: 10, rs: 16, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [325] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34623995\n",
      "Train loss (w/o reg) on all data: 0.3458604\n",
      "Test loss (w/o reg) on all data: 0.41268367\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022741314\n",
      "Norm of the params: 2.7551925\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41268. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [53] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3462855\n",
      "Train loss (w/o reg) on all data: 0.34591964\n",
      "Test loss (w/o reg) on all data: 0.41499478\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0003913766\n",
      "Norm of the params: 2.7049713\n",
      "                Loss: fixed   9 labels. Loss 0.41499. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38534918\n",
      "Train loss (w/o reg) on all data: 0.3850684\n",
      "Test loss (w/o reg) on all data: 0.40926573\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001013187\n",
      "Norm of the params: 2.3698606\n",
      "              Random: fixed   4 labels. Loss 0.40927. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40436167\n",
      "Train loss (w/o reg) on all data: 0.40407163\n",
      "Test loss (w/o reg) on all data: 0.40645024\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00072255195\n",
      "Norm of the params: 2.4085119\n",
      "Flipped loss: 0.40645. Accuracy: 0.800\n",
      "### Flips: 10, rs: 17, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [179] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3675205\n",
      "Train loss (w/o reg) on all data: 0.36711228\n",
      "Test loss (w/o reg) on all data: 0.38331553\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006864204\n",
      "Norm of the params: 2.8574154\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.38332. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [83] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35726875\n",
      "Train loss (w/o reg) on all data: 0.35683665\n",
      "Test loss (w/o reg) on all data: 0.41114098\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005338495\n",
      "Norm of the params: 2.9397006\n",
      "                Loss: fixed   3 labels. Loss 0.41114. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4040296\n",
      "Train loss (w/o reg) on all data: 0.4037513\n",
      "Test loss (w/o reg) on all data: 0.4141336\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0019006765\n",
      "Norm of the params: 2.3591843\n",
      "              Random: fixed   1 labels. Loss 0.41413. Accuracy 0.756.\n",
      "### Flips: 10, rs: 17, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35872054\n",
      "Train loss (w/o reg) on all data: 0.35828766\n",
      "Test loss (w/o reg) on all data: 0.37790045\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001139303\n",
      "Norm of the params: 2.9423428\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.37790. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [81] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34732047\n",
      "Train loss (w/o reg) on all data: 0.34685922\n",
      "Test loss (w/o reg) on all data: 0.40532205\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010901254\n",
      "Norm of the params: 3.0372958\n",
      "                Loss: fixed   4 labels. Loss 0.40532. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [363] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4032388\n",
      "Train loss (w/o reg) on all data: 0.40297064\n",
      "Test loss (w/o reg) on all data: 0.4180781\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0002360555\n",
      "Norm of the params: 2.3158395\n",
      "              Random: fixed   2 labels. Loss 0.41808. Accuracy 0.756.\n",
      "### Flips: 10, rs: 17, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [314] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34412953\n",
      "Train loss (w/o reg) on all data: 0.34366405\n",
      "Test loss (w/o reg) on all data: 0.3885497\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003400654\n",
      "Norm of the params: 3.0512056\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.38855. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3441296\n",
      "Train loss (w/o reg) on all data: 0.3436641\n",
      "Test loss (w/o reg) on all data: 0.38854256\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005564025\n",
      "Norm of the params: 3.0512056\n",
      "                Loss: fixed   5 labels. Loss 0.38854. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4032399\n",
      "Train loss (w/o reg) on all data: 0.4029716\n",
      "Test loss (w/o reg) on all data: 0.4182214\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0006696532\n",
      "Norm of the params: 2.3165367\n",
      "              Random: fixed   2 labels. Loss 0.41822. Accuracy 0.756.\n",
      "### Flips: 10, rs: 17, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [289] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3472246\n",
      "Train loss (w/o reg) on all data: 0.34682846\n",
      "Test loss (w/o reg) on all data: 0.38805297\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00050261\n",
      "Norm of the params: 2.8146906\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.38805. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [169] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3454663\n",
      "Train loss (w/o reg) on all data: 0.34502935\n",
      "Test loss (w/o reg) on all data: 0.39258182\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013870574\n",
      "Norm of the params: 2.9561398\n",
      "                Loss: fixed   6 labels. Loss 0.39258. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40348747\n",
      "Train loss (w/o reg) on all data: 0.40324694\n",
      "Test loss (w/o reg) on all data: 0.41459844\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0036368454\n",
      "Norm of the params: 2.1932642\n",
      "              Random: fixed   3 labels. Loss 0.41460. Accuracy 0.756.\n",
      "### Flips: 10, rs: 17, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34722477\n",
      "Train loss (w/o reg) on all data: 0.34682825\n",
      "Test loss (w/o reg) on all data: 0.38832214\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00062966044\n",
      "Norm of the params: 2.8161273\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.38832. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [153] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3464147\n",
      "Train loss (w/o reg) on all data: 0.3459939\n",
      "Test loss (w/o reg) on all data: 0.39628422\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0066381204\n",
      "Norm of the params: 2.9010098\n",
      "                Loss: fixed   7 labels. Loss 0.39628. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4034881\n",
      "Train loss (w/o reg) on all data: 0.4032479\n",
      "Test loss (w/o reg) on all data: 0.41469797\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00061228353\n",
      "Norm of the params: 2.1918406\n",
      "              Random: fixed   3 labels. Loss 0.41470. Accuracy 0.756.\n",
      "### Flips: 10, rs: 17, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3472248\n",
      "Train loss (w/o reg) on all data: 0.3468274\n",
      "Test loss (w/o reg) on all data: 0.38812807\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005062265\n",
      "Norm of the params: 2.8192527\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.38813. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [123] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3479741\n",
      "Train loss (w/o reg) on all data: 0.3475978\n",
      "Test loss (w/o reg) on all data: 0.3927542\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0024542739\n",
      "Norm of the params: 2.7433434\n",
      "                Loss: fixed   8 labels. Loss 0.39275. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40348724\n",
      "Train loss (w/o reg) on all data: 0.40324664\n",
      "Test loss (w/o reg) on all data: 0.41460377\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0008978547\n",
      "Norm of the params: 2.19365\n",
      "              Random: fixed   3 labels. Loss 0.41460. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [190] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss (w reg) on all data: 0.4306475\n",
      "Train loss (w/o reg) on all data: 0.4304308\n",
      "Test loss (w/o reg) on all data: 0.419251\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0012072804\n",
      "Norm of the params: 2.0817242\n",
      "Flipped loss: 0.41925. Accuracy: 0.756\n",
      "### Flips: 10, rs: 18, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [82] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3735673\n",
      "Train loss (w/o reg) on all data: 0.37326032\n",
      "Test loss (w/o reg) on all data: 0.3895373\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.009034279\n",
      "Norm of the params: 2.477826\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.38954. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [187] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38211063\n",
      "Train loss (w/o reg) on all data: 0.3817928\n",
      "Test loss (w/o reg) on all data: 0.42512414\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0050441986\n",
      "Norm of the params: 2.521163\n",
      "                Loss: fixed   4 labels. Loss 0.42512. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43064708\n",
      "Train loss (w/o reg) on all data: 0.43043178\n",
      "Test loss (w/o reg) on all data: 0.41860613\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.005943541\n",
      "Norm of the params: 2.0750873\n",
      "              Random: fixed   0 labels. Loss 0.41861. Accuracy 0.756.\n",
      "### Flips: 10, rs: 18, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [226] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3653528\n",
      "Train loss (w/o reg) on all data: 0.36501047\n",
      "Test loss (w/o reg) on all data: 0.40628654\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003210504\n",
      "Norm of the params: 2.6166565\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.40629. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34817165\n",
      "Train loss (w/o reg) on all data: 0.34783265\n",
      "Test loss (w/o reg) on all data: 0.42449638\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009079328\n",
      "Norm of the params: 2.6038635\n",
      "                Loss: fixed   8 labels. Loss 0.42450. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [187] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43011886\n",
      "Train loss (w/o reg) on all data: 0.42989507\n",
      "Test loss (w/o reg) on all data: 0.41288114\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0006449392\n",
      "Norm of the params: 2.1156347\n",
      "              Random: fixed   1 labels. Loss 0.41288. Accuracy 0.778.\n",
      "### Flips: 10, rs: 18, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [283] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3653527\n",
      "Train loss (w/o reg) on all data: 0.36501038\n",
      "Test loss (w/o reg) on all data: 0.40630573\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019434929\n",
      "Norm of the params: 2.6165133\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.40631. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [150] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34816754\n",
      "Train loss (w/o reg) on all data: 0.3478307\n",
      "Test loss (w/o reg) on all data: 0.42485318\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00303708\n",
      "Norm of the params: 2.5955055\n",
      "                Loss: fixed   8 labels. Loss 0.42485. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [175] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4219788\n",
      "Train loss (w/o reg) on all data: 0.42173094\n",
      "Test loss (w/o reg) on all data: 0.42538893\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00401791\n",
      "Norm of the params: 2.2265685\n",
      "              Random: fixed   2 labels. Loss 0.42539. Accuracy 0.756.\n",
      "### Flips: 10, rs: 18, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35871768\n",
      "Train loss (w/o reg) on all data: 0.3583905\n",
      "Test loss (w/o reg) on all data: 0.4099158\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00070578326\n",
      "Norm of the params: 2.5580206\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.40992. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [102] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34816906\n",
      "Train loss (w/o reg) on all data: 0.34783158\n",
      "Test loss (w/o reg) on all data: 0.42452884\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.000971229\n",
      "Norm of the params: 2.5979905\n",
      "                Loss: fixed   8 labels. Loss 0.42453. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [143] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40438005\n",
      "Train loss (w/o reg) on all data: 0.40410808\n",
      "Test loss (w/o reg) on all data: 0.40302405\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016302082\n",
      "Norm of the params: 2.3322475\n",
      "              Random: fixed   4 labels. Loss 0.40302. Accuracy 0.800.\n",
      "### Flips: 10, rs: 18, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [144] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3481802\n",
      "Train loss (w/o reg) on all data: 0.34784353\n",
      "Test loss (w/o reg) on all data: 0.42421272\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0032863943\n",
      "Norm of the params: 2.594949\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.42421. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [54] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34963688\n",
      "Train loss (w/o reg) on all data: 0.3492934\n",
      "Test loss (w/o reg) on all data: 0.41833055\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002795859\n",
      "Norm of the params: 2.6209805\n",
      "                Loss: fixed   9 labels. Loss 0.41833. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39608973\n",
      "Train loss (w/o reg) on all data: 0.3958185\n",
      "Test loss (w/o reg) on all data: 0.41394705\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017920318\n",
      "Norm of the params: 2.329062\n",
      "              Random: fixed   5 labels. Loss 0.41395. Accuracy 0.800.\n",
      "### Flips: 10, rs: 18, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34817004\n",
      "Train loss (w/o reg) on all data: 0.34783357\n",
      "Test loss (w/o reg) on all data: 0.4249618\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013996317\n",
      "Norm of the params: 2.5941586\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.42496. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [79] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34960347\n",
      "Train loss (w/o reg) on all data: 0.34925964\n",
      "Test loss (w/o reg) on all data: 0.4186039\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002182223\n",
      "Norm of the params: 2.6223068\n",
      "                Loss: fixed   9 labels. Loss 0.41860. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [222] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39608315\n",
      "Train loss (w/o reg) on all data: 0.39581162\n",
      "Test loss (w/o reg) on all data: 0.41369155\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00654427\n",
      "Norm of the params: 2.3303232\n",
      "              Random: fixed   5 labels. Loss 0.41369. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [100] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44819152\n",
      "Train loss (w/o reg) on all data: 0.4480237\n",
      "Test loss (w/o reg) on all data: 0.37741247\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0008540236\n",
      "Norm of the params: 1.8320361\n",
      "Flipped loss: 0.37741. Accuracy: 0.844\n",
      "### Flips: 10, rs: 19, checks: 10\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [78] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42300132\n",
      "Train loss (w/o reg) on all data: 0.42279944\n",
      "Test loss (w/o reg) on all data: 0.37293455\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00422665\n",
      "Norm of the params: 2.0094411\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.37293. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38505083\n",
      "Train loss (w/o reg) on all data: 0.38477656\n",
      "Test loss (w/o reg) on all data: 0.39378536\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006441713\n",
      "Norm of the params: 2.3421166\n",
      "                Loss: fixed   4 labels. Loss 0.39379. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [140] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4482042\n",
      "Train loss (w/o reg) on all data: 0.44803652\n",
      "Test loss (w/o reg) on all data: 0.37877342\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004934385\n",
      "Norm of the params: 1.8311969\n",
      "              Random: fixed   0 labels. Loss 0.37877. Accuracy 0.844.\n",
      "### Flips: 10, rs: 19, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3972353\n",
      "Train loss (w/o reg) on all data: 0.39697862\n",
      "Test loss (w/o reg) on all data: 0.38229623\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005564796\n",
      "Norm of the params: 2.2658296\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.38230. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [129] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3719816\n",
      "Train loss (w/o reg) on all data: 0.37168255\n",
      "Test loss (w/o reg) on all data: 0.3993941\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.000724259\n",
      "Norm of the params: 2.4455235\n",
      "                Loss: fixed   6 labels. Loss 0.39939. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [303] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44818893\n",
      "Train loss (w/o reg) on all data: 0.44801936\n",
      "Test loss (w/o reg) on all data: 0.37832823\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00037211378\n",
      "Norm of the params: 1.8416381\n",
      "              Random: fixed   0 labels. Loss 0.37833. Accuracy 0.844.\n",
      "### Flips: 10, rs: 19, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39282835\n",
      "Train loss (w/o reg) on all data: 0.39257935\n",
      "Test loss (w/o reg) on all data: 0.38402855\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014432753\n",
      "Norm of the params: 2.2316275\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.38403. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [153] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35721406\n",
      "Train loss (w/o reg) on all data: 0.3569052\n",
      "Test loss (w/o reg) on all data: 0.4086304\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00073942385\n",
      "Norm of the params: 2.4853842\n",
      "                Loss: fixed   9 labels. Loss 0.40863. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [130] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4481958\n",
      "Train loss (w/o reg) on all data: 0.4480255\n",
      "Test loss (w/o reg) on all data: 0.3781406\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0025075788\n",
      "Norm of the params: 1.8455037\n",
      "              Random: fixed   0 labels. Loss 0.37814. Accuracy 0.844.\n",
      "### Flips: 10, rs: 19, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [105] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3572157\n",
      "Train loss (w/o reg) on all data: 0.35690832\n",
      "Test loss (w/o reg) on all data: 0.4082256\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022509752\n",
      "Norm of the params: 2.4794598\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.40823. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3572157\n",
      "Train loss (w/o reg) on all data: 0.35690832\n",
      "Test loss (w/o reg) on all data: 0.40823096\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006207212\n",
      "Norm of the params: 2.4794598\n",
      "                Loss: fixed   9 labels. Loss 0.40823. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [135] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4305189\n",
      "Train loss (w/o reg) on all data: 0.4303204\n",
      "Test loss (w/o reg) on all data: 0.37607273\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0038167736\n",
      "Norm of the params: 1.9924588\n",
      "              Random: fixed   1 labels. Loss 0.37607. Accuracy 0.844.\n",
      "### Flips: 10, rs: 19, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [258] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35721233\n",
      "Train loss (w/o reg) on all data: 0.3569048\n",
      "Test loss (w/o reg) on all data: 0.40830475\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004734393\n",
      "Norm of the params: 2.480001\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.40830. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [11] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35721207\n",
      "Train loss (w/o reg) on all data: 0.3569043\n",
      "Test loss (w/o reg) on all data: 0.40808746\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013910212\n",
      "Norm of the params: 2.4809725\n",
      "                Loss: fixed   9 labels. Loss 0.40809. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [143] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43051934\n",
      "Train loss (w/o reg) on all data: 0.43032056\n",
      "Test loss (w/o reg) on all data: 0.3760462\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010651879\n",
      "Norm of the params: 1.993905\n",
      "              Random: fixed   1 labels. Loss 0.37605. Accuracy 0.844.\n",
      "### Flips: 10, rs: 19, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35721326\n",
      "Train loss (w/o reg) on all data: 0.35690534\n",
      "Test loss (w/o reg) on all data: 0.40842095\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011850189\n",
      "Norm of the params: 2.4816535\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.40842. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [195] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35923088\n",
      "Train loss (w/o reg) on all data: 0.3589109\n",
      "Test loss (w/o reg) on all data: 0.40702748\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00041606976\n",
      "Norm of the params: 2.529755\n",
      "                Loss: fixed  10 labels. Loss 0.40703. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42594478\n",
      "Train loss (w/o reg) on all data: 0.42575604\n",
      "Test loss (w/o reg) on all data: 0.3781438\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0024266054\n",
      "Norm of the params: 1.9428843\n",
      "              Random: fixed   2 labels. Loss 0.37814. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [280] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44539145\n",
      "Train loss (w/o reg) on all data: 0.44521743\n",
      "Test loss (w/o reg) on all data: 0.36789474\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0027014744\n",
      "Norm of the params: 1.8655617\n",
      "Flipped loss: 0.36789. Accuracy: 0.800\n",
      "### Flips: 10, rs: 20, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [128] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41055134\n",
      "Train loss (w/o reg) on all data: 0.4103178\n",
      "Test loss (w/o reg) on all data: 0.363109\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001123718\n",
      "Norm of the params: 2.1611526\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.36311. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38605526\n",
      "Train loss (w/o reg) on all data: 0.3857731\n",
      "Test loss (w/o reg) on all data: 0.3776215\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005758772\n",
      "Norm of the params: 2.3755746\n",
      "                Loss: fixed   4 labels. Loss 0.37762. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [318] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44538906\n",
      "Train loss (w/o reg) on all data: 0.4452149\n",
      "Test loss (w/o reg) on all data: 0.3673349\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00057145493\n",
      "Norm of the params: 1.8663344\n",
      "              Random: fixed   0 labels. Loss 0.36733. Accuracy 0.800.\n",
      "### Flips: 10, rs: 20, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.408223\n",
      "Train loss (w/o reg) on all data: 0.40797043\n",
      "Test loss (w/o reg) on all data: 0.3698153\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017499172\n",
      "Norm of the params: 2.2475383\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.36982. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [151] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38019276\n",
      "Train loss (w/o reg) on all data: 0.37992004\n",
      "Test loss (w/o reg) on all data: 0.3821636\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0067295986\n",
      "Norm of the params: 2.335492\n",
      "                Loss: fixed   5 labels. Loss 0.38216. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [149] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.431052\n",
      "Train loss (w/o reg) on all data: 0.43086457\n",
      "Test loss (w/o reg) on all data: 0.37366867\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010550949\n",
      "Norm of the params: 1.936131\n",
      "              Random: fixed   1 labels. Loss 0.37367. Accuracy 0.800.\n",
      "### Flips: 10, rs: 20, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40458784\n",
      "Train loss (w/o reg) on all data: 0.40433764\n",
      "Test loss (w/o reg) on all data: 0.36042553\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004609668\n",
      "Norm of the params: 2.23689\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.36043. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36443388\n",
      "Train loss (w/o reg) on all data: 0.3641423\n",
      "Test loss (w/o reg) on all data: 0.3957714\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0066248486\n",
      "Norm of the params: 2.4149425\n",
      "                Loss: fixed   8 labels. Loss 0.39577. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [288] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43104735\n",
      "Train loss (w/o reg) on all data: 0.43085894\n",
      "Test loss (w/o reg) on all data: 0.37336057\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016785376\n",
      "Norm of the params: 1.9411179\n",
      "              Random: fixed   1 labels. Loss 0.37336. Accuracy 0.800.\n",
      "### Flips: 10, rs: 20, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37335807\n",
      "Train loss (w/o reg) on all data: 0.37302992\n",
      "Test loss (w/o reg) on all data: 0.3903081\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011314652\n",
      "Norm of the params: 2.5618558\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.39031. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [132] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36086586\n",
      "Train loss (w/o reg) on all data: 0.36054677\n",
      "Test loss (w/o reg) on all data: 0.4041519\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003623299\n",
      "Norm of the params: 2.5262477\n",
      "                Loss: fixed   9 labels. Loss 0.40415. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [284] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43104747\n",
      "Train loss (w/o reg) on all data: 0.43085948\n",
      "Test loss (w/o reg) on all data: 0.37333816\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00077251357\n",
      "Norm of the params: 1.9389981\n",
      "              Random: fixed   1 labels. Loss 0.37334. Accuracy 0.800.\n",
      "### Flips: 10, rs: 20, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [133] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36761528\n",
      "Train loss (w/o reg) on all data: 0.36730042\n",
      "Test loss (w/o reg) on all data: 0.39436036\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012153474\n",
      "Norm of the params: 2.5094278\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.39436. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [163] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35923043\n",
      "Train loss (w/o reg) on all data: 0.35891065\n",
      "Test loss (w/o reg) on all data: 0.4066049\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0003393443\n",
      "Norm of the params: 2.5289147\n",
      "                Loss: fixed  10 labels. Loss 0.40660. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43044123\n",
      "Train loss (w/o reg) on all data: 0.430257\n",
      "Test loss (w/o reg) on all data: 0.3749227\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0026739682\n",
      "Norm of the params: 1.9196143\n",
      "              Random: fixed   2 labels. Loss 0.37492. Accuracy 0.800.\n",
      "### Flips: 10, rs: 20, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3661575\n",
      "Train loss (w/o reg) on all data: 0.36584467\n",
      "Test loss (w/o reg) on all data: 0.3956121\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.008035973\n",
      "Norm of the params: 2.501396\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.39561. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [99] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3592318\n",
      "Train loss (w/o reg) on all data: 0.3589136\n",
      "Test loss (w/o reg) on all data: 0.40628693\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00036155852\n",
      "Norm of the params: 2.5226743\n",
      "                Loss: fixed  10 labels. Loss 0.40629. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [150] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43044245\n",
      "Train loss (w/o reg) on all data: 0.43025836\n",
      "Test loss (w/o reg) on all data: 0.37511393\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015252024\n",
      "Norm of the params: 1.9187386\n",
      "              Random: fixed   2 labels. Loss 0.37511. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44151\n",
      "Train loss (w/o reg) on all data: 0.44131622\n",
      "Test loss (w/o reg) on all data: 0.41495165\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.009795405\n",
      "Norm of the params: 1.9685582\n",
      "Flipped loss: 0.41495. Accuracy: 0.800\n",
      "### Flips: 10, rs: 21, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [84] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40982533\n",
      "Train loss (w/o reg) on all data: 0.40960103\n",
      "Test loss (w/o reg) on all data: 0.3934227\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003980268\n",
      "Norm of the params: 2.11794\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.39342. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [145] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37266377\n",
      "Train loss (w/o reg) on all data: 0.372385\n",
      "Test loss (w/o reg) on all data: 0.3958206\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0015713443\n",
      "Norm of the params: 2.3612912\n",
      "                Loss: fixed   5 labels. Loss 0.39582. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44150817\n",
      "Train loss (w/o reg) on all data: 0.4413131\n",
      "Test loss (w/o reg) on all data: 0.41489202\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00056433\n",
      "Norm of the params: 1.9753344\n",
      "              Random: fixed   0 labels. Loss 0.41489. Accuracy 0.800.\n",
      "### Flips: 10, rs: 21, checks: 20\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38653067\n",
      "Train loss (w/o reg) on all data: 0.38626206\n",
      "Test loss (w/o reg) on all data: 0.40936208\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010655501\n",
      "Norm of the params: 2.3177383\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.40936. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [161] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36225057\n",
      "Train loss (w/o reg) on all data: 0.36192784\n",
      "Test loss (w/o reg) on all data: 0.39133486\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0004756538\n",
      "Norm of the params: 2.5406315\n",
      "                Loss: fixed   6 labels. Loss 0.39133. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [285] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.442012\n",
      "Train loss (w/o reg) on all data: 0.44181716\n",
      "Test loss (w/o reg) on all data: 0.42803717\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0024905226\n",
      "Norm of the params: 1.97408\n",
      "              Random: fixed   1 labels. Loss 0.42804. Accuracy 0.800.\n",
      "### Flips: 10, rs: 21, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [317] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3876644\n",
      "Train loss (w/o reg) on all data: 0.38740274\n",
      "Test loss (w/o reg) on all data: 0.40950775\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00052558404\n",
      "Norm of the params: 2.2876654\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.40951. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [284] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36224964\n",
      "Train loss (w/o reg) on all data: 0.36192673\n",
      "Test loss (w/o reg) on all data: 0.39154035\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007044863\n",
      "Norm of the params: 2.54126\n",
      "                Loss: fixed   6 labels. Loss 0.39154. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4255141\n",
      "Train loss (w/o reg) on all data: 0.42528808\n",
      "Test loss (w/o reg) on all data: 0.41539896\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00215851\n",
      "Norm of the params: 2.1261468\n",
      "              Random: fixed   2 labels. Loss 0.41540. Accuracy 0.800.\n",
      "### Flips: 10, rs: 21, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [337] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3766297\n",
      "Train loss (w/o reg) on all data: 0.37634447\n",
      "Test loss (w/o reg) on all data: 0.4082519\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00035399757\n",
      "Norm of the params: 2.3884857\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.40825. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [85] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36048153\n",
      "Train loss (w/o reg) on all data: 0.3601634\n",
      "Test loss (w/o reg) on all data: 0.39160395\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0023991861\n",
      "Norm of the params: 2.522414\n",
      "                Loss: fixed   7 labels. Loss 0.39160. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [369] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42550915\n",
      "Train loss (w/o reg) on all data: 0.42528397\n",
      "Test loss (w/o reg) on all data: 0.4148505\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00046712093\n",
      "Norm of the params: 2.122157\n",
      "              Random: fixed   2 labels. Loss 0.41485. Accuracy 0.800.\n",
      "### Flips: 10, rs: 21, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [371] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36129853\n",
      "Train loss (w/o reg) on all data: 0.3609735\n",
      "Test loss (w/o reg) on all data: 0.40713966\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002827791\n",
      "Norm of the params: 2.549638\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.40714. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [290] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36047497\n",
      "Train loss (w/o reg) on all data: 0.36015582\n",
      "Test loss (w/o reg) on all data: 0.39155346\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0003530208\n",
      "Norm of the params: 2.5265212\n",
      "                Loss: fixed   7 labels. Loss 0.39155. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41772285\n",
      "Train loss (w/o reg) on all data: 0.41748965\n",
      "Test loss (w/o reg) on all data: 0.41082323\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012298113\n",
      "Norm of the params: 2.159585\n",
      "              Random: fixed   3 labels. Loss 0.41082. Accuracy 0.800.\n",
      "### Flips: 10, rs: 21, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [348] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35923073\n",
      "Train loss (w/o reg) on all data: 0.35891023\n",
      "Test loss (w/o reg) on all data: 0.4067233\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006729349\n",
      "Norm of the params: 2.5318146\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.40672. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36247087\n",
      "Train loss (w/o reg) on all data: 0.3621528\n",
      "Test loss (w/o reg) on all data: 0.40408966\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007802505\n",
      "Norm of the params: 2.5221627\n",
      "                Loss: fixed   8 labels. Loss 0.40409. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41772023\n",
      "Train loss (w/o reg) on all data: 0.4174853\n",
      "Test loss (w/o reg) on all data: 0.4106398\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00024477826\n",
      "Norm of the params: 2.16769\n",
      "              Random: fixed   3 labels. Loss 0.41064. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45748147\n",
      "Train loss (w/o reg) on all data: 0.45735496\n",
      "Test loss (w/o reg) on all data: 0.40012512\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001252427\n",
      "Norm of the params: 1.5906452\n",
      "Flipped loss: 0.40013. Accuracy: 0.800\n",
      "### Flips: 10, rs: 22, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4069325\n",
      "Train loss (w/o reg) on all data: 0.40673584\n",
      "Test loss (w/o reg) on all data: 0.36673617\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010909629\n",
      "Norm of the params: 1.9833149\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.36674. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [135] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38553086\n",
      "Train loss (w/o reg) on all data: 0.38530606\n",
      "Test loss (w/o reg) on all data: 0.39096072\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0025791277\n",
      "Norm of the params: 2.1204262\n",
      "                Loss: fixed   5 labels. Loss 0.39096. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45748463\n",
      "Train loss (w/o reg) on all data: 0.45735738\n",
      "Test loss (w/o reg) on all data: 0.40137774\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012036959\n",
      "Norm of the params: 1.5953217\n",
      "              Random: fixed   0 labels. Loss 0.40138. Accuracy 0.800.\n",
      "### Flips: 10, rs: 22, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3818346\n",
      "Train loss (w/o reg) on all data: 0.38157877\n",
      "Test loss (w/o reg) on all data: 0.36033347\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0012258478\n",
      "Norm of the params: 2.2620144\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.36033. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [175] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37014893\n",
      "Train loss (w/o reg) on all data: 0.36989832\n",
      "Test loss (w/o reg) on all data: 0.39574915\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007363304\n",
      "Norm of the params: 2.2387724\n",
      "                Loss: fixed   7 labels. Loss 0.39575. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [282] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4574791\n",
      "Train loss (w/o reg) on all data: 0.45735192\n",
      "Test loss (w/o reg) on all data: 0.40055653\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0030591753\n",
      "Norm of the params: 1.5947094\n",
      "              Random: fixed   0 labels. Loss 0.40056. Accuracy 0.800.\n",
      "### Flips: 10, rs: 22, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3661566\n",
      "Train loss (w/o reg) on all data: 0.36584315\n",
      "Test loss (w/o reg) on all data: 0.39619386\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00080693513\n",
      "Norm of the params: 2.5038273\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.39619. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35625777\n",
      "Train loss (w/o reg) on all data: 0.3559607\n",
      "Test loss (w/o reg) on all data: 0.40023014\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00087886705\n",
      "Norm of the params: 2.4374957\n",
      "                Loss: fixed   9 labels. Loss 0.40023. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [317] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45748335\n",
      "Train loss (w/o reg) on all data: 0.45735705\n",
      "Test loss (w/o reg) on all data: 0.4011135\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001253264\n",
      "Norm of the params: 1.589287\n",
      "              Random: fixed   0 labels. Loss 0.40111. Accuracy 0.800.\n",
      "### Flips: 10, rs: 22, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36615458\n",
      "Train loss (w/o reg) on all data: 0.36584103\n",
      "Test loss (w/o reg) on all data: 0.39646238\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00047272403\n",
      "Norm of the params: 2.5041602\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.39646. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35625738\n",
      "Train loss (w/o reg) on all data: 0.3559605\n",
      "Test loss (w/o reg) on all data: 0.40011165\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0003248574\n",
      "Norm of the params: 2.4367151\n",
      "                Loss: fixed   9 labels. Loss 0.40011. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [282] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44375783\n",
      "Train loss (w/o reg) on all data: 0.44361624\n",
      "Test loss (w/o reg) on all data: 0.3971688\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002090977\n",
      "Norm of the params: 1.6828467\n",
      "              Random: fixed   1 labels. Loss 0.39717. Accuracy 0.800.\n",
      "### Flips: 10, rs: 22, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36615986\n",
      "Train loss (w/o reg) on all data: 0.36584702\n",
      "Test loss (w/o reg) on all data: 0.39599532\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0032428002\n",
      "Norm of the params: 2.5013878\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.39600. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [157] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35625744\n",
      "Train loss (w/o reg) on all data: 0.35596055\n",
      "Test loss (w/o reg) on all data: 0.40016076\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002296609\n",
      "Norm of the params: 2.4367921\n",
      "                Loss: fixed   9 labels. Loss 0.40016. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [347] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44375134\n",
      "Train loss (w/o reg) on all data: 0.4436097\n",
      "Test loss (w/o reg) on all data: 0.39678606\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00079906493\n",
      "Norm of the params: 1.6830548\n",
      "              Random: fixed   1 labels. Loss 0.39679. Accuracy 0.800.\n",
      "### Flips: 10, rs: 22, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [315] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36615366\n",
      "Train loss (w/o reg) on all data: 0.36583963\n",
      "Test loss (w/o reg) on all data: 0.39693612\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00026784223\n",
      "Norm of the params: 2.5061562\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.39694. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [115] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35626844\n",
      "Train loss (w/o reg) on all data: 0.3559681\n",
      "Test loss (w/o reg) on all data: 0.40133068\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010267383\n",
      "Norm of the params: 2.4509299\n",
      "                Loss: fixed   9 labels. Loss 0.40133. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [305] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43831798\n",
      "Train loss (w/o reg) on all data: 0.43817434\n",
      "Test loss (w/o reg) on all data: 0.40515736\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.006354714\n",
      "Norm of the params: 1.6949742\n",
      "              Random: fixed   2 labels. Loss 0.40516. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [406] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3982344\n",
      "Train loss (w/o reg) on all data: 0.39797285\n",
      "Test loss (w/o reg) on all data: 0.37990573\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020738803\n",
      "Norm of the params: 2.2871666\n",
      "Flipped loss: 0.37991. Accuracy: 0.800\n",
      "### Flips: 10, rs: 23, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [145] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3777519\n",
      "Train loss (w/o reg) on all data: 0.37744817\n",
      "Test loss (w/o reg) on all data: 0.38295728\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005897392\n",
      "Norm of the params: 2.4646282\n",
      "     Influence (LOO): fixed   1 labels. Loss 0.38296. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [100] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36040133\n",
      "Train loss (w/o reg) on all data: 0.3600502\n",
      "Test loss (w/o reg) on all data: 0.39750117\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003921115\n",
      "Norm of the params: 2.6500518\n",
      "                Loss: fixed   3 labels. Loss 0.39750. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [118] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3982546\n",
      "Train loss (w/o reg) on all data: 0.39798653\n",
      "Test loss (w/o reg) on all data: 0.38163126\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014441598\n",
      "Norm of the params: 2.3154671\n",
      "              Random: fixed   0 labels. Loss 0.38163. Accuracy 0.800.\n",
      "### Flips: 10, rs: 23, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [140] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36019272\n",
      "Train loss (w/o reg) on all data: 0.3598357\n",
      "Test loss (w/o reg) on all data: 0.4022784\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0003219427\n",
      "Norm of the params: 2.6721346\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.40228. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [103] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34158343\n",
      "Train loss (w/o reg) on all data: 0.34118554\n",
      "Test loss (w/o reg) on all data: 0.38843176\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011911165\n",
      "Norm of the params: 2.8210032\n",
      "                Loss: fixed   5 labels. Loss 0.38843. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [172] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40020126\n",
      "Train loss (w/o reg) on all data: 0.39993855\n",
      "Test loss (w/o reg) on all data: 0.39415783\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011856606\n",
      "Norm of the params: 2.2921476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Random: fixed   1 labels. Loss 0.39416. Accuracy 0.800.\n",
      "### Flips: 10, rs: 23, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36019292\n",
      "Train loss (w/o reg) on all data: 0.35983586\n",
      "Test loss (w/o reg) on all data: 0.40209377\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.000611796\n",
      "Norm of the params: 2.6723447\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.40209. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [97] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34158307\n",
      "Train loss (w/o reg) on all data: 0.3411855\n",
      "Test loss (w/o reg) on all data: 0.38827035\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005190716\n",
      "Norm of the params: 2.8198047\n",
      "                Loss: fixed   5 labels. Loss 0.38827. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [155] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39098588\n",
      "Train loss (w/o reg) on all data: 0.39072153\n",
      "Test loss (w/o reg) on all data: 0.40811732\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0050594644\n",
      "Norm of the params: 2.299348\n",
      "              Random: fixed   3 labels. Loss 0.40812. Accuracy 0.800.\n",
      "### Flips: 10, rs: 23, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34227148\n",
      "Train loss (w/o reg) on all data: 0.34187242\n",
      "Test loss (w/o reg) on all data: 0.4052671\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006285618\n",
      "Norm of the params: 2.8250868\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.40527. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [126] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34157875\n",
      "Train loss (w/o reg) on all data: 0.34118077\n",
      "Test loss (w/o reg) on all data: 0.38766152\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0028805926\n",
      "Norm of the params: 2.8212504\n",
      "                Loss: fixed   5 labels. Loss 0.38766. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [202] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3909859\n",
      "Train loss (w/o reg) on all data: 0.39072174\n",
      "Test loss (w/o reg) on all data: 0.4080786\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0044103274\n",
      "Norm of the params: 2.2985651\n",
      "              Random: fixed   3 labels. Loss 0.40808. Accuracy 0.800.\n",
      "### Flips: 10, rs: 23, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [160] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34206027\n",
      "Train loss (w/o reg) on all data: 0.3416913\n",
      "Test loss (w/o reg) on all data: 0.40905398\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00094269135\n",
      "Norm of the params: 2.7165856\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.40905. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [113] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34188858\n",
      "Train loss (w/o reg) on all data: 0.34149012\n",
      "Test loss (w/o reg) on all data: 0.39291355\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0026584694\n",
      "Norm of the params: 2.8230054\n",
      "                Loss: fixed   6 labels. Loss 0.39291. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39098096\n",
      "Train loss (w/o reg) on all data: 0.39071974\n",
      "Test loss (w/o reg) on all data: 0.40779677\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.006698153\n",
      "Norm of the params: 2.2856317\n",
      "              Random: fixed   3 labels. Loss 0.40780. Accuracy 0.800.\n",
      "### Flips: 10, rs: 23, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34206125\n",
      "Train loss (w/o reg) on all data: 0.34169263\n",
      "Test loss (w/o reg) on all data: 0.40906596\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0061739446\n",
      "Norm of the params: 2.7152\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.40907. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [169] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34188846\n",
      "Train loss (w/o reg) on all data: 0.34148985\n",
      "Test loss (w/o reg) on all data: 0.39284998\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00054412615\n",
      "Norm of the params: 2.8234558\n",
      "                Loss: fixed   6 labels. Loss 0.39285. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39098087\n",
      "Train loss (w/o reg) on all data: 0.3907187\n",
      "Test loss (w/o reg) on all data: 0.40762547\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020382085\n",
      "Norm of the params: 2.2898617\n",
      "              Random: fixed   3 labels. Loss 0.40763. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [392] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37828222\n",
      "Train loss (w/o reg) on all data: 0.3780042\n",
      "Test loss (w/o reg) on all data: 0.36548346\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010939927\n",
      "Norm of the params: 2.3581161\n",
      "Flipped loss: 0.36548. Accuracy: 0.822\n",
      "### Flips: 10, rs: 24, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.370318\n",
      "Train loss (w/o reg) on all data: 0.37002864\n",
      "Test loss (w/o reg) on all data: 0.38431287\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0035102244\n",
      "Norm of the params: 2.4055886\n",
      "     Influence (LOO): fixed   1 labels. Loss 0.38431. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34818253\n",
      "Train loss (w/o reg) on all data: 0.34780395\n",
      "Test loss (w/o reg) on all data: 0.3813797\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00073571305\n",
      "Norm of the params: 2.751626\n",
      "                Loss: fixed   2 labels. Loss 0.38138. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [136] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37828544\n",
      "Train loss (w/o reg) on all data: 0.37800673\n",
      "Test loss (w/o reg) on all data: 0.36551285\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004512131\n",
      "Norm of the params: 2.3609295\n",
      "              Random: fixed   0 labels. Loss 0.36551. Accuracy 0.822.\n",
      "### Flips: 10, rs: 24, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [278] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33590737\n",
      "Train loss (w/o reg) on all data: 0.33551097\n",
      "Test loss (w/o reg) on all data: 0.42173845\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005639667\n",
      "Norm of the params: 2.8157256\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.42174. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [107] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33429396\n",
      "Train loss (w/o reg) on all data: 0.33390117\n",
      "Test loss (w/o reg) on all data: 0.415054\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00056809536\n",
      "Norm of the params: 2.8028262\n",
      "                Loss: fixed   4 labels. Loss 0.41505. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37828103\n",
      "Train loss (w/o reg) on all data: 0.37800568\n",
      "Test loss (w/o reg) on all data: 0.36546156\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0023601484\n",
      "Norm of the params: 2.3466926\n",
      "              Random: fixed   0 labels. Loss 0.36546. Accuracy 0.822.\n",
      "### Flips: 10, rs: 24, checks: 30\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3289185\n",
      "Train loss (w/o reg) on all data: 0.328493\n",
      "Test loss (w/o reg) on all data: 0.4403788\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016974648\n",
      "Norm of the params: 2.9171603\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.44038. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [123] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32718223\n",
      "Train loss (w/o reg) on all data: 0.3267595\n",
      "Test loss (w/o reg) on all data: 0.43501404\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014138529\n",
      "Norm of the params: 2.9077191\n",
      "                Loss: fixed   5 labels. Loss 0.43501. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37828484\n",
      "Train loss (w/o reg) on all data: 0.37801045\n",
      "Test loss (w/o reg) on all data: 0.36517733\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0070293616\n",
      "Norm of the params: 2.3426187\n",
      "              Random: fixed   0 labels. Loss 0.36518. Accuracy 0.822.\n",
      "### Flips: 10, rs: 24, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [190] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3239353\n",
      "Train loss (w/o reg) on all data: 0.32351187\n",
      "Test loss (w/o reg) on all data: 0.45162293\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0029695928\n",
      "Norm of the params: 2.9100466\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.45162. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32393527\n",
      "Train loss (w/o reg) on all data: 0.32351184\n",
      "Test loss (w/o reg) on all data: 0.4516171\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00067808706\n",
      "Norm of the params: 2.9100466\n",
      "                Loss: fixed   6 labels. Loss 0.45162. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37376088\n",
      "Train loss (w/o reg) on all data: 0.37347764\n",
      "Test loss (w/o reg) on all data: 0.37666208\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013826815\n",
      "Norm of the params: 2.3800693\n",
      "              Random: fixed   1 labels. Loss 0.37666. Accuracy 0.800.\n",
      "### Flips: 10, rs: 24, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32393453\n",
      "Train loss (w/o reg) on all data: 0.32351148\n",
      "Test loss (w/o reg) on all data: 0.45119476\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0027960737\n",
      "Norm of the params: 2.9087424\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.45119. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [108] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32438904\n",
      "Train loss (w/o reg) on all data: 0.32394272\n",
      "Test loss (w/o reg) on all data: 0.45454326\n",
      "Train acc on all data:  0.8820754716981132\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0009051796\n",
      "Norm of the params: 2.9876814\n",
      "                Loss: fixed   7 labels. Loss 0.45454. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3737603\n",
      "Train loss (w/o reg) on all data: 0.37347743\n",
      "Test loss (w/o reg) on all data: 0.37666816\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00039917274\n",
      "Norm of the params: 2.3786407\n",
      "              Random: fixed   1 labels. Loss 0.37667. Accuracy 0.800.\n",
      "### Flips: 10, rs: 24, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32393464\n",
      "Train loss (w/o reg) on all data: 0.32351178\n",
      "Test loss (w/o reg) on all data: 0.45139194\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005814682\n",
      "Norm of the params: 2.9081316\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.45139. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [74] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.328091\n",
      "Train loss (w/o reg) on all data: 0.32767797\n",
      "Test loss (w/o reg) on all data: 0.4491359\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.000922037\n",
      "Norm of the params: 2.8741362\n",
      "                Loss: fixed   8 labels. Loss 0.44914. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [211] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3737632\n",
      "Train loss (w/o reg) on all data: 0.37347898\n",
      "Test loss (w/o reg) on all data: 0.37645057\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003104219\n",
      "Norm of the params: 2.38417\n",
      "              Random: fixed   1 labels. Loss 0.37645. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [272] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44739297\n",
      "Train loss (w/o reg) on all data: 0.44723627\n",
      "Test loss (w/o reg) on all data: 0.41424572\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002315336\n",
      "Norm of the params: 1.7702913\n",
      "Flipped loss: 0.41425. Accuracy: 0.800\n",
      "### Flips: 10, rs: 25, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40522775\n",
      "Train loss (w/o reg) on all data: 0.40501997\n",
      "Test loss (w/o reg) on all data: 0.4099082\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0022016754\n",
      "Norm of the params: 2.0385835\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.40991. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [173] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37316424\n",
      "Train loss (w/o reg) on all data: 0.3728346\n",
      "Test loss (w/o reg) on all data: 0.40425164\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00068462867\n",
      "Norm of the params: 2.5676248\n",
      "                Loss: fixed   5 labels. Loss 0.40425. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41981107\n",
      "Train loss (w/o reg) on all data: 0.41963455\n",
      "Test loss (w/o reg) on all data: 0.41244143\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018193893\n",
      "Norm of the params: 1.8789335\n",
      "              Random: fixed   2 labels. Loss 0.41244. Accuracy 0.822.\n",
      "### Flips: 10, rs: 25, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [160] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37516502\n",
      "Train loss (w/o reg) on all data: 0.3749057\n",
      "Test loss (w/o reg) on all data: 0.42116967\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0030696155\n",
      "Norm of the params: 2.2773187\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.42117. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34824342\n",
      "Train loss (w/o reg) on all data: 0.34788716\n",
      "Test loss (w/o reg) on all data: 0.41777804\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00048790776\n",
      "Norm of the params: 2.6692998\n",
      "                Loss: fixed   8 labels. Loss 0.41778. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41249767\n",
      "Train loss (w/o reg) on all data: 0.41231918\n",
      "Test loss (w/o reg) on all data: 0.4103254\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002612079\n",
      "Norm of the params: 1.8894275\n",
      "              Random: fixed   3 labels. Loss 0.41033. Accuracy 0.822.\n",
      "### Flips: 10, rs: 25, checks: 30\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [307] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36013728\n",
      "Train loss (w/o reg) on all data: 0.35982716\n",
      "Test loss (w/o reg) on all data: 0.42710173\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010431971\n",
      "Norm of the params: 2.4905148\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.42710. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [187] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34824207\n",
      "Train loss (w/o reg) on all data: 0.34788528\n",
      "Test loss (w/o reg) on all data: 0.41803467\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009668728\n",
      "Norm of the params: 2.6712813\n",
      "                Loss: fixed   8 labels. Loss 0.41803. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4178609\n",
      "Train loss (w/o reg) on all data: 0.41769183\n",
      "Test loss (w/o reg) on all data: 0.39795402\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002228788\n",
      "Norm of the params: 1.8387966\n",
      "              Random: fixed   4 labels. Loss 0.39795. Accuracy 0.822.\n",
      "### Flips: 10, rs: 25, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [266] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36013737\n",
      "Train loss (w/o reg) on all data: 0.3598286\n",
      "Test loss (w/o reg) on all data: 0.42706028\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00093062624\n",
      "Norm of the params: 2.4850287\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.42706. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [69] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34824586\n",
      "Train loss (w/o reg) on all data: 0.34789085\n",
      "Test loss (w/o reg) on all data: 0.41765264\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006600325\n",
      "Norm of the params: 2.6646051\n",
      "                Loss: fixed   8 labels. Loss 0.41765. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [278] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41786283\n",
      "Train loss (w/o reg) on all data: 0.41769284\n",
      "Test loss (w/o reg) on all data: 0.3983131\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00077473285\n",
      "Norm of the params: 1.843884\n",
      "              Random: fixed   4 labels. Loss 0.39831. Accuracy 0.822.\n",
      "### Flips: 10, rs: 25, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34824422\n",
      "Train loss (w/o reg) on all data: 0.34788638\n",
      "Test loss (w/o reg) on all data: 0.41725448\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011610676\n",
      "Norm of the params: 2.6751554\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41725. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34824425\n",
      "Train loss (w/o reg) on all data: 0.3478864\n",
      "Test loss (w/o reg) on all data: 0.41725096\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010149183\n",
      "Norm of the params: 2.6751554\n",
      "                Loss: fixed   8 labels. Loss 0.41725. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41785827\n",
      "Train loss (w/o reg) on all data: 0.41768906\n",
      "Test loss (w/o reg) on all data: 0.39839798\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009053788\n",
      "Norm of the params: 1.8396662\n",
      "              Random: fixed   4 labels. Loss 0.39840. Accuracy 0.822.\n",
      "### Flips: 10, rs: 25, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [279] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34824604\n",
      "Train loss (w/o reg) on all data: 0.34788916\n",
      "Test loss (w/o reg) on all data: 0.41728008\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009958744\n",
      "Norm of the params: 2.6716216\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41728. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [99] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3528073\n",
      "Train loss (w/o reg) on all data: 0.35246807\n",
      "Test loss (w/o reg) on all data: 0.42331243\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0004419023\n",
      "Norm of the params: 2.6047435\n",
      "                Loss: fixed   9 labels. Loss 0.42331. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [143] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41786316\n",
      "Train loss (w/o reg) on all data: 0.41769323\n",
      "Test loss (w/o reg) on all data: 0.39859378\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0077506173\n",
      "Norm of the params: 1.8435855\n",
      "              Random: fixed   4 labels. Loss 0.39859. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [118] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38036904\n",
      "Train loss (w/o reg) on all data: 0.38011435\n",
      "Test loss (w/o reg) on all data: 0.38544396\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011207946\n",
      "Norm of the params: 2.256887\n",
      "Flipped loss: 0.38544. Accuracy: 0.822\n",
      "### Flips: 10, rs: 26, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3317658\n",
      "Train loss (w/o reg) on all data: 0.33136547\n",
      "Test loss (w/o reg) on all data: 0.41899568\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001011937\n",
      "Norm of the params: 2.8295848\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.41900. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [211] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3262485\n",
      "Train loss (w/o reg) on all data: 0.32582194\n",
      "Test loss (w/o reg) on all data: 0.40116608\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0021557254\n",
      "Norm of the params: 2.9208677\n",
      "                Loss: fixed   3 labels. Loss 0.40117. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38038397\n",
      "Train loss (w/o reg) on all data: 0.3801278\n",
      "Test loss (w/o reg) on all data: 0.38602892\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.006000404\n",
      "Norm of the params: 2.2635949\n",
      "              Random: fixed   0 labels. Loss 0.38603. Accuracy 0.822.\n",
      "### Flips: 10, rs: 26, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [361] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31975925\n",
      "Train loss (w/o reg) on all data: 0.31930405\n",
      "Test loss (w/o reg) on all data: 0.42034262\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00044983582\n",
      "Norm of the params: 3.0172994\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.42034. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [71] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32076406\n",
      "Train loss (w/o reg) on all data: 0.32031652\n",
      "Test loss (w/o reg) on all data: 0.39855707\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00047700095\n",
      "Norm of the params: 2.9918096\n",
      "                Loss: fixed   4 labels. Loss 0.39856. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37467623\n",
      "Train loss (w/o reg) on all data: 0.3744083\n",
      "Test loss (w/o reg) on all data: 0.38112938\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005676893\n",
      "Norm of the params: 2.314816\n",
      "              Random: fixed   1 labels. Loss 0.38113. Accuracy 0.822.\n",
      "### Flips: 10, rs: 26, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31975952\n",
      "Train loss (w/o reg) on all data: 0.31930503\n",
      "Test loss (w/o reg) on all data: 0.41980556\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004660791\n",
      "Norm of the params: 3.0148995\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.41981. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [108] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32076216\n",
      "Train loss (w/o reg) on all data: 0.3203144\n",
      "Test loss (w/o reg) on all data: 0.3987934\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017117457\n",
      "Norm of the params: 2.9925177\n",
      "                Loss: fixed   4 labels. Loss 0.39879. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37467584\n",
      "Train loss (w/o reg) on all data: 0.3744082\n",
      "Test loss (w/o reg) on all data: 0.38118342\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00039965904\n",
      "Norm of the params: 2.3137171\n",
      "              Random: fixed   1 labels. Loss 0.38118. Accuracy 0.822.\n",
      "### Flips: 10, rs: 26, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [314] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31976187\n",
      "Train loss (w/o reg) on all data: 0.3193092\n",
      "Test loss (w/o reg) on all data: 0.4194683\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0031416616\n",
      "Norm of the params: 3.008831\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.41947. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [137] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3203605\n",
      "Train loss (w/o reg) on all data: 0.31992596\n",
      "Test loss (w/o reg) on all data: 0.4070556\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0004481483\n",
      "Norm of the params: 2.948013\n",
      "                Loss: fixed   5 labels. Loss 0.40706. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [108] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34104055\n",
      "Train loss (w/o reg) on all data: 0.34068066\n",
      "Test loss (w/o reg) on all data: 0.41145042\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0017093442\n",
      "Norm of the params: 2.6829357\n",
      "              Random: fixed   4 labels. Loss 0.41145. Accuracy 0.778.\n",
      "### Flips: 10, rs: 26, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [198] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31976008\n",
      "Train loss (w/o reg) on all data: 0.31930664\n",
      "Test loss (w/o reg) on all data: 0.4197211\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0049428823\n",
      "Norm of the params: 3.0114846\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.41972. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [133] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3189539\n",
      "Train loss (w/o reg) on all data: 0.31851128\n",
      "Test loss (w/o reg) on all data: 0.42951804\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.006536399\n",
      "Norm of the params: 2.9752667\n",
      "                Loss: fixed   6 labels. Loss 0.42952. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [201] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3410195\n",
      "Train loss (w/o reg) on all data: 0.34066445\n",
      "Test loss (w/o reg) on all data: 0.41111958\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0058304076\n",
      "Norm of the params: 2.6647785\n",
      "              Random: fixed   4 labels. Loss 0.41112. Accuracy 0.800.\n",
      "### Flips: 10, rs: 26, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31894833\n",
      "Train loss (w/o reg) on all data: 0.31851038\n",
      "Test loss (w/o reg) on all data: 0.42880553\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0017935808\n",
      "Norm of the params: 2.9595935\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.42881. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [107] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3232751\n",
      "Train loss (w/o reg) on all data: 0.32287776\n",
      "Test loss (w/o reg) on all data: 0.42320365\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017937059\n",
      "Norm of the params: 2.8189347\n",
      "                Loss: fixed   7 labels. Loss 0.42320. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35691118\n",
      "Train loss (w/o reg) on all data: 0.35660565\n",
      "Test loss (w/o reg) on all data: 0.41422784\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0005841047\n",
      "Norm of the params: 2.471947\n",
      "              Random: fixed   5 labels. Loss 0.41423. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [293] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43677166\n",
      "Train loss (w/o reg) on all data: 0.43662637\n",
      "Test loss (w/o reg) on all data: 0.38949\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007428918\n",
      "Norm of the params: 1.7046809\n",
      "Flipped loss: 0.38949. Accuracy: 0.822\n",
      "### Flips: 10, rs: 27, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [138] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41145775\n",
      "Train loss (w/o reg) on all data: 0.41127694\n",
      "Test loss (w/o reg) on all data: 0.38568398\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001250134\n",
      "Norm of the params: 1.9016224\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.38568. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [107] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3862608\n",
      "Train loss (w/o reg) on all data: 0.38603267\n",
      "Test loss (w/o reg) on all data: 0.39193726\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006684932\n",
      "Norm of the params: 2.1360004\n",
      "                Loss: fixed   4 labels. Loss 0.39194. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [158] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4282568\n",
      "Train loss (w/o reg) on all data: 0.4281037\n",
      "Test loss (w/o reg) on all data: 0.39081353\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004799247\n",
      "Norm of the params: 1.7499946\n",
      "              Random: fixed   1 labels. Loss 0.39081. Accuracy 0.822.\n",
      "### Flips: 10, rs: 27, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38095686\n",
      "Train loss (w/o reg) on all data: 0.38072428\n",
      "Test loss (w/o reg) on all data: 0.3866016\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00051221583\n",
      "Norm of the params: 2.15679\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.38660. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [258] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37729058\n",
      "Train loss (w/o reg) on all data: 0.37705818\n",
      "Test loss (w/o reg) on all data: 0.39310768\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0014846069\n",
      "Norm of the params: 2.155973\n",
      "                Loss: fixed   5 labels. Loss 0.39311. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [121] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42825598\n",
      "Train loss (w/o reg) on all data: 0.4281034\n",
      "Test loss (w/o reg) on all data: 0.39073515\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00050158845\n",
      "Norm of the params: 1.7468933\n",
      "              Random: fixed   1 labels. Loss 0.39074. Accuracy 0.822.\n",
      "### Flips: 10, rs: 27, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3746057\n",
      "Train loss (w/o reg) on all data: 0.37435874\n",
      "Test loss (w/o reg) on all data: 0.39982232\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0020073839\n",
      "Norm of the params: 2.222369\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.39982. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [90] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36175448\n",
      "Train loss (w/o reg) on all data: 0.36146688\n",
      "Test loss (w/o reg) on all data: 0.4095686\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0004552063\n",
      "Norm of the params: 2.3982782\n",
      "                Loss: fixed   8 labels. Loss 0.40957. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42515948\n",
      "Train loss (w/o reg) on all data: 0.4250003\n",
      "Test loss (w/o reg) on all data: 0.391902\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0025246474\n",
      "Norm of the params: 1.7841717\n",
      "              Random: fixed   2 labels. Loss 0.39190. Accuracy 0.822.\n",
      "### Flips: 10, rs: 27, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [293] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37574026\n",
      "Train loss (w/o reg) on all data: 0.37546793\n",
      "Test loss (w/o reg) on all data: 0.4014048\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014034521\n",
      "Norm of the params: 2.33377\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.40140. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [102] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3574769\n",
      "Train loss (w/o reg) on all data: 0.35718486\n",
      "Test loss (w/o reg) on all data: 0.40476778\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.006419226\n",
      "Norm of the params: 2.416744\n",
      "                Loss: fixed   9 labels. Loss 0.40477. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [150] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41147768\n",
      "Train loss (w/o reg) on all data: 0.41130012\n",
      "Test loss (w/o reg) on all data: 0.38636672\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0020290508\n",
      "Norm of the params: 1.8844514\n",
      "              Random: fixed   3 labels. Loss 0.38637. Accuracy 0.822.\n",
      "### Flips: 10, rs: 27, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [269] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35923046\n",
      "Train loss (w/o reg) on all data: 0.3589105\n",
      "Test loss (w/o reg) on all data: 0.4065137\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00073459465\n",
      "Norm of the params: 2.5296757\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.40651. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [131] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3574735\n",
      "Train loss (w/o reg) on all data: 0.3571809\n",
      "Test loss (w/o reg) on all data: 0.40544292\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0031483562\n",
      "Norm of the params: 2.4191425\n",
      "                Loss: fixed   9 labels. Loss 0.40544. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41145888\n",
      "Train loss (w/o reg) on all data: 0.41127843\n",
      "Test loss (w/o reg) on all data: 0.38571927\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0045124236\n",
      "Norm of the params: 1.8997761\n",
      "              Random: fixed   3 labels. Loss 0.38572. Accuracy 0.822.\n",
      "### Flips: 10, rs: 27, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [322] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3592306\n",
      "Train loss (w/o reg) on all data: 0.35891086\n",
      "Test loss (w/o reg) on all data: 0.40654618\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010322141\n",
      "Norm of the params: 2.5288064\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.40655. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35923064\n",
      "Train loss (w/o reg) on all data: 0.3589109\n",
      "Test loss (w/o reg) on all data: 0.40654364\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00049808825\n",
      "Norm of the params: 2.5288064\n",
      "                Loss: fixed  10 labels. Loss 0.40654. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [301] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40586117\n",
      "Train loss (w/o reg) on all data: 0.40567002\n",
      "Test loss (w/o reg) on all data: 0.39745513\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0054623755\n",
      "Norm of the params: 1.9553131\n",
      "              Random: fixed   4 labels. Loss 0.39746. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [321] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40489858\n",
      "Train loss (w/o reg) on all data: 0.4046184\n",
      "Test loss (w/o reg) on all data: 0.39200845\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0018638569\n",
      "Norm of the params: 2.367211\n",
      "Flipped loss: 0.39201. Accuracy: 0.844\n",
      "### Flips: 10, rs: 28, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38040906\n",
      "Train loss (w/o reg) on all data: 0.38005534\n",
      "Test loss (w/o reg) on all data: 0.41010377\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00063378597\n",
      "Norm of the params: 2.6598315\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.41010. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [150] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38093874\n",
      "Train loss (w/o reg) on all data: 0.3806211\n",
      "Test loss (w/o reg) on all data: 0.40320638\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0023117752\n",
      "Norm of the params: 2.5203946\n",
      "                Loss: fixed   2 labels. Loss 0.40321. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [128] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40490583\n",
      "Train loss (w/o reg) on all data: 0.40462315\n",
      "Test loss (w/o reg) on all data: 0.39235225\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001104833\n",
      "Norm of the params: 2.377749\n",
      "              Random: fixed   0 labels. Loss 0.39235. Accuracy 0.844.\n",
      "### Flips: 10, rs: 28, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [137] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3768409\n",
      "Train loss (w/o reg) on all data: 0.37648982\n",
      "Test loss (w/o reg) on all data: 0.39155158\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016758175\n",
      "Norm of the params: 2.6498194\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.39155. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36513722\n",
      "Train loss (w/o reg) on all data: 0.36475748\n",
      "Test loss (w/o reg) on all data: 0.4329038\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00069193955\n",
      "Norm of the params: 2.7559056\n",
      "                Loss: fixed   4 labels. Loss 0.43290. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [325] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40489838\n",
      "Train loss (w/o reg) on all data: 0.4046171\n",
      "Test loss (w/o reg) on all data: 0.39220893\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00036660847\n",
      "Norm of the params: 2.371867\n",
      "              Random: fixed   0 labels. Loss 0.39221. Accuracy 0.844.\n",
      "### Flips: 10, rs: 28, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3715044\n",
      "Train loss (w/o reg) on all data: 0.37120026\n",
      "Test loss (w/o reg) on all data: 0.37662518\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005468081\n",
      "Norm of the params: 2.4662592\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.37663. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [108] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36155838\n",
      "Train loss (w/o reg) on all data: 0.36118132\n",
      "Test loss (w/o reg) on all data: 0.41314244\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0030124136\n",
      "Norm of the params: 2.7461545\n",
      "                Loss: fixed   5 labels. Loss 0.41314. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [175] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4049048\n",
      "Train loss (w/o reg) on all data: 0.40462324\n",
      "Test loss (w/o reg) on all data: 0.3925031\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0018425221\n",
      "Norm of the params: 2.3730502\n",
      "              Random: fixed   0 labels. Loss 0.39250. Accuracy 0.844.\n",
      "### Flips: 10, rs: 28, checks: 40\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35471004\n",
      "Train loss (w/o reg) on all data: 0.35437554\n",
      "Test loss (w/o reg) on all data: 0.39834958\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012056632\n",
      "Norm of the params: 2.5864768\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.39835. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3615532\n",
      "Train loss (w/o reg) on all data: 0.36117595\n",
      "Test loss (w/o reg) on all data: 0.41373503\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0003469197\n",
      "Norm of the params: 2.7467287\n",
      "                Loss: fixed   5 labels. Loss 0.41374. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [308] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40489885\n",
      "Train loss (w/o reg) on all data: 0.40461725\n",
      "Test loss (w/o reg) on all data: 0.39236492\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0015873569\n",
      "Norm of the params: 2.3731904\n",
      "              Random: fixed   0 labels. Loss 0.39236. Accuracy 0.844.\n",
      "### Flips: 10, rs: 28, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [202] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35470617\n",
      "Train loss (w/o reg) on all data: 0.35436997\n",
      "Test loss (w/o reg) on all data: 0.39810592\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001996027\n",
      "Norm of the params: 2.593012\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.39811. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35470614\n",
      "Train loss (w/o reg) on all data: 0.35436994\n",
      "Test loss (w/o reg) on all data: 0.39810118\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00086302054\n",
      "Norm of the params: 2.593012\n",
      "                Loss: fixed   7 labels. Loss 0.39810. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [159] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38093957\n",
      "Train loss (w/o reg) on all data: 0.3806223\n",
      "Test loss (w/o reg) on all data: 0.40325192\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0020202172\n",
      "Norm of the params: 2.5190628\n",
      "              Random: fixed   2 labels. Loss 0.40325. Accuracy 0.822.\n",
      "### Flips: 10, rs: 28, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [195] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3547057\n",
      "Train loss (w/o reg) on all data: 0.35436884\n",
      "Test loss (w/o reg) on all data: 0.39853978\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00020477218\n",
      "Norm of the params: 2.5955906\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.39854. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [97] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35325405\n",
      "Train loss (w/o reg) on all data: 0.35292345\n",
      "Test loss (w/o reg) on all data: 0.40125325\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.000345922\n",
      "Norm of the params: 2.5713453\n",
      "                Loss: fixed   8 labels. Loss 0.40125. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37324402\n",
      "Train loss (w/o reg) on all data: 0.3729266\n",
      "Test loss (w/o reg) on all data: 0.41940305\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0003115565\n",
      "Norm of the params: 2.5195785\n",
      "              Random: fixed   3 labels. Loss 0.41940. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3766757\n",
      "Train loss (w/o reg) on all data: 0.37643278\n",
      "Test loss (w/o reg) on all data: 0.38669226\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0014864913\n",
      "Norm of the params: 2.2041728\n",
      "Flipped loss: 0.38669. Accuracy: 0.844\n",
      "### Flips: 10, rs: 29, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3452429\n",
      "Train loss (w/o reg) on all data: 0.34490845\n",
      "Test loss (w/o reg) on all data: 0.4136512\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00073494995\n",
      "Norm of the params: 2.586334\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.41365. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3329509\n",
      "Train loss (w/o reg) on all data: 0.3325844\n",
      "Test loss (w/o reg) on all data: 0.43458393\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0021699492\n",
      "Norm of the params: 2.7072709\n",
      "                Loss: fixed   3 labels. Loss 0.43458. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [190] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3766624\n",
      "Train loss (w/o reg) on all data: 0.3764212\n",
      "Test loss (w/o reg) on all data: 0.38584358\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00071195775\n",
      "Norm of the params: 2.1962917\n",
      "              Random: fixed   0 labels. Loss 0.38584. Accuracy 0.844.\n",
      "### Flips: 10, rs: 29, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3452436\n",
      "Train loss (w/o reg) on all data: 0.34490827\n",
      "Test loss (w/o reg) on all data: 0.41362804\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00047614763\n",
      "Norm of the params: 2.589764\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.41363. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3251155\n",
      "Train loss (w/o reg) on all data: 0.32472008\n",
      "Test loss (w/o reg) on all data: 0.44446525\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00187645\n",
      "Norm of the params: 2.8121538\n",
      "                Loss: fixed   4 labels. Loss 0.44447. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [190] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37667042\n",
      "Train loss (w/o reg) on all data: 0.37642828\n",
      "Test loss (w/o reg) on all data: 0.38687965\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0038056988\n",
      "Norm of the params: 2.2007203\n",
      "              Random: fixed   0 labels. Loss 0.38688. Accuracy 0.844.\n",
      "### Flips: 10, rs: 29, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [157] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33767068\n",
      "Train loss (w/o reg) on all data: 0.33730987\n",
      "Test loss (w/o reg) on all data: 0.4221854\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006957692\n",
      "Norm of the params: 2.6862974\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.42219. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [148] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3251173\n",
      "Train loss (w/o reg) on all data: 0.3247238\n",
      "Test loss (w/o reg) on all data: 0.44441733\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0028601019\n",
      "Norm of the params: 2.8053296\n",
      "                Loss: fixed   4 labels. Loss 0.44442. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37666404\n",
      "Train loss (w/o reg) on all data: 0.37642166\n",
      "Test loss (w/o reg) on all data: 0.38646555\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004914704\n",
      "Norm of the params: 2.2016916\n",
      "              Random: fixed   0 labels. Loss 0.38647. Accuracy 0.844.\n",
      "### Flips: 10, rs: 29, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32511762\n",
      "Train loss (w/o reg) on all data: 0.3247204\n",
      "Test loss (w/o reg) on all data: 0.44441864\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0028113083\n",
      "Norm of the params: 2.8185246\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.44442. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32113558\n",
      "Train loss (w/o reg) on all data: 0.32072917\n",
      "Test loss (w/o reg) on all data: 0.46103808\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00020012856\n",
      "Norm of the params: 2.851065\n",
      "                Loss: fixed   6 labels. Loss 0.46104. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.376663\n",
      "Train loss (w/o reg) on all data: 0.37642196\n",
      "Test loss (w/o reg) on all data: 0.3858039\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.005205217\n",
      "Norm of the params: 2.1956434\n",
      "              Random: fixed   0 labels. Loss 0.38580. Accuracy 0.844.\n",
      "### Flips: 10, rs: 29, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [346] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32232356\n",
      "Train loss (w/o reg) on all data: 0.32190415\n",
      "Test loss (w/o reg) on all data: 0.45717302\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0005877349\n",
      "Norm of the params: 2.896259\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.45717. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [124] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3227961\n",
      "Train loss (w/o reg) on all data: 0.32237273\n",
      "Test loss (w/o reg) on all data: 0.46107262\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0021084896\n",
      "Norm of the params: 2.9098454\n",
      "                Loss: fixed   7 labels. Loss 0.46107. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37581652\n",
      "Train loss (w/o reg) on all data: 0.37556732\n",
      "Test loss (w/o reg) on all data: 0.38957438\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00033323094\n",
      "Norm of the params: 2.2324584\n",
      "              Random: fixed   1 labels. Loss 0.38957. Accuracy 0.844.\n",
      "### Flips: 10, rs: 29, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32114524\n",
      "Train loss (w/o reg) on all data: 0.32073492\n",
      "Test loss (w/o reg) on all data: 0.4610105\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0041859993\n",
      "Norm of the params: 2.864693\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.46101. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32278946\n",
      "Train loss (w/o reg) on all data: 0.32236424\n",
      "Test loss (w/o reg) on all data: 0.46140102\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018664977\n",
      "Norm of the params: 2.9162698\n",
      "                Loss: fixed   7 labels. Loss 0.46140. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37582478\n",
      "Train loss (w/o reg) on all data: 0.37557685\n",
      "Test loss (w/o reg) on all data: 0.3898114\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010428636\n",
      "Norm of the params: 2.226761\n",
      "              Random: fixed   1 labels. Loss 0.38981. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40789184\n",
      "Train loss (w/o reg) on all data: 0.40769485\n",
      "Test loss (w/o reg) on all data: 0.3710596\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00073241955\n",
      "Norm of the params: 1.9849145\n",
      "Flipped loss: 0.37106. Accuracy: 0.844\n",
      "### Flips: 10, rs: 30, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37847644\n",
      "Train loss (w/o reg) on all data: 0.37822643\n",
      "Test loss (w/o reg) on all data: 0.37211978\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0036223887\n",
      "Norm of the params: 2.2360964\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.37212. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [140] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36080542\n",
      "Train loss (w/o reg) on all data: 0.360533\n",
      "Test loss (w/o reg) on all data: 0.38360357\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.005487465\n",
      "Norm of the params: 2.3341923\n",
      "                Loss: fixed   3 labels. Loss 0.38360. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41013312\n",
      "Train loss (w/o reg) on all data: 0.40993795\n",
      "Test loss (w/o reg) on all data: 0.36367115\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010649064\n",
      "Norm of the params: 1.9757009\n",
      "              Random: fixed   1 labels. Loss 0.36367. Accuracy 0.844.\n",
      "### Flips: 10, rs: 30, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36907\n",
      "Train loss (w/o reg) on all data: 0.36877835\n",
      "Test loss (w/o reg) on all data: 0.38221952\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0022026098\n",
      "Norm of the params: 2.4151587\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.38222. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34404668\n",
      "Train loss (w/o reg) on all data: 0.34371006\n",
      "Test loss (w/o reg) on all data: 0.41158605\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0016675784\n",
      "Norm of the params: 2.5946515\n",
      "                Loss: fixed   5 labels. Loss 0.41159. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4101305\n",
      "Train loss (w/o reg) on all data: 0.40993392\n",
      "Test loss (w/o reg) on all data: 0.36339068\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0026098564\n",
      "Norm of the params: 1.982827\n",
      "              Random: fixed   1 labels. Loss 0.36339. Accuracy 0.844.\n",
      "### Flips: 10, rs: 30, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [190] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34405518\n",
      "Train loss (w/o reg) on all data: 0.34371695\n",
      "Test loss (w/o reg) on all data: 0.4118612\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.003835888\n",
      "Norm of the params: 2.6008687\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.41186. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [193] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34011528\n",
      "Train loss (w/o reg) on all data: 0.3397853\n",
      "Test loss (w/o reg) on all data: 0.4121632\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00070103526\n",
      "Norm of the params: 2.568897\n",
      "                Loss: fixed   6 labels. Loss 0.41216. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [318] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41013238\n",
      "Train loss (w/o reg) on all data: 0.409935\n",
      "Test loss (w/o reg) on all data: 0.36332852\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0062240902\n",
      "Norm of the params: 1.9868077\n",
      "              Random: fixed   1 labels. Loss 0.36333. Accuracy 0.844.\n",
      "### Flips: 10, rs: 30, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [320] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3401202\n",
      "Train loss (w/o reg) on all data: 0.33978882\n",
      "Test loss (w/o reg) on all data: 0.41173512\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0007437815\n",
      "Norm of the params: 2.5743363\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.41174. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [93] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3391791\n",
      "Train loss (w/o reg) on all data: 0.33884424\n",
      "Test loss (w/o reg) on all data: 0.41323653\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00036622267\n",
      "Norm of the params: 2.5878935\n",
      "                Loss: fixed   7 labels. Loss 0.41324. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [359] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39461422\n",
      "Train loss (w/o reg) on all data: 0.39440513\n",
      "Test loss (w/o reg) on all data: 0.37128147\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00084237015\n",
      "Norm of the params: 2.0449696\n",
      "              Random: fixed   2 labels. Loss 0.37128. Accuracy 0.844.\n",
      "### Flips: 10, rs: 30, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [331] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34151027\n",
      "Train loss (w/o reg) on all data: 0.3411452\n",
      "Test loss (w/o reg) on all data: 0.42014194\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016805307\n",
      "Norm of the params: 2.702131\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.42014. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [146] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3391789\n",
      "Train loss (w/o reg) on all data: 0.33884418\n",
      "Test loss (w/o reg) on all data: 0.41417652\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0022231303\n",
      "Norm of the params: 2.587305\n",
      "                Loss: fixed   7 labels. Loss 0.41418. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [301] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38361576\n",
      "Train loss (w/o reg) on all data: 0.38339272\n",
      "Test loss (w/o reg) on all data: 0.3702939\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0016215838\n",
      "Norm of the params: 2.1121252\n",
      "              Random: fixed   3 labels. Loss 0.37029. Accuracy 0.844.\n",
      "### Flips: 10, rs: 30, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [332] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34151143\n",
      "Train loss (w/o reg) on all data: 0.34114584\n",
      "Test loss (w/o reg) on all data: 0.42012325\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0004320098\n",
      "Norm of the params: 2.7040687\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.42012. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [129] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34430856\n",
      "Train loss (w/o reg) on all data: 0.34398317\n",
      "Test loss (w/o reg) on all data: 0.40029657\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.007469286\n",
      "Norm of the params: 2.55104\n",
      "                Loss: fixed   8 labels. Loss 0.40030. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [139] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36365145\n",
      "Train loss (w/o reg) on all data: 0.3633785\n",
      "Test loss (w/o reg) on all data: 0.374698\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001619238\n",
      "Norm of the params: 2.3364506\n",
      "              Random: fixed   4 labels. Loss 0.37470. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4273604\n",
      "Train loss (w/o reg) on all data: 0.4271544\n",
      "Test loss (w/o reg) on all data: 0.34943452\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0022264803\n",
      "Norm of the params: 2.0298133\n",
      "Flipped loss: 0.34943. Accuracy: 0.844\n",
      "### Flips: 10, rs: 31, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [129] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40096965\n",
      "Train loss (w/o reg) on all data: 0.400724\n",
      "Test loss (w/o reg) on all data: 0.36476338\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.008270635\n",
      "Norm of the params: 2.21659\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.36476. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [102] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37685248\n",
      "Train loss (w/o reg) on all data: 0.3765507\n",
      "Test loss (w/o reg) on all data: 0.38007277\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00208477\n",
      "Norm of the params: 2.4567096\n",
      "                Loss: fixed   4 labels. Loss 0.38007. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42736545\n",
      "Train loss (w/o reg) on all data: 0.42715725\n",
      "Test loss (w/o reg) on all data: 0.35072777\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.008968927\n",
      "Norm of the params: 2.0406454\n",
      "              Random: fixed   0 labels. Loss 0.35073. Accuracy 0.844.\n",
      "### Flips: 10, rs: 31, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38146576\n",
      "Train loss (w/o reg) on all data: 0.38118654\n",
      "Test loss (w/o reg) on all data: 0.37771153\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.005454954\n",
      "Norm of the params: 2.3631039\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.37771. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [124] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37111872\n",
      "Train loss (w/o reg) on all data: 0.37081283\n",
      "Test loss (w/o reg) on all data: 0.38518763\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001072668\n",
      "Norm of the params: 2.473471\n",
      "                Loss: fixed   5 labels. Loss 0.38519. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [289] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42123133\n",
      "Train loss (w/o reg) on all data: 0.4210201\n",
      "Test loss (w/o reg) on all data: 0.353906\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00058018655\n",
      "Norm of the params: 2.055357\n",
      "              Random: fixed   1 labels. Loss 0.35391. Accuracy 0.844.\n",
      "### Flips: 10, rs: 31, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [189] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37156206\n",
      "Train loss (w/o reg) on all data: 0.37127605\n",
      "Test loss (w/o reg) on all data: 0.3888078\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0023087182\n",
      "Norm of the params: 2.391707\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.38881. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [90] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35982797\n",
      "Train loss (w/o reg) on all data: 0.35950845\n",
      "Test loss (w/o reg) on all data: 0.39836094\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004493106\n",
      "Norm of the params: 2.5278904\n",
      "                Loss: fixed   7 labels. Loss 0.39836. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42123488\n",
      "Train loss (w/o reg) on all data: 0.4210236\n",
      "Test loss (w/o reg) on all data: 0.35362512\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0039007678\n",
      "Norm of the params: 2.0555503\n",
      "              Random: fixed   1 labels. Loss 0.35363. Accuracy 0.844.\n",
      "### Flips: 10, rs: 31, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35982552\n",
      "Train loss (w/o reg) on all data: 0.35950476\n",
      "Test loss (w/o reg) on all data: 0.39874962\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009265532\n",
      "Norm of the params: 2.5328004\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.39875. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [75] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35677537\n",
      "Train loss (w/o reg) on all data: 0.35643676\n",
      "Test loss (w/o reg) on all data: 0.4092908\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015889624\n",
      "Norm of the params: 2.602377\n",
      "                Loss: fixed   8 labels. Loss 0.40929. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [133] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42124003\n",
      "Train loss (w/o reg) on all data: 0.4210273\n",
      "Test loss (w/o reg) on all data: 0.35363436\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00079943024\n",
      "Norm of the params: 2.0626595\n",
      "              Random: fixed   1 labels. Loss 0.35363. Accuracy 0.844.\n",
      "### Flips: 10, rs: 31, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [139] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3567799\n",
      "Train loss (w/o reg) on all data: 0.35644192\n",
      "Test loss (w/o reg) on all data: 0.4104277\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00096171343\n",
      "Norm of the params: 2.6000013\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41043. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [107] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35613933\n",
      "Train loss (w/o reg) on all data: 0.3557896\n",
      "Test loss (w/o reg) on all data: 0.41050076\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004706924\n",
      "Norm of the params: 2.644746\n",
      "                Loss: fixed   9 labels. Loss 0.41050. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [165] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40780556\n",
      "Train loss (w/o reg) on all data: 0.4075818\n",
      "Test loss (w/o reg) on all data: 0.36999047\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004188782\n",
      "Norm of the params: 2.1154761\n",
      "              Random: fixed   2 labels. Loss 0.36999. Accuracy 0.800.\n",
      "### Flips: 10, rs: 31, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [330] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35677117\n",
      "Train loss (w/o reg) on all data: 0.3564288\n",
      "Test loss (w/o reg) on all data: 0.41058105\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00063133036\n",
      "Norm of the params: 2.6167724\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41058. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [100] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3592384\n",
      "Train loss (w/o reg) on all data: 0.3589126\n",
      "Test loss (w/o reg) on all data: 0.40789825\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006419536\n",
      "Norm of the params: 2.5526814\n",
      "                Loss: fixed  10 labels. Loss 0.40790. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40779594\n",
      "Train loss (w/o reg) on all data: 0.40757105\n",
      "Test loss (w/o reg) on all data: 0.37072292\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00024286905\n",
      "Norm of the params: 2.1208231\n",
      "              Random: fixed   2 labels. Loss 0.37072. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [186] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4548845\n",
      "Train loss (w/o reg) on all data: 0.45475614\n",
      "Test loss (w/o reg) on all data: 0.386521\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00092642836\n",
      "Norm of the params: 1.6021906\n",
      "Flipped loss: 0.38652. Accuracy: 0.822\n",
      "### Flips: 10, rs: 32, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [127] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41431782\n",
      "Train loss (w/o reg) on all data: 0.41412696\n",
      "Test loss (w/o reg) on all data: 0.36558753\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0018098122\n",
      "Norm of the params: 1.9537889\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.36559. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [127] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38818085\n",
      "Train loss (w/o reg) on all data: 0.38793075\n",
      "Test loss (w/o reg) on all data: 0.38076895\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012474356\n",
      "Norm of the params: 2.2365057\n",
      "                Loss: fixed   5 labels. Loss 0.38077. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45488065\n",
      "Train loss (w/o reg) on all data: 0.45475268\n",
      "Test loss (w/o reg) on all data: 0.38569367\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00155249\n",
      "Norm of the params: 1.599855\n",
      "              Random: fixed   0 labels. Loss 0.38569. Accuracy 0.822.\n",
      "### Flips: 10, rs: 32, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [67] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4035586\n",
      "Train loss (w/o reg) on all data: 0.403335\n",
      "Test loss (w/o reg) on all data: 0.37729192\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002434386\n",
      "Norm of the params: 2.1147053\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.37729. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36569226\n",
      "Train loss (w/o reg) on all data: 0.36538544\n",
      "Test loss (w/o reg) on all data: 0.41057715\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013000655\n",
      "Norm of the params: 2.4771554\n",
      "                Loss: fixed   7 labels. Loss 0.41058. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [208] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45488006\n",
      "Train loss (w/o reg) on all data: 0.4547515\n",
      "Test loss (w/o reg) on all data: 0.3858212\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0026333837\n",
      "Norm of the params: 1.6036218\n",
      "              Random: fixed   0 labels. Loss 0.38582. Accuracy 0.822.\n",
      "### Flips: 10, rs: 32, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38732782\n",
      "Train loss (w/o reg) on all data: 0.38705948\n",
      "Test loss (w/o reg) on all data: 0.38677868\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0044752634\n",
      "Norm of the params: 2.316627\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.38678. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [108] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3656918\n",
      "Train loss (w/o reg) on all data: 0.36538467\n",
      "Test loss (w/o reg) on all data: 0.4107136\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0004957099\n",
      "Norm of the params: 2.47842\n",
      "                Loss: fixed   7 labels. Loss 0.41071. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45355546\n",
      "Train loss (w/o reg) on all data: 0.45342734\n",
      "Test loss (w/o reg) on all data: 0.38870007\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001394972\n",
      "Norm of the params: 1.6007735\n",
      "              Random: fixed   1 labels. Loss 0.38870. Accuracy 0.822.\n",
      "### Flips: 10, rs: 32, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [286] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3701253\n",
      "Train loss (w/o reg) on all data: 0.36981863\n",
      "Test loss (w/o reg) on all data: 0.39804757\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00045487582\n",
      "Norm of the params: 2.476505\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.39805. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35556608\n",
      "Train loss (w/o reg) on all data: 0.35523662\n",
      "Test loss (w/o reg) on all data: 0.41861594\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009615547\n",
      "Norm of the params: 2.5669322\n",
      "                Loss: fixed   9 labels. Loss 0.41862. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4535445\n",
      "Train loss (w/o reg) on all data: 0.45341542\n",
      "Test loss (w/o reg) on all data: 0.38889143\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005399162\n",
      "Norm of the params: 1.606774\n",
      "              Random: fixed   1 labels. Loss 0.38889. Accuracy 0.822.\n",
      "### Flips: 10, rs: 32, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [199] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3611058\n",
      "Train loss (w/o reg) on all data: 0.36079356\n",
      "Test loss (w/o reg) on all data: 0.41057947\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010393484\n",
      "Norm of the params: 2.4989738\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41058. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [130] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3555659\n",
      "Train loss (w/o reg) on all data: 0.3552373\n",
      "Test loss (w/o reg) on all data: 0.4185715\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.008302098\n",
      "Norm of the params: 2.5636466\n",
      "                Loss: fixed   9 labels. Loss 0.41857. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [165] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4535441\n",
      "Train loss (w/o reg) on all data: 0.45341504\n",
      "Test loss (w/o reg) on all data: 0.3890667\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00077031227\n",
      "Norm of the params: 1.6067452\n",
      "              Random: fixed   1 labels. Loss 0.38907. Accuracy 0.822.\n",
      "### Flips: 10, rs: 32, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36110553\n",
      "Train loss (w/o reg) on all data: 0.3607937\n",
      "Test loss (w/o reg) on all data: 0.41103965\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009516977\n",
      "Norm of the params: 2.4972482\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41104. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [114] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35556823\n",
      "Train loss (w/o reg) on all data: 0.3552414\n",
      "Test loss (w/o reg) on all data: 0.41833267\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018285137\n",
      "Norm of the params: 2.5566967\n",
      "                Loss: fixed   9 labels. Loss 0.41833. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45354378\n",
      "Train loss (w/o reg) on all data: 0.45341486\n",
      "Test loss (w/o reg) on all data: 0.38897172\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007128973\n",
      "Norm of the params: 1.6057043\n",
      "              Random: fixed   1 labels. Loss 0.38897. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41502336\n",
      "Train loss (w/o reg) on all data: 0.41478068\n",
      "Test loss (w/o reg) on all data: 0.38342172\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018544561\n",
      "Norm of the params: 2.2031364\n",
      "Flipped loss: 0.38342. Accuracy: 0.822\n",
      "### Flips: 10, rs: 33, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [118] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3766563\n",
      "Train loss (w/o reg) on all data: 0.37638026\n",
      "Test loss (w/o reg) on all data: 0.3905259\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00059008313\n",
      "Norm of the params: 2.3495445\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.39053. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [184] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34983724\n",
      "Train loss (w/o reg) on all data: 0.34947744\n",
      "Test loss (w/o reg) on all data: 0.42172164\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0003827073\n",
      "Norm of the params: 2.6825383\n",
      "                Loss: fixed   5 labels. Loss 0.42172. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [168] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4150225\n",
      "Train loss (w/o reg) on all data: 0.41478053\n",
      "Test loss (w/o reg) on all data: 0.3839244\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00047238753\n",
      "Norm of the params: 2.1998591\n",
      "              Random: fixed   0 labels. Loss 0.38392. Accuracy 0.822.\n",
      "### Flips: 10, rs: 33, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [114] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36042324\n",
      "Train loss (w/o reg) on all data: 0.36010402\n",
      "Test loss (w/o reg) on all data: 0.3943796\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001392613\n",
      "Norm of the params: 2.5266492\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.39438. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [148] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34157434\n",
      "Train loss (w/o reg) on all data: 0.3411832\n",
      "Test loss (w/o reg) on all data: 0.41339204\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0024373725\n",
      "Norm of the params: 2.7970335\n",
      "                Loss: fixed   6 labels. Loss 0.41339. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40564245\n",
      "Train loss (w/o reg) on all data: 0.40538645\n",
      "Test loss (w/o reg) on all data: 0.3949802\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014181304\n",
      "Norm of the params: 2.262692\n",
      "              Random: fixed   1 labels. Loss 0.39498. Accuracy 0.822.\n",
      "### Flips: 10, rs: 33, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3604187\n",
      "Train loss (w/o reg) on all data: 0.360099\n",
      "Test loss (w/o reg) on all data: 0.3952777\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012384217\n",
      "Norm of the params: 2.528738\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.39528. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [159] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33380932\n",
      "Train loss (w/o reg) on all data: 0.33342296\n",
      "Test loss (w/o reg) on all data: 0.41633075\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011366556\n",
      "Norm of the params: 2.779819\n",
      "                Loss: fixed   8 labels. Loss 0.41633. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [301] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4017357\n",
      "Train loss (w/o reg) on all data: 0.40148732\n",
      "Test loss (w/o reg) on all data: 0.40121812\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005132123\n",
      "Norm of the params: 2.2288225\n",
      "              Random: fixed   2 labels. Loss 0.40122. Accuracy 0.822.\n",
      "### Flips: 10, rs: 33, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [299] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3496646\n",
      "Train loss (w/o reg) on all data: 0.3492954\n",
      "Test loss (w/o reg) on all data: 0.39053735\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00061250845\n",
      "Norm of the params: 2.7173665\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.39054. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3338106\n",
      "Train loss (w/o reg) on all data: 0.3334264\n",
      "Test loss (w/o reg) on all data: 0.41600448\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010222003\n",
      "Norm of the params: 2.7720816\n",
      "                Loss: fixed   8 labels. Loss 0.41600. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [169] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39294344\n",
      "Train loss (w/o reg) on all data: 0.39266464\n",
      "Test loss (w/o reg) on all data: 0.39342797\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00070377917\n",
      "Norm of the params: 2.3614123\n",
      "              Random: fixed   3 labels. Loss 0.39343. Accuracy 0.822.\n",
      "### Flips: 10, rs: 33, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [143] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33381832\n",
      "Train loss (w/o reg) on all data: 0.33343568\n",
      "Test loss (w/o reg) on all data: 0.41510963\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00085929275\n",
      "Norm of the params: 2.766312\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41511. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33381826\n",
      "Train loss (w/o reg) on all data: 0.33343562\n",
      "Test loss (w/o reg) on all data: 0.41510633\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010423042\n",
      "Norm of the params: 2.7663121\n",
      "                Loss: fixed   8 labels. Loss 0.41511. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40021586\n",
      "Train loss (w/o reg) on all data: 0.3999408\n",
      "Test loss (w/o reg) on all data: 0.38509107\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014575693\n",
      "Norm of the params: 2.34549\n",
      "              Random: fixed   4 labels. Loss 0.38509. Accuracy 0.822.\n",
      "### Flips: 10, rs: 33, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33381033\n",
      "Train loss (w/o reg) on all data: 0.33342293\n",
      "Test loss (w/o reg) on all data: 0.4161695\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00082137424\n",
      "Norm of the params: 2.7835112\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41617. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [22] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33380964\n",
      "Train loss (w/o reg) on all data: 0.33342236\n",
      "Test loss (w/o reg) on all data: 0.41646278\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0002438624\n",
      "Norm of the params: 2.7831008\n",
      "                Loss: fixed   8 labels. Loss 0.41646. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4002151\n",
      "Train loss (w/o reg) on all data: 0.3999405\n",
      "Test loss (w/o reg) on all data: 0.3850427\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00055090105\n",
      "Norm of the params: 2.343544\n",
      "              Random: fixed   4 labels. Loss 0.38504. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42501977\n",
      "Train loss (w/o reg) on all data: 0.4248678\n",
      "Test loss (w/o reg) on all data: 0.3790522\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010309028\n",
      "Norm of the params: 1.7432749\n",
      "Flipped loss: 0.37905. Accuracy: 0.800\n",
      "### Flips: 10, rs: 34, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [272] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36959636\n",
      "Train loss (w/o reg) on all data: 0.36932424\n",
      "Test loss (w/o reg) on all data: 0.3842289\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0004987183\n",
      "Norm of the params: 2.3329308\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.38423. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [159] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35585183\n",
      "Train loss (w/o reg) on all data: 0.3555442\n",
      "Test loss (w/o reg) on all data: 0.386008\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00065289455\n",
      "Norm of the params: 2.4804533\n",
      "                Loss: fixed   4 labels. Loss 0.38601. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42501518\n",
      "Train loss (w/o reg) on all data: 0.42486206\n",
      "Test loss (w/o reg) on all data: 0.38004196\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00092636596\n",
      "Norm of the params: 1.7500185\n",
      "              Random: fixed   0 labels. Loss 0.38004. Accuracy 0.800.\n",
      "### Flips: 10, rs: 34, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [240] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35585147\n",
      "Train loss (w/o reg) on all data: 0.35554516\n",
      "Test loss (w/o reg) on all data: 0.38599223\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004384818\n",
      "Norm of the params: 2.4751556\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.38599. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [111] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34108147\n",
      "Train loss (w/o reg) on all data: 0.34073108\n",
      "Test loss (w/o reg) on all data: 0.41226107\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00061676354\n",
      "Norm of the params: 2.6472006\n",
      "                Loss: fixed   6 labels. Loss 0.41226. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42501685\n",
      "Train loss (w/o reg) on all data: 0.42486316\n",
      "Test loss (w/o reg) on all data: 0.3800123\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005886854\n",
      "Norm of the params: 1.7531891\n",
      "              Random: fixed   0 labels. Loss 0.38001. Accuracy 0.800.\n",
      "### Flips: 10, rs: 34, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [288] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3558507\n",
      "Train loss (w/o reg) on all data: 0.35554296\n",
      "Test loss (w/o reg) on all data: 0.38622263\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00042454037\n",
      "Norm of the params: 2.4809315\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.38622. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [139] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34108123\n",
      "Train loss (w/o reg) on all data: 0.34073028\n",
      "Test loss (w/o reg) on all data: 0.4124006\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001572073\n",
      "Norm of the params: 2.6493397\n",
      "                Loss: fixed   6 labels. Loss 0.41240. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [168] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4250211\n",
      "Train loss (w/o reg) on all data: 0.42486593\n",
      "Test loss (w/o reg) on all data: 0.3799586\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019403193\n",
      "Norm of the params: 1.7617475\n",
      "              Random: fixed   0 labels. Loss 0.37996. Accuracy 0.800.\n",
      "### Flips: 10, rs: 34, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34921384\n",
      "Train loss (w/o reg) on all data: 0.34887505\n",
      "Test loss (w/o reg) on all data: 0.38957223\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0032560984\n",
      "Norm of the params: 2.6029987\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.38957. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [134] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34108463\n",
      "Train loss (w/o reg) on all data: 0.34073448\n",
      "Test loss (w/o reg) on all data: 0.4120389\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007657733\n",
      "Norm of the params: 2.646349\n",
      "                Loss: fixed   6 labels. Loss 0.41204. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [309] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42501915\n",
      "Train loss (w/o reg) on all data: 0.42486608\n",
      "Test loss (w/o reg) on all data: 0.380082\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0028581505\n",
      "Norm of the params: 1.7496092\n",
      "              Random: fixed   0 labels. Loss 0.38008. Accuracy 0.800.\n",
      "### Flips: 10, rs: 34, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34921372\n",
      "Train loss (w/o reg) on all data: 0.34887448\n",
      "Test loss (w/o reg) on all data: 0.38952333\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00089077745\n",
      "Norm of the params: 2.6047797\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.38952. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [75] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34361196\n",
      "Train loss (w/o reg) on all data: 0.343266\n",
      "Test loss (w/o reg) on all data: 0.40065515\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0004526879\n",
      "Norm of the params: 2.6303844\n",
      "                Loss: fixed   7 labels. Loss 0.40066. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [211] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42501557\n",
      "Train loss (w/o reg) on all data: 0.42486244\n",
      "Test loss (w/o reg) on all data: 0.3799097\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002993177\n",
      "Norm of the params: 1.7500132\n",
      "              Random: fixed   0 labels. Loss 0.37991. Accuracy 0.800.\n",
      "### Flips: 10, rs: 34, checks: 60\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3505522\n",
      "Train loss (w/o reg) on all data: 0.35022298\n",
      "Test loss (w/o reg) on all data: 0.39834467\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.006347178\n",
      "Norm of the params: 2.5660672\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.39834. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [179] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34361038\n",
      "Train loss (w/o reg) on all data: 0.34326547\n",
      "Test loss (w/o reg) on all data: 0.4010806\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00030059746\n",
      "Norm of the params: 2.626373\n",
      "                Loss: fixed   7 labels. Loss 0.40108. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40715343\n",
      "Train loss (w/o reg) on all data: 0.40695375\n",
      "Test loss (w/o reg) on all data: 0.3886002\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00049243745\n",
      "Norm of the params: 1.9984252\n",
      "              Random: fixed   1 labels. Loss 0.38860. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46285093\n",
      "Train loss (w/o reg) on all data: 0.46266526\n",
      "Test loss (w/o reg) on all data: 0.40772378\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006634216\n",
      "Norm of the params: 1.9269559\n",
      "Flipped loss: 0.40772. Accuracy: 0.800\n",
      "### Flips: 10, rs: 35, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [96] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42243525\n",
      "Train loss (w/o reg) on all data: 0.4222054\n",
      "Test loss (w/o reg) on all data: 0.39676836\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010650874\n",
      "Norm of the params: 2.1441205\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.39677. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3967563\n",
      "Train loss (w/o reg) on all data: 0.39647323\n",
      "Test loss (w/o reg) on all data: 0.4039698\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0053186193\n",
      "Norm of the params: 2.3793838\n",
      "                Loss: fixed   5 labels. Loss 0.40397. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [135] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4628523\n",
      "Train loss (w/o reg) on all data: 0.46266514\n",
      "Test loss (w/o reg) on all data: 0.4084387\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004793983\n",
      "Norm of the params: 1.9346709\n",
      "              Random: fixed   0 labels. Loss 0.40844. Accuracy 0.800.\n",
      "### Flips: 10, rs: 35, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [189] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39247915\n",
      "Train loss (w/o reg) on all data: 0.39221585\n",
      "Test loss (w/o reg) on all data: 0.39071426\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003152588\n",
      "Norm of the params: 2.294854\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.39071. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3707758\n",
      "Train loss (w/o reg) on all data: 0.3704473\n",
      "Test loss (w/o reg) on all data: 0.41366315\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00054538966\n",
      "Norm of the params: 2.563131\n",
      "                Loss: fixed   8 labels. Loss 0.41366. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44720972\n",
      "Train loss (w/o reg) on all data: 0.44700792\n",
      "Test loss (w/o reg) on all data: 0.39803553\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017845626\n",
      "Norm of the params: 2.0089924\n",
      "              Random: fixed   1 labels. Loss 0.39804. Accuracy 0.800.\n",
      "### Flips: 10, rs: 35, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [155] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3773111\n",
      "Train loss (w/o reg) on all data: 0.37702626\n",
      "Test loss (w/o reg) on all data: 0.40408304\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.000959635\n",
      "Norm of the params: 2.3868952\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.40408. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [68] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3592503\n",
      "Train loss (w/o reg) on all data: 0.3589316\n",
      "Test loss (w/o reg) on all data: 0.40863127\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016239932\n",
      "Norm of the params: 2.524663\n",
      "                Loss: fixed  10 labels. Loss 0.40863. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [289] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44721076\n",
      "Train loss (w/o reg) on all data: 0.44701055\n",
      "Test loss (w/o reg) on all data: 0.39799488\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0034011337\n",
      "Norm of the params: 2.001081\n",
      "              Random: fixed   1 labels. Loss 0.39799. Accuracy 0.800.\n",
      "### Flips: 10, rs: 35, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3773089\n",
      "Train loss (w/o reg) on all data: 0.37702468\n",
      "Test loss (w/o reg) on all data: 0.40352473\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0026314503\n",
      "Norm of the params: 2.3841774\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.40352. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [87] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35923305\n",
      "Train loss (w/o reg) on all data: 0.35891345\n",
      "Test loss (w/o reg) on all data: 0.4074897\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008061097\n",
      "Norm of the params: 2.528208\n",
      "                Loss: fixed  10 labels. Loss 0.40749. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44720885\n",
      "Train loss (w/o reg) on all data: 0.4470076\n",
      "Test loss (w/o reg) on all data: 0.39838296\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010431452\n",
      "Norm of the params: 2.0062945\n",
      "              Random: fixed   1 labels. Loss 0.39838. Accuracy 0.800.\n",
      "### Flips: 10, rs: 35, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36844054\n",
      "Train loss (w/o reg) on all data: 0.36815304\n",
      "Test loss (w/o reg) on all data: 0.41384417\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00270816\n",
      "Norm of the params: 2.39791\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.41384. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [103] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35923332\n",
      "Train loss (w/o reg) on all data: 0.3589126\n",
      "Test loss (w/o reg) on all data: 0.40738237\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005184815\n",
      "Norm of the params: 2.532684\n",
      "                Loss: fixed  10 labels. Loss 0.40738. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44721144\n",
      "Train loss (w/o reg) on all data: 0.44701076\n",
      "Test loss (w/o reg) on all data: 0.39807132\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.010346183\n",
      "Norm of the params: 2.0034869\n",
      "              Random: fixed   1 labels. Loss 0.39807. Accuracy 0.800.\n",
      "### Flips: 10, rs: 35, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35923633\n",
      "Train loss (w/o reg) on all data: 0.35891312\n",
      "Test loss (w/o reg) on all data: 0.40758654\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016071473\n",
      "Norm of the params: 2.5424576\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.40759. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35923633\n",
      "Train loss (w/o reg) on all data: 0.35891312\n",
      "Test loss (w/o reg) on all data: 0.40759087\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012611017\n",
      "Norm of the params: 2.5424578\n",
      "                Loss: fixed  10 labels. Loss 0.40759. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [272] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44721112\n",
      "Train loss (w/o reg) on all data: 0.44700953\n",
      "Test loss (w/o reg) on all data: 0.39813572\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014781783\n",
      "Norm of the params: 2.0079327\n",
      "              Random: fixed   1 labels. Loss 0.39814. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [290] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40466684\n",
      "Train loss (w/o reg) on all data: 0.40446052\n",
      "Test loss (w/o reg) on all data: 0.40819713\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00028975846\n",
      "Norm of the params: 2.0313728\n",
      "Flipped loss: 0.40820. Accuracy: 0.800\n",
      "### Flips: 10, rs: 36, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3651365\n",
      "Train loss (w/o reg) on all data: 0.36483797\n",
      "Test loss (w/o reg) on all data: 0.43466344\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00085973414\n",
      "Norm of the params: 2.4434822\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.43466. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [139] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34027836\n",
      "Train loss (w/o reg) on all data: 0.33991444\n",
      "Test loss (w/o reg) on all data: 0.43245658\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00037505862\n",
      "Norm of the params: 2.697833\n",
      "                Loss: fixed   4 labels. Loss 0.43246. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41604015\n",
      "Train loss (w/o reg) on all data: 0.41585067\n",
      "Test loss (w/o reg) on all data: 0.39665028\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013590305\n",
      "Norm of the params: 1.9466983\n",
      "              Random: fixed   1 labels. Loss 0.39665. Accuracy 0.800.\n",
      "### Flips: 10, rs: 36, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [430] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33876184\n",
      "Train loss (w/o reg) on all data: 0.3383906\n",
      "Test loss (w/o reg) on all data: 0.45169327\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010059902\n",
      "Norm of the params: 2.7248409\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.45169. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [81] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34027788\n",
      "Train loss (w/o reg) on all data: 0.33991358\n",
      "Test loss (w/o reg) on all data: 0.43253586\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00023385849\n",
      "Norm of the params: 2.6993053\n",
      "                Loss: fixed   4 labels. Loss 0.43254. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4160374\n",
      "Train loss (w/o reg) on all data: 0.41584677\n",
      "Test loss (w/o reg) on all data: 0.39629686\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0030165398\n",
      "Norm of the params: 1.9526321\n",
      "              Random: fixed   1 labels. Loss 0.39630. Accuracy 0.800.\n",
      "### Flips: 10, rs: 36, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [349] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3360132\n",
      "Train loss (w/o reg) on all data: 0.3356351\n",
      "Test loss (w/o reg) on all data: 0.4574254\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0014666166\n",
      "Norm of the params: 2.7499487\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.45743. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [108] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33637884\n",
      "Train loss (w/o reg) on all data: 0.3360064\n",
      "Test loss (w/o reg) on all data: 0.43888542\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00023320415\n",
      "Norm of the params: 2.7292092\n",
      "                Loss: fixed   5 labels. Loss 0.43889. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [282] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41603833\n",
      "Train loss (w/o reg) on all data: 0.41584757\n",
      "Test loss (w/o reg) on all data: 0.39615774\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004090238\n",
      "Norm of the params: 1.9533346\n",
      "              Random: fixed   1 labels. Loss 0.39616. Accuracy 0.800.\n",
      "### Flips: 10, rs: 36, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [326] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3360143\n",
      "Train loss (w/o reg) on all data: 0.3356368\n",
      "Test loss (w/o reg) on all data: 0.4565823\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0034545087\n",
      "Norm of the params: 2.7477658\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.45658. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [90] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33638075\n",
      "Train loss (w/o reg) on all data: 0.33600894\n",
      "Test loss (w/o reg) on all data: 0.43842036\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00413951\n",
      "Norm of the params: 2.7270043\n",
      "                Loss: fixed   5 labels. Loss 0.43842. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [266] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41603974\n",
      "Train loss (w/o reg) on all data: 0.4158495\n",
      "Test loss (w/o reg) on all data: 0.39597076\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003204063\n",
      "Norm of the params: 1.9504858\n",
      "              Random: fixed   1 labels. Loss 0.39597. Accuracy 0.800.\n",
      "### Flips: 10, rs: 36, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [325] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33534458\n",
      "Train loss (w/o reg) on all data: 0.3349626\n",
      "Test loss (w/o reg) on all data: 0.4437555\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0005958856\n",
      "Norm of the params: 2.763958\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.44376. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [69] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33619994\n",
      "Train loss (w/o reg) on all data: 0.33582595\n",
      "Test loss (w/o reg) on all data: 0.4245572\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00048096257\n",
      "Norm of the params: 2.7349641\n",
      "                Loss: fixed   6 labels. Loss 0.42456. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [369] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4160377\n",
      "Train loss (w/o reg) on all data: 0.415846\n",
      "Test loss (w/o reg) on all data: 0.39666897\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014109621\n",
      "Norm of the params: 1.9580781\n",
      "              Random: fixed   1 labels. Loss 0.39667. Accuracy 0.800.\n",
      "### Flips: 10, rs: 36, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33534402\n",
      "Train loss (w/o reg) on all data: 0.33496425\n",
      "Test loss (w/o reg) on all data: 0.4431611\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0008116602\n",
      "Norm of the params: 2.7559943\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.44316. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [93] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33847922\n",
      "Train loss (w/o reg) on all data: 0.33810455\n",
      "Test loss (w/o reg) on all data: 0.41783252\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0006011249\n",
      "Norm of the params: 2.7374225\n",
      "                Loss: fixed   7 labels. Loss 0.41783. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4160368\n",
      "Train loss (w/o reg) on all data: 0.41584677\n",
      "Test loss (w/o reg) on all data: 0.39658213\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002330065\n",
      "Norm of the params: 1.9496124\n",
      "              Random: fixed   1 labels. Loss 0.39658. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [195] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41587642\n",
      "Train loss (w/o reg) on all data: 0.4156556\n",
      "Test loss (w/o reg) on all data: 0.37856814\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0023284815\n",
      "Norm of the params: 2.1013904\n",
      "Flipped loss: 0.37857. Accuracy: 0.800\n",
      "### Flips: 10, rs: 37, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41587633\n",
      "Train loss (w/o reg) on all data: 0.41565552\n",
      "Test loss (w/o reg) on all data: 0.3785725\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005048381\n",
      "Norm of the params: 2.1013904\n",
      "     Influence (LOO): fixed   0 labels. Loss 0.37857. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [280] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37591663\n",
      "Train loss (w/o reg) on all data: 0.3756308\n",
      "Test loss (w/o reg) on all data: 0.39870185\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009775342\n",
      "Norm of the params: 2.3909373\n",
      "                Loss: fixed   3 labels. Loss 0.39870. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4158754\n",
      "Train loss (w/o reg) on all data: 0.41565648\n",
      "Test loss (w/o reg) on all data: 0.3790657\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0021956523\n",
      "Norm of the params: 2.0924459\n",
      "              Random: fixed   0 labels. Loss 0.37907. Accuracy 0.800.\n",
      "### Flips: 10, rs: 37, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [173] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38186148\n",
      "Train loss (w/o reg) on all data: 0.38154203\n",
      "Test loss (w/o reg) on all data: 0.37978613\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00048487607\n",
      "Norm of the params: 2.5276651\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.37979. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36947\n",
      "Train loss (w/o reg) on all data: 0.3691856\n",
      "Test loss (w/o reg) on all data: 0.40742946\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00059922505\n",
      "Norm of the params: 2.3849561\n",
      "                Loss: fixed   4 labels. Loss 0.40743. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [115] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39795607\n",
      "Train loss (w/o reg) on all data: 0.39770246\n",
      "Test loss (w/o reg) on all data: 0.39547196\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016114574\n",
      "Norm of the params: 2.2522411\n",
      "              Random: fixed   2 labels. Loss 0.39547. Accuracy 0.800.\n",
      "### Flips: 10, rs: 37, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [94] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36725345\n",
      "Train loss (w/o reg) on all data: 0.36691347\n",
      "Test loss (w/o reg) on all data: 0.39204025\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00039406394\n",
      "Norm of the params: 2.6076393\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.39204. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [69] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35708135\n",
      "Train loss (w/o reg) on all data: 0.35672572\n",
      "Test loss (w/o reg) on all data: 0.39688975\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012993312\n",
      "Norm of the params: 2.6669369\n",
      "                Loss: fixed   7 labels. Loss 0.39689. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [140] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39794612\n",
      "Train loss (w/o reg) on all data: 0.3976938\n",
      "Test loss (w/o reg) on all data: 0.39495736\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001785362\n",
      "Norm of the params: 2.2464023\n",
      "              Random: fixed   2 labels. Loss 0.39496. Accuracy 0.800.\n",
      "### Flips: 10, rs: 37, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [195] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3570718\n",
      "Train loss (w/o reg) on all data: 0.35671118\n",
      "Test loss (w/o reg) on all data: 0.39788496\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.007982627\n",
      "Norm of the params: 2.685546\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.39788. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [92] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35447004\n",
      "Train loss (w/o reg) on all data: 0.35412806\n",
      "Test loss (w/o reg) on all data: 0.4099767\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0003689792\n",
      "Norm of the params: 2.615326\n",
      "                Loss: fixed   8 labels. Loss 0.40998. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39794526\n",
      "Train loss (w/o reg) on all data: 0.39769235\n",
      "Test loss (w/o reg) on all data: 0.39484277\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007129612\n",
      "Norm of the params: 2.2490115\n",
      "              Random: fixed   2 labels. Loss 0.39484. Accuracy 0.800.\n",
      "### Flips: 10, rs: 37, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [116] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35448128\n",
      "Train loss (w/o reg) on all data: 0.35413674\n",
      "Test loss (w/o reg) on all data: 0.40988863\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00088087854\n",
      "Norm of the params: 2.6250534\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.40989. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35448128\n",
      "Train loss (w/o reg) on all data: 0.35413674\n",
      "Test loss (w/o reg) on all data: 0.40989578\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0029465277\n",
      "Norm of the params: 2.625053\n",
      "                Loss: fixed   8 labels. Loss 0.40990. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39421874\n",
      "Train loss (w/o reg) on all data: 0.39395124\n",
      "Test loss (w/o reg) on all data: 0.391915\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008387717\n",
      "Norm of the params: 2.3130312\n",
      "              Random: fixed   3 labels. Loss 0.39191. Accuracy 0.800.\n",
      "### Flips: 10, rs: 37, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [286] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35446742\n",
      "Train loss (w/o reg) on all data: 0.3541222\n",
      "Test loss (w/o reg) on all data: 0.4108169\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006238737\n",
      "Norm of the params: 2.62766\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41082. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35446745\n",
      "Train loss (w/o reg) on all data: 0.35412222\n",
      "Test loss (w/o reg) on all data: 0.41081527\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00043633368\n",
      "Norm of the params: 2.62766\n",
      "                Loss: fixed   8 labels. Loss 0.41082. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39422128\n",
      "Train loss (w/o reg) on all data: 0.39395216\n",
      "Test loss (w/o reg) on all data: 0.39234754\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00091002096\n",
      "Norm of the params: 2.3200085\n",
      "              Random: fixed   3 labels. Loss 0.39235. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45227116\n",
      "Train loss (w/o reg) on all data: 0.45210558\n",
      "Test loss (w/o reg) on all data: 0.37417322\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.011737805\n",
      "Norm of the params: 1.8198025\n",
      "Flipped loss: 0.37417. Accuracy: 0.778\n",
      "### Flips: 10, rs: 38, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [130] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40078926\n",
      "Train loss (w/o reg) on all data: 0.40057975\n",
      "Test loss (w/o reg) on all data: 0.3828138\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011232173\n",
      "Norm of the params: 2.0469763\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.38281. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36248565\n",
      "Train loss (w/o reg) on all data: 0.36218673\n",
      "Test loss (w/o reg) on all data: 0.39461\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0008924008\n",
      "Norm of the params: 2.4450576\n",
      "                Loss: fixed   6 labels. Loss 0.39461. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [120] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4523043\n",
      "Train loss (w/o reg) on all data: 0.45213738\n",
      "Test loss (w/o reg) on all data: 0.37534624\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0033504162\n",
      "Norm of the params: 1.827108\n",
      "              Random: fixed   0 labels. Loss 0.37535. Accuracy 0.778.\n",
      "### Flips: 10, rs: 38, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3893848\n",
      "Train loss (w/o reg) on all data: 0.38914382\n",
      "Test loss (w/o reg) on all data: 0.3818348\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0003962845\n",
      "Norm of the params: 2.1953125\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.38183. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35708117\n",
      "Train loss (w/o reg) on all data: 0.35676286\n",
      "Test loss (w/o reg) on all data: 0.40232766\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00024809613\n",
      "Norm of the params: 2.5232012\n",
      "                Loss: fixed   7 labels. Loss 0.40233. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [141] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43975022\n",
      "Train loss (w/o reg) on all data: 0.43955338\n",
      "Test loss (w/o reg) on all data: 0.37428293\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0016025945\n",
      "Norm of the params: 1.9841418\n",
      "              Random: fixed   1 labels. Loss 0.37428. Accuracy 0.778.\n",
      "### Flips: 10, rs: 38, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37539297\n",
      "Train loss (w/o reg) on all data: 0.37510577\n",
      "Test loss (w/o reg) on all data: 0.38377026\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0023244612\n",
      "Norm of the params: 2.3966684\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.38377. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35072643\n",
      "Train loss (w/o reg) on all data: 0.3503859\n",
      "Test loss (w/o reg) on all data: 0.41971174\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002263557\n",
      "Norm of the params: 2.609659\n",
      "                Loss: fixed   8 labels. Loss 0.41971. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4397278\n",
      "Train loss (w/o reg) on all data: 0.4395311\n",
      "Test loss (w/o reg) on all data: 0.37320873\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0009985868\n",
      "Norm of the params: 1.9836162\n",
      "              Random: fixed   1 labels. Loss 0.37321. Accuracy 0.778.\n",
      "### Flips: 10, rs: 38, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [315] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35072964\n",
      "Train loss (w/o reg) on all data: 0.35039034\n",
      "Test loss (w/o reg) on all data: 0.41915622\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0056109177\n",
      "Norm of the params: 2.6049967\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41916. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [60] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3470772\n",
      "Train loss (w/o reg) on all data: 0.34672827\n",
      "Test loss (w/o reg) on all data: 0.42321715\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0003242763\n",
      "Norm of the params: 2.6416404\n",
      "                Loss: fixed   9 labels. Loss 0.42322. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [335] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42750418\n",
      "Train loss (w/o reg) on all data: 0.4272759\n",
      "Test loss (w/o reg) on all data: 0.3716525\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0035151367\n",
      "Norm of the params: 2.1367767\n",
      "              Random: fixed   2 labels. Loss 0.37165. Accuracy 0.778.\n",
      "### Flips: 10, rs: 38, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [226] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35072985\n",
      "Train loss (w/o reg) on all data: 0.35039103\n",
      "Test loss (w/o reg) on all data: 0.41850442\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00632026\n",
      "Norm of the params: 2.6031835\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41850. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [78] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34707698\n",
      "Train loss (w/o reg) on all data: 0.34672892\n",
      "Test loss (w/o reg) on all data: 0.42289373\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014669392\n",
      "Norm of the params: 2.6384227\n",
      "                Loss: fixed   9 labels. Loss 0.42289. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4102003\n",
      "Train loss (w/o reg) on all data: 0.40994963\n",
      "Test loss (w/o reg) on all data: 0.3759925\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0007864829\n",
      "Norm of the params: 2.2390187\n",
      "              Random: fixed   3 labels. Loss 0.37599. Accuracy 0.778.\n",
      "### Flips: 10, rs: 38, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [347] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35072696\n",
      "Train loss (w/o reg) on all data: 0.35038733\n",
      "Test loss (w/o reg) on all data: 0.4193617\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0014020454\n",
      "Norm of the params: 2.606213\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41936. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [84] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3470773\n",
      "Train loss (w/o reg) on all data: 0.34672874\n",
      "Test loss (w/o reg) on all data: 0.42313886\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009074679\n",
      "Norm of the params: 2.6403322\n",
      "                Loss: fixed   9 labels. Loss 0.42314. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [324] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41019818\n",
      "Train loss (w/o reg) on all data: 0.40994802\n",
      "Test loss (w/o reg) on all data: 0.37610677\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0007840904\n",
      "Norm of the params: 2.236792\n",
      "              Random: fixed   3 labels. Loss 0.37611. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4029069\n",
      "Train loss (w/o reg) on all data: 0.40265146\n",
      "Test loss (w/o reg) on all data: 0.39279583\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007043687\n",
      "Norm of the params: 2.260249\n",
      "Flipped loss: 0.39280. Accuracy: 0.822\n",
      "### Flips: 10, rs: 39, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [163] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37658557\n",
      "Train loss (w/o reg) on all data: 0.37626022\n",
      "Test loss (w/o reg) on all data: 0.40141705\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0031834834\n",
      "Norm of the params: 2.5508533\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.40142. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [147] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36452243\n",
      "Train loss (w/o reg) on all data: 0.36416546\n",
      "Test loss (w/o reg) on all data: 0.41703397\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00034766484\n",
      "Norm of the params: 2.6719303\n",
      "                Loss: fixed   3 labels. Loss 0.41703. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [179] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38568553\n",
      "Train loss (w/o reg) on all data: 0.38536385\n",
      "Test loss (w/o reg) on all data: 0.39432475\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00047784665\n",
      "Norm of the params: 2.5364413\n",
      "              Random: fixed   1 labels. Loss 0.39432. Accuracy 0.822.\n",
      "### Flips: 10, rs: 39, checks: 20\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [147] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36711445\n",
      "Train loss (w/o reg) on all data: 0.3668194\n",
      "Test loss (w/o reg) on all data: 0.39459318\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015088298\n",
      "Norm of the params: 2.4291503\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.39459. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [142] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36452848\n",
      "Train loss (w/o reg) on all data: 0.36417425\n",
      "Test loss (w/o reg) on all data: 0.41667217\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007448513\n",
      "Norm of the params: 2.6616852\n",
      "                Loss: fixed   3 labels. Loss 0.41667. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [152] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38568705\n",
      "Train loss (w/o reg) on all data: 0.38536474\n",
      "Test loss (w/o reg) on all data: 0.3944162\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0027327312\n",
      "Norm of the params: 2.538888\n",
      "              Random: fixed   1 labels. Loss 0.39442. Accuracy 0.822.\n",
      "### Flips: 10, rs: 39, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [101] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36711425\n",
      "Train loss (w/o reg) on all data: 0.36681905\n",
      "Test loss (w/o reg) on all data: 0.39448178\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009756471\n",
      "Norm of the params: 2.4297292\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.39448. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [88] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35512358\n",
      "Train loss (w/o reg) on all data: 0.35480496\n",
      "Test loss (w/o reg) on all data: 0.40730423\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0043363557\n",
      "Norm of the params: 2.5243337\n",
      "                Loss: fixed   5 labels. Loss 0.40730. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3856841\n",
      "Train loss (w/o reg) on all data: 0.3853659\n",
      "Test loss (w/o reg) on all data: 0.3936834\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012316989\n",
      "Norm of the params: 2.5226567\n",
      "              Random: fixed   1 labels. Loss 0.39368. Accuracy 0.822.\n",
      "### Flips: 10, rs: 39, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36710796\n",
      "Train loss (w/o reg) on all data: 0.36681238\n",
      "Test loss (w/o reg) on all data: 0.39354315\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014430551\n",
      "Norm of the params: 2.4313297\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.39354. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35288978\n",
      "Train loss (w/o reg) on all data: 0.3525785\n",
      "Test loss (w/o reg) on all data: 0.4137996\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0025825552\n",
      "Norm of the params: 2.4951935\n",
      "                Loss: fixed   8 labels. Loss 0.41380. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37658352\n",
      "Train loss (w/o reg) on all data: 0.37625834\n",
      "Test loss (w/o reg) on all data: 0.4014141\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014822133\n",
      "Norm of the params: 2.5501678\n",
      "              Random: fixed   2 labels. Loss 0.40141. Accuracy 0.822.\n",
      "### Flips: 10, rs: 39, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35227907\n",
      "Train loss (w/o reg) on all data: 0.35194898\n",
      "Test loss (w/o reg) on all data: 0.40893984\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012699257\n",
      "Norm of the params: 2.569444\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.40894. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [124] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35289377\n",
      "Train loss (w/o reg) on all data: 0.35257944\n",
      "Test loss (w/o reg) on all data: 0.41473448\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008956288\n",
      "Norm of the params: 2.5072477\n",
      "                Loss: fixed   8 labels. Loss 0.41473. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [130] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37351045\n",
      "Train loss (w/o reg) on all data: 0.37320882\n",
      "Test loss (w/o reg) on all data: 0.3972775\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0043266397\n",
      "Norm of the params: 2.456101\n",
      "              Random: fixed   3 labels. Loss 0.39728. Accuracy 0.800.\n",
      "### Flips: 10, rs: 39, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [173] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35227993\n",
      "Train loss (w/o reg) on all data: 0.3519488\n",
      "Test loss (w/o reg) on all data: 0.40867445\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0035955422\n",
      "Norm of the params: 2.573432\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.40867. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35289028\n",
      "Train loss (w/o reg) on all data: 0.35257614\n",
      "Test loss (w/o reg) on all data: 0.41435063\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.000821301\n",
      "Norm of the params: 2.5065327\n",
      "                Loss: fixed   8 labels. Loss 0.41435. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37350532\n",
      "Train loss (w/o reg) on all data: 0.37320155\n",
      "Test loss (w/o reg) on all data: 0.39842144\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006969613\n",
      "Norm of the params: 2.4648502\n",
      "              Random: fixed   3 labels. Loss 0.39842. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4822416\n",
      "Train loss (w/o reg) on all data: 0.4821199\n",
      "Test loss (w/o reg) on all data: 0.37712857\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0009704584\n",
      "Norm of the params: 1.5602154\n",
      "Flipped loss: 0.37713. Accuracy: 0.844\n",
      "### Flips: 20, rs: 0, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42360875\n",
      "Train loss (w/o reg) on all data: 0.42339772\n",
      "Test loss (w/o reg) on all data: 0.3709699\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011380618\n",
      "Norm of the params: 2.054361\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.37097. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3836935\n",
      "Train loss (w/o reg) on all data: 0.38336003\n",
      "Test loss (w/o reg) on all data: 0.37264964\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0039789425\n",
      "Norm of the params: 2.5824165\n",
      "                Loss: fixed   7 labels. Loss 0.37265. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [146] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4844755\n",
      "Train loss (w/o reg) on all data: 0.48435444\n",
      "Test loss (w/o reg) on all data: 0.39332443\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.008038322\n",
      "Norm of the params: 1.5560367\n",
      "              Random: fixed   2 labels. Loss 0.39332. Accuracy 0.822.\n",
      "### Flips: 20, rs: 0, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [272] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3869844\n",
      "Train loss (w/o reg) on all data: 0.38667527\n",
      "Test loss (w/o reg) on all data: 0.3751183\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013236594\n",
      "Norm of the params: 2.4864695\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.37512. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [127] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37795487\n",
      "Train loss (w/o reg) on all data: 0.37760487\n",
      "Test loss (w/o reg) on all data: 0.37787047\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00083012955\n",
      "Norm of the params: 2.645752\n",
      "                Loss: fixed   8 labels. Loss 0.37787. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4807482\n",
      "Train loss (w/o reg) on all data: 0.48062587\n",
      "Test loss (w/o reg) on all data: 0.3938209\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004645426\n",
      "Norm of the params: 1.5642221\n",
      "              Random: fixed   3 labels. Loss 0.39382. Accuracy 0.822.\n",
      "### Flips: 20, rs: 0, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38092208\n",
      "Train loss (w/o reg) on all data: 0.38065085\n",
      "Test loss (w/o reg) on all data: 0.3700873\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00311674\n",
      "Norm of the params: 2.3290534\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.37009. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3617394\n",
      "Train loss (w/o reg) on all data: 0.36138746\n",
      "Test loss (w/o reg) on all data: 0.39530534\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0047141495\n",
      "Norm of the params: 2.6530926\n",
      "                Loss: fixed  13 labels. Loss 0.39531. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4807494\n",
      "Train loss (w/o reg) on all data: 0.48062742\n",
      "Test loss (w/o reg) on all data: 0.3935817\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.010904864\n",
      "Norm of the params: 1.5620142\n",
      "              Random: fixed   3 labels. Loss 0.39358. Accuracy 0.822.\n",
      "### Flips: 20, rs: 0, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [300] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36356345\n",
      "Train loss (w/o reg) on all data: 0.36322558\n",
      "Test loss (w/o reg) on all data: 0.365525\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00043087517\n",
      "Norm of the params: 2.5995264\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.36553. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3533113\n",
      "Train loss (w/o reg) on all data: 0.35293168\n",
      "Test loss (w/o reg) on all data: 0.39567608\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016406318\n",
      "Norm of the params: 2.7554262\n",
      "                Loss: fixed  15 labels. Loss 0.39568. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46422872\n",
      "Train loss (w/o reg) on all data: 0.46407595\n",
      "Test loss (w/o reg) on all data: 0.38633737\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0022557382\n",
      "Norm of the params: 1.748015\n",
      "              Random: fixed   4 labels. Loss 0.38634. Accuracy 0.822.\n",
      "### Flips: 20, rs: 0, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [195] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3635669\n",
      "Train loss (w/o reg) on all data: 0.36322954\n",
      "Test loss (w/o reg) on all data: 0.3654455\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0036289068\n",
      "Norm of the params: 2.597599\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.36545. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [199] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34837922\n",
      "Train loss (w/o reg) on all data: 0.34802547\n",
      "Test loss (w/o reg) on all data: 0.38957193\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.003891217\n",
      "Norm of the params: 2.6599004\n",
      "                Loss: fixed  17 labels. Loss 0.38957. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [199] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46422714\n",
      "Train loss (w/o reg) on all data: 0.46407455\n",
      "Test loss (w/o reg) on all data: 0.3859722\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019507579\n",
      "Norm of the params: 1.7469726\n",
      "              Random: fixed   4 labels. Loss 0.38597. Accuracy 0.822.\n",
      "### Flips: 20, rs: 0, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [290] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35822043\n",
      "Train loss (w/o reg) on all data: 0.35786286\n",
      "Test loss (w/o reg) on all data: 0.37322274\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0025947415\n",
      "Norm of the params: 2.6741858\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.37322. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [137] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35088345\n",
      "Train loss (w/o reg) on all data: 0.35053632\n",
      "Test loss (w/o reg) on all data: 0.3918359\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00079149456\n",
      "Norm of the params: 2.6349537\n",
      "                Loss: fixed  19 labels. Loss 0.39184. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46422744\n",
      "Train loss (w/o reg) on all data: 0.46407497\n",
      "Test loss (w/o reg) on all data: 0.38586017\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00089430367\n",
      "Norm of the params: 1.7462279\n",
      "              Random: fixed   4 labels. Loss 0.38586. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44515193\n",
      "Train loss (w/o reg) on all data: 0.4449682\n",
      "Test loss (w/o reg) on all data: 0.46315354\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0015692929\n",
      "Norm of the params: 1.9169191\n",
      "Flipped loss: 0.46315. Accuracy: 0.756\n",
      "### Flips: 20, rs: 1, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [108] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39906403\n",
      "Train loss (w/o reg) on all data: 0.39887878\n",
      "Test loss (w/o reg) on all data: 0.4283152\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00079019467\n",
      "Norm of the params: 1.9248785\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.42832. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38098148\n",
      "Train loss (w/o reg) on all data: 0.38071233\n",
      "Test loss (w/o reg) on all data: 0.45810372\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0021065269\n",
      "Norm of the params: 2.3201077\n",
      "                Loss: fixed   5 labels. Loss 0.45810. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43838704\n",
      "Train loss (w/o reg) on all data: 0.438218\n",
      "Test loss (w/o reg) on all data: 0.4590239\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0010113085\n",
      "Norm of the params: 1.8387179\n",
      "              Random: fixed   1 labels. Loss 0.45902. Accuracy 0.756.\n",
      "### Flips: 20, rs: 1, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37776482\n",
      "Train loss (w/o reg) on all data: 0.37753347\n",
      "Test loss (w/o reg) on all data: 0.4435472\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0020832862\n",
      "Norm of the params: 2.1510692\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.44355. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34473333\n",
      "Train loss (w/o reg) on all data: 0.34441745\n",
      "Test loss (w/o reg) on all data: 0.45091256\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008880855\n",
      "Norm of the params: 2.513515\n",
      "                Loss: fixed   9 labels. Loss 0.45091. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4383869\n",
      "Train loss (w/o reg) on all data: 0.43821615\n",
      "Test loss (w/o reg) on all data: 0.4593224\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0033987164\n",
      "Norm of the params: 1.8479508\n",
      "              Random: fixed   1 labels. Loss 0.45932. Accuracy 0.756.\n",
      "### Flips: 20, rs: 1, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [335] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35240373\n",
      "Train loss (w/o reg) on all data: 0.35209808\n",
      "Test loss (w/o reg) on all data: 0.4763766\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0009362436\n",
      "Norm of the params: 2.4725115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Influence (LOO): fixed  10 labels. Loss 0.47638. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [175] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3326704\n",
      "Train loss (w/o reg) on all data: 0.3323331\n",
      "Test loss (w/o reg) on all data: 0.45920834\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0027150654\n",
      "Norm of the params: 2.5972843\n",
      "                Loss: fixed  11 labels. Loss 0.45921. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42795774\n",
      "Train loss (w/o reg) on all data: 0.42777553\n",
      "Test loss (w/o reg) on all data: 0.46360028\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0018500114\n",
      "Norm of the params: 1.9089893\n",
      "              Random: fixed   2 labels. Loss 0.46360. Accuracy 0.756.\n",
      "### Flips: 20, rs: 1, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32827747\n",
      "Train loss (w/o reg) on all data: 0.32790685\n",
      "Test loss (w/o reg) on all data: 0.47622842\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016869799\n",
      "Norm of the params: 2.72254\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.47623. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [145] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3249693\n",
      "Train loss (w/o reg) on all data: 0.32459307\n",
      "Test loss (w/o reg) on all data: 0.44860747\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005740559\n",
      "Norm of the params: 2.743059\n",
      "                Loss: fixed  13 labels. Loss 0.44861. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [321] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41933602\n",
      "Train loss (w/o reg) on all data: 0.41914552\n",
      "Test loss (w/o reg) on all data: 0.46089727\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00071790675\n",
      "Norm of the params: 1.9518522\n",
      "              Random: fixed   4 labels. Loss 0.46090. Accuracy 0.756.\n",
      "### Flips: 20, rs: 1, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [362] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33199707\n",
      "Train loss (w/o reg) on all data: 0.3316603\n",
      "Test loss (w/o reg) on all data: 0.46300253\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0018443589\n",
      "Norm of the params: 2.5952842\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.46300. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [105] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32831058\n",
      "Train loss (w/o reg) on all data: 0.32794595\n",
      "Test loss (w/o reg) on all data: 0.45840666\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0030020992\n",
      "Norm of the params: 2.7004354\n",
      "                Loss: fixed  15 labels. Loss 0.45841. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [307] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41440302\n",
      "Train loss (w/o reg) on all data: 0.4141937\n",
      "Test loss (w/o reg) on all data: 0.46591368\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0017789634\n",
      "Norm of the params: 2.046175\n",
      "              Random: fixed   5 labels. Loss 0.46591. Accuracy 0.756.\n",
      "### Flips: 20, rs: 1, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [431] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33047915\n",
      "Train loss (w/o reg) on all data: 0.33012283\n",
      "Test loss (w/o reg) on all data: 0.44659215\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020428083\n",
      "Norm of the params: 2.6694944\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.44659. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [82] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.332391\n",
      "Train loss (w/o reg) on all data: 0.33203608\n",
      "Test loss (w/o reg) on all data: 0.44270325\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00021035485\n",
      "Norm of the params: 2.6642244\n",
      "                Loss: fixed  17 labels. Loss 0.44270. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [350] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4193316\n",
      "Train loss (w/o reg) on all data: 0.41911748\n",
      "Test loss (w/o reg) on all data: 0.46086574\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.008429588\n",
      "Norm of the params: 2.0694923\n",
      "              Random: fixed   6 labels. Loss 0.46087. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [258] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5050057\n",
      "Train loss (w/o reg) on all data: 0.5048569\n",
      "Test loss (w/o reg) on all data: 0.41135404\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0038657049\n",
      "Norm of the params: 1.7251422\n",
      "Flipped loss: 0.41135. Accuracy: 0.822\n",
      "### Flips: 20, rs: 2, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46622273\n",
      "Train loss (w/o reg) on all data: 0.4660461\n",
      "Test loss (w/o reg) on all data: 0.37555984\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011659169\n",
      "Norm of the params: 1.8794906\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.37556. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [92] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41507238\n",
      "Train loss (w/o reg) on all data: 0.41482514\n",
      "Test loss (w/o reg) on all data: 0.37016588\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015984091\n",
      "Norm of the params: 2.2236505\n",
      "                Loss: fixed   7 labels. Loss 0.37017. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49520993\n",
      "Train loss (w/o reg) on all data: 0.49504974\n",
      "Test loss (w/o reg) on all data: 0.40491292\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.007036581\n",
      "Norm of the params: 1.7899415\n",
      "              Random: fixed   1 labels. Loss 0.40491. Accuracy 0.822.\n",
      "### Flips: 20, rs: 2, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [129] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44395828\n",
      "Train loss (w/o reg) on all data: 0.44375142\n",
      "Test loss (w/o reg) on all data: 0.37357903\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0030653784\n",
      "Norm of the params: 2.0340564\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.37358. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [140] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38011363\n",
      "Train loss (w/o reg) on all data: 0.3798104\n",
      "Test loss (w/o reg) on all data: 0.38755685\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0030399484\n",
      "Norm of the params: 2.4626272\n",
      "                Loss: fixed  11 labels. Loss 0.38756. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [302] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4626521\n",
      "Train loss (w/o reg) on all data: 0.46246508\n",
      "Test loss (w/o reg) on all data: 0.39559466\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013847254\n",
      "Norm of the params: 1.9340311\n",
      "              Random: fixed   4 labels. Loss 0.39559. Accuracy 0.822.\n",
      "### Flips: 20, rs: 2, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [327] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39919803\n",
      "Train loss (w/o reg) on all data: 0.39890966\n",
      "Test loss (w/o reg) on all data: 0.37893054\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00295735\n",
      "Norm of the params: 2.4015088\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.37893. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3657339\n",
      "Train loss (w/o reg) on all data: 0.3653897\n",
      "Test loss (w/o reg) on all data: 0.3973748\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015201478\n",
      "Norm of the params: 2.6237438\n",
      "                Loss: fixed  13 labels. Loss 0.39737. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44692305\n",
      "Train loss (w/o reg) on all data: 0.44673717\n",
      "Test loss (w/o reg) on all data: 0.3766546\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012043564\n",
      "Norm of the params: 1.9281449\n",
      "              Random: fixed   7 labels. Loss 0.37665. Accuracy 0.822.\n",
      "### Flips: 20, rs: 2, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3851152\n",
      "Train loss (w/o reg) on all data: 0.384794\n",
      "Test loss (w/o reg) on all data: 0.37677443\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00091402093\n",
      "Norm of the params: 2.534645\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.37677. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3619688\n",
      "Train loss (w/o reg) on all data: 0.36160907\n",
      "Test loss (w/o reg) on all data: 0.40691718\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015198484\n",
      "Norm of the params: 2.6821613\n",
      "                Loss: fixed  14 labels. Loss 0.40692. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4469281\n",
      "Train loss (w/o reg) on all data: 0.44674337\n",
      "Test loss (w/o reg) on all data: 0.376825\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00088207243\n",
      "Norm of the params: 1.9222041\n",
      "              Random: fixed   7 labels. Loss 0.37683. Accuracy 0.822.\n",
      "### Flips: 20, rs: 2, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3776887\n",
      "Train loss (w/o reg) on all data: 0.3773297\n",
      "Test loss (w/o reg) on all data: 0.3842128\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009973327\n",
      "Norm of the params: 2.679526\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.38421. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [94] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35342982\n",
      "Train loss (w/o reg) on all data: 0.35304302\n",
      "Test loss (w/o reg) on all data: 0.3970562\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011230629\n",
      "Norm of the params: 2.781413\n",
      "                Loss: fixed  16 labels. Loss 0.39706. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [292] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4469286\n",
      "Train loss (w/o reg) on all data: 0.44674438\n",
      "Test loss (w/o reg) on all data: 0.37658292\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0044380846\n",
      "Norm of the params: 1.9194027\n",
      "              Random: fixed   7 labels. Loss 0.37658. Accuracy 0.822.\n",
      "### Flips: 20, rs: 2, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [353] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3654833\n",
      "Train loss (w/o reg) on all data: 0.36510915\n",
      "Test loss (w/o reg) on all data: 0.39383644\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006542413\n",
      "Norm of the params: 2.7355766\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.39384. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35415193\n",
      "Train loss (w/o reg) on all data: 0.35378712\n",
      "Test loss (w/o reg) on all data: 0.40169302\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00049325504\n",
      "Norm of the params: 2.7011015\n",
      "                Loss: fixed  17 labels. Loss 0.40169. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [315] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44692352\n",
      "Train loss (w/o reg) on all data: 0.44673765\n",
      "Test loss (w/o reg) on all data: 0.37663087\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0037347535\n",
      "Norm of the params: 1.9281607\n",
      "              Random: fixed   7 labels. Loss 0.37663. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [211] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46422425\n",
      "Train loss (w/o reg) on all data: 0.46410078\n",
      "Test loss (w/o reg) on all data: 0.38801447\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.006885033\n",
      "Norm of the params: 1.5715023\n",
      "Flipped loss: 0.38801. Accuracy: 0.822\n",
      "### Flips: 20, rs: 3, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [116] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44596142\n",
      "Train loss (w/o reg) on all data: 0.4458201\n",
      "Test loss (w/o reg) on all data: 0.38341525\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0041063866\n",
      "Norm of the params: 1.6812876\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.38342. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [299] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40771225\n",
      "Train loss (w/o reg) on all data: 0.407518\n",
      "Test loss (w/o reg) on all data: 0.4149075\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0020031177\n",
      "Norm of the params: 1.9710766\n",
      "                Loss: fixed   5 labels. Loss 0.41491. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4587704\n",
      "Train loss (w/o reg) on all data: 0.4586354\n",
      "Test loss (w/o reg) on all data: 0.3814289\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00091542077\n",
      "Norm of the params: 1.6431528\n",
      "              Random: fixed   1 labels. Loss 0.38143. Accuracy 0.844.\n",
      "### Flips: 20, rs: 3, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41641703\n",
      "Train loss (w/o reg) on all data: 0.41619772\n",
      "Test loss (w/o reg) on all data: 0.3705544\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0028904038\n",
      "Norm of the params: 2.094308\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.37055. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3533048\n",
      "Train loss (w/o reg) on all data: 0.35295498\n",
      "Test loss (w/o reg) on all data: 0.4418515\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00044941113\n",
      "Norm of the params: 2.6450257\n",
      "                Loss: fixed  10 labels. Loss 0.44185. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45877677\n",
      "Train loss (w/o reg) on all data: 0.45864254\n",
      "Test loss (w/o reg) on all data: 0.38223028\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.009057842\n",
      "Norm of the params: 1.6383942\n",
      "              Random: fixed   1 labels. Loss 0.38223. Accuracy 0.844.\n",
      "### Flips: 20, rs: 3, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40622503\n",
      "Train loss (w/o reg) on all data: 0.40599945\n",
      "Test loss (w/o reg) on all data: 0.38514006\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003018037\n",
      "Norm of the params: 2.1240063\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.38514. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3439118\n",
      "Train loss (w/o reg) on all data: 0.34351847\n",
      "Test loss (w/o reg) on all data: 0.4385053\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015289859\n",
      "Norm of the params: 2.8047388\n",
      "                Loss: fixed  11 labels. Loss 0.43851. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [309] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44071168\n",
      "Train loss (w/o reg) on all data: 0.4405516\n",
      "Test loss (w/o reg) on all data: 0.3953023\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00035484295\n",
      "Norm of the params: 1.7892009\n",
      "              Random: fixed   3 labels. Loss 0.39530. Accuracy 0.822.\n",
      "### Flips: 20, rs: 3, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3784224\n",
      "Train loss (w/o reg) on all data: 0.37816387\n",
      "Test loss (w/o reg) on all data: 0.40492257\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007319144\n",
      "Norm of the params: 2.2739327\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.40492. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [199] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33555627\n",
      "Train loss (w/o reg) on all data: 0.3351439\n",
      "Test loss (w/o reg) on all data: 0.42742592\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00064352364\n",
      "Norm of the params: 2.87183\n",
      "                Loss: fixed  14 labels. Loss 0.42743. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [167] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43009272\n",
      "Train loss (w/o reg) on all data: 0.42991322\n",
      "Test loss (w/o reg) on all data: 0.39403614\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0025085264\n",
      "Norm of the params: 1.894792\n",
      "              Random: fixed   4 labels. Loss 0.39404. Accuracy 0.844.\n",
      "### Flips: 20, rs: 3, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34103674\n",
      "Train loss (w/o reg) on all data: 0.3406421\n",
      "Test loss (w/o reg) on all data: 0.41869467\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0034302142\n",
      "Norm of the params: 2.8094616\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.41869. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [86] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33785403\n",
      "Train loss (w/o reg) on all data: 0.33744547\n",
      "Test loss (w/o reg) on all data: 0.42150655\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008118844\n",
      "Norm of the params: 2.8584995\n",
      "                Loss: fixed  15 labels. Loss 0.42151. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [284] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41152695\n",
      "Train loss (w/o reg) on all data: 0.41132075\n",
      "Test loss (w/o reg) on all data: 0.38621193\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.003774494\n",
      "Norm of the params: 2.0308268\n",
      "              Random: fixed   6 labels. Loss 0.38621. Accuracy 0.844.\n",
      "### Flips: 20, rs: 3, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [330] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33555862\n",
      "Train loss (w/o reg) on all data: 0.33514354\n",
      "Test loss (w/o reg) on all data: 0.4272033\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00076666725\n",
      "Norm of the params: 2.8813145\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.42720. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [74] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33784753\n",
      "Train loss (w/o reg) on all data: 0.33743697\n",
      "Test loss (w/o reg) on all data: 0.42203242\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00030975835\n",
      "Norm of the params: 2.8655076\n",
      "                Loss: fixed  15 labels. Loss 0.42203. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41152686\n",
      "Train loss (w/o reg) on all data: 0.4113204\n",
      "Test loss (w/o reg) on all data: 0.3865299\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0044998014\n",
      "Norm of the params: 2.0321312\n",
      "              Random: fixed   6 labels. Loss 0.38653. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [295] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48759463\n",
      "Train loss (w/o reg) on all data: 0.48742995\n",
      "Test loss (w/o reg) on all data: 0.3833052\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0027935943\n",
      "Norm of the params: 1.8149356\n",
      "Flipped loss: 0.38331. Accuracy: 0.844\n",
      "### Flips: 20, rs: 4, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [167] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45851538\n",
      "Train loss (w/o reg) on all data: 0.4582981\n",
      "Test loss (w/o reg) on all data: 0.36442325\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002261239\n",
      "Norm of the params: 2.0846863\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.36442. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [240] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42280853\n",
      "Train loss (w/o reg) on all data: 0.42251226\n",
      "Test loss (w/o reg) on all data: 0.37483057\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0040522963\n",
      "Norm of the params: 2.4341977\n",
      "                Loss: fixed   6 labels. Loss 0.37483. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [136] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48011824\n",
      "Train loss (w/o reg) on all data: 0.47994646\n",
      "Test loss (w/o reg) on all data: 0.3944244\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012715398\n",
      "Norm of the params: 1.8534917\n",
      "              Random: fixed   1 labels. Loss 0.39442. Accuracy 0.800.\n",
      "### Flips: 20, rs: 4, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [264] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44813338\n",
      "Train loss (w/o reg) on all data: 0.44785726\n",
      "Test loss (w/o reg) on all data: 0.37256816\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0027591544\n",
      "Norm of the params: 2.3499677\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.37257. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [319] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3861548\n",
      "Train loss (w/o reg) on all data: 0.38582054\n",
      "Test loss (w/o reg) on all data: 0.40706336\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0009341956\n",
      "Norm of the params: 2.5856042\n",
      "                Loss: fixed  10 labels. Loss 0.40706. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47219968\n",
      "Train loss (w/o reg) on all data: 0.4720083\n",
      "Test loss (w/o reg) on all data: 0.3865551\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009317231\n",
      "Norm of the params: 1.956539\n",
      "              Random: fixed   2 labels. Loss 0.38656. Accuracy 0.822.\n",
      "### Flips: 20, rs: 4, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43066618\n",
      "Train loss (w/o reg) on all data: 0.43038306\n",
      "Test loss (w/o reg) on all data: 0.36841604\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0023983223\n",
      "Norm of the params: 2.3795478\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.36842. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36312053\n",
      "Train loss (w/o reg) on all data: 0.3627474\n",
      "Test loss (w/o reg) on all data: 0.41162553\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0037823005\n",
      "Norm of the params: 2.731734\n",
      "                Loss: fixed  14 labels. Loss 0.41163. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [208] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47219843\n",
      "Train loss (w/o reg) on all data: 0.47200692\n",
      "Test loss (w/o reg) on all data: 0.3868904\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0049960422\n",
      "Norm of the params: 1.9570433\n",
      "              Random: fixed   2 labels. Loss 0.38689. Accuracy 0.822.\n",
      "### Flips: 20, rs: 4, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41685408\n",
      "Train loss (w/o reg) on all data: 0.41660106\n",
      "Test loss (w/o reg) on all data: 0.3506968\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010610116\n",
      "Norm of the params: 2.2495608\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.35070. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3627516\n",
      "Train loss (w/o reg) on all data: 0.36240244\n",
      "Test loss (w/o reg) on all data: 0.4026068\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00085549464\n",
      "Norm of the params: 2.6426265\n",
      "                Loss: fixed  15 labels. Loss 0.40261. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4675672\n",
      "Train loss (w/o reg) on all data: 0.46737555\n",
      "Test loss (w/o reg) on all data: 0.39442998\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011214846\n",
      "Norm of the params: 1.9578352\n",
      "              Random: fixed   3 labels. Loss 0.39443. Accuracy 0.800.\n",
      "### Flips: 20, rs: 4, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [101] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.390193\n",
      "Train loss (w/o reg) on all data: 0.38992643\n",
      "Test loss (w/o reg) on all data: 0.37621748\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004884096\n",
      "Norm of the params: 2.308875\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.37622. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [309] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35902986\n",
      "Train loss (w/o reg) on all data: 0.3587052\n",
      "Test loss (w/o reg) on all data: 0.40425032\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018296947\n",
      "Norm of the params: 2.548205\n",
      "                Loss: fixed  17 labels. Loss 0.40425. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [303] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46756315\n",
      "Train loss (w/o reg) on all data: 0.4673736\n",
      "Test loss (w/o reg) on all data: 0.39406744\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020633377\n",
      "Norm of the params: 1.9470401\n",
      "              Random: fixed   3 labels. Loss 0.39407. Accuracy 0.800.\n",
      "### Flips: 20, rs: 4, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [283] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38763767\n",
      "Train loss (w/o reg) on all data: 0.38736454\n",
      "Test loss (w/o reg) on all data: 0.37088513\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00027370892\n",
      "Norm of the params: 2.337297\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.37089. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [143] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35512418\n",
      "Train loss (w/o reg) on all data: 0.3547871\n",
      "Test loss (w/o reg) on all data: 0.40068775\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016596975\n",
      "Norm of the params: 2.5964453\n",
      "                Loss: fixed  18 labels. Loss 0.40069. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46079072\n",
      "Train loss (w/o reg) on all data: 0.4606027\n",
      "Test loss (w/o reg) on all data: 0.38831148\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008209525\n",
      "Norm of the params: 1.939262\n",
      "              Random: fixed   4 labels. Loss 0.38831. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [122] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5085465\n",
      "Train loss (w/o reg) on all data: 0.50839543\n",
      "Test loss (w/o reg) on all data: 0.3839074\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012253616\n",
      "Norm of the params: 1.7379683\n",
      "Flipped loss: 0.38391. Accuracy: 0.800\n",
      "### Flips: 20, rs: 5, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43895265\n",
      "Train loss (w/o reg) on all data: 0.43875208\n",
      "Test loss (w/o reg) on all data: 0.36770177\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012076976\n",
      "Norm of the params: 2.0028183\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.36770. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4063549\n",
      "Train loss (w/o reg) on all data: 0.40608898\n",
      "Test loss (w/o reg) on all data: 0.38336432\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007677848\n",
      "Norm of the params: 2.3061454\n",
      "                Loss: fixed   7 labels. Loss 0.38336. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5034032\n",
      "Train loss (w/o reg) on all data: 0.503263\n",
      "Test loss (w/o reg) on all data: 0.374366\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002515019\n",
      "Norm of the params: 1.6745027\n",
      "              Random: fixed   1 labels. Loss 0.37437. Accuracy 0.800.\n",
      "### Flips: 20, rs: 5, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [153] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42193487\n",
      "Train loss (w/o reg) on all data: 0.42169124\n",
      "Test loss (w/o reg) on all data: 0.37619662\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00049349054\n",
      "Norm of the params: 2.2074678\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.37620. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35861668\n",
      "Train loss (w/o reg) on all data: 0.35825592\n",
      "Test loss (w/o reg) on all data: 0.39723775\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008194851\n",
      "Norm of the params: 2.6860762\n",
      "                Loss: fixed  12 labels. Loss 0.39724. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5034032\n",
      "Train loss (w/o reg) on all data: 0.50326407\n",
      "Test loss (w/o reg) on all data: 0.37420085\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0029280197\n",
      "Norm of the params: 1.6679882\n",
      "              Random: fixed   1 labels. Loss 0.37420. Accuracy 0.800.\n",
      "### Flips: 20, rs: 5, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41234785\n",
      "Train loss (w/o reg) on all data: 0.41210175\n",
      "Test loss (w/o reg) on all data: 0.36895624\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009444705\n",
      "Norm of the params: 2.2185526\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.36896. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35178378\n",
      "Train loss (w/o reg) on all data: 0.35142395\n",
      "Test loss (w/o reg) on all data: 0.40998942\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019832836\n",
      "Norm of the params: 2.6826408\n",
      "                Loss: fixed  13 labels. Loss 0.40999. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50340384\n",
      "Train loss (w/o reg) on all data: 0.50326455\n",
      "Test loss (w/o reg) on all data: 0.37412912\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001491097\n",
      "Norm of the params: 1.6691815\n",
      "              Random: fixed   1 labels. Loss 0.37413. Accuracy 0.800.\n",
      "### Flips: 20, rs: 5, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3894936\n",
      "Train loss (w/o reg) on all data: 0.3891955\n",
      "Test loss (w/o reg) on all data: 0.38900056\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0036335639\n",
      "Norm of the params: 2.441814\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.38900. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [120] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34794396\n",
      "Train loss (w/o reg) on all data: 0.34756765\n",
      "Test loss (w/o reg) on all data: 0.4072421\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016556262\n",
      "Norm of the params: 2.7433543\n",
      "                Loss: fixed  14 labels. Loss 0.40724. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49167204\n",
      "Train loss (w/o reg) on all data: 0.49152416\n",
      "Test loss (w/o reg) on all data: 0.35818252\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003552695\n",
      "Norm of the params: 1.7197052\n",
      "              Random: fixed   2 labels. Loss 0.35818. Accuracy 0.822.\n",
      "### Flips: 20, rs: 5, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [322] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38594234\n",
      "Train loss (w/o reg) on all data: 0.3856602\n",
      "Test loss (w/o reg) on all data: 0.3914903\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0029461253\n",
      "Norm of the params: 2.3754327\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.39149. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [323] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34259054\n",
      "Train loss (w/o reg) on all data: 0.3421915\n",
      "Test loss (w/o reg) on all data: 0.41308132\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0023551388\n",
      "Norm of the params: 2.8250601\n",
      "                Loss: fixed  15 labels. Loss 0.41308. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46567202\n",
      "Train loss (w/o reg) on all data: 0.46551672\n",
      "Test loss (w/o reg) on all data: 0.34331957\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0017112663\n",
      "Norm of the params: 1.7623773\n",
      "              Random: fixed   5 labels. Loss 0.34332. Accuracy 0.844.\n",
      "### Flips: 20, rs: 5, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [303] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36331064\n",
      "Train loss (w/o reg) on all data: 0.3629899\n",
      "Test loss (w/o reg) on all data: 0.39275965\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0045379493\n",
      "Norm of the params: 2.5327318\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.39276. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34500018\n",
      "Train loss (w/o reg) on all data: 0.3446161\n",
      "Test loss (w/o reg) on all data: 0.41002223\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.000979325\n",
      "Norm of the params: 2.7715669\n",
      "                Loss: fixed  16 labels. Loss 0.41002. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [295] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4626048\n",
      "Train loss (w/o reg) on all data: 0.46245483\n",
      "Test loss (w/o reg) on all data: 0.35062668\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0020111678\n",
      "Norm of the params: 1.7317705\n",
      "              Random: fixed   6 labels. Loss 0.35063. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4651807\n",
      "Train loss (w/o reg) on all data: 0.4649931\n",
      "Test loss (w/o reg) on all data: 0.40205112\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014522896\n",
      "Norm of the params: 1.9370797\n",
      "Flipped loss: 0.40205. Accuracy: 0.800\n",
      "### Flips: 20, rs: 6, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39501187\n",
      "Train loss (w/o reg) on all data: 0.39469862\n",
      "Test loss (w/o reg) on all data: 0.39385074\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012245846\n",
      "Norm of the params: 2.5029979\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.39385. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [279] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37880343\n",
      "Train loss (w/o reg) on all data: 0.3783845\n",
      "Test loss (w/o reg) on all data: 0.4063591\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003132308\n",
      "Norm of the params: 2.8945725\n",
      "                Loss: fixed   6 labels. Loss 0.40636. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4537735\n",
      "Train loss (w/o reg) on all data: 0.45355484\n",
      "Test loss (w/o reg) on all data: 0.39733025\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007378979\n",
      "Norm of the params: 2.0911896\n",
      "              Random: fixed   1 labels. Loss 0.39733. Accuracy 0.800.\n",
      "### Flips: 20, rs: 6, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [302] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3691786\n",
      "Train loss (w/o reg) on all data: 0.3687966\n",
      "Test loss (w/o reg) on all data: 0.4031573\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005469284\n",
      "Norm of the params: 2.7640755\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.40316. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35558796\n",
      "Train loss (w/o reg) on all data: 0.35511255\n",
      "Test loss (w/o reg) on all data: 0.44369477\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00032021763\n",
      "Norm of the params: 3.0835302\n",
      "                Loss: fixed   9 labels. Loss 0.44369. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4537678\n",
      "Train loss (w/o reg) on all data: 0.4535497\n",
      "Test loss (w/o reg) on all data: 0.3985471\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019653891\n",
      "Norm of the params: 2.088555\n",
      "              Random: fixed   1 labels. Loss 0.39855. Accuracy 0.800.\n",
      "### Flips: 20, rs: 6, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [300] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3675658\n",
      "Train loss (w/o reg) on all data: 0.36718994\n",
      "Test loss (w/o reg) on all data: 0.4067487\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001565064\n",
      "Norm of the params: 2.7418013\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.40675. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [152] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3406403\n",
      "Train loss (w/o reg) on all data: 0.34008235\n",
      "Test loss (w/o reg) on all data: 0.43584344\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016674468\n",
      "Norm of the params: 3.3404922\n",
      "                Loss: fixed  12 labels. Loss 0.43584. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [363] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4402813\n",
      "Train loss (w/o reg) on all data: 0.44005355\n",
      "Test loss (w/o reg) on all data: 0.39246643\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00032936205\n",
      "Norm of the params: 2.134199\n",
      "              Random: fixed   4 labels. Loss 0.39247. Accuracy 0.800.\n",
      "### Flips: 20, rs: 6, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [377] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34068772\n",
      "Train loss (w/o reg) on all data: 0.3402162\n",
      "Test loss (w/o reg) on all data: 0.43161058\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0013467204\n",
      "Norm of the params: 3.0709202\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.43161. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33473864\n",
      "Train loss (w/o reg) on all data: 0.33421627\n",
      "Test loss (w/o reg) on all data: 0.43105367\n",
      "Train acc on all data:  0.8820754716981132\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0023611733\n",
      "Norm of the params: 3.2323027\n",
      "                Loss: fixed  13 labels. Loss 0.43105. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [331] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4336945\n",
      "Train loss (w/o reg) on all data: 0.4334472\n",
      "Test loss (w/o reg) on all data: 0.40371773\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015315883\n",
      "Norm of the params: 2.2239795\n",
      "              Random: fixed   5 labels. Loss 0.40372. Accuracy 0.778.\n",
      "### Flips: 20, rs: 6, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34068736\n",
      "Train loss (w/o reg) on all data: 0.34021515\n",
      "Test loss (w/o reg) on all data: 0.4316791\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011309172\n",
      "Norm of the params: 3.073193\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.43168. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [112] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33486143\n",
      "Train loss (w/o reg) on all data: 0.3343216\n",
      "Test loss (w/o reg) on all data: 0.43892235\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0009413905\n",
      "Norm of the params: 3.2858071\n",
      "                Loss: fixed  15 labels. Loss 0.43892. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43369794\n",
      "Train loss (w/o reg) on all data: 0.43345204\n",
      "Test loss (w/o reg) on all data: 0.4038317\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015345566\n",
      "Norm of the params: 2.217666\n",
      "              Random: fixed   5 labels. Loss 0.40383. Accuracy 0.778.\n",
      "### Flips: 20, rs: 6, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34200302\n",
      "Train loss (w/o reg) on all data: 0.34159318\n",
      "Test loss (w/o reg) on all data: 0.4257953\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012698113\n",
      "Norm of the params: 2.8630064\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.42580. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [171] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33673888\n",
      "Train loss (w/o reg) on all data: 0.336247\n",
      "Test loss (w/o reg) on all data: 0.43199888\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005839381\n",
      "Norm of the params: 3.1364985\n",
      "                Loss: fixed  17 labels. Loss 0.43200. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44643927\n",
      "Train loss (w/o reg) on all data: 0.44622713\n",
      "Test loss (w/o reg) on all data: 0.40041962\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0017820542\n",
      "Norm of the params: 2.059749\n",
      "              Random: fixed   6 labels. Loss 0.40042. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49181303\n",
      "Train loss (w/o reg) on all data: 0.49168003\n",
      "Test loss (w/o reg) on all data: 0.37691465\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0023378218\n",
      "Norm of the params: 1.6309651\n",
      "Flipped loss: 0.37691. Accuracy: 0.844\n",
      "### Flips: 20, rs: 7, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [151] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47200513\n",
      "Train loss (w/o reg) on all data: 0.4718443\n",
      "Test loss (w/o reg) on all data: 0.3828056\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010642258\n",
      "Norm of the params: 1.7935033\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.38281. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41881913\n",
      "Train loss (w/o reg) on all data: 0.4185307\n",
      "Test loss (w/o reg) on all data: 0.3610044\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0014356571\n",
      "Norm of the params: 2.4018135\n",
      "                Loss: fixed   7 labels. Loss 0.36100. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [199] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4849961\n",
      "Train loss (w/o reg) on all data: 0.48484856\n",
      "Test loss (w/o reg) on all data: 0.36774734\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0010638197\n",
      "Norm of the params: 1.7179022\n",
      "              Random: fixed   2 labels. Loss 0.36775. Accuracy 0.867.\n",
      "### Flips: 20, rs: 7, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4314738\n",
      "Train loss (w/o reg) on all data: 0.43125743\n",
      "Test loss (w/o reg) on all data: 0.38649648\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009931312\n",
      "Norm of the params: 2.08016\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.38650. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38272318\n",
      "Train loss (w/o reg) on all data: 0.38239908\n",
      "Test loss (w/o reg) on all data: 0.37899378\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00090557116\n",
      "Norm of the params: 2.5460129\n",
      "                Loss: fixed  11 labels. Loss 0.37899. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48056364\n",
      "Train loss (w/o reg) on all data: 0.4804261\n",
      "Test loss (w/o reg) on all data: 0.37145534\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00039895842\n",
      "Norm of the params: 1.6586231\n",
      "              Random: fixed   3 labels. Loss 0.37146. Accuracy 0.844.\n",
      "### Flips: 20, rs: 7, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41442627\n",
      "Train loss (w/o reg) on all data: 0.41418374\n",
      "Test loss (w/o reg) on all data: 0.39725304\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00069326226\n",
      "Norm of the params: 2.2024415\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.39725. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36853346\n",
      "Train loss (w/o reg) on all data: 0.36823323\n",
      "Test loss (w/o reg) on all data: 0.3886737\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004288832\n",
      "Norm of the params: 2.4504335\n",
      "                Loss: fixed  13 labels. Loss 0.38867. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [372] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4584145\n",
      "Train loss (w/o reg) on all data: 0.45823497\n",
      "Test loss (w/o reg) on all data: 0.35078737\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002414441\n",
      "Norm of the params: 1.8949528\n",
      "              Random: fixed   5 labels. Loss 0.35079. Accuracy 0.822.\n",
      "### Flips: 20, rs: 7, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39434233\n",
      "Train loss (w/o reg) on all data: 0.3940672\n",
      "Test loss (w/o reg) on all data: 0.39911813\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0029176755\n",
      "Norm of the params: 2.3458424\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.39912. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35611615\n",
      "Train loss (w/o reg) on all data: 0.35579824\n",
      "Test loss (w/o reg) on all data: 0.3957168\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0073179775\n",
      "Norm of the params: 2.5214589\n",
      "                Loss: fixed  15 labels. Loss 0.39572. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45374095\n",
      "Train loss (w/o reg) on all data: 0.45355844\n",
      "Test loss (w/o reg) on all data: 0.36073613\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0067580203\n",
      "Norm of the params: 1.9104909\n",
      "              Random: fixed   7 labels. Loss 0.36074. Accuracy 0.822.\n",
      "### Flips: 20, rs: 7, checks: 50\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [442] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36298403\n",
      "Train loss (w/o reg) on all data: 0.36262107\n",
      "Test loss (w/o reg) on all data: 0.39736655\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 7.4386386e-05\n",
      "Norm of the params: 2.6942627\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.39737. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [104] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34966126\n",
      "Train loss (w/o reg) on all data: 0.3492957\n",
      "Test loss (w/o reg) on all data: 0.40393642\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0024217411\n",
      "Norm of the params: 2.7038593\n",
      "                Loss: fixed  16 labels. Loss 0.40394. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [226] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44627577\n",
      "Train loss (w/o reg) on all data: 0.44607508\n",
      "Test loss (w/o reg) on all data: 0.3640737\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0025629883\n",
      "Norm of the params: 2.0034447\n",
      "              Random: fixed   9 labels. Loss 0.36407. Accuracy 0.822.\n",
      "### Flips: 20, rs: 7, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [295] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3597563\n",
      "Train loss (w/o reg) on all data: 0.35942748\n",
      "Test loss (w/o reg) on all data: 0.3991109\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009673288\n",
      "Norm of the params: 2.5643928\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.39911. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [171] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34965187\n",
      "Train loss (w/o reg) on all data: 0.34929037\n",
      "Test loss (w/o reg) on all data: 0.40503395\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006190194\n",
      "Norm of the params: 2.6888669\n",
      "                Loss: fixed  16 labels. Loss 0.40503. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44263512\n",
      "Train loss (w/o reg) on all data: 0.44243658\n",
      "Test loss (w/o reg) on all data: 0.3668489\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011457524\n",
      "Norm of the params: 1.9926585\n",
      "              Random: fixed  10 labels. Loss 0.36685. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [184] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50494844\n",
      "Train loss (w/o reg) on all data: 0.5048395\n",
      "Test loss (w/o reg) on all data: 0.38674855\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0064682686\n",
      "Norm of the params: 1.4762579\n",
      "Flipped loss: 0.38675. Accuracy: 0.800\n",
      "### Flips: 20, rs: 8, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [143] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43517765\n",
      "Train loss (w/o reg) on all data: 0.435008\n",
      "Test loss (w/o reg) on all data: 0.36875173\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00079092686\n",
      "Norm of the params: 1.842018\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.36875. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39695665\n",
      "Train loss (w/o reg) on all data: 0.3967342\n",
      "Test loss (w/o reg) on all data: 0.36537942\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.000656014\n",
      "Norm of the params: 2.1092129\n",
      "                Loss: fixed   7 labels. Loss 0.36538. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [122] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49681678\n",
      "Train loss (w/o reg) on all data: 0.49670094\n",
      "Test loss (w/o reg) on all data: 0.37596032\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001443996\n",
      "Norm of the params: 1.522095\n",
      "              Random: fixed   1 labels. Loss 0.37596. Accuracy 0.822.\n",
      "### Flips: 20, rs: 8, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [292] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38901415\n",
      "Train loss (w/o reg) on all data: 0.38877243\n",
      "Test loss (w/o reg) on all data: 0.37850413\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013613879\n",
      "Norm of the params: 2.1987698\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.37850. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [115] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36469132\n",
      "Train loss (w/o reg) on all data: 0.36441803\n",
      "Test loss (w/o reg) on all data: 0.3724577\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0028031014\n",
      "Norm of the params: 2.3378785\n",
      "                Loss: fixed  10 labels. Loss 0.37246. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47927257\n",
      "Train loss (w/o reg) on all data: 0.4791437\n",
      "Test loss (w/o reg) on all data: 0.3520057\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0065662824\n",
      "Norm of the params: 1.6054726\n",
      "              Random: fixed   3 labels. Loss 0.35201. Accuracy 0.867.\n",
      "### Flips: 20, rs: 8, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3903856\n",
      "Train loss (w/o reg) on all data: 0.39013913\n",
      "Test loss (w/o reg) on all data: 0.39069107\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001776468\n",
      "Norm of the params: 2.2202563\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.39069. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [116] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3646914\n",
      "Train loss (w/o reg) on all data: 0.3644192\n",
      "Test loss (w/o reg) on all data: 0.37231517\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00945572\n",
      "Norm of the params: 2.333356\n",
      "                Loss: fixed  10 labels. Loss 0.37232. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45457742\n",
      "Train loss (w/o reg) on all data: 0.45441592\n",
      "Test loss (w/o reg) on all data: 0.35056755\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0013895996\n",
      "Norm of the params: 1.7972633\n",
      "              Random: fixed   5 labels. Loss 0.35057. Accuracy 0.867.\n",
      "### Flips: 20, rs: 8, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [372] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3598388\n",
      "Train loss (w/o reg) on all data: 0.35951003\n",
      "Test loss (w/o reg) on all data: 0.3986706\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001247232\n",
      "Norm of the params: 2.5642827\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.39867. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [45] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35126638\n",
      "Train loss (w/o reg) on all data: 0.35096398\n",
      "Test loss (w/o reg) on all data: 0.38324615\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018434251\n",
      "Norm of the params: 2.4593134\n",
      "                Loss: fixed  13 labels. Loss 0.38325. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.454581\n",
      "Train loss (w/o reg) on all data: 0.45441738\n",
      "Test loss (w/o reg) on all data: 0.35116354\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.000784445\n",
      "Norm of the params: 1.8089999\n",
      "              Random: fixed   5 labels. Loss 0.35116. Accuracy 0.867.\n",
      "### Flips: 20, rs: 8, checks: 50\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [329] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35008457\n",
      "Train loss (w/o reg) on all data: 0.34974647\n",
      "Test loss (w/o reg) on all data: 0.409841\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012754516\n",
      "Norm of the params: 2.600385\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.40984. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [135] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34737524\n",
      "Train loss (w/o reg) on all data: 0.34703913\n",
      "Test loss (w/o reg) on all data: 0.39826134\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002032573\n",
      "Norm of the params: 2.5926785\n",
      "                Loss: fixed  14 labels. Loss 0.39826. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [314] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4545786\n",
      "Train loss (w/o reg) on all data: 0.45441756\n",
      "Test loss (w/o reg) on all data: 0.3506674\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0014099327\n",
      "Norm of the params: 1.7947892\n",
      "              Random: fixed   5 labels. Loss 0.35067. Accuracy 0.867.\n",
      "### Flips: 20, rs: 8, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [332] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3500849\n",
      "Train loss (w/o reg) on all data: 0.34974658\n",
      "Test loss (w/o reg) on all data: 0.40988904\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019878612\n",
      "Norm of the params: 2.6011815\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.40989. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [157] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3404326\n",
      "Train loss (w/o reg) on all data: 0.34006888\n",
      "Test loss (w/o reg) on all data: 0.42170697\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0038022534\n",
      "Norm of the params: 2.6971328\n",
      "                Loss: fixed  17 labels. Loss 0.42171. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45460317\n",
      "Train loss (w/o reg) on all data: 0.45444068\n",
      "Test loss (w/o reg) on all data: 0.3523152\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0036473263\n",
      "Norm of the params: 1.8027309\n",
      "              Random: fixed   5 labels. Loss 0.35232. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48543417\n",
      "Train loss (w/o reg) on all data: 0.4852906\n",
      "Test loss (w/o reg) on all data: 0.4012637\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010315799\n",
      "Norm of the params: 1.6947097\n",
      "Flipped loss: 0.40126. Accuracy: 0.778\n",
      "### Flips: 20, rs: 9, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [158] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42172956\n",
      "Train loss (w/o reg) on all data: 0.42148125\n",
      "Test loss (w/o reg) on all data: 0.39076683\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0039831754\n",
      "Norm of the params: 2.228549\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.39077. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [94] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4167938\n",
      "Train loss (w/o reg) on all data: 0.41655248\n",
      "Test loss (w/o reg) on all data: 0.3816673\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002653206\n",
      "Norm of the params: 2.196869\n",
      "                Loss: fixed   5 labels. Loss 0.38167. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [117] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47525096\n",
      "Train loss (w/o reg) on all data: 0.47508982\n",
      "Test loss (w/o reg) on all data: 0.40635437\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0021857757\n",
      "Norm of the params: 1.7952381\n",
      "              Random: fixed   2 labels. Loss 0.40635. Accuracy 0.778.\n",
      "### Flips: 20, rs: 9, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41964823\n",
      "Train loss (w/o reg) on all data: 0.41939002\n",
      "Test loss (w/o reg) on all data: 0.40138322\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0014979681\n",
      "Norm of the params: 2.2724693\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.40138. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [134] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36368302\n",
      "Train loss (w/o reg) on all data: 0.36332494\n",
      "Test loss (w/o reg) on all data: 0.40063494\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.006549052\n",
      "Norm of the params: 2.67614\n",
      "                Loss: fixed  11 labels. Loss 0.40063. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47523418\n",
      "Train loss (w/o reg) on all data: 0.47507444\n",
      "Test loss (w/o reg) on all data: 0.40439558\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0014896959\n",
      "Norm of the params: 1.7873776\n",
      "              Random: fixed   2 labels. Loss 0.40440. Accuracy 0.778.\n",
      "### Flips: 20, rs: 9, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40227938\n",
      "Train loss (w/o reg) on all data: 0.40199423\n",
      "Test loss (w/o reg) on all data: 0.40593708\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002445853\n",
      "Norm of the params: 2.3880765\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.40594. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [171] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34747177\n",
      "Train loss (w/o reg) on all data: 0.34705576\n",
      "Test loss (w/o reg) on all data: 0.4132597\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0003726284\n",
      "Norm of the params: 2.8845036\n",
      "                Loss: fixed  14 labels. Loss 0.41326. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47613925\n",
      "Train loss (w/o reg) on all data: 0.47598144\n",
      "Test loss (w/o reg) on all data: 0.40303522\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00054132147\n",
      "Norm of the params: 1.7764927\n",
      "              Random: fixed   3 labels. Loss 0.40304. Accuracy 0.778.\n",
      "### Flips: 20, rs: 9, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [186] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.379755\n",
      "Train loss (w/o reg) on all data: 0.37944034\n",
      "Test loss (w/o reg) on all data: 0.39100346\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007189321\n",
      "Norm of the params: 2.5085466\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.39100. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34106407\n",
      "Train loss (w/o reg) on all data: 0.34061965\n",
      "Test loss (w/o reg) on all data: 0.4281047\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00019230982\n",
      "Norm of the params: 2.9813523\n",
      "                Loss: fixed  16 labels. Loss 0.42810. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46703258\n",
      "Train loss (w/o reg) on all data: 0.46688032\n",
      "Test loss (w/o reg) on all data: 0.3953923\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00086861895\n",
      "Norm of the params: 1.7451278\n",
      "              Random: fixed   6 labels. Loss 0.39539. Accuracy 0.800.\n",
      "### Flips: 20, rs: 9, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [280] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35188496\n",
      "Train loss (w/o reg) on all data: 0.35148978\n",
      "Test loss (w/o reg) on all data: 0.40839154\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001200016\n",
      "Norm of the params: 2.8113372\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.40839. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [159] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3412107\n",
      "Train loss (w/o reg) on all data: 0.34077933\n",
      "Test loss (w/o reg) on all data: 0.42578378\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0007670382\n",
      "Norm of the params: 2.9371676\n",
      "                Loss: fixed  17 labels. Loss 0.42578. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4765247\n",
      "Train loss (w/o reg) on all data: 0.47638774\n",
      "Test loss (w/o reg) on all data: 0.39322197\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.008282206\n",
      "Norm of the params: 1.6551515\n",
      "              Random: fixed   7 labels. Loss 0.39322. Accuracy 0.800.\n",
      "### Flips: 20, rs: 9, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3462869\n",
      "Train loss (w/o reg) on all data: 0.3459204\n",
      "Test loss (w/o reg) on all data: 0.41516572\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001486932\n",
      "Norm of the params: 2.7072954\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.41517. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34121042\n",
      "Train loss (w/o reg) on all data: 0.34077877\n",
      "Test loss (w/o reg) on all data: 0.4254034\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0003028322\n",
      "Norm of the params: 2.9381847\n",
      "                Loss: fixed  17 labels. Loss 0.42540. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4649867\n",
      "Train loss (w/o reg) on all data: 0.46483418\n",
      "Test loss (w/o reg) on all data: 0.41259474\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0022170267\n",
      "Norm of the params: 1.7466252\n",
      "              Random: fixed   9 labels. Loss 0.41259. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4860461\n",
      "Train loss (w/o reg) on all data: 0.48591653\n",
      "Test loss (w/o reg) on all data: 0.4310332\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005411694\n",
      "Norm of the params: 1.6099244\n",
      "Flipped loss: 0.43103. Accuracy: 0.800\n",
      "### Flips: 20, rs: 10, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [179] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4373085\n",
      "Train loss (w/o reg) on all data: 0.4371158\n",
      "Test loss (w/o reg) on all data: 0.43278706\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.006282702\n",
      "Norm of the params: 1.9631433\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.43279. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [127] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.392245\n",
      "Train loss (w/o reg) on all data: 0.39195934\n",
      "Test loss (w/o reg) on all data: 0.4101876\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0018013193\n",
      "Norm of the params: 2.3902605\n",
      "                Loss: fixed   7 labels. Loss 0.41019. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46370405\n",
      "Train loss (w/o reg) on all data: 0.4635532\n",
      "Test loss (w/o reg) on all data: 0.41801053\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020555938\n",
      "Norm of the params: 1.7370225\n",
      "              Random: fixed   2 labels. Loss 0.41801. Accuracy 0.800.\n",
      "### Flips: 20, rs: 10, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [138] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38629848\n",
      "Train loss (w/o reg) on all data: 0.38607928\n",
      "Test loss (w/o reg) on all data: 0.41825894\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004302724\n",
      "Norm of the params: 2.0938506\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.41826. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35628578\n",
      "Train loss (w/o reg) on all data: 0.35593653\n",
      "Test loss (w/o reg) on all data: 0.4316831\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0035966905\n",
      "Norm of the params: 2.6429129\n",
      "                Loss: fixed  11 labels. Loss 0.43168. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46369806\n",
      "Train loss (w/o reg) on all data: 0.46354935\n",
      "Test loss (w/o reg) on all data: 0.4174179\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0069134603\n",
      "Norm of the params: 1.7245506\n",
      "              Random: fixed   2 labels. Loss 0.41742. Accuracy 0.800.\n",
      "### Flips: 20, rs: 10, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3609007\n",
      "Train loss (w/o reg) on all data: 0.36064962\n",
      "Test loss (w/o reg) on all data: 0.44003013\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012158129\n",
      "Norm of the params: 2.2409048\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.44003. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34116855\n",
      "Train loss (w/o reg) on all data: 0.3408498\n",
      "Test loss (w/o reg) on all data: 0.43439382\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004317144\n",
      "Norm of the params: 2.5249631\n",
      "                Loss: fixed  14 labels. Loss 0.43439. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [208] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46369967\n",
      "Train loss (w/o reg) on all data: 0.4635501\n",
      "Test loss (w/o reg) on all data: 0.41748315\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008162742\n",
      "Norm of the params: 1.7296278\n",
      "              Random: fixed   2 labels. Loss 0.41748. Accuracy 0.800.\n",
      "### Flips: 20, rs: 10, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [327] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34310713\n",
      "Train loss (w/o reg) on all data: 0.342753\n",
      "Test loss (w/o reg) on all data: 0.43359834\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00062191195\n",
      "Norm of the params: 2.6613812\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.43360. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34116727\n",
      "Train loss (w/o reg) on all data: 0.3408474\n",
      "Test loss (w/o reg) on all data: 0.43453023\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0003132274\n",
      "Norm of the params: 2.5292797\n",
      "                Loss: fixed  14 labels. Loss 0.43453. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46369722\n",
      "Train loss (w/o reg) on all data: 0.46354792\n",
      "Test loss (w/o reg) on all data: 0.41698268\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007430303\n",
      "Norm of the params: 1.7279888\n",
      "              Random: fixed   2 labels. Loss 0.41698. Accuracy 0.800.\n",
      "### Flips: 20, rs: 10, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [279] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3431085\n",
      "Train loss (w/o reg) on all data: 0.34275302\n",
      "Test loss (w/o reg) on all data: 0.43349528\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001637999\n",
      "Norm of the params: 2.6663582\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.43350. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [65] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34200478\n",
      "Train loss (w/o reg) on all data: 0.34166372\n",
      "Test loss (w/o reg) on all data: 0.43633363\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005280425\n",
      "Norm of the params: 2.611708\n",
      "                Loss: fixed  16 labels. Loss 0.43633. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [289] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4682531\n",
      "Train loss (w/o reg) on all data: 0.4681104\n",
      "Test loss (w/o reg) on all data: 0.40610477\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011435144\n",
      "Norm of the params: 1.6893921\n",
      "              Random: fixed   3 labels. Loss 0.40610. Accuracy 0.800.\n",
      "### Flips: 20, rs: 10, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [328] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34310752\n",
      "Train loss (w/o reg) on all data: 0.34275234\n",
      "Test loss (w/o reg) on all data: 0.43395063\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0029626852\n",
      "Norm of the params: 2.66522\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.43395. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [75] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34200475\n",
      "Train loss (w/o reg) on all data: 0.34166372\n",
      "Test loss (w/o reg) on all data: 0.436334\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00034296804\n",
      "Norm of the params: 2.6116724\n",
      "                Loss: fixed  16 labels. Loss 0.43633. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4615464\n",
      "Train loss (w/o reg) on all data: 0.4614007\n",
      "Test loss (w/o reg) on all data: 0.41991648\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011493625\n",
      "Norm of the params: 1.7070335\n",
      "              Random: fixed   4 labels. Loss 0.41992. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4611619\n",
      "Train loss (w/o reg) on all data: 0.46101278\n",
      "Test loss (w/o reg) on all data: 0.37399492\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00040645697\n",
      "Norm of the params: 1.7270433\n",
      "Flipped loss: 0.37399. Accuracy: 0.822\n",
      "### Flips: 20, rs: 11, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44303787\n",
      "Train loss (w/o reg) on all data: 0.44289887\n",
      "Test loss (w/o reg) on all data: 0.3839637\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0024840282\n",
      "Norm of the params: 1.6674064\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.38396. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [161] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41097572\n",
      "Train loss (w/o reg) on all data: 0.41079208\n",
      "Test loss (w/o reg) on all data: 0.40255043\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006998991\n",
      "Norm of the params: 1.9164537\n",
      "                Loss: fixed   4 labels. Loss 0.40255. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46116614\n",
      "Train loss (w/o reg) on all data: 0.46101803\n",
      "Test loss (w/o reg) on all data: 0.3737327\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.010183095\n",
      "Norm of the params: 1.7211254\n",
      "              Random: fixed   0 labels. Loss 0.37373. Accuracy 0.822.\n",
      "### Flips: 20, rs: 11, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40963244\n",
      "Train loss (w/o reg) on all data: 0.40945086\n",
      "Test loss (w/o reg) on all data: 0.40801603\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007529486\n",
      "Norm of the params: 1.9056723\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.40802. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36398298\n",
      "Train loss (w/o reg) on all data: 0.36372223\n",
      "Test loss (w/o reg) on all data: 0.42369652\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009953479\n",
      "Norm of the params: 2.2835474\n",
      "                Loss: fixed   9 labels. Loss 0.42370. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4611621\n",
      "Train loss (w/o reg) on all data: 0.461013\n",
      "Test loss (w/o reg) on all data: 0.3742661\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0036824776\n",
      "Norm of the params: 1.7268716\n",
      "              Random: fixed   0 labels. Loss 0.37427. Accuracy 0.822.\n",
      "### Flips: 20, rs: 11, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3928142\n",
      "Train loss (w/o reg) on all data: 0.39258158\n",
      "Test loss (w/o reg) on all data: 0.43739593\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00048812898\n",
      "Norm of the params: 2.156898\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.43740. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [105] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35132387\n",
      "Train loss (w/o reg) on all data: 0.35104594\n",
      "Test loss (w/o reg) on all data: 0.43837467\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0022699994\n",
      "Norm of the params: 2.3577192\n",
      "                Loss: fixed  11 labels. Loss 0.43837. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [363] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44175488\n",
      "Train loss (w/o reg) on all data: 0.44157898\n",
      "Test loss (w/o reg) on all data: 0.36568448\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001478664\n",
      "Norm of the params: 1.8755842\n",
      "              Random: fixed   3 labels. Loss 0.36568. Accuracy 0.822.\n",
      "### Flips: 20, rs: 11, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [211] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35684365\n",
      "Train loss (w/o reg) on all data: 0.35653982\n",
      "Test loss (w/o reg) on all data: 0.4395929\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006306441\n",
      "Norm of the params: 2.4650843\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.43959. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3470924\n",
      "Train loss (w/o reg) on all data: 0.34681645\n",
      "Test loss (w/o reg) on all data: 0.4365465\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.005387217\n",
      "Norm of the params: 2.3492181\n",
      "                Loss: fixed  12 labels. Loss 0.43655. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4417546\n",
      "Train loss (w/o reg) on all data: 0.44157872\n",
      "Test loss (w/o reg) on all data: 0.36606005\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.005587499\n",
      "Norm of the params: 1.8755876\n",
      "              Random: fixed   3 labels. Loss 0.36606. Accuracy 0.822.\n",
      "### Flips: 20, rs: 11, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35428703\n",
      "Train loss (w/o reg) on all data: 0.35396332\n",
      "Test loss (w/o reg) on all data: 0.4285697\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0034164246\n",
      "Norm of the params: 2.5444386\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.42857. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [184] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34665972\n",
      "Train loss (w/o reg) on all data: 0.34639138\n",
      "Test loss (w/o reg) on all data: 0.4325875\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0021674845\n",
      "Norm of the params: 2.3166037\n",
      "                Loss: fixed  14 labels. Loss 0.43259. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [266] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43597957\n",
      "Train loss (w/o reg) on all data: 0.43578336\n",
      "Test loss (w/o reg) on all data: 0.37458137\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00029657024\n",
      "Norm of the params: 1.9809937\n",
      "              Random: fixed   4 labels. Loss 0.37458. Accuracy 0.822.\n",
      "### Flips: 20, rs: 11, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [159] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35428932\n",
      "Train loss (w/o reg) on all data: 0.3539659\n",
      "Test loss (w/o reg) on all data: 0.42837554\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014196422\n",
      "Norm of the params: 2.5433278\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.42838. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [198] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34973106\n",
      "Train loss (w/o reg) on all data: 0.34942505\n",
      "Test loss (w/o reg) on all data: 0.39441833\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00053624687\n",
      "Norm of the params: 2.473919\n",
      "                Loss: fixed  17 labels. Loss 0.39442. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4359845\n",
      "Train loss (w/o reg) on all data: 0.43578807\n",
      "Test loss (w/o reg) on all data: 0.3744832\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00065577304\n",
      "Norm of the params: 1.9820977\n",
      "              Random: fixed   4 labels. Loss 0.37448. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45881835\n",
      "Train loss (w/o reg) on all data: 0.45863286\n",
      "Test loss (w/o reg) on all data: 0.3888649\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.012310972\n",
      "Norm of the params: 1.9260892\n",
      "Flipped loss: 0.38886. Accuracy: 0.822\n",
      "### Flips: 20, rs: 12, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [184] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4004952\n",
      "Train loss (w/o reg) on all data: 0.40013692\n",
      "Test loss (w/o reg) on all data: 0.3939053\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003651288\n",
      "Norm of the params: 2.676895\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.39391. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [226] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36456296\n",
      "Train loss (w/o reg) on all data: 0.36411443\n",
      "Test loss (w/o reg) on all data: 0.39059526\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004195319\n",
      "Norm of the params: 2.9950547\n",
      "                Loss: fixed   7 labels. Loss 0.39060. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4588152\n",
      "Train loss (w/o reg) on all data: 0.45862803\n",
      "Test loss (w/o reg) on all data: 0.38938004\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014519254\n",
      "Norm of the params: 1.9347785\n",
      "              Random: fixed   0 labels. Loss 0.38938. Accuracy 0.822.\n",
      "### Flips: 20, rs: 12, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38066715\n",
      "Train loss (w/o reg) on all data: 0.38028717\n",
      "Test loss (w/o reg) on all data: 0.39047813\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00038161778\n",
      "Norm of the params: 2.756722\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.39048. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [149] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3481622\n",
      "Train loss (w/o reg) on all data: 0.347706\n",
      "Test loss (w/o reg) on all data: 0.4001243\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0026646508\n",
      "Norm of the params: 3.0206351\n",
      "                Loss: fixed   9 labels. Loss 0.40012. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4556458\n",
      "Train loss (w/o reg) on all data: 0.45546976\n",
      "Test loss (w/o reg) on all data: 0.39775455\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00054313126\n",
      "Norm of the params: 1.876414\n",
      "              Random: fixed   2 labels. Loss 0.39775. Accuracy 0.844.\n",
      "### Flips: 20, rs: 12, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [324] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35270816\n",
      "Train loss (w/o reg) on all data: 0.352287\n",
      "Test loss (w/o reg) on all data: 0.3967258\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017820232\n",
      "Norm of the params: 2.9023242\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.39673. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [120] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.345447\n",
      "Train loss (w/o reg) on all data: 0.34501156\n",
      "Test loss (w/o reg) on all data: 0.40576336\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0003602384\n",
      "Norm of the params: 2.9511225\n",
      "                Loss: fixed  10 labels. Loss 0.40576. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [240] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42741635\n",
      "Train loss (w/o reg) on all data: 0.42716822\n",
      "Test loss (w/o reg) on all data: 0.40799692\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010188265\n",
      "Norm of the params: 2.227714\n",
      "              Random: fixed   4 labels. Loss 0.40800. Accuracy 0.822.\n",
      "### Flips: 20, rs: 12, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35112038\n",
      "Train loss (w/o reg) on all data: 0.35073823\n",
      "Test loss (w/o reg) on all data: 0.40248933\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0044616167\n",
      "Norm of the params: 2.7646637\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.40249. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [61] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3405303\n",
      "Train loss (w/o reg) on all data: 0.34014603\n",
      "Test loss (w/o reg) on all data: 0.41531214\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001533424\n",
      "Norm of the params: 2.7722077\n",
      "                Loss: fixed  12 labels. Loss 0.41531. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4165665\n",
      "Train loss (w/o reg) on all data: 0.4162827\n",
      "Test loss (w/o reg) on all data: 0.391115\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005803884\n",
      "Norm of the params: 2.3824036\n",
      "              Random: fixed   6 labels. Loss 0.39112. Accuracy 0.822.\n",
      "### Flips: 20, rs: 12, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [173] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3511135\n",
      "Train loss (w/o reg) on all data: 0.35073036\n",
      "Test loss (w/o reg) on all data: 0.4026621\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00057058886\n",
      "Norm of the params: 2.7681549\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.40266. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [138] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34066942\n",
      "Train loss (w/o reg) on all data: 0.34027746\n",
      "Test loss (w/o reg) on all data: 0.39994258\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00071017025\n",
      "Norm of the params: 2.7998917\n",
      "                Loss: fixed  13 labels. Loss 0.39994. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41656765\n",
      "Train loss (w/o reg) on all data: 0.41628262\n",
      "Test loss (w/o reg) on all data: 0.39103958\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006974682\n",
      "Norm of the params: 2.3876243\n",
      "              Random: fixed   6 labels. Loss 0.39104. Accuracy 0.822.\n",
      "### Flips: 20, rs: 12, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [150] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35281873\n",
      "Train loss (w/o reg) on all data: 0.3524702\n",
      "Test loss (w/o reg) on all data: 0.40353265\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.008345074\n",
      "Norm of the params: 2.64023\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.40353. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33959723\n",
      "Train loss (w/o reg) on all data: 0.3392118\n",
      "Test loss (w/o reg) on all data: 0.4070622\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00053334027\n",
      "Norm of the params: 2.7764409\n",
      "                Loss: fixed  14 labels. Loss 0.40706. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [185] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4094248\n",
      "Train loss (w/o reg) on all data: 0.40912485\n",
      "Test loss (w/o reg) on all data: 0.40122044\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008665071\n",
      "Norm of the params: 2.4493747\n",
      "              Random: fixed   7 labels. Loss 0.40122. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4711418\n",
      "Train loss (w/o reg) on all data: 0.47099838\n",
      "Test loss (w/o reg) on all data: 0.35168287\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0008111146\n",
      "Norm of the params: 1.6935592\n",
      "Flipped loss: 0.35168. Accuracy: 0.844\n",
      "### Flips: 20, rs: 13, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44420615\n",
      "Train loss (w/o reg) on all data: 0.44401067\n",
      "Test loss (w/o reg) on all data: 0.3361469\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00029554797\n",
      "Norm of the params: 1.9772122\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.33615. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [386] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37251058\n",
      "Train loss (w/o reg) on all data: 0.37209314\n",
      "Test loss (w/o reg) on all data: 0.34093165\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011647411\n",
      "Norm of the params: 2.889403\n",
      "                Loss: fixed   7 labels. Loss 0.34093. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4716798\n",
      "Train loss (w/o reg) on all data: 0.47154328\n",
      "Test loss (w/o reg) on all data: 0.3605749\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.010138148\n",
      "Norm of the params: 1.6524198\n",
      "              Random: fixed   3 labels. Loss 0.36057. Accuracy 0.822.\n",
      "### Flips: 20, rs: 13, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [362] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4108897\n",
      "Train loss (w/o reg) on all data: 0.41060358\n",
      "Test loss (w/o reg) on all data: 0.34275317\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0022131028\n",
      "Norm of the params: 2.3920248\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.34275. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36845756\n",
      "Train loss (w/o reg) on all data: 0.36807021\n",
      "Test loss (w/o reg) on all data: 0.342189\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00042091706\n",
      "Norm of the params: 2.7833328\n",
      "                Loss: fixed   8 labels. Loss 0.34219. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44594795\n",
      "Train loss (w/o reg) on all data: 0.4457478\n",
      "Test loss (w/o reg) on all data: 0.3530912\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0013532133\n",
      "Norm of the params: 2.000827\n",
      "              Random: fixed   6 labels. Loss 0.35309. Accuracy 0.844.\n",
      "### Flips: 20, rs: 13, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [374] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.382574\n",
      "Train loss (w/o reg) on all data: 0.382246\n",
      "Test loss (w/o reg) on all data: 0.3511636\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0005058945\n",
      "Norm of the params: 2.561229\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.35116. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.343396\n",
      "Train loss (w/o reg) on all data: 0.34294003\n",
      "Test loss (w/o reg) on all data: 0.37956014\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010854305\n",
      "Norm of the params: 3.0198977\n",
      "                Loss: fixed  12 labels. Loss 0.37956. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [387] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44594502\n",
      "Train loss (w/o reg) on all data: 0.44574553\n",
      "Test loss (w/o reg) on all data: 0.35325405\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0008274539\n",
      "Norm of the params: 1.9974483\n",
      "              Random: fixed   6 labels. Loss 0.35325. Accuracy 0.844.\n",
      "### Flips: 20, rs: 13, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [317] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3637637\n",
      "Train loss (w/o reg) on all data: 0.36340296\n",
      "Test loss (w/o reg) on all data: 0.3574735\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0020540939\n",
      "Norm of the params: 2.686034\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.35747. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [189] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.339672\n",
      "Train loss (w/o reg) on all data: 0.33920258\n",
      "Test loss (w/o reg) on all data: 0.38372293\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0024279836\n",
      "Norm of the params: 3.0640283\n",
      "                Loss: fixed  13 labels. Loss 0.38372. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [291] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4317027\n",
      "Train loss (w/o reg) on all data: 0.43146238\n",
      "Test loss (w/o reg) on all data: 0.34996143\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0013201191\n",
      "Norm of the params: 2.192416\n",
      "              Random: fixed   7 labels. Loss 0.34996. Accuracy 0.844.\n",
      "### Flips: 20, rs: 13, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [438] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34408596\n",
      "Train loss (w/o reg) on all data: 0.34370616\n",
      "Test loss (w/o reg) on all data: 0.38508\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014093929\n",
      "Norm of the params: 2.7560916\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.38508. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3399695\n",
      "Train loss (w/o reg) on all data: 0.33953005\n",
      "Test loss (w/o reg) on all data: 0.39066714\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00070078997\n",
      "Norm of the params: 2.9645355\n",
      "                Loss: fixed  15 labels. Loss 0.39067. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [374] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43418828\n",
      "Train loss (w/o reg) on all data: 0.4339499\n",
      "Test loss (w/o reg) on all data: 0.35465428\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002512599\n",
      "Norm of the params: 2.1835678\n",
      "              Random: fixed   8 labels. Loss 0.35465. Accuracy 0.822.\n",
      "### Flips: 20, rs: 13, checks: 60\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [375] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35058895\n",
      "Train loss (w/o reg) on all data: 0.35024083\n",
      "Test loss (w/o reg) on all data: 0.41033775\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0024434347\n",
      "Norm of the params: 2.63869\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.41034. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34440747\n",
      "Train loss (w/o reg) on all data: 0.3440398\n",
      "Test loss (w/o reg) on all data: 0.39315736\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0022469016\n",
      "Norm of the params: 2.711682\n",
      "                Loss: fixed  17 labels. Loss 0.39316. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [364] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44047678\n",
      "Train loss (w/o reg) on all data: 0.44025192\n",
      "Test loss (w/o reg) on all data: 0.36410806\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0021288178\n",
      "Norm of the params: 2.1205926\n",
      "              Random: fixed   9 labels. Loss 0.36411. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [284] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47952944\n",
      "Train loss (w/o reg) on all data: 0.47939998\n",
      "Test loss (w/o reg) on all data: 0.38926268\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0022948154\n",
      "Norm of the params: 1.6091827\n",
      "Flipped loss: 0.38926. Accuracy: 0.822\n",
      "### Flips: 20, rs: 14, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4273629\n",
      "Train loss (w/o reg) on all data: 0.42717534\n",
      "Test loss (w/o reg) on all data: 0.37346396\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0039595054\n",
      "Norm of the params: 1.9366782\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.37346. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39222264\n",
      "Train loss (w/o reg) on all data: 0.39195064\n",
      "Test loss (w/o reg) on all data: 0.40642956\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002322973\n",
      "Norm of the params: 2.3324454\n",
      "                Loss: fixed   7 labels. Loss 0.40643. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4701322\n",
      "Train loss (w/o reg) on all data: 0.46998724\n",
      "Test loss (w/o reg) on all data: 0.39969766\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0049531064\n",
      "Norm of the params: 1.7026303\n",
      "              Random: fixed   1 labels. Loss 0.39970. Accuracy 0.822.\n",
      "### Flips: 20, rs: 14, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [144] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38017502\n",
      "Train loss (w/o reg) on all data: 0.37987792\n",
      "Test loss (w/o reg) on all data: 0.41243058\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0029167286\n",
      "Norm of the params: 2.4376152\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.41243. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36410183\n",
      "Train loss (w/o reg) on all data: 0.36374944\n",
      "Test loss (w/o reg) on all data: 0.43078867\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.005898494\n",
      "Norm of the params: 2.6547081\n",
      "                Loss: fixed  10 labels. Loss 0.43079. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [305] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4725364\n",
      "Train loss (w/o reg) on all data: 0.4723901\n",
      "Test loss (w/o reg) on all data: 0.399946\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001212461\n",
      "Norm of the params: 1.7107903\n",
      "              Random: fixed   2 labels. Loss 0.39995. Accuracy 0.822.\n",
      "### Flips: 20, rs: 14, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3801694\n",
      "Train loss (w/o reg) on all data: 0.37987512\n",
      "Test loss (w/o reg) on all data: 0.4127669\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00074610475\n",
      "Norm of the params: 2.425988\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.41277. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [179] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3405826\n",
      "Train loss (w/o reg) on all data: 0.34020472\n",
      "Test loss (w/o reg) on all data: 0.45695037\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008754051\n",
      "Norm of the params: 2.7491632\n",
      "                Loss: fixed  14 labels. Loss 0.45695. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.472128\n",
      "Train loss (w/o reg) on all data: 0.4719829\n",
      "Test loss (w/o reg) on all data: 0.39534932\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0022503203\n",
      "Norm of the params: 1.7035844\n",
      "              Random: fixed   3 labels. Loss 0.39535. Accuracy 0.822.\n",
      "### Flips: 20, rs: 14, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37492007\n",
      "Train loss (w/o reg) on all data: 0.3746198\n",
      "Test loss (w/o reg) on all data: 0.40431306\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0003524134\n",
      "Norm of the params: 2.4505658\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.40431. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [195] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33815634\n",
      "Train loss (w/o reg) on all data: 0.3377828\n",
      "Test loss (w/o reg) on all data: 0.43921632\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007252514\n",
      "Norm of the params: 2.7332687\n",
      "                Loss: fixed  16 labels. Loss 0.43922. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [185] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45779797\n",
      "Train loss (w/o reg) on all data: 0.4576294\n",
      "Test loss (w/o reg) on all data: 0.39433482\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010576496\n",
      "Norm of the params: 1.8360636\n",
      "              Random: fixed   4 labels. Loss 0.39433. Accuracy 0.800.\n",
      "### Flips: 20, rs: 14, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35913318\n",
      "Train loss (w/o reg) on all data: 0.35880744\n",
      "Test loss (w/o reg) on all data: 0.40653497\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0002669948\n",
      "Norm of the params: 2.5524151\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.40653. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33814976\n",
      "Train loss (w/o reg) on all data: 0.33777556\n",
      "Test loss (w/o reg) on all data: 0.43968615\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00072559225\n",
      "Norm of the params: 2.7356825\n",
      "                Loss: fixed  16 labels. Loss 0.43969. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [323] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45779654\n",
      "Train loss (w/o reg) on all data: 0.45763016\n",
      "Test loss (w/o reg) on all data: 0.39420107\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.017662557\n",
      "Norm of the params: 1.8241816\n",
      "              Random: fixed   4 labels. Loss 0.39420. Accuracy 0.800.\n",
      "### Flips: 20, rs: 14, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [128] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34406513\n",
      "Train loss (w/o reg) on all data: 0.3437002\n",
      "Test loss (w/o reg) on all data: 0.41946635\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013781714\n",
      "Norm of the params: 2.701625\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.41947. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [103] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33597392\n",
      "Train loss (w/o reg) on all data: 0.33559316\n",
      "Test loss (w/o reg) on all data: 0.43336564\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00022414271\n",
      "Norm of the params: 2.7595363\n",
      "                Loss: fixed  17 labels. Loss 0.43337. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45258474\n",
      "Train loss (w/o reg) on all data: 0.45241386\n",
      "Test loss (w/o reg) on all data: 0.40223202\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00058141816\n",
      "Norm of the params: 1.8486332\n",
      "              Random: fixed   5 labels. Loss 0.40223. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [119] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51112705\n",
      "Train loss (w/o reg) on all data: 0.51101005\n",
      "Test loss (w/o reg) on all data: 0.36271176\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0026700487\n",
      "Norm of the params: 1.5299234\n",
      "Flipped loss: 0.36271. Accuracy: 0.844\n",
      "### Flips: 20, rs: 15, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [112] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45202005\n",
      "Train loss (w/o reg) on all data: 0.45183307\n",
      "Test loss (w/o reg) on all data: 0.34437406\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0073068\n",
      "Norm of the params: 1.9337842\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.34437. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42899516\n",
      "Train loss (w/o reg) on all data: 0.42874873\n",
      "Test loss (w/o reg) on all data: 0.33589226\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0056875143\n",
      "Norm of the params: 2.220131\n",
      "                Loss: fixed   7 labels. Loss 0.33589. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50693375\n",
      "Train loss (w/o reg) on all data: 0.5068171\n",
      "Test loss (w/o reg) on all data: 0.35655874\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0019018999\n",
      "Norm of the params: 1.5274255\n",
      "              Random: fixed   1 labels. Loss 0.35656. Accuracy 0.844.\n",
      "### Flips: 20, rs: 15, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42064738\n",
      "Train loss (w/o reg) on all data: 0.42039675\n",
      "Test loss (w/o reg) on all data: 0.34024036\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0014280343\n",
      "Norm of the params: 2.2388995\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.34024. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38197976\n",
      "Train loss (w/o reg) on all data: 0.38169917\n",
      "Test loss (w/o reg) on all data: 0.37169057\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010567199\n",
      "Norm of the params: 2.3688827\n",
      "                Loss: fixed  12 labels. Loss 0.37169. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5069356\n",
      "Train loss (w/o reg) on all data: 0.50681907\n",
      "Test loss (w/o reg) on all data: 0.3569101\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011976488\n",
      "Norm of the params: 1.5266203\n",
      "              Random: fixed   1 labels. Loss 0.35691. Accuracy 0.844.\n",
      "### Flips: 20, rs: 15, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4016131\n",
      "Train loss (w/o reg) on all data: 0.40132952\n",
      "Test loss (w/o reg) on all data: 0.3476252\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00093205995\n",
      "Norm of the params: 2.3815165\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.34763. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [84] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37660134\n",
      "Train loss (w/o reg) on all data: 0.3763011\n",
      "Test loss (w/o reg) on all data: 0.35925135\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.009321611\n",
      "Norm of the params: 2.4504602\n",
      "                Loss: fixed  13 labels. Loss 0.35925. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5069364\n",
      "Train loss (w/o reg) on all data: 0.5068195\n",
      "Test loss (w/o reg) on all data: 0.35692576\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0020655515\n",
      "Norm of the params: 1.5288612\n",
      "              Random: fixed   1 labels. Loss 0.35693. Accuracy 0.844.\n",
      "### Flips: 20, rs: 15, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [312] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40161258\n",
      "Train loss (w/o reg) on all data: 0.40132925\n",
      "Test loss (w/o reg) on all data: 0.34785053\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0027052548\n",
      "Norm of the params: 2.38042\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.34785. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [389] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3653466\n",
      "Train loss (w/o reg) on all data: 0.36501053\n",
      "Test loss (w/o reg) on all data: 0.3617084\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00046733057\n",
      "Norm of the params: 2.5925698\n",
      "                Loss: fixed  14 labels. Loss 0.36171. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49820676\n",
      "Train loss (w/o reg) on all data: 0.49810043\n",
      "Test loss (w/o reg) on all data: 0.3568041\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016416085\n",
      "Norm of the params: 1.4583881\n",
      "              Random: fixed   3 labels. Loss 0.35680. Accuracy 0.822.\n",
      "### Flips: 20, rs: 15, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [433] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38523442\n",
      "Train loss (w/o reg) on all data: 0.38495156\n",
      "Test loss (w/o reg) on all data: 0.35842574\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011976619\n",
      "Norm of the params: 2.378498\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.35843. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [155] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36279747\n",
      "Train loss (w/o reg) on all data: 0.36244482\n",
      "Test loss (w/o reg) on all data: 0.37690505\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0029902398\n",
      "Norm of the params: 2.6557126\n",
      "                Loss: fixed  16 labels. Loss 0.37691. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49820527\n",
      "Train loss (w/o reg) on all data: 0.4980985\n",
      "Test loss (w/o reg) on all data: 0.356877\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.011838749\n",
      "Norm of the params: 1.461343\n",
      "              Random: fixed   3 labels. Loss 0.35688. Accuracy 0.822.\n",
      "### Flips: 20, rs: 15, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [354] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36322704\n",
      "Train loss (w/o reg) on all data: 0.36289638\n",
      "Test loss (w/o reg) on all data: 0.39068964\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013842084\n",
      "Norm of the params: 2.571596\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.39069. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [100] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3621488\n",
      "Train loss (w/o reg) on all data: 0.36183634\n",
      "Test loss (w/o reg) on all data: 0.38686883\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.011669345\n",
      "Norm of the params: 2.499768\n",
      "                Loss: fixed  19 labels. Loss 0.38687. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [344] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48260656\n",
      "Train loss (w/o reg) on all data: 0.48248306\n",
      "Test loss (w/o reg) on all data: 0.34140906\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0014296635\n",
      "Norm of the params: 1.5715672\n",
      "              Random: fixed   4 labels. Loss 0.34141. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46318752\n",
      "Train loss (w/o reg) on all data: 0.46300095\n",
      "Test loss (w/o reg) on all data: 0.38967964\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005197071\n",
      "Norm of the params: 1.9316653\n",
      "Flipped loss: 0.38968. Accuracy: 0.822\n",
      "### Flips: 20, rs: 16, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [280] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3898538\n",
      "Train loss (w/o reg) on all data: 0.38955215\n",
      "Test loss (w/o reg) on all data: 0.39523965\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009396946\n",
      "Norm of the params: 2.4563024\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.39524. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [269] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35430127\n",
      "Train loss (w/o reg) on all data: 0.35390055\n",
      "Test loss (w/o reg) on all data: 0.42125803\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0039650165\n",
      "Norm of the params: 2.8310087\n",
      "                Loss: fixed   8 labels. Loss 0.42126. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4631886\n",
      "Train loss (w/o reg) on all data: 0.46300247\n",
      "Test loss (w/o reg) on all data: 0.38998213\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.010569381\n",
      "Norm of the params: 1.9293507\n",
      "              Random: fixed   0 labels. Loss 0.38998. Accuracy 0.822.\n",
      "### Flips: 20, rs: 16, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [280] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3505837\n",
      "Train loss (w/o reg) on all data: 0.35022613\n",
      "Test loss (w/o reg) on all data: 0.40259022\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0004908758\n",
      "Norm of the params: 2.6742563\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.40259. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33167234\n",
      "Train loss (w/o reg) on all data: 0.33118838\n",
      "Test loss (w/o reg) on all data: 0.4355543\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016952762\n",
      "Norm of the params: 3.1111248\n",
      "                Loss: fixed  10 labels. Loss 0.43555. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4631922\n",
      "Train loss (w/o reg) on all data: 0.46300632\n",
      "Test loss (w/o reg) on all data: 0.38960725\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018001545\n",
      "Norm of the params: 1.9280319\n",
      "              Random: fixed   0 labels. Loss 0.38961. Accuracy 0.822.\n",
      "### Flips: 20, rs: 16, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33280128\n",
      "Train loss (w/o reg) on all data: 0.33236605\n",
      "Test loss (w/o reg) on all data: 0.42469254\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0006937024\n",
      "Norm of the params: 2.9504058\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.42469. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [106] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3249393\n",
      "Train loss (w/o reg) on all data: 0.324504\n",
      "Test loss (w/o reg) on all data: 0.42540258\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001839959\n",
      "Norm of the params: 2.950659\n",
      "                Loss: fixed  12 labels. Loss 0.42540. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [374] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44198847\n",
      "Train loss (w/o reg) on all data: 0.44178274\n",
      "Test loss (w/o reg) on all data: 0.381595\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006410796\n",
      "Norm of the params: 2.0284195\n",
      "              Random: fixed   1 labels. Loss 0.38159. Accuracy 0.800.\n",
      "### Flips: 20, rs: 16, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [350] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32200876\n",
      "Train loss (w/o reg) on all data: 0.32156205\n",
      "Test loss (w/o reg) on all data: 0.43766305\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00060933694\n",
      "Norm of the params: 2.989009\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.43766. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [122] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31415063\n",
      "Train loss (w/o reg) on all data: 0.31369603\n",
      "Test loss (w/o reg) on all data: 0.44566208\n",
      "Train acc on all data:  0.8867924528301887\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005200158\n",
      "Norm of the params: 3.0152621\n",
      "                Loss: fixed  14 labels. Loss 0.44566. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [348] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44199264\n",
      "Train loss (w/o reg) on all data: 0.4417872\n",
      "Test loss (w/o reg) on all data: 0.38183612\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017328149\n",
      "Norm of the params: 2.0269494\n",
      "              Random: fixed   1 labels. Loss 0.38184. Accuracy 0.800.\n",
      "### Flips: 20, rs: 16, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [393] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32442075\n",
      "Train loss (w/o reg) on all data: 0.32402477\n",
      "Test loss (w/o reg) on all data: 0.43324614\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0043380065\n",
      "Norm of the params: 2.814209\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.43325. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3141501\n",
      "Train loss (w/o reg) on all data: 0.31369185\n",
      "Test loss (w/o reg) on all data: 0.4458143\n",
      "Train acc on all data:  0.8867924528301887\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007032005\n",
      "Norm of the params: 3.0273445\n",
      "                Loss: fixed  14 labels. Loss 0.44581. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44099522\n",
      "Train loss (w/o reg) on all data: 0.44078106\n",
      "Test loss (w/o reg) on all data: 0.38112217\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014161186\n",
      "Norm of the params: 2.0696523\n",
      "              Random: fixed   4 labels. Loss 0.38112. Accuracy 0.800.\n",
      "### Flips: 20, rs: 16, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [427] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31855923\n",
      "Train loss (w/o reg) on all data: 0.31816217\n",
      "Test loss (w/o reg) on all data: 0.4338229\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00046477007\n",
      "Norm of the params: 2.8180063\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.43382. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31443095\n",
      "Train loss (w/o reg) on all data: 0.31395283\n",
      "Test loss (w/o reg) on all data: 0.4427277\n",
      "Train acc on all data:  0.8915094339622641\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00026814864\n",
      "Norm of the params: 3.0923514\n",
      "                Loss: fixed  15 labels. Loss 0.44273. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [286] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4409932\n",
      "Train loss (w/o reg) on all data: 0.4407781\n",
      "Test loss (w/o reg) on all data: 0.3809586\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00431697\n",
      "Norm of the params: 2.0740218\n",
      "              Random: fixed   4 labels. Loss 0.38096. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [177] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48504865\n",
      "Train loss (w/o reg) on all data: 0.4848679\n",
      "Test loss (w/o reg) on all data: 0.42867318\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009336171\n",
      "Norm of the params: 1.9013863\n",
      "Flipped loss: 0.42867. Accuracy: 0.800\n",
      "### Flips: 20, rs: 17, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42288125\n",
      "Train loss (w/o reg) on all data: 0.422574\n",
      "Test loss (w/o reg) on all data: 0.41357702\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.000584249\n",
      "Norm of the params: 2.478812\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.41358. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [88] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40373525\n",
      "Train loss (w/o reg) on all data: 0.40341228\n",
      "Test loss (w/o reg) on all data: 0.42415988\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014044881\n",
      "Norm of the params: 2.5414937\n",
      "                Loss: fixed   7 labels. Loss 0.42416. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48106742\n",
      "Train loss (w/o reg) on all data: 0.48087183\n",
      "Test loss (w/o reg) on all data: 0.43446\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00089098554\n",
      "Norm of the params: 1.9779062\n",
      "              Random: fixed   2 labels. Loss 0.43446. Accuracy 0.778.\n",
      "### Flips: 20, rs: 17, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [134] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39509544\n",
      "Train loss (w/o reg) on all data: 0.39476633\n",
      "Test loss (w/o reg) on all data: 0.4228578\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0042412565\n",
      "Norm of the params: 2.565548\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.42286. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [278] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37494615\n",
      "Train loss (w/o reg) on all data: 0.37457252\n",
      "Test loss (w/o reg) on all data: 0.41802287\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013325668\n",
      "Norm of the params: 2.7335677\n",
      "                Loss: fixed  10 labels. Loss 0.41802. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48106685\n",
      "Train loss (w/o reg) on all data: 0.48087046\n",
      "Test loss (w/o reg) on all data: 0.43380538\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00065000856\n",
      "Norm of the params: 1.9818794\n",
      "              Random: fixed   2 labels. Loss 0.43381. Accuracy 0.778.\n",
      "### Flips: 20, rs: 17, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3848077\n",
      "Train loss (w/o reg) on all data: 0.38446224\n",
      "Test loss (w/o reg) on all data: 0.40220815\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011127982\n",
      "Norm of the params: 2.6285326\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.40221. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36343646\n",
      "Train loss (w/o reg) on all data: 0.36306\n",
      "Test loss (w/o reg) on all data: 0.42653817\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0027581004\n",
      "Norm of the params: 2.7439914\n",
      "                Loss: fixed  12 labels. Loss 0.42654. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [147] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4810683\n",
      "Train loss (w/o reg) on all data: 0.4808719\n",
      "Test loss (w/o reg) on all data: 0.43438306\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0040910323\n",
      "Norm of the params: 1.9820772\n",
      "              Random: fixed   2 labels. Loss 0.43438. Accuracy 0.778.\n",
      "### Flips: 20, rs: 17, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36659098\n",
      "Train loss (w/o reg) on all data: 0.36625588\n",
      "Test loss (w/o reg) on all data: 0.39310005\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00094041356\n",
      "Norm of the params: 2.588804\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.39310. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [293] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35684377\n",
      "Train loss (w/o reg) on all data: 0.3564604\n",
      "Test loss (w/o reg) on all data: 0.42823783\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0032660407\n",
      "Norm of the params: 2.7690713\n",
      "                Loss: fixed  14 labels. Loss 0.42824. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [291] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46059042\n",
      "Train loss (w/o reg) on all data: 0.46037295\n",
      "Test loss (w/o reg) on all data: 0.42473\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0005763796\n",
      "Norm of the params: 2.0854661\n",
      "              Random: fixed   4 labels. Loss 0.42473. Accuracy 0.778.\n",
      "### Flips: 20, rs: 17, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [300] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.358273\n",
      "Train loss (w/o reg) on all data: 0.35793686\n",
      "Test loss (w/o reg) on all data: 0.40278724\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001009361\n",
      "Norm of the params: 2.5928009\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.40279. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [135] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3524445\n",
      "Train loss (w/o reg) on all data: 0.35209894\n",
      "Test loss (w/o reg) on all data: 0.4038043\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007195955\n",
      "Norm of the params: 2.6289375\n",
      "                Loss: fixed  16 labels. Loss 0.40380. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44233814\n",
      "Train loss (w/o reg) on all data: 0.4420615\n",
      "Test loss (w/o reg) on all data: 0.4427193\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0007617444\n",
      "Norm of the params: 2.3520882\n",
      "              Random: fixed   6 labels. Loss 0.44272. Accuracy 0.778.\n",
      "### Flips: 20, rs: 17, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35251376\n",
      "Train loss (w/o reg) on all data: 0.3521781\n",
      "Test loss (w/o reg) on all data: 0.40632105\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005355195\n",
      "Norm of the params: 2.590959\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.40632. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [149] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3526491\n",
      "Train loss (w/o reg) on all data: 0.35228992\n",
      "Test loss (w/o reg) on all data: 0.41017705\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00044484253\n",
      "Norm of the params: 2.6801732\n",
      "                Loss: fixed  17 labels. Loss 0.41018. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44646528\n",
      "Train loss (w/o reg) on all data: 0.4461996\n",
      "Test loss (w/o reg) on all data: 0.44709414\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0016844007\n",
      "Norm of the params: 2.3051689\n",
      "              Random: fixed   7 labels. Loss 0.44709. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [313] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52638745\n",
      "Train loss (w/o reg) on all data: 0.52628386\n",
      "Test loss (w/o reg) on all data: 0.4311341\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012499272\n",
      "Norm of the params: 1.439406\n",
      "Flipped loss: 0.43113. Accuracy: 0.800\n",
      "### Flips: 20, rs: 18, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [110] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46967417\n",
      "Train loss (w/o reg) on all data: 0.46954763\n",
      "Test loss (w/o reg) on all data: 0.39658374\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0036195829\n",
      "Norm of the params: 1.5909287\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.39658. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42508766\n",
      "Train loss (w/o reg) on all data: 0.42492443\n",
      "Test loss (w/o reg) on all data: 0.3921374\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00039585002\n",
      "Norm of the params: 1.8067949\n",
      "                Loss: fixed   8 labels. Loss 0.39214. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52638537\n",
      "Train loss (w/o reg) on all data: 0.5262807\n",
      "Test loss (w/o reg) on all data: 0.43151885\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004691444\n",
      "Norm of the params: 1.4466889\n",
      "              Random: fixed   0 labels. Loss 0.43152. Accuracy 0.800.\n",
      "### Flips: 20, rs: 18, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4254384\n",
      "Train loss (w/o reg) on all data: 0.42520592\n",
      "Test loss (w/o reg) on all data: 0.40247428\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0026733573\n",
      "Norm of the params: 2.1563663\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.40247. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38204852\n",
      "Train loss (w/o reg) on all data: 0.3817931\n",
      "Test loss (w/o reg) on all data: 0.39612278\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005650171\n",
      "Norm of the params: 2.260137\n",
      "                Loss: fixed  12 labels. Loss 0.39612. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5204795\n",
      "Train loss (w/o reg) on all data: 0.5203771\n",
      "Test loss (w/o reg) on all data: 0.41820037\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017450481\n",
      "Norm of the params: 1.4311415\n",
      "              Random: fixed   1 labels. Loss 0.41820. Accuracy 0.800.\n",
      "### Flips: 20, rs: 18, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4091713\n",
      "Train loss (w/o reg) on all data: 0.40889642\n",
      "Test loss (w/o reg) on all data: 0.40734464\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017110667\n",
      "Norm of the params: 2.3447654\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.40734. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37132835\n",
      "Train loss (w/o reg) on all data: 0.37105542\n",
      "Test loss (w/o reg) on all data: 0.3941323\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010678272\n",
      "Norm of the params: 2.3363266\n",
      "                Loss: fixed  13 labels. Loss 0.39413. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [140] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5204855\n",
      "Train loss (w/o reg) on all data: 0.5203834\n",
      "Test loss (w/o reg) on all data: 0.4184743\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017253036\n",
      "Norm of the params: 1.429006\n",
      "              Random: fixed   1 labels. Loss 0.41847. Accuracy 0.800.\n",
      "### Flips: 20, rs: 18, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40917137\n",
      "Train loss (w/o reg) on all data: 0.40889618\n",
      "Test loss (w/o reg) on all data: 0.40737703\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017382613\n",
      "Norm of the params: 2.3460083\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.40738. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [195] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37132952\n",
      "Train loss (w/o reg) on all data: 0.37105647\n",
      "Test loss (w/o reg) on all data: 0.39394972\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0039888364\n",
      "Norm of the params: 2.3369017\n",
      "                Loss: fixed  13 labels. Loss 0.39395. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52031094\n",
      "Train loss (w/o reg) on all data: 0.5202053\n",
      "Test loss (w/o reg) on all data: 0.40732494\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0038121773\n",
      "Norm of the params: 1.4535971\n",
      "              Random: fixed   2 labels. Loss 0.40732. Accuracy 0.800.\n",
      "### Flips: 20, rs: 18, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [208] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3966565\n",
      "Train loss (w/o reg) on all data: 0.39637116\n",
      "Test loss (w/o reg) on all data: 0.40661132\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009111048\n",
      "Norm of the params: 2.388965\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.40661. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [157] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35944557\n",
      "Train loss (w/o reg) on all data: 0.3591146\n",
      "Test loss (w/o reg) on all data: 0.407\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013479404\n",
      "Norm of the params: 2.5728948\n",
      "                Loss: fixed  18 labels. Loss 0.40700. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [314] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5009644\n",
      "Train loss (w/o reg) on all data: 0.5008379\n",
      "Test loss (w/o reg) on all data: 0.39396334\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008800781\n",
      "Norm of the params: 1.5905874\n",
      "              Random: fixed   4 labels. Loss 0.39396. Accuracy 0.800.\n",
      "### Flips: 20, rs: 18, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37854818\n",
      "Train loss (w/o reg) on all data: 0.37825674\n",
      "Test loss (w/o reg) on all data: 0.4168442\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00095093297\n",
      "Norm of the params: 2.4143238\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.41684. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [95] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3594334\n",
      "Train loss (w/o reg) on all data: 0.359105\n",
      "Test loss (w/o reg) on all data: 0.40696523\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011341404\n",
      "Norm of the params: 2.5629191\n",
      "                Loss: fixed  18 labels. Loss 0.40697. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [330] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49289975\n",
      "Train loss (w/o reg) on all data: 0.49277392\n",
      "Test loss (w/o reg) on all data: 0.39016593\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0025468362\n",
      "Norm of the params: 1.5862787\n",
      "              Random: fixed   5 labels. Loss 0.39017. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [333] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45798495\n",
      "Train loss (w/o reg) on all data: 0.45781693\n",
      "Test loss (w/o reg) on all data: 0.46333912\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00092199247\n",
      "Norm of the params: 1.8331882\n",
      "Flipped loss: 0.46334. Accuracy: 0.756\n",
      "### Flips: 20, rs: 19, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [85] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4082994\n",
      "Train loss (w/o reg) on all data: 0.40805793\n",
      "Test loss (w/o reg) on all data: 0.42887175\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001087674\n",
      "Norm of the params: 2.1975145\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.42887. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39170045\n",
      "Train loss (w/o reg) on all data: 0.39141563\n",
      "Test loss (w/o reg) on all data: 0.46036166\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0031723282\n",
      "Norm of the params: 2.3867195\n",
      "                Loss: fixed   5 labels. Loss 0.46036. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4605603\n",
      "Train loss (w/o reg) on all data: 0.46040118\n",
      "Test loss (w/o reg) on all data: 0.44759932\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0002421931\n",
      "Norm of the params: 1.7839733\n",
      "              Random: fixed   1 labels. Loss 0.44760. Accuracy 0.756.\n",
      "### Flips: 20, rs: 19, checks: 20\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [112] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36700433\n",
      "Train loss (w/o reg) on all data: 0.3667079\n",
      "Test loss (w/o reg) on all data: 0.4394786\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0007194962\n",
      "Norm of the params: 2.434977\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.43948. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36924076\n",
      "Train loss (w/o reg) on all data: 0.368871\n",
      "Test loss (w/o reg) on all data: 0.44638252\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00058906706\n",
      "Norm of the params: 2.7194521\n",
      "                Loss: fixed   7 labels. Loss 0.44638. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [293] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4580659\n",
      "Train loss (w/o reg) on all data: 0.45790648\n",
      "Test loss (w/o reg) on all data: 0.44812968\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.006553911\n",
      "Norm of the params: 1.7854875\n",
      "              Random: fixed   2 labels. Loss 0.44813. Accuracy 0.756.\n",
      "### Flips: 20, rs: 19, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [290] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37255383\n",
      "Train loss (w/o reg) on all data: 0.37225825\n",
      "Test loss (w/o reg) on all data: 0.44822395\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0040842374\n",
      "Norm of the params: 2.4313364\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.44822. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [189] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34186158\n",
      "Train loss (w/o reg) on all data: 0.3414506\n",
      "Test loss (w/o reg) on all data: 0.45615917\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0030197157\n",
      "Norm of the params: 2.8670115\n",
      "                Loss: fixed  12 labels. Loss 0.45616. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [135] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4516611\n",
      "Train loss (w/o reg) on all data: 0.45149228\n",
      "Test loss (w/o reg) on all data: 0.44262815\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0016667818\n",
      "Norm of the params: 1.8376098\n",
      "              Random: fixed   3 labels. Loss 0.44263. Accuracy 0.756.\n",
      "### Flips: 20, rs: 19, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [318] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36321995\n",
      "Train loss (w/o reg) on all data: 0.36289996\n",
      "Test loss (w/o reg) on all data: 0.45678037\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.000860999\n",
      "Norm of the params: 2.5297956\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.45678. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [159] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33927932\n",
      "Train loss (w/o reg) on all data: 0.33887205\n",
      "Test loss (w/o reg) on all data: 0.45900849\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0008625571\n",
      "Norm of the params: 2.854087\n",
      "                Loss: fixed  13 labels. Loss 0.45901. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45165437\n",
      "Train loss (w/o reg) on all data: 0.4514851\n",
      "Test loss (w/o reg) on all data: 0.44343397\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0014284994\n",
      "Norm of the params: 1.8400499\n",
      "              Random: fixed   3 labels. Loss 0.44343. Accuracy 0.756.\n",
      "### Flips: 20, rs: 19, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34968638\n",
      "Train loss (w/o reg) on all data: 0.34932286\n",
      "Test loss (w/o reg) on all data: 0.44292662\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0009353535\n",
      "Norm of the params: 2.6964378\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.44293. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33458012\n",
      "Train loss (w/o reg) on all data: 0.33419898\n",
      "Test loss (w/o reg) on all data: 0.45657742\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0045003654\n",
      "Norm of the params: 2.760931\n",
      "                Loss: fixed  14 labels. Loss 0.45658. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [201] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45165625\n",
      "Train loss (w/o reg) on all data: 0.45148712\n",
      "Test loss (w/o reg) on all data: 0.44309327\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00074880663\n",
      "Norm of the params: 1.839184\n",
      "              Random: fixed   3 labels. Loss 0.44309. Accuracy 0.756.\n",
      "### Flips: 20, rs: 19, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [322] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3458596\n",
      "Train loss (w/o reg) on all data: 0.34550163\n",
      "Test loss (w/o reg) on all data: 0.45244378\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00067904126\n",
      "Norm of the params: 2.675685\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.45244. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [187] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3345793\n",
      "Train loss (w/o reg) on all data: 0.33419922\n",
      "Test loss (w/o reg) on all data: 0.45650655\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0021722857\n",
      "Norm of the params: 2.757036\n",
      "                Loss: fixed  14 labels. Loss 0.45651. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44367206\n",
      "Train loss (w/o reg) on all data: 0.44346914\n",
      "Test loss (w/o reg) on all data: 0.464861\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0006268482\n",
      "Norm of the params: 2.014536\n",
      "              Random: fixed   5 labels. Loss 0.46486. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5243825\n",
      "Train loss (w/o reg) on all data: 0.5242813\n",
      "Test loss (w/o reg) on all data: 0.4134435\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017928714\n",
      "Norm of the params: 1.4224066\n",
      "Flipped loss: 0.41344. Accuracy: 0.800\n",
      "### Flips: 20, rs: 20, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [151] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4737947\n",
      "Train loss (w/o reg) on all data: 0.47362953\n",
      "Test loss (w/o reg) on all data: 0.41096538\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.006023782\n",
      "Norm of the params: 1.817565\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.41097. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43663505\n",
      "Train loss (w/o reg) on all data: 0.43643585\n",
      "Test loss (w/o reg) on all data: 0.3820892\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00047090504\n",
      "Norm of the params: 1.9960576\n",
      "                Loss: fixed   7 labels. Loss 0.38209. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50791335\n",
      "Train loss (w/o reg) on all data: 0.50779384\n",
      "Test loss (w/o reg) on all data: 0.41552496\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0025403143\n",
      "Norm of the params: 1.5458175\n",
      "              Random: fixed   2 labels. Loss 0.41552. Accuracy 0.800.\n",
      "### Flips: 20, rs: 20, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [147] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41664454\n",
      "Train loss (w/o reg) on all data: 0.41642347\n",
      "Test loss (w/o reg) on all data: 0.3835294\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018622224\n",
      "Norm of the params: 2.1027958\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.38353. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [126] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4036588\n",
      "Train loss (w/o reg) on all data: 0.40340078\n",
      "Test loss (w/o reg) on all data: 0.40666217\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008640376\n",
      "Norm of the params: 2.2717104\n",
      "                Loss: fixed  11 labels. Loss 0.40666. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [285] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.502899\n",
      "Train loss (w/o reg) on all data: 0.50277084\n",
      "Test loss (w/o reg) on all data: 0.41072604\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019116432\n",
      "Norm of the params: 1.6010587\n",
      "              Random: fixed   3 labels. Loss 0.41073. Accuracy 0.800.\n",
      "### Flips: 20, rs: 20, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40730402\n",
      "Train loss (w/o reg) on all data: 0.4070579\n",
      "Test loss (w/o reg) on all data: 0.38724098\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002306986\n",
      "Norm of the params: 2.2186236\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.38724. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3763723\n",
      "Train loss (w/o reg) on all data: 0.3760661\n",
      "Test loss (w/o reg) on all data: 0.42956755\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00037519165\n",
      "Norm of the params: 2.4747229\n",
      "                Loss: fixed  15 labels. Loss 0.42957. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49263015\n",
      "Train loss (w/o reg) on all data: 0.49248597\n",
      "Test loss (w/o reg) on all data: 0.40760934\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0042960984\n",
      "Norm of the params: 1.6982226\n",
      "              Random: fixed   4 labels. Loss 0.40761. Accuracy 0.800.\n",
      "### Flips: 20, rs: 20, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39526853\n",
      "Train loss (w/o reg) on all data: 0.3949986\n",
      "Test loss (w/o reg) on all data: 0.3926422\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010462337\n",
      "Norm of the params: 2.3234565\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.39264. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36712134\n",
      "Train loss (w/o reg) on all data: 0.36680192\n",
      "Test loss (w/o reg) on all data: 0.42416975\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012095197\n",
      "Norm of the params: 2.5274806\n",
      "                Loss: fixed  17 labels. Loss 0.42417. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4926334\n",
      "Train loss (w/o reg) on all data: 0.4924897\n",
      "Test loss (w/o reg) on all data: 0.40727478\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0033847846\n",
      "Norm of the params: 1.6953663\n",
      "              Random: fixed   4 labels. Loss 0.40727. Accuracy 0.800.\n",
      "### Flips: 20, rs: 20, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [240] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38095385\n",
      "Train loss (w/o reg) on all data: 0.38067088\n",
      "Test loss (w/o reg) on all data: 0.39305657\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013466296\n",
      "Norm of the params: 2.3789926\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.39306. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.364822\n",
      "Train loss (w/o reg) on all data: 0.36448306\n",
      "Test loss (w/o reg) on all data: 0.4115224\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.006442633\n",
      "Norm of the params: 2.6035862\n",
      "                Loss: fixed  18 labels. Loss 0.41152. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [342] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4801202\n",
      "Train loss (w/o reg) on all data: 0.47995773\n",
      "Test loss (w/o reg) on all data: 0.40148723\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008147299\n",
      "Norm of the params: 1.802692\n",
      "              Random: fixed   5 labels. Loss 0.40149. Accuracy 0.800.\n",
      "### Flips: 20, rs: 20, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [154] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37214196\n",
      "Train loss (w/o reg) on all data: 0.37185842\n",
      "Test loss (w/o reg) on all data: 0.40732235\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005313499\n",
      "Norm of the params: 2.3813581\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.40732. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [132] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35923913\n",
      "Train loss (w/o reg) on all data: 0.3589228\n",
      "Test loss (w/o reg) on all data: 0.4051252\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0026745673\n",
      "Norm of the params: 2.5152247\n",
      "                Loss: fixed  20 labels. Loss 0.40513. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47408953\n",
      "Train loss (w/o reg) on all data: 0.47392902\n",
      "Test loss (w/o reg) on all data: 0.41007233\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011888314\n",
      "Norm of the params: 1.7916765\n",
      "              Random: fixed   6 labels. Loss 0.41007. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49122772\n",
      "Train loss (w/o reg) on all data: 0.49107984\n",
      "Test loss (w/o reg) on all data: 0.41600788\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001267649\n",
      "Norm of the params: 1.7197641\n",
      "Flipped loss: 0.41601. Accuracy: 0.822\n",
      "### Flips: 20, rs: 21, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44233173\n",
      "Train loss (w/o reg) on all data: 0.44213697\n",
      "Test loss (w/o reg) on all data: 0.42861515\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016532008\n",
      "Norm of the params: 1.9736463\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.42862. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3867366\n",
      "Train loss (w/o reg) on all data: 0.3864302\n",
      "Test loss (w/o reg) on all data: 0.42683458\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015640992\n",
      "Norm of the params: 2.475503\n",
      "                Loss: fixed   8 labels. Loss 0.42683. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [285] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49455452\n",
      "Train loss (w/o reg) on all data: 0.49441412\n",
      "Test loss (w/o reg) on all data: 0.41789505\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0052468097\n",
      "Norm of the params: 1.6757617\n",
      "              Random: fixed   1 labels. Loss 0.41790. Accuracy 0.800.\n",
      "### Flips: 20, rs: 21, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41978133\n",
      "Train loss (w/o reg) on all data: 0.41955075\n",
      "Test loss (w/o reg) on all data: 0.421717\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002402446\n",
      "Norm of the params: 2.1474736\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.42172. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [226] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3677552\n",
      "Train loss (w/o reg) on all data: 0.36737993\n",
      "Test loss (w/o reg) on all data: 0.43322098\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016719167\n",
      "Norm of the params: 2.739626\n",
      "                Loss: fixed  10 labels. Loss 0.43322. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49456143\n",
      "Train loss (w/o reg) on all data: 0.4944209\n",
      "Test loss (w/o reg) on all data: 0.41769573\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016717382\n",
      "Norm of the params: 1.6766515\n",
      "              Random: fixed   1 labels. Loss 0.41770. Accuracy 0.822.\n",
      "### Flips: 20, rs: 21, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37858212\n",
      "Train loss (w/o reg) on all data: 0.37829214\n",
      "Test loss (w/o reg) on all data: 0.41165897\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.005333357\n",
      "Norm of the params: 2.4082062\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.41166. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35858005\n",
      "Train loss (w/o reg) on all data: 0.35820308\n",
      "Test loss (w/o reg) on all data: 0.42654437\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007901272\n",
      "Norm of the params: 2.7458417\n",
      "                Loss: fixed  12 labels. Loss 0.42654. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [302] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4945554\n",
      "Train loss (w/o reg) on all data: 0.49441436\n",
      "Test loss (w/o reg) on all data: 0.41829702\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003456148\n",
      "Norm of the params: 1.6796372\n",
      "              Random: fixed   1 labels. Loss 0.41830. Accuracy 0.800.\n",
      "### Flips: 20, rs: 21, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36498988\n",
      "Train loss (w/o reg) on all data: 0.36465347\n",
      "Test loss (w/o reg) on all data: 0.4024869\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00042299824\n",
      "Norm of the params: 2.5938642\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.40249. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [92] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3558607\n",
      "Train loss (w/o reg) on all data: 0.3554979\n",
      "Test loss (w/o reg) on all data: 0.4240056\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0020142037\n",
      "Norm of the params: 2.6937082\n",
      "                Loss: fixed  13 labels. Loss 0.42401. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [376] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4945528\n",
      "Train loss (w/o reg) on all data: 0.49441254\n",
      "Test loss (w/o reg) on all data: 0.4180056\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019364607\n",
      "Norm of the params: 1.674793\n",
      "              Random: fixed   1 labels. Loss 0.41801. Accuracy 0.822.\n",
      "### Flips: 20, rs: 21, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36499044\n",
      "Train loss (w/o reg) on all data: 0.36465415\n",
      "Test loss (w/o reg) on all data: 0.402644\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00036812882\n",
      "Norm of the params: 2.5934033\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.40264. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [193] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35476917\n",
      "Train loss (w/o reg) on all data: 0.3544133\n",
      "Test loss (w/o reg) on all data: 0.43110093\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017186842\n",
      "Norm of the params: 2.6678798\n",
      "                Loss: fixed  14 labels. Loss 0.43110. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [305] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4691338\n",
      "Train loss (w/o reg) on all data: 0.46895996\n",
      "Test loss (w/o reg) on all data: 0.41459155\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020469672\n",
      "Norm of the params: 1.8645947\n",
      "              Random: fixed   3 labels. Loss 0.41459. Accuracy 0.800.\n",
      "### Flips: 20, rs: 21, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [240] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36136928\n",
      "Train loss (w/o reg) on all data: 0.3610406\n",
      "Test loss (w/o reg) on all data: 0.40178338\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00078649056\n",
      "Norm of the params: 2.5638843\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.40178. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [105] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35262305\n",
      "Train loss (w/o reg) on all data: 0.35228938\n",
      "Test loss (w/o reg) on all data: 0.416835\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005296517\n",
      "Norm of the params: 2.5832677\n",
      "                Loss: fixed  16 labels. Loss 0.41684. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [199] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45164278\n",
      "Train loss (w/o reg) on all data: 0.45143723\n",
      "Test loss (w/o reg) on all data: 0.42480308\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00112354\n",
      "Norm of the params: 2.0275066\n",
      "              Random: fixed   5 labels. Loss 0.42480. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [187] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.502071\n",
      "Train loss (w/o reg) on all data: 0.5019462\n",
      "Test loss (w/o reg) on all data: 0.38843548\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009676722\n",
      "Norm of the params: 1.5801294\n",
      "Flipped loss: 0.38844. Accuracy: 0.822\n",
      "### Flips: 20, rs: 22, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45358694\n",
      "Train loss (w/o reg) on all data: 0.45342377\n",
      "Test loss (w/o reg) on all data: 0.3701921\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0033858533\n",
      "Norm of the params: 1.806533\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.37019. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3853785\n",
      "Train loss (w/o reg) on all data: 0.38503504\n",
      "Test loss (w/o reg) on all data: 0.38171765\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0025613704\n",
      "Norm of the params: 2.6209226\n",
      "                Loss: fixed   8 labels. Loss 0.38172. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49541074\n",
      "Train loss (w/o reg) on all data: 0.49528098\n",
      "Test loss (w/o reg) on all data: 0.3955509\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0035644474\n",
      "Norm of the params: 1.610912\n",
      "              Random: fixed   1 labels. Loss 0.39555. Accuracy 0.822.\n",
      "### Flips: 20, rs: 22, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [202] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3945601\n",
      "Train loss (w/o reg) on all data: 0.39430016\n",
      "Test loss (w/o reg) on all data: 0.37357834\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.003749905\n",
      "Norm of the params: 2.2800906\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.37358. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3509107\n",
      "Train loss (w/o reg) on all data: 0.350509\n",
      "Test loss (w/o reg) on all data: 0.40169096\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0061682365\n",
      "Norm of the params: 2.8344495\n",
      "                Loss: fixed  12 labels. Loss 0.40169. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48345372\n",
      "Train loss (w/o reg) on all data: 0.48330602\n",
      "Test loss (w/o reg) on all data: 0.39091343\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0021704051\n",
      "Norm of the params: 1.7187741\n",
      "              Random: fixed   2 labels. Loss 0.39091. Accuracy 0.800.\n",
      "### Flips: 20, rs: 22, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3779932\n",
      "Train loss (w/o reg) on all data: 0.37768278\n",
      "Test loss (w/o reg) on all data: 0.38249227\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0004537907\n",
      "Norm of the params: 2.4917\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.38249. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33563027\n",
      "Train loss (w/o reg) on all data: 0.33522567\n",
      "Test loss (w/o reg) on all data: 0.4270911\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00062375807\n",
      "Norm of the params: 2.8446596\n",
      "                Loss: fixed  14 labels. Loss 0.42709. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [173] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47036394\n",
      "Train loss (w/o reg) on all data: 0.4701983\n",
      "Test loss (w/o reg) on all data: 0.3850773\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00066238805\n",
      "Norm of the params: 1.8200605\n",
      "              Random: fixed   3 labels. Loss 0.38508. Accuracy 0.822.\n",
      "### Flips: 20, rs: 22, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35998476\n",
      "Train loss (w/o reg) on all data: 0.35964486\n",
      "Test loss (w/o reg) on all data: 0.38390684\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00051937404\n",
      "Norm of the params: 2.6073325\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.38391. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3326995\n",
      "Train loss (w/o reg) on all data: 0.3322825\n",
      "Test loss (w/o reg) on all data: 0.42959112\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0030490113\n",
      "Norm of the params: 2.8879128\n",
      "                Loss: fixed  15 labels. Loss 0.42959. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46333176\n",
      "Train loss (w/o reg) on all data: 0.46315712\n",
      "Test loss (w/o reg) on all data: 0.39155006\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013502764\n",
      "Norm of the params: 1.8688784\n",
      "              Random: fixed   5 labels. Loss 0.39155. Accuracy 0.822.\n",
      "### Flips: 20, rs: 22, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [130] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35138315\n",
      "Train loss (w/o reg) on all data: 0.35103688\n",
      "Test loss (w/o reg) on all data: 0.39566278\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0003439898\n",
      "Norm of the params: 2.6315815\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.39566. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32901224\n",
      "Train loss (w/o reg) on all data: 0.32863012\n",
      "Test loss (w/o reg) on all data: 0.42728174\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00057884504\n",
      "Norm of the params: 2.764538\n",
      "                Loss: fixed  16 labels. Loss 0.42728. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [338] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46332943\n",
      "Train loss (w/o reg) on all data: 0.46315572\n",
      "Test loss (w/o reg) on all data: 0.3916618\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012995973\n",
      "Norm of the params: 1.863912\n",
      "              Random: fixed   5 labels. Loss 0.39166. Accuracy 0.822.\n",
      "### Flips: 20, rs: 22, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [334] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33616114\n",
      "Train loss (w/o reg) on all data: 0.33578146\n",
      "Test loss (w/o reg) on all data: 0.41642508\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016908648\n",
      "Norm of the params: 2.7557037\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.41643. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [112] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32901943\n",
      "Train loss (w/o reg) on all data: 0.32863718\n",
      "Test loss (w/o reg) on all data: 0.42689013\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00066895154\n",
      "Norm of the params: 2.7649338\n",
      "                Loss: fixed  16 labels. Loss 0.42689. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [342] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45975938\n",
      "Train loss (w/o reg) on all data: 0.45957518\n",
      "Test loss (w/o reg) on all data: 0.3946707\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014220554\n",
      "Norm of the params: 1.9193612\n",
      "              Random: fixed   6 labels. Loss 0.39467. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47750196\n",
      "Train loss (w/o reg) on all data: 0.47739345\n",
      "Test loss (w/o reg) on all data: 0.4123294\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001525398\n",
      "Norm of the params: 1.473161\n",
      "Flipped loss: 0.41233. Accuracy: 0.822\n",
      "### Flips: 20, rs: 23, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40731746\n",
      "Train loss (w/o reg) on all data: 0.40712154\n",
      "Test loss (w/o reg) on all data: 0.3778806\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001872583\n",
      "Norm of the params: 1.9794416\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.37788. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36844462\n",
      "Train loss (w/o reg) on all data: 0.36814845\n",
      "Test loss (w/o reg) on all data: 0.4116976\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008499982\n",
      "Norm of the params: 2.4338212\n",
      "                Loss: fixed   8 labels. Loss 0.41170. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4775017\n",
      "Train loss (w/o reg) on all data: 0.4773936\n",
      "Test loss (w/o reg) on all data: 0.41245082\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006788234\n",
      "Norm of the params: 1.4704013\n",
      "              Random: fixed   0 labels. Loss 0.41245. Accuracy 0.822.\n",
      "### Flips: 20, rs: 23, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [305] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3814859\n",
      "Train loss (w/o reg) on all data: 0.38120508\n",
      "Test loss (w/o reg) on all data: 0.41360852\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0030363146\n",
      "Norm of the params: 2.3699145\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.41361. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [372] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3492644\n",
      "Train loss (w/o reg) on all data: 0.3489179\n",
      "Test loss (w/o reg) on all data: 0.4132665\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006396457\n",
      "Norm of the params: 2.6325803\n",
      "                Loss: fixed  10 labels. Loss 0.41327. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44994813\n",
      "Train loss (w/o reg) on all data: 0.44979975\n",
      "Test loss (w/o reg) on all data: 0.41938272\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.014995977\n",
      "Norm of the params: 1.7226475\n",
      "              Random: fixed   2 labels. Loss 0.41938. Accuracy 0.822.\n",
      "### Flips: 20, rs: 23, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34992447\n",
      "Train loss (w/o reg) on all data: 0.34957463\n",
      "Test loss (w/o reg) on all data: 0.41843534\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00172656\n",
      "Norm of the params: 2.6451344\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.41844. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [310] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33631966\n",
      "Train loss (w/o reg) on all data: 0.3359413\n",
      "Test loss (w/o reg) on all data: 0.4182417\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.000956301\n",
      "Norm of the params: 2.7507699\n",
      "                Loss: fixed  12 labels. Loss 0.41824. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [266] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4499502\n",
      "Train loss (w/o reg) on all data: 0.44980258\n",
      "Test loss (w/o reg) on all data: 0.41916683\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0065521016\n",
      "Norm of the params: 1.7181762\n",
      "              Random: fixed   2 labels. Loss 0.41917. Accuracy 0.822.\n",
      "### Flips: 20, rs: 23, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34992492\n",
      "Train loss (w/o reg) on all data: 0.34957284\n",
      "Test loss (w/o reg) on all data: 0.41849294\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006989433\n",
      "Norm of the params: 2.6536396\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.41849. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33276677\n",
      "Train loss (w/o reg) on all data: 0.33233723\n",
      "Test loss (w/o reg) on all data: 0.43572396\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00054947886\n",
      "Norm of the params: 2.9309912\n",
      "                Loss: fixed  15 labels. Loss 0.43572. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44995025\n",
      "Train loss (w/o reg) on all data: 0.4498007\n",
      "Test loss (w/o reg) on all data: 0.41969252\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00051720947\n",
      "Norm of the params: 1.7293627\n",
      "              Random: fixed   2 labels. Loss 0.41969. Accuracy 0.822.\n",
      "### Flips: 20, rs: 23, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34522244\n",
      "Train loss (w/o reg) on all data: 0.34487668\n",
      "Test loss (w/o reg) on all data: 0.4271485\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011793749\n",
      "Norm of the params: 2.6297207\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.42715. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [222] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33170757\n",
      "Train loss (w/o reg) on all data: 0.33127695\n",
      "Test loss (w/o reg) on all data: 0.43606263\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019537797\n",
      "Norm of the params: 2.9346611\n",
      "                Loss: fixed  16 labels. Loss 0.43606. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44995272\n",
      "Train loss (w/o reg) on all data: 0.44980296\n",
      "Test loss (w/o reg) on all data: 0.42024595\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00079922564\n",
      "Norm of the params: 1.7306142\n",
      "              Random: fixed   2 labels. Loss 0.42025. Accuracy 0.822.\n",
      "### Flips: 20, rs: 23, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33251497\n",
      "Train loss (w/o reg) on all data: 0.33211842\n",
      "Test loss (w/o reg) on all data: 0.4185789\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005051591\n",
      "Norm of the params: 2.8161638\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.41858. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [61] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3298578\n",
      "Train loss (w/o reg) on all data: 0.3294557\n",
      "Test loss (w/o reg) on all data: 0.4289359\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00095239625\n",
      "Norm of the params: 2.8357756\n",
      "                Loss: fixed  17 labels. Loss 0.42894. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [313] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46247482\n",
      "Train loss (w/o reg) on all data: 0.46233755\n",
      "Test loss (w/o reg) on all data: 0.4116171\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.007177208\n",
      "Norm of the params: 1.6569624\n",
      "              Random: fixed   3 labels. Loss 0.41162. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50973415\n",
      "Train loss (w/o reg) on all data: 0.509615\n",
      "Test loss (w/o reg) on all data: 0.44005176\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001092402\n",
      "Norm of the params: 1.5435667\n",
      "Flipped loss: 0.44005. Accuracy: 0.778\n",
      "### Flips: 20, rs: 24, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [104] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4633912\n",
      "Train loss (w/o reg) on all data: 0.46321407\n",
      "Test loss (w/o reg) on all data: 0.39388478\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010121411\n",
      "Norm of the params: 1.8822536\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.39388. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40215766\n",
      "Train loss (w/o reg) on all data: 0.40182588\n",
      "Test loss (w/o reg) on all data: 0.41058162\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019288312\n",
      "Norm of the params: 2.5760102\n",
      "                Loss: fixed   8 labels. Loss 0.41058. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.509736\n",
      "Train loss (w/o reg) on all data: 0.50961614\n",
      "Test loss (w/o reg) on all data: 0.44031554\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0026460162\n",
      "Norm of the params: 1.5484009\n",
      "              Random: fixed   0 labels. Loss 0.44032. Accuracy 0.778.\n",
      "### Flips: 20, rs: 24, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [172] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4232886\n",
      "Train loss (w/o reg) on all data: 0.42306316\n",
      "Test loss (w/o reg) on all data: 0.39823675\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0028370155\n",
      "Norm of the params: 2.1234183\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.39824. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [324] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38813135\n",
      "Train loss (w/o reg) on all data: 0.38780054\n",
      "Test loss (w/o reg) on all data: 0.40962017\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00054819346\n",
      "Norm of the params: 2.572148\n",
      "                Loss: fixed  10 labels. Loss 0.40962. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [157] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5097375\n",
      "Train loss (w/o reg) on all data: 0.5096166\n",
      "Test loss (w/o reg) on all data: 0.4403141\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.005357728\n",
      "Norm of the params: 1.554713\n",
      "              Random: fixed   0 labels. Loss 0.44031. Accuracy 0.778.\n",
      "### Flips: 20, rs: 24, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38958496\n",
      "Train loss (w/o reg) on all data: 0.38930517\n",
      "Test loss (w/o reg) on all data: 0.40647075\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012740607\n",
      "Norm of the params: 2.3654575\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.40647. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [92] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36356914\n",
      "Train loss (w/o reg) on all data: 0.36325467\n",
      "Test loss (w/o reg) on all data: 0.42386818\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018790532\n",
      "Norm of the params: 2.507889\n",
      "                Loss: fixed  14 labels. Loss 0.42387. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [308] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48773262\n",
      "Train loss (w/o reg) on all data: 0.48758385\n",
      "Test loss (w/o reg) on all data: 0.41261375\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016732215\n",
      "Norm of the params: 1.7248721\n",
      "              Random: fixed   2 labels. Loss 0.41261. Accuracy 0.800.\n",
      "### Flips: 20, rs: 24, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [201] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38022035\n",
      "Train loss (w/o reg) on all data: 0.37989974\n",
      "Test loss (w/o reg) on all data: 0.4072342\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0039944383\n",
      "Norm of the params: 2.5322769\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.40723. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [102] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3596476\n",
      "Train loss (w/o reg) on all data: 0.3593244\n",
      "Test loss (w/o reg) on all data: 0.40663832\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0035864857\n",
      "Norm of the params: 2.5424469\n",
      "                Loss: fixed  15 labels. Loss 0.40664. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [211] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48773268\n",
      "Train loss (w/o reg) on all data: 0.487583\n",
      "Test loss (w/o reg) on all data: 0.41299757\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009468053\n",
      "Norm of the params: 1.7301346\n",
      "              Random: fixed   2 labels. Loss 0.41300. Accuracy 0.800.\n",
      "### Flips: 20, rs: 24, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35778302\n",
      "Train loss (w/o reg) on all data: 0.3574382\n",
      "Test loss (w/o reg) on all data: 0.42260945\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0034552594\n",
      "Norm of the params: 2.626069\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.42261. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35964647\n",
      "Train loss (w/o reg) on all data: 0.3593275\n",
      "Test loss (w/o reg) on all data: 0.40645045\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00044740527\n",
      "Norm of the params: 2.5257807\n",
      "                Loss: fixed  15 labels. Loss 0.40645. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4877323\n",
      "Train loss (w/o reg) on all data: 0.48758253\n",
      "Test loss (w/o reg) on all data: 0.41295165\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011849459\n",
      "Norm of the params: 1.7306577\n",
      "              Random: fixed   2 labels. Loss 0.41295. Accuracy 0.800.\n",
      "### Flips: 20, rs: 24, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [120] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35744914\n",
      "Train loss (w/o reg) on all data: 0.35712677\n",
      "Test loss (w/o reg) on all data: 0.42035523\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0029139626\n",
      "Norm of the params: 2.539203\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.42036. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [109] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35552332\n",
      "Train loss (w/o reg) on all data: 0.35515678\n",
      "Test loss (w/o reg) on all data: 0.4137928\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0034995517\n",
      "Norm of the params: 2.7075756\n",
      "                Loss: fixed  18 labels. Loss 0.41379. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48965365\n",
      "Train loss (w/o reg) on all data: 0.4895078\n",
      "Test loss (w/o reg) on all data: 0.41279832\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0027233765\n",
      "Norm of the params: 1.7079064\n",
      "              Random: fixed   3 labels. Loss 0.41280. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [186] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5079371\n",
      "Train loss (w/o reg) on all data: 0.50782835\n",
      "Test loss (w/o reg) on all data: 0.37114534\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00074385584\n",
      "Norm of the params: 1.474602\n",
      "Flipped loss: 0.37115. Accuracy: 0.844\n",
      "### Flips: 20, rs: 25, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [133] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48054972\n",
      "Train loss (w/o reg) on all data: 0.48040074\n",
      "Test loss (w/o reg) on all data: 0.36408785\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003671788\n",
      "Norm of the params: 1.7262422\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.36409. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42789152\n",
      "Train loss (w/o reg) on all data: 0.42763537\n",
      "Test loss (w/o reg) on all data: 0.35671493\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0044096247\n",
      "Norm of the params: 2.2634099\n",
      "                Loss: fixed   7 labels. Loss 0.35671. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [282] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49614608\n",
      "Train loss (w/o reg) on all data: 0.49604034\n",
      "Test loss (w/o reg) on all data: 0.38742226\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00081785774\n",
      "Norm of the params: 1.454167\n",
      "              Random: fixed   2 labels. Loss 0.38742. Accuracy 0.822.\n",
      "### Flips: 20, rs: 25, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [141] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4576612\n",
      "Train loss (w/o reg) on all data: 0.4575114\n",
      "Test loss (w/o reg) on all data: 0.36740226\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011158427\n",
      "Norm of the params: 1.731031\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.36740. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38460764\n",
      "Train loss (w/o reg) on all data: 0.38427582\n",
      "Test loss (w/o reg) on all data: 0.37259725\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015546622\n",
      "Norm of the params: 2.5760643\n",
      "                Loss: fixed  12 labels. Loss 0.37260. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [331] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48698896\n",
      "Train loss (w/o reg) on all data: 0.4868726\n",
      "Test loss (w/o reg) on all data: 0.38027763\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0021797589\n",
      "Norm of the params: 1.5255016\n",
      "              Random: fixed   3 labels. Loss 0.38028. Accuracy 0.822.\n",
      "### Flips: 20, rs: 25, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43593317\n",
      "Train loss (w/o reg) on all data: 0.435743\n",
      "Test loss (w/o reg) on all data: 0.36851734\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009615787\n",
      "Norm of the params: 1.9502442\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.36852. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36721098\n",
      "Train loss (w/o reg) on all data: 0.36685756\n",
      "Test loss (w/o reg) on all data: 0.3894871\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011534054\n",
      "Norm of the params: 2.6586297\n",
      "                Loss: fixed  15 labels. Loss 0.38949. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [171] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48698926\n",
      "Train loss (w/o reg) on all data: 0.48687306\n",
      "Test loss (w/o reg) on all data: 0.38011342\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018627612\n",
      "Norm of the params: 1.524555\n",
      "              Random: fixed   3 labels. Loss 0.38011. Accuracy 0.822.\n",
      "### Flips: 20, rs: 25, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40817726\n",
      "Train loss (w/o reg) on all data: 0.40794176\n",
      "Test loss (w/o reg) on all data: 0.36750033\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0022780928\n",
      "Norm of the params: 2.1702027\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.36750. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35475335\n",
      "Train loss (w/o reg) on all data: 0.35438815\n",
      "Test loss (w/o reg) on all data: 0.41662228\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0024509013\n",
      "Norm of the params: 2.7025414\n",
      "                Loss: fixed  17 labels. Loss 0.41662. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48431483\n",
      "Train loss (w/o reg) on all data: 0.48419923\n",
      "Test loss (w/o reg) on all data: 0.3827614\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0021774143\n",
      "Norm of the params: 1.5205674\n",
      "              Random: fixed   4 labels. Loss 0.38276. Accuracy 0.822.\n",
      "### Flips: 20, rs: 25, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [211] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38903376\n",
      "Train loss (w/o reg) on all data: 0.38876155\n",
      "Test loss (w/o reg) on all data: 0.37557632\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012079136\n",
      "Norm of the params: 2.3332865\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.37558. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [240] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35567304\n",
      "Train loss (w/o reg) on all data: 0.35532966\n",
      "Test loss (w/o reg) on all data: 0.4107858\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00043623886\n",
      "Norm of the params: 2.6205645\n",
      "                Loss: fixed  19 labels. Loss 0.41079. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [377] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48431307\n",
      "Train loss (w/o reg) on all data: 0.48419785\n",
      "Test loss (w/o reg) on all data: 0.3828667\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011439886\n",
      "Norm of the params: 1.5179498\n",
      "              Random: fixed   4 labels. Loss 0.38287. Accuracy 0.822.\n",
      "### Flips: 20, rs: 25, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37638888\n",
      "Train loss (w/o reg) on all data: 0.37610197\n",
      "Test loss (w/o reg) on all data: 0.38264042\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007241261\n",
      "Norm of the params: 2.3954313\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.38264. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [160] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35567528\n",
      "Train loss (w/o reg) on all data: 0.35533395\n",
      "Test loss (w/o reg) on all data: 0.41036037\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013204674\n",
      "Norm of the params: 2.6127648\n",
      "                Loss: fixed  19 labels. Loss 0.41036. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [301] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47803357\n",
      "Train loss (w/o reg) on all data: 0.47791743\n",
      "Test loss (w/o reg) on all data: 0.39030898\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0033154904\n",
      "Norm of the params: 1.5241455\n",
      "              Random: fixed   5 labels. Loss 0.39031. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52157736\n",
      "Train loss (w/o reg) on all data: 0.5214736\n",
      "Test loss (w/o reg) on all data: 0.40693384\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0036474757\n",
      "Norm of the params: 1.4405297\n",
      "Flipped loss: 0.40693. Accuracy: 0.778\n",
      "### Flips: 20, rs: 26, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [290] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46343964\n",
      "Train loss (w/o reg) on all data: 0.46329087\n",
      "Test loss (w/o reg) on all data: 0.37601048\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00064622174\n",
      "Norm of the params: 1.7249461\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.37601. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [139] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41749582\n",
      "Train loss (w/o reg) on all data: 0.41726524\n",
      "Test loss (w/o reg) on all data: 0.37687752\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0129890395\n",
      "Norm of the params: 2.1475205\n",
      "                Loss: fixed   8 labels. Loss 0.37688. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5215799\n",
      "Train loss (w/o reg) on all data: 0.52147436\n",
      "Test loss (w/o reg) on all data: 0.40728047\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0040638153\n",
      "Norm of the params: 1.4528211\n",
      "              Random: fixed   0 labels. Loss 0.40728. Accuracy 0.778.\n",
      "### Flips: 20, rs: 26, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4310615\n",
      "Train loss (w/o reg) on all data: 0.43086118\n",
      "Test loss (w/o reg) on all data: 0.3816264\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006113672\n",
      "Norm of the params: 2.0017145\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.38163. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [264] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37577173\n",
      "Train loss (w/o reg) on all data: 0.37545696\n",
      "Test loss (w/o reg) on all data: 0.38021928\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017822497\n",
      "Norm of the params: 2.5090375\n",
      "                Loss: fixed  12 labels. Loss 0.38022. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [258] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5175471\n",
      "Train loss (w/o reg) on all data: 0.517439\n",
      "Test loss (w/o reg) on all data: 0.41659847\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002651074\n",
      "Norm of the params: 1.4699987\n",
      "              Random: fixed   1 labels. Loss 0.41660. Accuracy 0.778.\n",
      "### Flips: 20, rs: 26, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [269] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41564527\n",
      "Train loss (w/o reg) on all data: 0.41542333\n",
      "Test loss (w/o reg) on all data: 0.3890498\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015779037\n",
      "Norm of the params: 2.106841\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.38905. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36395475\n",
      "Train loss (w/o reg) on all data: 0.36364794\n",
      "Test loss (w/o reg) on all data: 0.40616226\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002405358\n",
      "Norm of the params: 2.4771953\n",
      "                Loss: fixed  14 labels. Loss 0.40616. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51754\n",
      "Train loss (w/o reg) on all data: 0.5174331\n",
      "Test loss (w/o reg) on all data: 0.4163991\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0027654935\n",
      "Norm of the params: 1.4618531\n",
      "              Random: fixed   1 labels. Loss 0.41640. Accuracy 0.778.\n",
      "### Flips: 20, rs: 26, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [324] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss (w reg) on all data: 0.40297037\n",
      "Train loss (w/o reg) on all data: 0.40272972\n",
      "Test loss (w/o reg) on all data: 0.39475006\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0038999044\n",
      "Norm of the params: 2.1939096\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.39475. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35476527\n",
      "Train loss (w/o reg) on all data: 0.3544338\n",
      "Test loss (w/o reg) on all data: 0.4315609\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007738325\n",
      "Norm of the params: 2.5747073\n",
      "                Loss: fixed  16 labels. Loss 0.43156. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51124597\n",
      "Train loss (w/o reg) on all data: 0.51113325\n",
      "Test loss (w/o reg) on all data: 0.41196796\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0029296458\n",
      "Norm of the params: 1.5012987\n",
      "              Random: fixed   3 labels. Loss 0.41197. Accuracy 0.778.\n",
      "### Flips: 20, rs: 26, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [284] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37100753\n",
      "Train loss (w/o reg) on all data: 0.37072107\n",
      "Test loss (w/o reg) on all data: 0.41835994\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003842619\n",
      "Norm of the params: 2.3936207\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.41836. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [113] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34939638\n",
      "Train loss (w/o reg) on all data: 0.34906325\n",
      "Test loss (w/o reg) on all data: 0.43107623\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.007006191\n",
      "Norm of the params: 2.5812368\n",
      "                Loss: fixed  17 labels. Loss 0.43108. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [363] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47069117\n",
      "Train loss (w/o reg) on all data: 0.47054026\n",
      "Test loss (w/o reg) on all data: 0.38778397\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00046450619\n",
      "Norm of the params: 1.7373363\n",
      "              Random: fixed   6 labels. Loss 0.38778. Accuracy 0.800.\n",
      "### Flips: 20, rs: 26, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35134163\n",
      "Train loss (w/o reg) on all data: 0.35101658\n",
      "Test loss (w/o reg) on all data: 0.4217644\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011833776\n",
      "Norm of the params: 2.5497725\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.42176. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34939548\n",
      "Train loss (w/o reg) on all data: 0.34906158\n",
      "Test loss (w/o reg) on all data: 0.43160883\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010913637\n",
      "Norm of the params: 2.5841467\n",
      "                Loss: fixed  17 labels. Loss 0.43161. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [359] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46948734\n",
      "Train loss (w/o reg) on all data: 0.46934062\n",
      "Test loss (w/o reg) on all data: 0.38301083\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005855322\n",
      "Norm of the params: 1.7129761\n",
      "              Random: fixed   7 labels. Loss 0.38301. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [285] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5014096\n",
      "Train loss (w/o reg) on all data: 0.50128764\n",
      "Test loss (w/o reg) on all data: 0.39033467\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0037196577\n",
      "Norm of the params: 1.5617099\n",
      "Flipped loss: 0.39033. Accuracy: 0.822\n",
      "### Flips: 20, rs: 27, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [121] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43577808\n",
      "Train loss (w/o reg) on all data: 0.4356166\n",
      "Test loss (w/o reg) on all data: 0.36299437\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0019919924\n",
      "Norm of the params: 1.7969964\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.36299. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39364478\n",
      "Train loss (w/o reg) on all data: 0.3933699\n",
      "Test loss (w/o reg) on all data: 0.35862288\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00055662135\n",
      "Norm of the params: 2.344584\n",
      "                Loss: fixed   8 labels. Loss 0.35862. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [153] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48789126\n",
      "Train loss (w/o reg) on all data: 0.48776716\n",
      "Test loss (w/o reg) on all data: 0.38441008\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.003635112\n",
      "Norm of the params: 1.5754961\n",
      "              Random: fixed   1 labels. Loss 0.38441. Accuracy 0.844.\n",
      "### Flips: 20, rs: 27, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4001639\n",
      "Train loss (w/o reg) on all data: 0.39993316\n",
      "Test loss (w/o reg) on all data: 0.3511922\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00046496297\n",
      "Norm of the params: 2.148148\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.35119. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [190] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37293372\n",
      "Train loss (w/o reg) on all data: 0.37262326\n",
      "Test loss (w/o reg) on all data: 0.36155722\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0005489765\n",
      "Norm of the params: 2.4918113\n",
      "                Loss: fixed  11 labels. Loss 0.36156. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [211] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48777887\n",
      "Train loss (w/o reg) on all data: 0.48764214\n",
      "Test loss (w/o reg) on all data: 0.39440134\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012519414\n",
      "Norm of the params: 1.6536483\n",
      "              Random: fixed   3 labels. Loss 0.39440. Accuracy 0.822.\n",
      "### Flips: 20, rs: 27, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40398124\n",
      "Train loss (w/o reg) on all data: 0.40373784\n",
      "Test loss (w/o reg) on all data: 0.35495675\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007605672\n",
      "Norm of the params: 2.2062721\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.35496. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35754314\n",
      "Train loss (w/o reg) on all data: 0.35720006\n",
      "Test loss (w/o reg) on all data: 0.3586436\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0020111746\n",
      "Norm of the params: 2.6194987\n",
      "                Loss: fixed  13 labels. Loss 0.35864. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47730792\n",
      "Train loss (w/o reg) on all data: 0.47716093\n",
      "Test loss (w/o reg) on all data: 0.39946568\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0028690617\n",
      "Norm of the params: 1.7144922\n",
      "              Random: fixed   4 labels. Loss 0.39947. Accuracy 0.822.\n",
      "### Flips: 20, rs: 27, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3826924\n",
      "Train loss (w/o reg) on all data: 0.38241217\n",
      "Test loss (w/o reg) on all data: 0.3470003\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0003979182\n",
      "Norm of the params: 2.3674464\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.34700. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [139] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34932715\n",
      "Train loss (w/o reg) on all data: 0.34898916\n",
      "Test loss (w/o reg) on all data: 0.38298428\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0034738563\n",
      "Norm of the params: 2.5999033\n",
      "                Loss: fixed  15 labels. Loss 0.38298. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4773015\n",
      "Train loss (w/o reg) on all data: 0.47715306\n",
      "Test loss (w/o reg) on all data: 0.39983967\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.000536218\n",
      "Norm of the params: 1.7230127\n",
      "              Random: fixed   4 labels. Loss 0.39984. Accuracy 0.822.\n",
      "### Flips: 20, rs: 27, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36466345\n",
      "Train loss (w/o reg) on all data: 0.36437815\n",
      "Test loss (w/o reg) on all data: 0.3746635\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.000430497\n",
      "Norm of the params: 2.388677\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.37466. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [293] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34932515\n",
      "Train loss (w/o reg) on all data: 0.34898585\n",
      "Test loss (w/o reg) on all data: 0.3826596\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00087413855\n",
      "Norm of the params: 2.605022\n",
      "                Loss: fixed  15 labels. Loss 0.38266. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46812627\n",
      "Train loss (w/o reg) on all data: 0.46796396\n",
      "Test loss (w/o reg) on all data: 0.4067599\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008389511\n",
      "Norm of the params: 1.8017534\n",
      "              Random: fixed   5 labels. Loss 0.40676. Accuracy 0.800.\n",
      "### Flips: 20, rs: 27, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [335] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35811403\n",
      "Train loss (w/o reg) on all data: 0.35781103\n",
      "Test loss (w/o reg) on all data: 0.38948122\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007735283\n",
      "Norm of the params: 2.4616609\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.38948. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [222] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35036284\n",
      "Train loss (w/o reg) on all data: 0.35000795\n",
      "Test loss (w/o reg) on all data: 0.3811593\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0007227777\n",
      "Norm of the params: 2.6641893\n",
      "                Loss: fixed  16 labels. Loss 0.38116. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [300] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4761375\n",
      "Train loss (w/o reg) on all data: 0.47598547\n",
      "Test loss (w/o reg) on all data: 0.41059747\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017193514\n",
      "Norm of the params: 1.7436329\n",
      "              Random: fixed   6 labels. Loss 0.41060. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [222] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47227767\n",
      "Train loss (w/o reg) on all data: 0.47215113\n",
      "Test loss (w/o reg) on all data: 0.47828802\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.005998664\n",
      "Norm of the params: 1.5908266\n",
      "Flipped loss: 0.47829. Accuracy: 0.778\n",
      "### Flips: 20, rs: 28, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3993749\n",
      "Train loss (w/o reg) on all data: 0.39916444\n",
      "Test loss (w/o reg) on all data: 0.4851741\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0004350205\n",
      "Norm of the params: 2.0516458\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.48517. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [173] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3770559\n",
      "Train loss (w/o reg) on all data: 0.37680852\n",
      "Test loss (w/o reg) on all data: 0.47574794\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00074414955\n",
      "Norm of the params: 2.2243423\n",
      "                Loss: fixed   7 labels. Loss 0.47575. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4514783\n",
      "Train loss (w/o reg) on all data: 0.4513313\n",
      "Test loss (w/o reg) on all data: 0.46326265\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0013595496\n",
      "Norm of the params: 1.7146522\n",
      "              Random: fixed   2 labels. Loss 0.46326. Accuracy 0.778.\n",
      "### Flips: 20, rs: 28, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35217434\n",
      "Train loss (w/o reg) on all data: 0.3518734\n",
      "Test loss (w/o reg) on all data: 0.45043787\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0055859825\n",
      "Norm of the params: 2.4532967\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.45044. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [153] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34501454\n",
      "Train loss (w/o reg) on all data: 0.3446757\n",
      "Test loss (w/o reg) on all data: 0.4564574\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003129342\n",
      "Norm of the params: 2.6032398\n",
      "                Loss: fixed  10 labels. Loss 0.45646. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44262588\n",
      "Train loss (w/o reg) on all data: 0.44248062\n",
      "Test loss (w/o reg) on all data: 0.45519117\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.012502062\n",
      "Norm of the params: 1.7044113\n",
      "              Random: fixed   3 labels. Loss 0.45519. Accuracy 0.778.\n",
      "### Flips: 20, rs: 28, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [208] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3291621\n",
      "Train loss (w/o reg) on all data: 0.32879\n",
      "Test loss (w/o reg) on all data: 0.474012\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0046243635\n",
      "Norm of the params: 2.7279072\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.47401. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [23] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32915974\n",
      "Train loss (w/o reg) on all data: 0.32878888\n",
      "Test loss (w/o reg) on all data: 0.47322664\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009672977\n",
      "Norm of the params: 2.7234976\n",
      "                Loss: fixed  12 labels. Loss 0.47323. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [290] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44166592\n",
      "Train loss (w/o reg) on all data: 0.4415233\n",
      "Test loss (w/o reg) on all data: 0.44941962\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00063314755\n",
      "Norm of the params: 1.6888617\n",
      "              Random: fixed   4 labels. Loss 0.44942. Accuracy 0.800.\n",
      "### Flips: 20, rs: 28, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [284] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32864162\n",
      "Train loss (w/o reg) on all data: 0.3282614\n",
      "Test loss (w/o reg) on all data: 0.49172693\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0010912891\n",
      "Norm of the params: 2.7575953\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.49173. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32916042\n",
      "Train loss (w/o reg) on all data: 0.3287912\n",
      "Test loss (w/o reg) on all data: 0.4734265\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0041380096\n",
      "Norm of the params: 2.7174191\n",
      "                Loss: fixed  12 labels. Loss 0.47343. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4450552\n",
      "Train loss (w/o reg) on all data: 0.44491175\n",
      "Test loss (w/o reg) on all data: 0.4281509\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0036011362\n",
      "Norm of the params: 1.6937301\n",
      "              Random: fixed   6 labels. Loss 0.42815. Accuracy 0.800.\n",
      "### Flips: 20, rs: 28, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32691023\n",
      "Train loss (w/o reg) on all data: 0.32654724\n",
      "Test loss (w/o reg) on all data: 0.48177025\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0016791562\n",
      "Norm of the params: 2.694417\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.48177. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [184] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3246151\n",
      "Train loss (w/o reg) on all data: 0.3241929\n",
      "Test loss (w/o reg) on all data: 0.4894879\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0003371132\n",
      "Norm of the params: 2.9057875\n",
      "                Loss: fixed  14 labels. Loss 0.48949. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [323] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4272084\n",
      "Train loss (w/o reg) on all data: 0.42702737\n",
      "Test loss (w/o reg) on all data: 0.436433\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0029515484\n",
      "Norm of the params: 1.9028077\n",
      "              Random: fixed   7 labels. Loss 0.43643. Accuracy 0.800.\n",
      "### Flips: 20, rs: 28, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [208] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3269071\n",
      "Train loss (w/o reg) on all data: 0.326545\n",
      "Test loss (w/o reg) on all data: 0.48178694\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0005214373\n",
      "Norm of the params: 2.6910446\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.48179. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.324616\n",
      "Train loss (w/o reg) on all data: 0.32419452\n",
      "Test loss (w/o reg) on all data: 0.4890578\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0045157373\n",
      "Norm of the params: 2.9033442\n",
      "                Loss: fixed  14 labels. Loss 0.48906. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40872866\n",
      "Train loss (w/o reg) on all data: 0.4085003\n",
      "Test loss (w/o reg) on all data: 0.42512062\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0025402757\n",
      "Norm of the params: 2.1369655\n",
      "              Random: fixed   9 labels. Loss 0.42512. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.496455\n",
      "Train loss (w/o reg) on all data: 0.49633768\n",
      "Test loss (w/o reg) on all data: 0.4109202\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0039478657\n",
      "Norm of the params: 1.5319011\n",
      "Flipped loss: 0.41092. Accuracy: 0.800\n",
      "### Flips: 20, rs: 29, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [211] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44591695\n",
      "Train loss (w/o reg) on all data: 0.44573078\n",
      "Test loss (w/o reg) on all data: 0.4063239\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002104367\n",
      "Norm of the params: 1.929639\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.40632. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [185] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41901192\n",
      "Train loss (w/o reg) on all data: 0.41877544\n",
      "Test loss (w/o reg) on all data: 0.4174978\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010902997\n",
      "Norm of the params: 2.1748118\n",
      "                Loss: fixed   7 labels. Loss 0.41750. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4935176\n",
      "Train loss (w/o reg) on all data: 0.49339122\n",
      "Test loss (w/o reg) on all data: 0.41739988\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.006251471\n",
      "Norm of the params: 1.5900086\n",
      "              Random: fixed   3 labels. Loss 0.41740. Accuracy 0.800.\n",
      "### Flips: 20, rs: 29, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [118] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43046382\n",
      "Train loss (w/o reg) on all data: 0.43028352\n",
      "Test loss (w/o reg) on all data: 0.3989877\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017920197\n",
      "Norm of the params: 1.8989844\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.39899. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39038965\n",
      "Train loss (w/o reg) on all data: 0.39014983\n",
      "Test loss (w/o reg) on all data: 0.42064586\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005899725\n",
      "Norm of the params: 2.1900744\n",
      "                Loss: fixed  10 labels. Loss 0.42065. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4819523\n",
      "Train loss (w/o reg) on all data: 0.4818162\n",
      "Test loss (w/o reg) on all data: 0.42412457\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0021136645\n",
      "Norm of the params: 1.6498113\n",
      "              Random: fixed   4 labels. Loss 0.42412. Accuracy 0.800.\n",
      "### Flips: 20, rs: 29, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [130] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41970667\n",
      "Train loss (w/o reg) on all data: 0.41950908\n",
      "Test loss (w/o reg) on all data: 0.3886166\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0025862562\n",
      "Norm of the params: 1.9879193\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.38862. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36471033\n",
      "Train loss (w/o reg) on all data: 0.36444333\n",
      "Test loss (w/o reg) on all data: 0.41637638\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0064426307\n",
      "Norm of the params: 2.3108516\n",
      "                Loss: fixed  14 labels. Loss 0.41638. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [202] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48195288\n",
      "Train loss (w/o reg) on all data: 0.4818164\n",
      "Test loss (w/o reg) on all data: 0.42393526\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0024494904\n",
      "Norm of the params: 1.6521183\n",
      "              Random: fixed   4 labels. Loss 0.42394. Accuracy 0.800.\n",
      "### Flips: 20, rs: 29, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39565495\n",
      "Train loss (w/o reg) on all data: 0.39542776\n",
      "Test loss (w/o reg) on all data: 0.39417198\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006412789\n",
      "Norm of the params: 2.1316545\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.39417. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3530225\n",
      "Train loss (w/o reg) on all data: 0.3527234\n",
      "Test loss (w/o reg) on all data: 0.43710056\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010410649\n",
      "Norm of the params: 2.445743\n",
      "                Loss: fixed  16 labels. Loss 0.43710. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [309] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48195082\n",
      "Train loss (w/o reg) on all data: 0.48181418\n",
      "Test loss (w/o reg) on all data: 0.4237701\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004276425\n",
      "Norm of the params: 1.6531554\n",
      "              Random: fixed   4 labels. Loss 0.42377. Accuracy 0.800.\n",
      "### Flips: 20, rs: 29, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37141004\n",
      "Train loss (w/o reg) on all data: 0.3711455\n",
      "Test loss (w/o reg) on all data: 0.4131015\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0045109848\n",
      "Norm of the params: 2.300283\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.41310. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [86] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35499087\n",
      "Train loss (w/o reg) on all data: 0.35468775\n",
      "Test loss (w/o reg) on all data: 0.43024716\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017555492\n",
      "Norm of the params: 2.4621344\n",
      "                Loss: fixed  17 labels. Loss 0.43025. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [291] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48194984\n",
      "Train loss (w/o reg) on all data: 0.4818144\n",
      "Test loss (w/o reg) on all data: 0.42394745\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015257678\n",
      "Norm of the params: 1.6458141\n",
      "              Random: fixed   4 labels. Loss 0.42395. Accuracy 0.800.\n",
      "### Flips: 20, rs: 29, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [154] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37141597\n",
      "Train loss (w/o reg) on all data: 0.37114996\n",
      "Test loss (w/o reg) on all data: 0.41373456\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.009062159\n",
      "Norm of the params: 2.3065467\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.41373. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35554585\n",
      "Train loss (w/o reg) on all data: 0.355236\n",
      "Test loss (w/o reg) on all data: 0.41307026\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0003865227\n",
      "Norm of the params: 2.4893959\n",
      "                Loss: fixed  18 labels. Loss 0.41307. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4747356\n",
      "Train loss (w/o reg) on all data: 0.47459272\n",
      "Test loss (w/o reg) on all data: 0.42819548\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.002930474\n",
      "Norm of the params: 1.6903882\n",
      "              Random: fixed   6 labels. Loss 0.42820. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [82] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47985768\n",
      "Train loss (w/o reg) on all data: 0.47975188\n",
      "Test loss (w/o reg) on all data: 0.38938576\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.006430091\n",
      "Norm of the params: 1.4546802\n",
      "Flipped loss: 0.38939. Accuracy: 0.822\n",
      "### Flips: 20, rs: 30, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44478676\n",
      "Train loss (w/o reg) on all data: 0.44463745\n",
      "Test loss (w/o reg) on all data: 0.38112712\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0031546995\n",
      "Norm of the params: 1.7280827\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.38113. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [226] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39485642\n",
      "Train loss (w/o reg) on all data: 0.3946698\n",
      "Test loss (w/o reg) on all data: 0.39780438\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006515365\n",
      "Norm of the params: 1.9319441\n",
      "                Loss: fixed   7 labels. Loss 0.39780. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [173] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4797806\n",
      "Train loss (w/o reg) on all data: 0.4796731\n",
      "Test loss (w/o reg) on all data: 0.3882155\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00071845343\n",
      "Norm of the params: 1.4665625\n",
      "              Random: fixed   0 labels. Loss 0.38822. Accuracy 0.822.\n",
      "### Flips: 20, rs: 30, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [146] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43748352\n",
      "Train loss (w/o reg) on all data: 0.4373316\n",
      "Test loss (w/o reg) on all data: 0.3917578\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022988261\n",
      "Norm of the params: 1.7431375\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.39176. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38004518\n",
      "Train loss (w/o reg) on all data: 0.37981287\n",
      "Test loss (w/o reg) on all data: 0.40684932\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00047378417\n",
      "Norm of the params: 2.1554625\n",
      "                Loss: fixed   9 labels. Loss 0.40685. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47974548\n",
      "Train loss (w/o reg) on all data: 0.47964042\n",
      "Test loss (w/o reg) on all data: 0.38473347\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0044729332\n",
      "Norm of the params: 1.4494458\n",
      "              Random: fixed   1 labels. Loss 0.38473. Accuracy 0.822.\n",
      "### Flips: 20, rs: 30, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [357] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40713748\n",
      "Train loss (w/o reg) on all data: 0.4069498\n",
      "Test loss (w/o reg) on all data: 0.41601673\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0003610889\n",
      "Norm of the params: 1.93744\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.41602. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [169] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36429465\n",
      "Train loss (w/o reg) on all data: 0.36403376\n",
      "Test loss (w/o reg) on all data: 0.43420026\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00037737563\n",
      "Norm of the params: 2.2842207\n",
      "                Loss: fixed  11 labels. Loss 0.43420. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47974607\n",
      "Train loss (w/o reg) on all data: 0.4796417\n",
      "Test loss (w/o reg) on all data: 0.38428137\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011435202\n",
      "Norm of the params: 1.4447193\n",
      "              Random: fixed   1 labels. Loss 0.38428. Accuracy 0.822.\n",
      "### Flips: 20, rs: 30, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [339] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39907882\n",
      "Train loss (w/o reg) on all data: 0.3988518\n",
      "Test loss (w/o reg) on all data: 0.4148777\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.000804319\n",
      "Norm of the params: 2.130722\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.41488. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36046508\n",
      "Train loss (w/o reg) on all data: 0.36017552\n",
      "Test loss (w/o reg) on all data: 0.43348503\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0004886394\n",
      "Norm of the params: 2.4064476\n",
      "                Loss: fixed  12 labels. Loss 0.43349. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47514585\n",
      "Train loss (w/o reg) on all data: 0.47503942\n",
      "Test loss (w/o reg) on all data: 0.38936472\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00060628465\n",
      "Norm of the params: 1.4589597\n",
      "              Random: fixed   2 labels. Loss 0.38936. Accuracy 0.822.\n",
      "### Flips: 20, rs: 30, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [326] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35836706\n",
      "Train loss (w/o reg) on all data: 0.3580808\n",
      "Test loss (w/o reg) on all data: 0.4384657\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.000948112\n",
      "Norm of the params: 2.3927474\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.43847. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35702983\n",
      "Train loss (w/o reg) on all data: 0.3567626\n",
      "Test loss (w/o reg) on all data: 0.41945565\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.009179334\n",
      "Norm of the params: 2.3118348\n",
      "                Loss: fixed  15 labels. Loss 0.41946. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46255726\n",
      "Train loss (w/o reg) on all data: 0.4624455\n",
      "Test loss (w/o reg) on all data: 0.3936816\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013571095\n",
      "Norm of the params: 1.4950686\n",
      "              Random: fixed   3 labels. Loss 0.39368. Accuracy 0.822.\n",
      "### Flips: 20, rs: 30, checks: 60\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35650924\n",
      "Train loss (w/o reg) on all data: 0.3562144\n",
      "Test loss (w/o reg) on all data: 0.4321515\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010232318\n",
      "Norm of the params: 2.4283564\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.43215. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35752735\n",
      "Train loss (w/o reg) on all data: 0.3572246\n",
      "Test loss (w/o reg) on all data: 0.4128633\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002198764\n",
      "Norm of the params: 2.4606316\n",
      "                Loss: fixed  17 labels. Loss 0.41286. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [315] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.462557\n",
      "Train loss (w/o reg) on all data: 0.46244514\n",
      "Test loss (w/o reg) on all data: 0.39389664\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.009583891\n",
      "Norm of the params: 1.4957279\n",
      "              Random: fixed   3 labels. Loss 0.39390. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46821478\n",
      "Train loss (w/o reg) on all data: 0.46809885\n",
      "Test loss (w/o reg) on all data: 0.42402458\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0013935837\n",
      "Norm of the params: 1.522652\n",
      "Flipped loss: 0.42402. Accuracy: 0.756\n",
      "### Flips: 20, rs: 31, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43089154\n",
      "Train loss (w/o reg) on all data: 0.43071318\n",
      "Test loss (w/o reg) on all data: 0.4117575\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0009203304\n",
      "Norm of the params: 1.8887348\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.41176. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [105] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40093\n",
      "Train loss (w/o reg) on all data: 0.40071732\n",
      "Test loss (w/o reg) on all data: 0.45105454\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.001013996\n",
      "Norm of the params: 2.0623126\n",
      "                Loss: fixed   5 labels. Loss 0.45105. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4568628\n",
      "Train loss (w/o reg) on all data: 0.45673382\n",
      "Test loss (w/o reg) on all data: 0.44138816\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0002704249\n",
      "Norm of the params: 1.6061481\n",
      "              Random: fixed   1 labels. Loss 0.44139. Accuracy 0.733.\n",
      "### Flips: 20, rs: 31, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [372] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4013071\n",
      "Train loss (w/o reg) on all data: 0.4011049\n",
      "Test loss (w/o reg) on all data: 0.38594544\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0019447902\n",
      "Norm of the params: 2.0109584\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.38595. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [143] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3862775\n",
      "Train loss (w/o reg) on all data: 0.38605946\n",
      "Test loss (w/o reg) on all data: 0.42617184\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0020434102\n",
      "Norm of the params: 2.0881753\n",
      "                Loss: fixed   7 labels. Loss 0.42617. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45686352\n",
      "Train loss (w/o reg) on all data: 0.45673564\n",
      "Test loss (w/o reg) on all data: 0.44112346\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0023602054\n",
      "Norm of the params: 1.5992119\n",
      "              Random: fixed   1 labels. Loss 0.44112. Accuracy 0.733.\n",
      "### Flips: 20, rs: 31, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [392] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38356408\n",
      "Train loss (w/o reg) on all data: 0.3833513\n",
      "Test loss (w/o reg) on all data: 0.41379517\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0004557492\n",
      "Norm of the params: 2.0629709\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.41380. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35758469\n",
      "Train loss (w/o reg) on all data: 0.3573147\n",
      "Test loss (w/o reg) on all data: 0.4662592\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012782164\n",
      "Norm of the params: 2.3237636\n",
      "                Loss: fixed  12 labels. Loss 0.46626. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [347] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4545234\n",
      "Train loss (w/o reg) on all data: 0.45437932\n",
      "Test loss (w/o reg) on all data: 0.4247111\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0025738347\n",
      "Norm of the params: 1.697641\n",
      "              Random: fixed   2 labels. Loss 0.42471. Accuracy 0.756.\n",
      "### Flips: 20, rs: 31, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [365] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37454522\n",
      "Train loss (w/o reg) on all data: 0.3742945\n",
      "Test loss (w/o reg) on all data: 0.41232932\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0005217831\n",
      "Norm of the params: 2.239334\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.41233. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3466061\n",
      "Train loss (w/o reg) on all data: 0.3463121\n",
      "Test loss (w/o reg) on all data: 0.4213782\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011375538\n",
      "Norm of the params: 2.4249208\n",
      "                Loss: fixed  15 labels. Loss 0.42138. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4472784\n",
      "Train loss (w/o reg) on all data: 0.44712132\n",
      "Test loss (w/o reg) on all data: 0.4099026\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0005695832\n",
      "Norm of the params: 1.7725787\n",
      "              Random: fixed   3 labels. Loss 0.40990. Accuracy 0.756.\n",
      "### Flips: 20, rs: 31, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3594611\n",
      "Train loss (w/o reg) on all data: 0.35918087\n",
      "Test loss (w/o reg) on all data: 0.41312578\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0006525467\n",
      "Norm of the params: 2.367434\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.41313. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [102] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3453325\n",
      "Train loss (w/o reg) on all data: 0.3450488\n",
      "Test loss (w/o reg) on all data: 0.42281112\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0075424123\n",
      "Norm of the params: 2.38209\n",
      "                Loss: fixed  16 labels. Loss 0.42281. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43480483\n",
      "Train loss (w/o reg) on all data: 0.4346517\n",
      "Test loss (w/o reg) on all data: 0.40889156\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0046494505\n",
      "Norm of the params: 1.7500404\n",
      "              Random: fixed   5 labels. Loss 0.40889. Accuracy 0.800.\n",
      "### Flips: 20, rs: 31, checks: 60\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [258] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35037225\n",
      "Train loss (w/o reg) on all data: 0.3500611\n",
      "Test loss (w/o reg) on all data: 0.43022448\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.000950325\n",
      "Norm of the params: 2.4946887\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.43022. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [126] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34530947\n",
      "Train loss (w/o reg) on all data: 0.34501734\n",
      "Test loss (w/o reg) on all data: 0.42563117\n",
      "Train acc on all data:  0.8820754716981132\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0039165975\n",
      "Norm of the params: 2.417054\n",
      "                Loss: fixed  16 labels. Loss 0.42563. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4317037\n",
      "Train loss (w/o reg) on all data: 0.43155667\n",
      "Test loss (w/o reg) on all data: 0.41967824\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015762552\n",
      "Norm of the params: 1.7147151\n",
      "              Random: fixed   6 labels. Loss 0.41968. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44113386\n",
      "Train loss (w/o reg) on all data: 0.4409873\n",
      "Test loss (w/o reg) on all data: 0.36332363\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00067071937\n",
      "Norm of the params: 1.7121903\n",
      "Flipped loss: 0.36332. Accuracy: 0.844\n",
      "### Flips: 20, rs: 32, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3994003\n",
      "Train loss (w/o reg) on all data: 0.39916784\n",
      "Test loss (w/o reg) on all data: 0.37453833\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0046965014\n",
      "Norm of the params: 2.1562345\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.37454. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [111] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37670782\n",
      "Train loss (w/o reg) on all data: 0.37646887\n",
      "Test loss (w/o reg) on all data: 0.4012938\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0024902853\n",
      "Norm of the params: 2.186079\n",
      "                Loss: fixed   5 labels. Loss 0.40129. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43274602\n",
      "Train loss (w/o reg) on all data: 0.43258235\n",
      "Test loss (w/o reg) on all data: 0.36611062\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0048924894\n",
      "Norm of the params: 1.8092119\n",
      "              Random: fixed   1 labels. Loss 0.36611. Accuracy 0.822.\n",
      "### Flips: 20, rs: 32, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [371] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36698866\n",
      "Train loss (w/o reg) on all data: 0.36664528\n",
      "Test loss (w/o reg) on all data: 0.38332137\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00058310555\n",
      "Norm of the params: 2.6205962\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.38332. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34896296\n",
      "Train loss (w/o reg) on all data: 0.34864664\n",
      "Test loss (w/o reg) on all data: 0.42664883\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009766166\n",
      "Norm of the params: 2.5151982\n",
      "                Loss: fixed   8 labels. Loss 0.42665. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [324] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43274844\n",
      "Train loss (w/o reg) on all data: 0.43258524\n",
      "Test loss (w/o reg) on all data: 0.36622298\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001136991\n",
      "Norm of the params: 1.8066502\n",
      "              Random: fixed   1 labels. Loss 0.36622. Accuracy 0.822.\n",
      "### Flips: 20, rs: 32, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [383] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35899702\n",
      "Train loss (w/o reg) on all data: 0.3586526\n",
      "Test loss (w/o reg) on all data: 0.3924645\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0003063227\n",
      "Norm of the params: 2.6246536\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.39246. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.324171\n",
      "Train loss (w/o reg) on all data: 0.3237856\n",
      "Test loss (w/o reg) on all data: 0.44898596\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00088425656\n",
      "Norm of the params: 2.7763839\n",
      "                Loss: fixed  12 labels. Loss 0.44899. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [429] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43274537\n",
      "Train loss (w/o reg) on all data: 0.43258157\n",
      "Test loss (w/o reg) on all data: 0.36585173\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00071397785\n",
      "Norm of the params: 1.8098851\n",
      "              Random: fixed   1 labels. Loss 0.36585. Accuracy 0.822.\n",
      "### Flips: 20, rs: 32, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33242187\n",
      "Train loss (w/o reg) on all data: 0.33202013\n",
      "Test loss (w/o reg) on all data: 0.4255009\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009905118\n",
      "Norm of the params: 2.8346078\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.42550. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [120] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31864387\n",
      "Train loss (w/o reg) on all data: 0.31823882\n",
      "Test loss (w/o reg) on all data: 0.44011617\n",
      "Train acc on all data:  0.8820754716981132\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0024358775\n",
      "Norm of the params: 2.8461769\n",
      "                Loss: fixed  14 labels. Loss 0.44012. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [386] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43275034\n",
      "Train loss (w/o reg) on all data: 0.43258598\n",
      "Test loss (w/o reg) on all data: 0.36616924\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0030347088\n",
      "Norm of the params: 1.8130635\n",
      "              Random: fixed   1 labels. Loss 0.36617. Accuracy 0.822.\n",
      "### Flips: 20, rs: 32, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [295] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3333995\n",
      "Train loss (w/o reg) on all data: 0.33303452\n",
      "Test loss (w/o reg) on all data: 0.4179104\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0023754458\n",
      "Norm of the params: 2.701803\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.41791. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [169] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3206812\n",
      "Train loss (w/o reg) on all data: 0.32027775\n",
      "Test loss (w/o reg) on all data: 0.4329389\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0027399622\n",
      "Norm of the params: 2.8406062\n",
      "                Loss: fixed  15 labels. Loss 0.43294. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43658563\n",
      "Train loss (w/o reg) on all data: 0.43642408\n",
      "Test loss (w/o reg) on all data: 0.35722423\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0008776286\n",
      "Norm of the params: 1.7976061\n",
      "              Random: fixed   2 labels. Loss 0.35722. Accuracy 0.844.\n",
      "### Flips: 20, rs: 32, checks: 60\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3231035\n",
      "Train loss (w/o reg) on all data: 0.32272783\n",
      "Test loss (w/o reg) on all data: 0.43430993\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018082509\n",
      "Norm of the params: 2.7409663\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.43431. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [136] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.320681\n",
      "Train loss (w/o reg) on all data: 0.32027882\n",
      "Test loss (w/o reg) on all data: 0.43272722\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022170017\n",
      "Norm of the params: 2.8361344\n",
      "                Loss: fixed  15 labels. Loss 0.43273. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41741747\n",
      "Train loss (w/o reg) on all data: 0.4172173\n",
      "Test loss (w/o reg) on all data: 0.35639188\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004629876\n",
      "Norm of the params: 2.0008192\n",
      "              Random: fixed   4 labels. Loss 0.35639. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [283] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4662519\n",
      "Train loss (w/o reg) on all data: 0.46616003\n",
      "Test loss (w/o reg) on all data: 0.4230696\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0050576893\n",
      "Norm of the params: 1.3555094\n",
      "Flipped loss: 0.42307. Accuracy: 0.800\n",
      "### Flips: 20, rs: 33, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [283] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41814572\n",
      "Train loss (w/o reg) on all data: 0.41801134\n",
      "Test loss (w/o reg) on all data: 0.4161461\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00034498653\n",
      "Norm of the params: 1.6394261\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.41615. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [388] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40514797\n",
      "Train loss (w/o reg) on all data: 0.4049623\n",
      "Test loss (w/o reg) on all data: 0.423453\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010027484\n",
      "Norm of the params: 1.9270705\n",
      "                Loss: fixed   4 labels. Loss 0.42345. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46624467\n",
      "Train loss (w/o reg) on all data: 0.46615285\n",
      "Test loss (w/o reg) on all data: 0.42271233\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00079562794\n",
      "Norm of the params: 1.3551072\n",
      "              Random: fixed   0 labels. Loss 0.42271. Accuracy 0.800.\n",
      "### Flips: 20, rs: 33, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [314] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38382503\n",
      "Train loss (w/o reg) on all data: 0.3836449\n",
      "Test loss (w/o reg) on all data: 0.38319468\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00058026327\n",
      "Norm of the params: 1.8980176\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.38319. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38118246\n",
      "Train loss (w/o reg) on all data: 0.3809648\n",
      "Test loss (w/o reg) on all data: 0.39917123\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0026485887\n",
      "Norm of the params: 2.0864792\n",
      "                Loss: fixed   7 labels. Loss 0.39917. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46624526\n",
      "Train loss (w/o reg) on all data: 0.46615356\n",
      "Test loss (w/o reg) on all data: 0.4226077\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.007195746\n",
      "Norm of the params: 1.3542929\n",
      "              Random: fixed   0 labels. Loss 0.42261. Accuracy 0.800.\n",
      "### Flips: 20, rs: 33, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [195] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36607596\n",
      "Train loss (w/o reg) on all data: 0.36587122\n",
      "Test loss (w/o reg) on all data: 0.39903027\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0037572542\n",
      "Norm of the params: 2.0236237\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.39903. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35908714\n",
      "Train loss (w/o reg) on all data: 0.3588331\n",
      "Test loss (w/o reg) on all data: 0.40536517\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004810641\n",
      "Norm of the params: 2.2540603\n",
      "                Loss: fixed  11 labels. Loss 0.40537. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44352219\n",
      "Train loss (w/o reg) on all data: 0.44342405\n",
      "Test loss (w/o reg) on all data: 0.4275324\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008101527\n",
      "Norm of the params: 1.4010549\n",
      "              Random: fixed   3 labels. Loss 0.42753. Accuracy 0.800.\n",
      "### Flips: 20, rs: 33, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36818942\n",
      "Train loss (w/o reg) on all data: 0.3679601\n",
      "Test loss (w/o reg) on all data: 0.40350595\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.005448299\n",
      "Norm of the params: 2.1415615\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.40351. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34691128\n",
      "Train loss (w/o reg) on all data: 0.3466579\n",
      "Test loss (w/o reg) on all data: 0.41297057\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006028429\n",
      "Norm of the params: 2.2511768\n",
      "                Loss: fixed  13 labels. Loss 0.41297. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44353044\n",
      "Train loss (w/o reg) on all data: 0.44343206\n",
      "Test loss (w/o reg) on all data: 0.4282029\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.005608919\n",
      "Norm of the params: 1.4026984\n",
      "              Random: fixed   3 labels. Loss 0.42820. Accuracy 0.800.\n",
      "### Flips: 20, rs: 33, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [399] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34041038\n",
      "Train loss (w/o reg) on all data: 0.34007746\n",
      "Test loss (w/o reg) on all data: 0.40658486\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0012023799\n",
      "Norm of the params: 2.58045\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.40658. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3361135\n",
      "Train loss (w/o reg) on all data: 0.33579478\n",
      "Test loss (w/o reg) on all data: 0.40386268\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0056763063\n",
      "Norm of the params: 2.5247822\n",
      "                Loss: fixed  15 labels. Loss 0.40386. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [310] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43128684\n",
      "Train loss (w/o reg) on all data: 0.43116617\n",
      "Test loss (w/o reg) on all data: 0.42987907\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012205853\n",
      "Norm of the params: 1.5535452\n",
      "              Random: fixed   5 labels. Loss 0.42988. Accuracy 0.800.\n",
      "### Flips: 20, rs: 33, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [490] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34040806\n",
      "Train loss (w/o reg) on all data: 0.34007773\n",
      "Test loss (w/o reg) on all data: 0.40608406\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0009589925\n",
      "Norm of the params: 2.5702796\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.40608. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [148] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33161288\n",
      "Train loss (w/o reg) on all data: 0.3312879\n",
      "Test loss (w/o reg) on all data: 0.40546066\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0018584435\n",
      "Norm of the params: 2.5494957\n",
      "                Loss: fixed  16 labels. Loss 0.40546. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [342] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42983735\n",
      "Train loss (w/o reg) on all data: 0.42970678\n",
      "Test loss (w/o reg) on all data: 0.43165153\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0004571439\n",
      "Norm of the params: 1.6160269\n",
      "              Random: fixed   6 labels. Loss 0.43165. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45325992\n",
      "Train loss (w/o reg) on all data: 0.45310557\n",
      "Test loss (w/o reg) on all data: 0.43069497\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00965557\n",
      "Norm of the params: 1.756989\n",
      "Flipped loss: 0.43069. Accuracy: 0.822\n",
      "### Flips: 20, rs: 34, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [70] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41173393\n",
      "Train loss (w/o reg) on all data: 0.41152263\n",
      "Test loss (w/o reg) on all data: 0.4126121\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0032450429\n",
      "Norm of the params: 2.0557666\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.41261. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [137] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40461066\n",
      "Train loss (w/o reg) on all data: 0.40440592\n",
      "Test loss (w/o reg) on all data: 0.40237135\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009490547\n",
      "Norm of the params: 2.0235572\n",
      "                Loss: fixed   4 labels. Loss 0.40237. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44682214\n",
      "Train loss (w/o reg) on all data: 0.44665226\n",
      "Test loss (w/o reg) on all data: 0.44252464\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00097258494\n",
      "Norm of the params: 1.8432096\n",
      "              Random: fixed   1 labels. Loss 0.44252. Accuracy 0.822.\n",
      "### Flips: 20, rs: 34, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40340224\n",
      "Train loss (w/o reg) on all data: 0.4031762\n",
      "Test loss (w/o reg) on all data: 0.41716367\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00036349398\n",
      "Norm of the params: 2.1262386\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.41716. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [172] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38482124\n",
      "Train loss (w/o reg) on all data: 0.38458169\n",
      "Test loss (w/o reg) on all data: 0.418563\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00094295474\n",
      "Norm of the params: 2.188778\n",
      "                Loss: fixed   7 labels. Loss 0.41856. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [167] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4495813\n",
      "Train loss (w/o reg) on all data: 0.44941163\n",
      "Test loss (w/o reg) on all data: 0.45072433\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0038230075\n",
      "Norm of the params: 1.8420237\n",
      "              Random: fixed   2 labels. Loss 0.45072. Accuracy 0.800.\n",
      "### Flips: 20, rs: 34, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [199] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3734972\n",
      "Train loss (w/o reg) on all data: 0.37324998\n",
      "Test loss (w/o reg) on all data: 0.41386986\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018282941\n",
      "Norm of the params: 2.223581\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.41387. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [140] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35781074\n",
      "Train loss (w/o reg) on all data: 0.35750538\n",
      "Test loss (w/o reg) on all data: 0.4358793\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.005468212\n",
      "Norm of the params: 2.4712029\n",
      "                Loss: fixed  11 labels. Loss 0.43588. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [163] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4553661\n",
      "Train loss (w/o reg) on all data: 0.45520586\n",
      "Test loss (w/o reg) on all data: 0.448722\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0054988195\n",
      "Norm of the params: 1.7903116\n",
      "              Random: fixed   4 labels. Loss 0.44872. Accuracy 0.822.\n",
      "### Flips: 20, rs: 34, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [133] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36611235\n",
      "Train loss (w/o reg) on all data: 0.36585975\n",
      "Test loss (w/o reg) on all data: 0.4046424\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.003308933\n",
      "Norm of the params: 2.2476227\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.40464. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [98] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34233448\n",
      "Train loss (w/o reg) on all data: 0.3420112\n",
      "Test loss (w/o reg) on all data: 0.42317247\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002685869\n",
      "Norm of the params: 2.542667\n",
      "                Loss: fixed  13 labels. Loss 0.42317. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45612463\n",
      "Train loss (w/o reg) on all data: 0.4559608\n",
      "Test loss (w/o reg) on all data: 0.44043228\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0072355517\n",
      "Norm of the params: 1.8101074\n",
      "              Random: fixed   5 labels. Loss 0.44043. Accuracy 0.822.\n",
      "### Flips: 20, rs: 34, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3491749\n",
      "Train loss (w/o reg) on all data: 0.3488612\n",
      "Test loss (w/o reg) on all data: 0.41714376\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00221024\n",
      "Norm of the params: 2.5048108\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.41714. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [0] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3491749\n",
      "Train loss (w/o reg) on all data: 0.3488612\n",
      "Test loss (w/o reg) on all data: 0.41715142\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006848992\n",
      "Norm of the params: 2.5048106\n",
      "                Loss: fixed  14 labels. Loss 0.41715. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46248275\n",
      "Train loss (w/o reg) on all data: 0.4623212\n",
      "Test loss (w/o reg) on all data: 0.4302416\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00067666866\n",
      "Norm of the params: 1.7975312\n",
      "              Random: fixed   6 labels. Loss 0.43024. Accuracy 0.822.\n",
      "### Flips: 20, rs: 34, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35021186\n",
      "Train loss (w/o reg) on all data: 0.34991336\n",
      "Test loss (w/o reg) on all data: 0.4194338\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0023926161\n",
      "Norm of the params: 2.4433637\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.41943. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [172] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34917176\n",
      "Train loss (w/o reg) on all data: 0.3488607\n",
      "Test loss (w/o reg) on all data: 0.416799\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00095478527\n",
      "Norm of the params: 2.4941394\n",
      "                Loss: fixed  14 labels. Loss 0.41680. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4558693\n",
      "Train loss (w/o reg) on all data: 0.45570096\n",
      "Test loss (w/o reg) on all data: 0.43990928\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010833837\n",
      "Norm of the params: 1.8348708\n",
      "              Random: fixed   7 labels. Loss 0.43991. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [299] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48388752\n",
      "Train loss (w/o reg) on all data: 0.4837746\n",
      "Test loss (w/o reg) on all data: 0.40518653\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001088805\n",
      "Norm of the params: 1.5028137\n",
      "Flipped loss: 0.40519. Accuracy: 0.822\n",
      "### Flips: 20, rs: 35, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [139] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4391632\n",
      "Train loss (w/o reg) on all data: 0.4390339\n",
      "Test loss (w/o reg) on all data: 0.42216563\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0004078486\n",
      "Norm of the params: 1.6081457\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.42217. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4072496\n",
      "Train loss (w/o reg) on all data: 0.40707704\n",
      "Test loss (w/o reg) on all data: 0.41984186\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004320125\n",
      "Norm of the params: 1.8577538\n",
      "                Loss: fixed   6 labels. Loss 0.41984. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47906205\n",
      "Train loss (w/o reg) on all data: 0.47895017\n",
      "Test loss (w/o reg) on all data: 0.39240023\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001538085\n",
      "Norm of the params: 1.4959364\n",
      "              Random: fixed   1 labels. Loss 0.39240. Accuracy 0.822.\n",
      "### Flips: 20, rs: 35, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [171] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4181005\n",
      "Train loss (w/o reg) on all data: 0.41793302\n",
      "Test loss (w/o reg) on all data: 0.44013205\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.00084622746\n",
      "Norm of the params: 1.8302568\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.44013. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37490365\n",
      "Train loss (w/o reg) on all data: 0.37464467\n",
      "Test loss (w/o reg) on all data: 0.4220037\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001801662\n",
      "Norm of the params: 2.275891\n",
      "                Loss: fixed  10 labels. Loss 0.42200. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4790639\n",
      "Train loss (w/o reg) on all data: 0.47895265\n",
      "Test loss (w/o reg) on all data: 0.39213353\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0021303832\n",
      "Norm of the params: 1.4915849\n",
      "              Random: fixed   1 labels. Loss 0.39213. Accuracy 0.822.\n",
      "### Flips: 20, rs: 35, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [352] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3930883\n",
      "Train loss (w/o reg) on all data: 0.39289355\n",
      "Test loss (w/o reg) on all data: 0.44618568\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0006853405\n",
      "Norm of the params: 1.9736332\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.44619. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3519344\n",
      "Train loss (w/o reg) on all data: 0.35165\n",
      "Test loss (w/o reg) on all data: 0.44211942\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0004989314\n",
      "Norm of the params: 2.385001\n",
      "                Loss: fixed  13 labels. Loss 0.44212. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47107872\n",
      "Train loss (w/o reg) on all data: 0.47096443\n",
      "Test loss (w/o reg) on all data: 0.3899822\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0031517236\n",
      "Norm of the params: 1.5118995\n",
      "              Random: fixed   2 labels. Loss 0.38998. Accuracy 0.822.\n",
      "### Flips: 20, rs: 35, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37516323\n",
      "Train loss (w/o reg) on all data: 0.3749093\n",
      "Test loss (w/o reg) on all data: 0.4525532\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.005776333\n",
      "Norm of the params: 2.2534919\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.45255. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [132] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34117153\n",
      "Train loss (w/o reg) on all data: 0.34083125\n",
      "Test loss (w/o reg) on all data: 0.45775384\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00065133587\n",
      "Norm of the params: 2.6087968\n",
      "                Loss: fixed  15 labels. Loss 0.45775. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [348] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45643434\n",
      "Train loss (w/o reg) on all data: 0.45630112\n",
      "Test loss (w/o reg) on all data: 0.38456005\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00048954895\n",
      "Norm of the params: 1.6323136\n",
      "              Random: fixed   3 labels. Loss 0.38456. Accuracy 0.822.\n",
      "### Flips: 20, rs: 35, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36324307\n",
      "Train loss (w/o reg) on all data: 0.3629667\n",
      "Test loss (w/o reg) on all data: 0.4453866\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00058983703\n",
      "Norm of the params: 2.351128\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.44539. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [97] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34065086\n",
      "Train loss (w/o reg) on all data: 0.34030274\n",
      "Test loss (w/o reg) on all data: 0.4554066\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009250575\n",
      "Norm of the params: 2.6386664\n",
      "                Loss: fixed  16 labels. Loss 0.45541. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [300] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4564348\n",
      "Train loss (w/o reg) on all data: 0.4563018\n",
      "Test loss (w/o reg) on all data: 0.38443333\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0026030398\n",
      "Norm of the params: 1.6308124\n",
      "              Random: fixed   3 labels. Loss 0.38443. Accuracy 0.822.\n",
      "### Flips: 20, rs: 35, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [301] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35501477\n",
      "Train loss (w/o reg) on all data: 0.35470587\n",
      "Test loss (w/o reg) on all data: 0.4370255\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.013178537\n",
      "Norm of the params: 2.4855328\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.43703. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34064984\n",
      "Train loss (w/o reg) on all data: 0.3402995\n",
      "Test loss (w/o reg) on all data: 0.45604944\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018983708\n",
      "Norm of the params: 2.6470797\n",
      "                Loss: fixed  16 labels. Loss 0.45605. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [414] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45525792\n",
      "Train loss (w/o reg) on all data: 0.45512906\n",
      "Test loss (w/o reg) on all data: 0.37858173\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0030180963\n",
      "Norm of the params: 1.6054549\n",
      "              Random: fixed   5 labels. Loss 0.37858. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [154] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4780757\n",
      "Train loss (w/o reg) on all data: 0.4779255\n",
      "Test loss (w/o reg) on all data: 0.4094177\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00036378964\n",
      "Norm of the params: 1.7331908\n",
      "Flipped loss: 0.40942. Accuracy: 0.844\n",
      "### Flips: 20, rs: 36, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [184] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40178007\n",
      "Train loss (w/o reg) on all data: 0.40151408\n",
      "Test loss (w/o reg) on all data: 0.36361402\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00043399216\n",
      "Norm of the params: 2.3064735\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.36361. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [84] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40141764\n",
      "Train loss (w/o reg) on all data: 0.40112966\n",
      "Test loss (w/o reg) on all data: 0.36665443\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.006937834\n",
      "Norm of the params: 2.3999317\n",
      "                Loss: fixed   6 labels. Loss 0.36665. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4772783\n",
      "Train loss (w/o reg) on all data: 0.47714046\n",
      "Test loss (w/o reg) on all data: 0.40931782\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0004846649\n",
      "Norm of the params: 1.6603816\n",
      "              Random: fixed   1 labels. Loss 0.40932. Accuracy 0.844.\n",
      "### Flips: 20, rs: 36, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3735907\n",
      "Train loss (w/o reg) on all data: 0.37324032\n",
      "Test loss (w/o reg) on all data: 0.36236313\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014295811\n",
      "Norm of the params: 2.6471941\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.36236. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35905898\n",
      "Train loss (w/o reg) on all data: 0.35863388\n",
      "Test loss (w/o reg) on all data: 0.36607483\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00057590584\n",
      "Norm of the params: 2.9157825\n",
      "                Loss: fixed  10 labels. Loss 0.36607. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4772803\n",
      "Train loss (w/o reg) on all data: 0.477143\n",
      "Test loss (w/o reg) on all data: 0.40927997\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0019044332\n",
      "Norm of the params: 1.65713\n",
      "              Random: fixed   1 labels. Loss 0.40928. Accuracy 0.844.\n",
      "### Flips: 20, rs: 36, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35754147\n",
      "Train loss (w/o reg) on all data: 0.35712448\n",
      "Test loss (w/o reg) on all data: 0.3617094\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004743688\n",
      "Norm of the params: 2.8879254\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.36171. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [99] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3538375\n",
      "Train loss (w/o reg) on all data: 0.3534124\n",
      "Test loss (w/o reg) on all data: 0.3761861\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00024231474\n",
      "Norm of the params: 2.9157848\n",
      "                Loss: fixed  11 labels. Loss 0.37619. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46842238\n",
      "Train loss (w/o reg) on all data: 0.46826842\n",
      "Test loss (w/o reg) on all data: 0.39278048\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002150323\n",
      "Norm of the params: 1.7547656\n",
      "              Random: fixed   3 labels. Loss 0.39278. Accuracy 0.844.\n",
      "### Flips: 20, rs: 36, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35331663\n",
      "Train loss (w/o reg) on all data: 0.35292518\n",
      "Test loss (w/o reg) on all data: 0.37511724\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001628716\n",
      "Norm of the params: 2.7980857\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.37512. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [95] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35279527\n",
      "Train loss (w/o reg) on all data: 0.35238233\n",
      "Test loss (w/o reg) on all data: 0.3807396\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009893399\n",
      "Norm of the params: 2.8737857\n",
      "                Loss: fixed  12 labels. Loss 0.38074. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4684312\n",
      "Train loss (w/o reg) on all data: 0.46827847\n",
      "Test loss (w/o reg) on all data: 0.39258918\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0054371054\n",
      "Norm of the params: 1.7476971\n",
      "              Random: fixed   3 labels. Loss 0.39259. Accuracy 0.844.\n",
      "### Flips: 20, rs: 36, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [361] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35238412\n",
      "Train loss (w/o reg) on all data: 0.35199532\n",
      "Test loss (w/o reg) on all data: 0.38076544\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00047121293\n",
      "Norm of the params: 2.7885478\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.38077. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [187] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34818426\n",
      "Train loss (w/o reg) on all data: 0.34778658\n",
      "Test loss (w/o reg) on all data: 0.381229\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00044993524\n",
      "Norm of the params: 2.8202405\n",
      "                Loss: fixed  14 labels. Loss 0.38123. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46842676\n",
      "Train loss (w/o reg) on all data: 0.46827286\n",
      "Test loss (w/o reg) on all data: 0.39260772\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00119368\n",
      "Norm of the params: 1.7543877\n",
      "              Random: fixed   3 labels. Loss 0.39261. Accuracy 0.844.\n",
      "### Flips: 20, rs: 36, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35046455\n",
      "Train loss (w/o reg) on all data: 0.3500854\n",
      "Test loss (w/o reg) on all data: 0.37625763\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001759381\n",
      "Norm of the params: 2.753695\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.37626. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [141] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34574616\n",
      "Train loss (w/o reg) on all data: 0.3453387\n",
      "Test loss (w/o reg) on all data: 0.3875017\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.000762223\n",
      "Norm of the params: 2.8546352\n",
      "                Loss: fixed  15 labels. Loss 0.38750. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [362] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46113136\n",
      "Train loss (w/o reg) on all data: 0.46095824\n",
      "Test loss (w/o reg) on all data: 0.3963278\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010390384\n",
      "Norm of the params: 1.8607659\n",
      "              Random: fixed   4 labels. Loss 0.39633. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [362] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46754828\n",
      "Train loss (w/o reg) on all data: 0.46739557\n",
      "Test loss (w/o reg) on all data: 0.43587664\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0007815212\n",
      "Norm of the params: 1.7475418\n",
      "Flipped loss: 0.43588. Accuracy: 0.756\n",
      "### Flips: 20, rs: 37, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4139644\n",
      "Train loss (w/o reg) on all data: 0.41372773\n",
      "Test loss (w/o reg) on all data: 0.43933636\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00053451123\n",
      "Norm of the params: 2.1755443\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.43934. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [168] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38757572\n",
      "Train loss (w/o reg) on all data: 0.38723114\n",
      "Test loss (w/o reg) on all data: 0.41248047\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.007440179\n",
      "Norm of the params: 2.625172\n",
      "                Loss: fixed   6 labels. Loss 0.41248. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4587784\n",
      "Train loss (w/o reg) on all data: 0.45861456\n",
      "Test loss (w/o reg) on all data: 0.41889408\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0011541203\n",
      "Norm of the params: 1.8103026\n",
      "              Random: fixed   1 labels. Loss 0.41889. Accuracy 0.756.\n",
      "### Flips: 20, rs: 37, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38486114\n",
      "Train loss (w/o reg) on all data: 0.38463268\n",
      "Test loss (w/o reg) on all data: 0.42019135\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0017248094\n",
      "Norm of the params: 2.1376147\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.42019. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35697505\n",
      "Train loss (w/o reg) on all data: 0.3565429\n",
      "Test loss (w/o reg) on all data: 0.39358342\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005157227\n",
      "Norm of the params: 2.9399173\n",
      "                Loss: fixed  10 labels. Loss 0.39358. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45364738\n",
      "Train loss (w/o reg) on all data: 0.4534723\n",
      "Test loss (w/o reg) on all data: 0.43186185\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00043907407\n",
      "Norm of the params: 1.8713675\n",
      "              Random: fixed   2 labels. Loss 0.43186. Accuracy 0.756.\n",
      "### Flips: 20, rs: 37, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3733332\n",
      "Train loss (w/o reg) on all data: 0.37306702\n",
      "Test loss (w/o reg) on all data: 0.429013\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0031829719\n",
      "Norm of the params: 2.3072124\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.42901. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [168] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34402564\n",
      "Train loss (w/o reg) on all data: 0.343608\n",
      "Test loss (w/o reg) on all data: 0.42223457\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.000991824\n",
      "Norm of the params: 2.8901136\n",
      "                Loss: fixed  13 labels. Loss 0.42223. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45364952\n",
      "Train loss (w/o reg) on all data: 0.45347252\n",
      "Test loss (w/o reg) on all data: 0.43258908\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0022958224\n",
      "Norm of the params: 1.8814429\n",
      "              Random: fixed   2 labels. Loss 0.43259. Accuracy 0.756.\n",
      "### Flips: 20, rs: 37, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35883173\n",
      "Train loss (w/o reg) on all data: 0.35851654\n",
      "Test loss (w/o reg) on all data: 0.4292987\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00047232164\n",
      "Norm of the params: 2.5107481\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.42930. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33748487\n",
      "Train loss (w/o reg) on all data: 0.33709866\n",
      "Test loss (w/o reg) on all data: 0.41786623\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0039749234\n",
      "Norm of the params: 2.7791893\n",
      "                Loss: fixed  14 labels. Loss 0.41787. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4395884\n",
      "Train loss (w/o reg) on all data: 0.4393793\n",
      "Test loss (w/o reg) on all data: 0.44870958\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.002623767\n",
      "Norm of the params: 2.0449636\n",
      "              Random: fixed   4 labels. Loss 0.44871. Accuracy 0.756.\n",
      "### Flips: 20, rs: 37, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3588301\n",
      "Train loss (w/o reg) on all data: 0.35851553\n",
      "Test loss (w/o reg) on all data: 0.42890036\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0014437148\n",
      "Norm of the params: 2.5082948\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.42890. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [269] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33748576\n",
      "Train loss (w/o reg) on all data: 0.3370991\n",
      "Test loss (w/o reg) on all data: 0.4175163\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0009418203\n",
      "Norm of the params: 2.780802\n",
      "                Loss: fixed  14 labels. Loss 0.41752. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [300] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43958867\n",
      "Train loss (w/o reg) on all data: 0.43937927\n",
      "Test loss (w/o reg) on all data: 0.44869825\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00072895293\n",
      "Norm of the params: 2.0464327\n",
      "              Random: fixed   4 labels. Loss 0.44870. Accuracy 0.756.\n",
      "### Flips: 20, rs: 37, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [336] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34241137\n",
      "Train loss (w/o reg) on all data: 0.34205508\n",
      "Test loss (w/o reg) on all data: 0.415073\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00056483754\n",
      "Norm of the params: 2.6694188\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.41507. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [198] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33748534\n",
      "Train loss (w/o reg) on all data: 0.33709955\n",
      "Test loss (w/o reg) on all data: 0.41735968\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00049510546\n",
      "Norm of the params: 2.7777054\n",
      "                Loss: fixed  14 labels. Loss 0.41736. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [111] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4198314\n",
      "Train loss (w/o reg) on all data: 0.4195785\n",
      "Test loss (w/o reg) on all data: 0.45231766\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0011602127\n",
      "Norm of the params: 2.2489824\n",
      "              Random: fixed   5 labels. Loss 0.45232. Accuracy 0.756.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [280] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46815005\n",
      "Train loss (w/o reg) on all data: 0.4680107\n",
      "Test loss (w/o reg) on all data: 0.41858864\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009822446\n",
      "Norm of the params: 1.6694216\n",
      "Flipped loss: 0.41859. Accuracy: 0.800\n",
      "### Flips: 20, rs: 38, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4192528\n",
      "Train loss (w/o reg) on all data: 0.41902938\n",
      "Test loss (w/o reg) on all data: 0.42458382\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014816868\n",
      "Norm of the params: 2.113922\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.42458. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3598352\n",
      "Train loss (w/o reg) on all data: 0.35946342\n",
      "Test loss (w/o reg) on all data: 0.4585328\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007605049\n",
      "Norm of the params: 2.7267897\n",
      "                Loss: fixed   8 labels. Loss 0.45853. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46093985\n",
      "Train loss (w/o reg) on all data: 0.46080145\n",
      "Test loss (w/o reg) on all data: 0.42812636\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001002245\n",
      "Norm of the params: 1.6637053\n",
      "              Random: fixed   1 labels. Loss 0.42813. Accuracy 0.800.\n",
      "### Flips: 20, rs: 38, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3924034\n",
      "Train loss (w/o reg) on all data: 0.39212024\n",
      "Test loss (w/o reg) on all data: 0.4388841\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.000556637\n",
      "Norm of the params: 2.379747\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.43888. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3505874\n",
      "Train loss (w/o reg) on all data: 0.35021412\n",
      "Test loss (w/o reg) on all data: 0.47444353\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014190687\n",
      "Norm of the params: 2.7322645\n",
      "                Loss: fixed   9 labels. Loss 0.47444. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4609421\n",
      "Train loss (w/o reg) on all data: 0.4608036\n",
      "Test loss (w/o reg) on all data: 0.4285489\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003991191\n",
      "Norm of the params: 1.6642853\n",
      "              Random: fixed   1 labels. Loss 0.42855. Accuracy 0.800.\n",
      "### Flips: 20, rs: 38, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37099853\n",
      "Train loss (w/o reg) on all data: 0.37069687\n",
      "Test loss (w/o reg) on all data: 0.4238027\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.007824011\n",
      "Norm of the params: 2.4562652\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.42380. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [165] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33987433\n",
      "Train loss (w/o reg) on all data: 0.3394668\n",
      "Test loss (w/o reg) on all data: 0.47369426\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001825117\n",
      "Norm of the params: 2.8548527\n",
      "                Loss: fixed  11 labels. Loss 0.47369. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45188528\n",
      "Train loss (w/o reg) on all data: 0.45173603\n",
      "Test loss (w/o reg) on all data: 0.42269036\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.000894441\n",
      "Norm of the params: 1.7277709\n",
      "              Random: fixed   2 labels. Loss 0.42269. Accuracy 0.800.\n",
      "### Flips: 20, rs: 38, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [323] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34451383\n",
      "Train loss (w/o reg) on all data: 0.344135\n",
      "Test loss (w/o reg) on all data: 0.43837112\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006339603\n",
      "Norm of the params: 2.7526584\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.43837. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [208] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33405328\n",
      "Train loss (w/o reg) on all data: 0.33368236\n",
      "Test loss (w/o reg) on all data: 0.45350474\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00057367334\n",
      "Norm of the params: 2.7236629\n",
      "                Loss: fixed  14 labels. Loss 0.45350. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.430921\n",
      "Train loss (w/o reg) on all data: 0.43074107\n",
      "Test loss (w/o reg) on all data: 0.42603844\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0033160984\n",
      "Norm of the params: 1.8969345\n",
      "              Random: fixed   4 labels. Loss 0.42604. Accuracy 0.800.\n",
      "### Flips: 20, rs: 38, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33755243\n",
      "Train loss (w/o reg) on all data: 0.33718842\n",
      "Test loss (w/o reg) on all data: 0.45209843\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006897604\n",
      "Norm of the params: 2.6982043\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.45210. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [64] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33210465\n",
      "Train loss (w/o reg) on all data: 0.33173633\n",
      "Test loss (w/o reg) on all data: 0.4556918\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00060483965\n",
      "Norm of the params: 2.7141325\n",
      "                Loss: fixed  15 labels. Loss 0.45569. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43314296\n",
      "Train loss (w/o reg) on all data: 0.4329565\n",
      "Test loss (w/o reg) on all data: 0.4249848\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00080774305\n",
      "Norm of the params: 1.9311485\n",
      "              Random: fixed   5 labels. Loss 0.42498. Accuracy 0.778.\n",
      "### Flips: 20, rs: 38, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [132] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3340675\n",
      "Train loss (w/o reg) on all data: 0.33370087\n",
      "Test loss (w/o reg) on all data: 0.45376202\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001108653\n",
      "Norm of the params: 2.7079067\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.45376. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [76] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3321044\n",
      "Train loss (w/o reg) on all data: 0.3317386\n",
      "Test loss (w/o reg) on all data: 0.455351\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007380705\n",
      "Norm of the params: 2.7049458\n",
      "                Loss: fixed  15 labels. Loss 0.45535. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43119553\n",
      "Train loss (w/o reg) on all data: 0.43100673\n",
      "Test loss (w/o reg) on all data: 0.42573908\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0013479502\n",
      "Norm of the params: 1.9432361\n",
      "              Random: fixed   6 labels. Loss 0.42574. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [291] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46156725\n",
      "Train loss (w/o reg) on all data: 0.46134683\n",
      "Test loss (w/o reg) on all data: 0.4569831\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012198735\n",
      "Norm of the params: 2.0996335\n",
      "Flipped loss: 0.45698. Accuracy: 0.778\n",
      "### Flips: 20, rs: 39, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [90] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4253002\n",
      "Train loss (w/o reg) on all data: 0.42502022\n",
      "Test loss (w/o reg) on all data: 0.47983047\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.01168051\n",
      "Norm of the params: 2.3664618\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.47983. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [95] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39955273\n",
      "Train loss (w/o reg) on all data: 0.3992332\n",
      "Test loss (w/o reg) on all data: 0.49149033\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0065707066\n",
      "Norm of the params: 2.5280426\n",
      "                Loss: fixed   5 labels. Loss 0.49149. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4395025\n",
      "Train loss (w/o reg) on all data: 0.43926087\n",
      "Test loss (w/o reg) on all data: 0.44898397\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0009963078\n",
      "Norm of the params: 2.198392\n",
      "              Random: fixed   2 labels. Loss 0.44898. Accuracy 0.778.\n",
      "### Flips: 20, rs: 39, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40682742\n",
      "Train loss (w/o reg) on all data: 0.40653223\n",
      "Test loss (w/o reg) on all data: 0.4631858\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0021192355\n",
      "Norm of the params: 2.4297445\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.46319. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [150] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36729544\n",
      "Train loss (w/o reg) on all data: 0.36690903\n",
      "Test loss (w/o reg) on all data: 0.4711925\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011844805\n",
      "Norm of the params: 2.7800002\n",
      "                Loss: fixed   9 labels. Loss 0.47119. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43070763\n",
      "Train loss (w/o reg) on all data: 0.4304419\n",
      "Test loss (w/o reg) on all data: 0.45878774\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0009945807\n",
      "Norm of the params: 2.3053808\n",
      "              Random: fixed   3 labels. Loss 0.45879. Accuracy 0.778.\n",
      "### Flips: 20, rs: 39, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39828283\n",
      "Train loss (w/o reg) on all data: 0.3979777\n",
      "Test loss (w/o reg) on all data: 0.45511404\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0035267323\n",
      "Norm of the params: 2.4703078\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.45511. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35947654\n",
      "Train loss (w/o reg) on all data: 0.35907835\n",
      "Test loss (w/o reg) on all data: 0.4701993\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.007923669\n",
      "Norm of the params: 2.822017\n",
      "                Loss: fixed  10 labels. Loss 0.47020. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42064527\n",
      "Train loss (w/o reg) on all data: 0.42037597\n",
      "Test loss (w/o reg) on all data: 0.46541765\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0019158596\n",
      "Norm of the params: 2.3207552\n",
      "              Random: fixed   4 labels. Loss 0.46542. Accuracy 0.778.\n",
      "### Flips: 20, rs: 39, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37317097\n",
      "Train loss (w/o reg) on all data: 0.37280747\n",
      "Test loss (w/o reg) on all data: 0.46123394\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0054299505\n",
      "Norm of the params: 2.6962545\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.46123. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [201] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35321766\n",
      "Train loss (w/o reg) on all data: 0.3528365\n",
      "Test loss (w/o reg) on all data: 0.47453892\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0007957647\n",
      "Norm of the params: 2.7610395\n",
      "                Loss: fixed  11 labels. Loss 0.47454. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [303] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42064273\n",
      "Train loss (w/o reg) on all data: 0.42037448\n",
      "Test loss (w/o reg) on all data: 0.4653934\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00033930718\n",
      "Norm of the params: 2.3162374\n",
      "              Random: fixed   4 labels. Loss 0.46539. Accuracy 0.778.\n",
      "### Flips: 20, rs: 39, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [339] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3454334\n",
      "Train loss (w/o reg) on all data: 0.34496897\n",
      "Test loss (w/o reg) on all data: 0.43197584\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003230207\n",
      "Norm of the params: 3.0477946\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.43198. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35365242\n",
      "Train loss (w/o reg) on all data: 0.3532745\n",
      "Test loss (w/o reg) on all data: 0.46764758\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00056977174\n",
      "Norm of the params: 2.7492847\n",
      "                Loss: fixed  12 labels. Loss 0.46765. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40872356\n",
      "Train loss (w/o reg) on all data: 0.408441\n",
      "Test loss (w/o reg) on all data: 0.4739197\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0014091013\n",
      "Norm of the params: 2.3771775\n",
      "              Random: fixed   5 labels. Loss 0.47392. Accuracy 0.778.\n",
      "### Flips: 20, rs: 39, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34543353\n",
      "Train loss (w/o reg) on all data: 0.3449705\n",
      "Test loss (w/o reg) on all data: 0.43214515\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006151769\n",
      "Norm of the params: 3.0431817\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.43215. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34324783\n",
      "Train loss (w/o reg) on all data: 0.3428296\n",
      "Test loss (w/o reg) on all data: 0.45243725\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002508133\n",
      "Norm of the params: 2.8920937\n",
      "                Loss: fixed  14 labels. Loss 0.45244. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [374] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40757474\n",
      "Train loss (w/o reg) on all data: 0.40728945\n",
      "Test loss (w/o reg) on all data: 0.473767\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00090188027\n",
      "Norm of the params: 2.3886855\n",
      "              Random: fixed   7 labels. Loss 0.47377. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49209267\n",
      "Train loss (w/o reg) on all data: 0.49197537\n",
      "Test loss (w/o reg) on all data: 0.42687798\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0014201311\n",
      "Norm of the params: 1.5316119\n",
      "Flipped loss: 0.42688. Accuracy: 0.778\n",
      "### Flips: 30, rs: 0, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45816398\n",
      "Train loss (w/o reg) on all data: 0.45800456\n",
      "Test loss (w/o reg) on all data: 0.39519322\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0013880485\n",
      "Norm of the params: 1.7856494\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.39519. Accuracy 0.756.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4252562\n",
      "Train loss (w/o reg) on all data: 0.42506906\n",
      "Test loss (w/o reg) on all data: 0.40754718\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.004789346\n",
      "Norm of the params: 1.9345267\n",
      "                Loss: fixed   6 labels. Loss 0.40755. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49208978\n",
      "Train loss (w/o reg) on all data: 0.4919728\n",
      "Test loss (w/o reg) on all data: 0.42711255\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0006401216\n",
      "Norm of the params: 1.5294424\n",
      "              Random: fixed   0 labels. Loss 0.42711. Accuracy 0.778.\n",
      "### Flips: 30, rs: 0, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [279] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43711066\n",
      "Train loss (w/o reg) on all data: 0.4369096\n",
      "Test loss (w/o reg) on all data: 0.3778001\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0003849784\n",
      "Norm of the params: 2.0053282\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.37780. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39147416\n",
      "Train loss (w/o reg) on all data: 0.39124912\n",
      "Test loss (w/o reg) on all data: 0.41719675\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.009534739\n",
      "Norm of the params: 2.12156\n",
      "                Loss: fixed  10 labels. Loss 0.41720. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [222] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48614508\n",
      "Train loss (w/o reg) on all data: 0.4860096\n",
      "Test loss (w/o reg) on all data: 0.422938\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020242927\n",
      "Norm of the params: 1.646068\n",
      "              Random: fixed   2 labels. Loss 0.42294. Accuracy 0.800.\n",
      "### Flips: 30, rs: 0, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37172404\n",
      "Train loss (w/o reg) on all data: 0.37134966\n",
      "Test loss (w/o reg) on all data: 0.3790463\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010126408\n",
      "Norm of the params: 2.7363374\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.37905. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [140] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35199594\n",
      "Train loss (w/o reg) on all data: 0.35165542\n",
      "Test loss (w/o reg) on all data: 0.4159341\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00850574\n",
      "Norm of the params: 2.6096356\n",
      "                Loss: fixed  16 labels. Loss 0.41593. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49737215\n",
      "Train loss (w/o reg) on all data: 0.49725366\n",
      "Test loss (w/o reg) on all data: 0.41267908\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0024462384\n",
      "Norm of the params: 1.5393602\n",
      "              Random: fixed   4 labels. Loss 0.41268. Accuracy 0.756.\n",
      "### Flips: 30, rs: 0, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [195] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35930327\n",
      "Train loss (w/o reg) on all data: 0.3589269\n",
      "Test loss (w/o reg) on all data: 0.39177832\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0024435797\n",
      "Norm of the params: 2.743582\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.39178. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [95] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33316824\n",
      "Train loss (w/o reg) on all data: 0.33272013\n",
      "Test loss (w/o reg) on all data: 0.42065784\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010237829\n",
      "Norm of the params: 2.9936461\n",
      "                Loss: fixed  19 labels. Loss 0.42066. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.498345\n",
      "Train loss (w/o reg) on all data: 0.4982256\n",
      "Test loss (w/o reg) on all data: 0.41044846\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0052192686\n",
      "Norm of the params: 1.5451508\n",
      "              Random: fixed   5 labels. Loss 0.41045. Accuracy 0.822.\n",
      "### Flips: 30, rs: 0, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3526121\n",
      "Train loss (w/o reg) on all data: 0.35227692\n",
      "Test loss (w/o reg) on all data: 0.41051614\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00057530764\n",
      "Norm of the params: 2.5892065\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.41052. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33108672\n",
      "Train loss (w/o reg) on all data: 0.33066314\n",
      "Test loss (w/o reg) on all data: 0.42390895\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0016647863\n",
      "Norm of the params: 2.9106278\n",
      "                Loss: fixed  22 labels. Loss 0.42391. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [348] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49834296\n",
      "Train loss (w/o reg) on all data: 0.49822423\n",
      "Test loss (w/o reg) on all data: 0.41019493\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019214497\n",
      "Norm of the params: 1.5410799\n",
      "              Random: fixed   5 labels. Loss 0.41019. Accuracy 0.822.\n",
      "### Flips: 30, rs: 0, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [317] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34399873\n",
      "Train loss (w/o reg) on all data: 0.3436311\n",
      "Test loss (w/o reg) on all data: 0.4040413\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001221977\n",
      "Norm of the params: 2.7115731\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.40404. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32552722\n",
      "Train loss (w/o reg) on all data: 0.32507792\n",
      "Test loss (w/o reg) on all data: 0.44288632\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.010471601\n",
      "Norm of the params: 2.9976945\n",
      "                Loss: fixed  23 labels. Loss 0.44289. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [392] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5086694\n",
      "Train loss (w/o reg) on all data: 0.5085676\n",
      "Test loss (w/o reg) on all data: 0.40189824\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0066381358\n",
      "Norm of the params: 1.4268821\n",
      "              Random: fixed   7 labels. Loss 0.40190. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52316105\n",
      "Train loss (w/o reg) on all data: 0.52304554\n",
      "Test loss (w/o reg) on all data: 0.42780736\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0032467605\n",
      "Norm of the params: 1.5200531\n",
      "Flipped loss: 0.42781. Accuracy: 0.822\n",
      "### Flips: 30, rs: 1, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [103] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.472899\n",
      "Train loss (w/o reg) on all data: 0.4727075\n",
      "Test loss (w/o reg) on all data: 0.41836566\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010869955\n",
      "Norm of the params: 1.9569812\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.41837. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [154] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43279713\n",
      "Train loss (w/o reg) on all data: 0.43255487\n",
      "Test loss (w/o reg) on all data: 0.40336508\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0075861737\n",
      "Norm of the params: 2.2012026\n",
      "                Loss: fixed   8 labels. Loss 0.40337. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [140] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52316076\n",
      "Train loss (w/o reg) on all data: 0.5230461\n",
      "Test loss (w/o reg) on all data: 0.4277398\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0026887495\n",
      "Norm of the params: 1.5144236\n",
      "              Random: fixed   0 labels. Loss 0.42774. Accuracy 0.822.\n",
      "### Flips: 30, rs: 1, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [169] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44243544\n",
      "Train loss (w/o reg) on all data: 0.44222292\n",
      "Test loss (w/o reg) on all data: 0.41130388\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0007455672\n",
      "Norm of the params: 2.0616386\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.41130. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [302] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3742645\n",
      "Train loss (w/o reg) on all data: 0.3739558\n",
      "Test loss (w/o reg) on all data: 0.42377234\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00030149653\n",
      "Norm of the params: 2.4848194\n",
      "                Loss: fixed  14 labels. Loss 0.42377. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51176614\n",
      "Train loss (w/o reg) on all data: 0.5116439\n",
      "Test loss (w/o reg) on all data: 0.41522858\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0021290106\n",
      "Norm of the params: 1.5638231\n",
      "              Random: fixed   2 labels. Loss 0.41523. Accuracy 0.822.\n",
      "### Flips: 30, rs: 1, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [340] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42764273\n",
      "Train loss (w/o reg) on all data: 0.42741027\n",
      "Test loss (w/o reg) on all data: 0.41101837\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0003822466\n",
      "Norm of the params: 2.156143\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.41102. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34984306\n",
      "Train loss (w/o reg) on all data: 0.34946522\n",
      "Test loss (w/o reg) on all data: 0.41628194\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002382841\n",
      "Norm of the params: 2.7489555\n",
      "                Loss: fixed  17 labels. Loss 0.41628. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4988243\n",
      "Train loss (w/o reg) on all data: 0.4986994\n",
      "Test loss (w/o reg) on all data: 0.3966273\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011532917\n",
      "Norm of the params: 1.5805174\n",
      "              Random: fixed   5 labels. Loss 0.39663. Accuracy 0.822.\n",
      "### Flips: 30, rs: 1, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [98] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42520145\n",
      "Train loss (w/o reg) on all data: 0.424996\n",
      "Test loss (w/o reg) on all data: 0.4164755\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0019536165\n",
      "Norm of the params: 2.0271304\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.41648. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34482184\n",
      "Train loss (w/o reg) on all data: 0.34442383\n",
      "Test loss (w/o reg) on all data: 0.41997656\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015885942\n",
      "Norm of the params: 2.8213732\n",
      "                Loss: fixed  18 labels. Loss 0.41998. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4853671\n",
      "Train loss (w/o reg) on all data: 0.48521826\n",
      "Test loss (w/o reg) on all data: 0.4046924\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0024509497\n",
      "Norm of the params: 1.7253652\n",
      "              Random: fixed   7 labels. Loss 0.40469. Accuracy 0.822.\n",
      "### Flips: 30, rs: 1, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [134] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40436432\n",
      "Train loss (w/o reg) on all data: 0.404135\n",
      "Test loss (w/o reg) on all data: 0.42311367\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0021414845\n",
      "Norm of the params: 2.1416895\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.42311. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [159] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34166\n",
      "Train loss (w/o reg) on all data: 0.34127456\n",
      "Test loss (w/o reg) on all data: 0.40665677\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008821432\n",
      "Norm of the params: 2.7764065\n",
      "                Loss: fixed  20 labels. Loss 0.40666. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [383] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4762546\n",
      "Train loss (w/o reg) on all data: 0.47609296\n",
      "Test loss (w/o reg) on all data: 0.3945496\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0015965363\n",
      "Norm of the params: 1.798088\n",
      "              Random: fixed   8 labels. Loss 0.39455. Accuracy 0.844.\n",
      "### Flips: 30, rs: 1, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [330] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3972477\n",
      "Train loss (w/o reg) on all data: 0.3970087\n",
      "Test loss (w/o reg) on all data: 0.42819512\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00018865286\n",
      "Norm of the params: 2.1863737\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.42820. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3394307\n",
      "Train loss (w/o reg) on all data: 0.3391035\n",
      "Test loss (w/o reg) on all data: 0.41688886\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0018479725\n",
      "Norm of the params: 2.5581322\n",
      "                Loss: fixed  24 labels. Loss 0.41689. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47099018\n",
      "Train loss (w/o reg) on all data: 0.47082686\n",
      "Test loss (w/o reg) on all data: 0.3820157\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013907618\n",
      "Norm of the params: 1.8073574\n",
      "              Random: fixed  10 labels. Loss 0.38202. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54517365\n",
      "Train loss (w/o reg) on all data: 0.5450846\n",
      "Test loss (w/o reg) on all data: 0.47020188\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0014173641\n",
      "Norm of the params: 1.3344353\n",
      "Flipped loss: 0.47020. Accuracy: 0.778\n",
      "### Flips: 30, rs: 2, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [116] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4831524\n",
      "Train loss (w/o reg) on all data: 0.48299524\n",
      "Test loss (w/o reg) on all data: 0.42298537\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0004477425\n",
      "Norm of the params: 1.772916\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.42299. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43823144\n",
      "Train loss (w/o reg) on all data: 0.437958\n",
      "Test loss (w/o reg) on all data: 0.47148803\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005607314\n",
      "Norm of the params: 2.3385577\n",
      "                Loss: fixed   9 labels. Loss 0.47149. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5451754\n",
      "Train loss (w/o reg) on all data: 0.54508704\n",
      "Test loss (w/o reg) on all data: 0.47081876\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0014661773\n",
      "Norm of the params: 1.3291483\n",
      "              Random: fixed   0 labels. Loss 0.47082. Accuracy 0.778.\n",
      "### Flips: 30, rs: 2, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43584177\n",
      "Train loss (w/o reg) on all data: 0.43557277\n",
      "Test loss (w/o reg) on all data: 0.42213142\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0021913203\n",
      "Norm of the params: 2.319413\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.42213. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [161] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38529968\n",
      "Train loss (w/o reg) on all data: 0.38491678\n",
      "Test loss (w/o reg) on all data: 0.4800576\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0020458747\n",
      "Norm of the params: 2.7673507\n",
      "                Loss: fixed  15 labels. Loss 0.48006. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.545016\n",
      "Train loss (w/o reg) on all data: 0.54493403\n",
      "Test loss (w/o reg) on all data: 0.46616203\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0023265255\n",
      "Norm of the params: 1.2803453\n",
      "              Random: fixed   1 labels. Loss 0.46616. Accuracy 0.778.\n",
      "### Flips: 30, rs: 2, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [331] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4309861\n",
      "Train loss (w/o reg) on all data: 0.43073058\n",
      "Test loss (w/o reg) on all data: 0.43410042\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013779983\n",
      "Norm of the params: 2.2606845\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.43410. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36396772\n",
      "Train loss (w/o reg) on all data: 0.36361387\n",
      "Test loss (w/o reg) on all data: 0.46484286\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0005281177\n",
      "Norm of the params: 2.6602037\n",
      "                Loss: fixed  18 labels. Loss 0.46484. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.525379\n",
      "Train loss (w/o reg) on all data: 0.52529943\n",
      "Test loss (w/o reg) on all data: 0.4533165\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00076276646\n",
      "Norm of the params: 1.2616837\n",
      "              Random: fixed   4 labels. Loss 0.45332. Accuracy 0.800.\n",
      "### Flips: 30, rs: 2, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42208934\n",
      "Train loss (w/o reg) on all data: 0.42180708\n",
      "Test loss (w/o reg) on all data: 0.43504393\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0014133402\n",
      "Norm of the params: 2.3759916\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.43504. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35069758\n",
      "Train loss (w/o reg) on all data: 0.3503159\n",
      "Test loss (w/o reg) on all data: 0.4662511\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0006999754\n",
      "Norm of the params: 2.7628663\n",
      "                Loss: fixed  20 labels. Loss 0.46625. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [258] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4879265\n",
      "Train loss (w/o reg) on all data: 0.48782602\n",
      "Test loss (w/o reg) on all data: 0.43926036\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018784828\n",
      "Norm of the params: 1.4177426\n",
      "              Random: fixed   8 labels. Loss 0.43926. Accuracy 0.822.\n",
      "### Flips: 30, rs: 2, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3958944\n",
      "Train loss (w/o reg) on all data: 0.3955798\n",
      "Test loss (w/o reg) on all data: 0.42918238\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0037004868\n",
      "Norm of the params: 2.5084302\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.42918. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34314445\n",
      "Train loss (w/o reg) on all data: 0.3427962\n",
      "Test loss (w/o reg) on all data: 0.47171882\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00038455307\n",
      "Norm of the params: 2.639104\n",
      "                Loss: fixed  22 labels. Loss 0.47172. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [171] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4814333\n",
      "Train loss (w/o reg) on all data: 0.48131958\n",
      "Test loss (w/o reg) on all data: 0.44662634\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015398146\n",
      "Norm of the params: 1.5081254\n",
      "              Random: fixed   9 labels. Loss 0.44663. Accuracy 0.822.\n",
      "### Flips: 30, rs: 2, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3789529\n",
      "Train loss (w/o reg) on all data: 0.37859556\n",
      "Test loss (w/o reg) on all data: 0.41477153\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.006428858\n",
      "Norm of the params: 2.6733277\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.41477. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [310] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34195116\n",
      "Train loss (w/o reg) on all data: 0.34161168\n",
      "Test loss (w/o reg) on all data: 0.48145723\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0010842133\n",
      "Norm of the params: 2.6056337\n",
      "                Loss: fixed  23 labels. Loss 0.48146. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4345975\n",
      "Train loss (w/o reg) on all data: 0.43441778\n",
      "Test loss (w/o reg) on all data: 0.4523208\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0006403424\n",
      "Norm of the params: 1.8958035\n",
      "              Random: fixed  13 labels. Loss 0.45232. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5408472\n",
      "Train loss (w/o reg) on all data: 0.5407607\n",
      "Test loss (w/o reg) on all data: 0.37751156\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009383391\n",
      "Norm of the params: 1.3152769\n",
      "Flipped loss: 0.37751. Accuracy: 0.800\n",
      "### Flips: 30, rs: 3, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [104] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4985993\n",
      "Train loss (w/o reg) on all data: 0.49847034\n",
      "Test loss (w/o reg) on all data: 0.37483388\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.000775742\n",
      "Norm of the params: 1.6060237\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.37483. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4408134\n",
      "Train loss (w/o reg) on all data: 0.4405819\n",
      "Test loss (w/o reg) on all data: 0.3669453\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015011332\n",
      "Norm of the params: 2.151702\n",
      "                Loss: fixed   8 labels. Loss 0.36695. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [90] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5395236\n",
      "Train loss (w/o reg) on all data: 0.53943443\n",
      "Test loss (w/o reg) on all data: 0.3736288\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004031857\n",
      "Norm of the params: 1.3354264\n",
      "              Random: fixed   1 labels. Loss 0.37363. Accuracy 0.844.\n",
      "### Flips: 30, rs: 3, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [142] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46105295\n",
      "Train loss (w/o reg) on all data: 0.46087003\n",
      "Test loss (w/o reg) on all data: 0.38246045\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0017160263\n",
      "Norm of the params: 1.9127886\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.38246. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [222] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3855824\n",
      "Train loss (w/o reg) on all data: 0.3852001\n",
      "Test loss (w/o reg) on all data: 0.40233213\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0039477325\n",
      "Norm of the params: 2.765001\n",
      "                Loss: fixed  13 labels. Loss 0.40233. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52018625\n",
      "Train loss (w/o reg) on all data: 0.5200876\n",
      "Test loss (w/o reg) on all data: 0.38840657\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002118974\n",
      "Norm of the params: 1.4046451\n",
      "              Random: fixed   4 labels. Loss 0.38841. Accuracy 0.800.\n",
      "### Flips: 30, rs: 3, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42390025\n",
      "Train loss (w/o reg) on all data: 0.42365748\n",
      "Test loss (w/o reg) on all data: 0.38354173\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020775665\n",
      "Norm of the params: 2.2034624\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.38354. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36101392\n",
      "Train loss (w/o reg) on all data: 0.36059397\n",
      "Test loss (w/o reg) on all data: 0.42463452\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00081841357\n",
      "Norm of the params: 2.898116\n",
      "                Loss: fixed  17 labels. Loss 0.42463. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [165] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50918305\n",
      "Train loss (w/o reg) on all data: 0.50906724\n",
      "Test loss (w/o reg) on all data: 0.3836363\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004060633\n",
      "Norm of the params: 1.521737\n",
      "              Random: fixed   5 labels. Loss 0.38364. Accuracy 0.800.\n",
      "### Flips: 30, rs: 3, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41419178\n",
      "Train loss (w/o reg) on all data: 0.413948\n",
      "Test loss (w/o reg) on all data: 0.38340324\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0038723422\n",
      "Norm of the params: 2.2080758\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.38340. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34469712\n",
      "Train loss (w/o reg) on all data: 0.344287\n",
      "Test loss (w/o reg) on all data: 0.41805646\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0019373413\n",
      "Norm of the params: 2.8639102\n",
      "                Loss: fixed  21 labels. Loss 0.41806. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4966658\n",
      "Train loss (w/o reg) on all data: 0.4965336\n",
      "Test loss (w/o reg) on all data: 0.38073894\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00062149396\n",
      "Norm of the params: 1.6260012\n",
      "              Random: fixed   6 labels. Loss 0.38074. Accuracy 0.800.\n",
      "### Flips: 30, rs: 3, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4111\n",
      "Train loss (w/o reg) on all data: 0.4108536\n",
      "Test loss (w/o reg) on all data: 0.37196672\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00043269934\n",
      "Norm of the params: 2.2198758\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.37197. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3403537\n",
      "Train loss (w/o reg) on all data: 0.3399556\n",
      "Test loss (w/o reg) on all data: 0.4027779\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00097110955\n",
      "Norm of the params: 2.821671\n",
      "                Loss: fixed  24 labels. Loss 0.40278. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48494273\n",
      "Train loss (w/o reg) on all data: 0.48479277\n",
      "Test loss (w/o reg) on all data: 0.38550627\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022789978\n",
      "Norm of the params: 1.7317998\n",
      "              Random: fixed   7 labels. Loss 0.38551. Accuracy 0.800.\n",
      "### Flips: 30, rs: 3, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38676313\n",
      "Train loss (w/o reg) on all data: 0.38646397\n",
      "Test loss (w/o reg) on all data: 0.375282\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022713896\n",
      "Norm of the params: 2.4460323\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.37528. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [280] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34035423\n",
      "Train loss (w/o reg) on all data: 0.3399574\n",
      "Test loss (w/o reg) on all data: 0.40246585\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006255686\n",
      "Norm of the params: 2.817276\n",
      "                Loss: fixed  24 labels. Loss 0.40247. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48051962\n",
      "Train loss (w/o reg) on all data: 0.4803691\n",
      "Test loss (w/o reg) on all data: 0.36981934\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.021697395\n",
      "Norm of the params: 1.7350389\n",
      "              Random: fixed   9 labels. Loss 0.36982. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5404477\n",
      "Train loss (w/o reg) on all data: 0.54036146\n",
      "Test loss (w/o reg) on all data: 0.44908062\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005553738\n",
      "Norm of the params: 1.313358\n",
      "Flipped loss: 0.44908. Accuracy: 0.822\n",
      "### Flips: 30, rs: 4, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [143] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5157074\n",
      "Train loss (w/o reg) on all data: 0.51557034\n",
      "Test loss (w/o reg) on all data: 0.44157138\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0077553918\n",
      "Norm of the params: 1.6556077\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.44157. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [284] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45705974\n",
      "Train loss (w/o reg) on all data: 0.4568835\n",
      "Test loss (w/o reg) on all data: 0.39059168\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0024651694\n",
      "Norm of the params: 1.8775597\n",
      "                Loss: fixed   8 labels. Loss 0.39059. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [211] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5404495\n",
      "Train loss (w/o reg) on all data: 0.5403625\n",
      "Test loss (w/o reg) on all data: 0.4493394\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001596991\n",
      "Norm of the params: 1.3192756\n",
      "              Random: fixed   0 labels. Loss 0.44934. Accuracy 0.822.\n",
      "### Flips: 30, rs: 4, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47063538\n",
      "Train loss (w/o reg) on all data: 0.47045517\n",
      "Test loss (w/o reg) on all data: 0.41136208\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00080483267\n",
      "Norm of the params: 1.89845\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.41136. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [101] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39648703\n",
      "Train loss (w/o reg) on all data: 0.39619672\n",
      "Test loss (w/o reg) on all data: 0.38226148\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.006564119\n",
      "Norm of the params: 2.409632\n",
      "                Loss: fixed  14 labels. Loss 0.38226. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [282] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5365148\n",
      "Train loss (w/o reg) on all data: 0.5364277\n",
      "Test loss (w/o reg) on all data: 0.44665876\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0031765983\n",
      "Norm of the params: 1.32002\n",
      "              Random: fixed   1 labels. Loss 0.44666. Accuracy 0.822.\n",
      "### Flips: 30, rs: 4, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44356117\n",
      "Train loss (w/o reg) on all data: 0.4433419\n",
      "Test loss (w/o reg) on all data: 0.40773535\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018539702\n",
      "Norm of the params: 2.09403\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.40774. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [302] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.383036\n",
      "Train loss (w/o reg) on all data: 0.38273486\n",
      "Test loss (w/o reg) on all data: 0.3890158\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00044890455\n",
      "Norm of the params: 2.454072\n",
      "                Loss: fixed  16 labels. Loss 0.38902. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53651386\n",
      "Train loss (w/o reg) on all data: 0.5364272\n",
      "Test loss (w/o reg) on all data: 0.44693384\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013143587\n",
      "Norm of the params: 1.3166133\n",
      "              Random: fixed   1 labels. Loss 0.44693. Accuracy 0.822.\n",
      "### Flips: 30, rs: 4, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3930559\n",
      "Train loss (w/o reg) on all data: 0.39280277\n",
      "Test loss (w/o reg) on all data: 0.39733297\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00081564166\n",
      "Norm of the params: 2.249941\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.39733. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [115] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37199703\n",
      "Train loss (w/o reg) on all data: 0.3716968\n",
      "Test loss (w/o reg) on all data: 0.40652195\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00612516\n",
      "Norm of the params: 2.4504216\n",
      "                Loss: fixed  20 labels. Loss 0.40652. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [285] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5365124\n",
      "Train loss (w/o reg) on all data: 0.53642607\n",
      "Test loss (w/o reg) on all data: 0.44700825\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011949259\n",
      "Norm of the params: 1.3136268\n",
      "              Random: fixed   1 labels. Loss 0.44701. Accuracy 0.822.\n",
      "### Flips: 30, rs: 4, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [189] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37902102\n",
      "Train loss (w/o reg) on all data: 0.3787408\n",
      "Test loss (w/o reg) on all data: 0.39478505\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013418448\n",
      "Norm of the params: 2.367422\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.39479. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34689358\n",
      "Train loss (w/o reg) on all data: 0.34652588\n",
      "Test loss (w/o reg) on all data: 0.42239222\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006233827\n",
      "Norm of the params: 2.711826\n",
      "                Loss: fixed  24 labels. Loss 0.42239. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5215108\n",
      "Train loss (w/o reg) on all data: 0.52141786\n",
      "Test loss (w/o reg) on all data: 0.43814027\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0045457995\n",
      "Norm of the params: 1.3633224\n",
      "              Random: fixed   3 labels. Loss 0.43814. Accuracy 0.822.\n",
      "### Flips: 30, rs: 4, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [465] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3516758\n",
      "Train loss (w/o reg) on all data: 0.3513215\n",
      "Test loss (w/o reg) on all data: 0.40686828\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00063698617\n",
      "Norm of the params: 2.6620274\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.40687. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [168] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34454688\n",
      "Train loss (w/o reg) on all data: 0.34419417\n",
      "Test loss (w/o reg) on all data: 0.4223887\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008841136\n",
      "Norm of the params: 2.6560056\n",
      "                Loss: fixed  26 labels. Loss 0.42239. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [139] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5113462\n",
      "Train loss (w/o reg) on all data: 0.51124924\n",
      "Test loss (w/o reg) on all data: 0.4298176\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014188633\n",
      "Norm of the params: 1.3926691\n",
      "              Random: fixed   5 labels. Loss 0.42982. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5044184\n",
      "Train loss (w/o reg) on all data: 0.50434107\n",
      "Test loss (w/o reg) on all data: 0.45799989\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.003154304\n",
      "Norm of the params: 1.2435378\n",
      "Flipped loss: 0.45800. Accuracy: 0.778\n",
      "### Flips: 30, rs: 5, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45045084\n",
      "Train loss (w/o reg) on all data: 0.4502978\n",
      "Test loss (w/o reg) on all data: 0.4250273\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002500461\n",
      "Norm of the params: 1.7494175\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.42503. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [114] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39923465\n",
      "Train loss (w/o reg) on all data: 0.39901057\n",
      "Test loss (w/o reg) on all data: 0.45110613\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0022055386\n",
      "Norm of the params: 2.1170585\n",
      "                Loss: fixed   8 labels. Loss 0.45111. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49399874\n",
      "Train loss (w/o reg) on all data: 0.4939062\n",
      "Test loss (w/o reg) on all data: 0.4538283\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0031816112\n",
      "Norm of the params: 1.3604809\n",
      "              Random: fixed   2 labels. Loss 0.45383. Accuracy 0.778.\n",
      "### Flips: 30, rs: 5, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41839904\n",
      "Train loss (w/o reg) on all data: 0.41820592\n",
      "Test loss (w/o reg) on all data: 0.41343468\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0004449406\n",
      "Norm of the params: 1.9653301\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.41343. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35792384\n",
      "Train loss (w/o reg) on all data: 0.35762075\n",
      "Test loss (w/o reg) on all data: 0.49094197\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005008587\n",
      "Norm of the params: 2.4620163\n",
      "                Loss: fixed  12 labels. Loss 0.49094. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [373] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4838785\n",
      "Train loss (w/o reg) on all data: 0.48377395\n",
      "Test loss (w/o reg) on all data: 0.44796914\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.003319855\n",
      "Norm of the params: 1.4459395\n",
      "              Random: fixed   4 labels. Loss 0.44797. Accuracy 0.778.\n",
      "### Flips: 30, rs: 5, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [379] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37342334\n",
      "Train loss (w/o reg) on all data: 0.37318015\n",
      "Test loss (w/o reg) on all data: 0.42365047\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012791355\n",
      "Norm of the params: 2.2053893\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.42365. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33845723\n",
      "Train loss (w/o reg) on all data: 0.33808184\n",
      "Test loss (w/o reg) on all data: 0.48255104\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.000736285\n",
      "Norm of the params: 2.739987\n",
      "                Loss: fixed  15 labels. Loss 0.48255. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47737327\n",
      "Train loss (w/o reg) on all data: 0.47725606\n",
      "Test loss (w/o reg) on all data: 0.41997796\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0025861897\n",
      "Norm of the params: 1.5311664\n",
      "              Random: fixed   8 labels. Loss 0.41998. Accuracy 0.800.\n",
      "### Flips: 30, rs: 5, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [302] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3626767\n",
      "Train loss (w/o reg) on all data: 0.3624023\n",
      "Test loss (w/o reg) on all data: 0.44431955\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00094163505\n",
      "Norm of the params: 2.3427672\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.44432. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [137] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33139738\n",
      "Train loss (w/o reg) on all data: 0.33105144\n",
      "Test loss (w/o reg) on all data: 0.46608767\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016274821\n",
      "Norm of the params: 2.63041\n",
      "                Loss: fixed  19 labels. Loss 0.46609. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46999666\n",
      "Train loss (w/o reg) on all data: 0.46988067\n",
      "Test loss (w/o reg) on all data: 0.4053863\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00043486196\n",
      "Norm of the params: 1.5231454\n",
      "              Random: fixed  10 labels. Loss 0.40539. Accuracy 0.800.\n",
      "### Flips: 30, rs: 5, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3522386\n",
      "Train loss (w/o reg) on all data: 0.35192138\n",
      "Test loss (w/o reg) on all data: 0.42702034\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00047036173\n",
      "Norm of the params: 2.518758\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.42702. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [173] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.324781\n",
      "Train loss (w/o reg) on all data: 0.32436845\n",
      "Test loss (w/o reg) on all data: 0.4810795\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0006285222\n",
      "Norm of the params: 2.8724232\n",
      "                Loss: fixed  22 labels. Loss 0.48108. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [322] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45577446\n",
      "Train loss (w/o reg) on all data: 0.45564184\n",
      "Test loss (w/o reg) on all data: 0.39341497\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0030455235\n",
      "Norm of the params: 1.62867\n",
      "              Random: fixed  11 labels. Loss 0.39341. Accuracy 0.822.\n",
      "### Flips: 30, rs: 5, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33148324\n",
      "Train loss (w/o reg) on all data: 0.3311121\n",
      "Test loss (w/o reg) on all data: 0.46292886\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001628186\n",
      "Norm of the params: 2.7245228\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.46293. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [106] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32802767\n",
      "Train loss (w/o reg) on all data: 0.32763636\n",
      "Test loss (w/o reg) on all data: 0.45017594\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007531828\n",
      "Norm of the params: 2.7975585\n",
      "                Loss: fixed  25 labels. Loss 0.45018. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [343] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4623511\n",
      "Train loss (w/o reg) on all data: 0.46222687\n",
      "Test loss (w/o reg) on all data: 0.3840482\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00076814776\n",
      "Norm of the params: 1.5763776\n",
      "              Random: fixed  12 labels. Loss 0.38405. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [150] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5370055\n",
      "Train loss (w/o reg) on all data: 0.53689\n",
      "Test loss (w/o reg) on all data: 0.35674283\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00091396354\n",
      "Norm of the params: 1.5200121\n",
      "Flipped loss: 0.35674. Accuracy: 0.844\n",
      "### Flips: 30, rs: 6, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50771534\n",
      "Train loss (w/o reg) on all data: 0.5075631\n",
      "Test loss (w/o reg) on all data: 0.3367978\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0024577486\n",
      "Norm of the params: 1.7448345\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.33680. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46000046\n",
      "Train loss (w/o reg) on all data: 0.4598006\n",
      "Test loss (w/o reg) on all data: 0.34063753\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013344887\n",
      "Norm of the params: 1.9992069\n",
      "                Loss: fixed   7 labels. Loss 0.34064. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [211] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.531303\n",
      "Train loss (w/o reg) on all data: 0.5311894\n",
      "Test loss (w/o reg) on all data: 0.36266178\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0018211923\n",
      "Norm of the params: 1.5074441\n",
      "              Random: fixed   1 labels. Loss 0.36266. Accuracy 0.844.\n",
      "### Flips: 30, rs: 6, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.491041\n",
      "Train loss (w/o reg) on all data: 0.4908676\n",
      "Test loss (w/o reg) on all data: 0.3353672\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0009350896\n",
      "Norm of the params: 1.8621562\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.33537. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41002163\n",
      "Train loss (w/o reg) on all data: 0.4097608\n",
      "Test loss (w/o reg) on all data: 0.37914073\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0033863613\n",
      "Norm of the params: 2.283934\n",
      "                Loss: fixed  12 labels. Loss 0.37914. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51296437\n",
      "Train loss (w/o reg) on all data: 0.5128397\n",
      "Test loss (w/o reg) on all data: 0.36638272\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0041657425\n",
      "Norm of the params: 1.5792547\n",
      "              Random: fixed   3 labels. Loss 0.36638. Accuracy 0.822.\n",
      "### Flips: 30, rs: 6, checks: 30\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [144] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46668777\n",
      "Train loss (w/o reg) on all data: 0.4664676\n",
      "Test loss (w/o reg) on all data: 0.326947\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0032293128\n",
      "Norm of the params: 2.0985258\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.32695. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38968894\n",
      "Train loss (w/o reg) on all data: 0.38936824\n",
      "Test loss (w/o reg) on all data: 0.35712564\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001835204\n",
      "Norm of the params: 2.5325825\n",
      "                Loss: fixed  16 labels. Loss 0.35713. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [269] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51296604\n",
      "Train loss (w/o reg) on all data: 0.5128417\n",
      "Test loss (w/o reg) on all data: 0.36668104\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0043781283\n",
      "Norm of the params: 1.5769752\n",
      "              Random: fixed   3 labels. Loss 0.36668. Accuracy 0.822.\n",
      "### Flips: 30, rs: 6, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45176664\n",
      "Train loss (w/o reg) on all data: 0.45155224\n",
      "Test loss (w/o reg) on all data: 0.33985427\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0013182124\n",
      "Norm of the params: 2.0707905\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.33985. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37614378\n",
      "Train loss (w/o reg) on all data: 0.3758106\n",
      "Test loss (w/o reg) on all data: 0.3557518\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0042934003\n",
      "Norm of the params: 2.5814853\n",
      "                Loss: fixed  18 labels. Loss 0.35575. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5003683\n",
      "Train loss (w/o reg) on all data: 0.50025105\n",
      "Test loss (w/o reg) on all data: 0.3688249\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0030112723\n",
      "Norm of the params: 1.5314\n",
      "              Random: fixed   6 labels. Loss 0.36882. Accuracy 0.800.\n",
      "### Flips: 30, rs: 6, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45182285\n",
      "Train loss (w/o reg) on all data: 0.4516224\n",
      "Test loss (w/o reg) on all data: 0.34154904\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00519764\n",
      "Norm of the params: 2.0021925\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.34155. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [148] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3682073\n",
      "Train loss (w/o reg) on all data: 0.36786252\n",
      "Test loss (w/o reg) on all data: 0.37611964\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0019854729\n",
      "Norm of the params: 2.6259747\n",
      "                Loss: fixed  21 labels. Loss 0.37612. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [302] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48952046\n",
      "Train loss (w/o reg) on all data: 0.48939702\n",
      "Test loss (w/o reg) on all data: 0.36900008\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0023130118\n",
      "Norm of the params: 1.5712637\n",
      "              Random: fixed   8 labels. Loss 0.36900. Accuracy 0.800.\n",
      "### Flips: 30, rs: 6, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41630793\n",
      "Train loss (w/o reg) on all data: 0.4160595\n",
      "Test loss (w/o reg) on all data: 0.3423584\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011076363\n",
      "Norm of the params: 2.2290335\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.34236. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36314183\n",
      "Train loss (w/o reg) on all data: 0.3627957\n",
      "Test loss (w/o reg) on all data: 0.3802126\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00072664884\n",
      "Norm of the params: 2.631075\n",
      "                Loss: fixed  24 labels. Loss 0.38021. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47949642\n",
      "Train loss (w/o reg) on all data: 0.47934803\n",
      "Test loss (w/o reg) on all data: 0.36700282\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00095382956\n",
      "Norm of the params: 1.7227697\n",
      "              Random: fixed   9 labels. Loss 0.36700. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.527476\n",
      "Train loss (w/o reg) on all data: 0.5273839\n",
      "Test loss (w/o reg) on all data: 0.4887118\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009662326\n",
      "Norm of the params: 1.3572185\n",
      "Flipped loss: 0.48871. Accuracy: 0.800\n",
      "### Flips: 30, rs: 7, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [93] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50183797\n",
      "Train loss (w/o reg) on all data: 0.5017266\n",
      "Test loss (w/o reg) on all data: 0.46284828\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0036642964\n",
      "Norm of the params: 1.4924463\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.46285. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4413017\n",
      "Train loss (w/o reg) on all data: 0.4411308\n",
      "Test loss (w/o reg) on all data: 0.505156\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0033130797\n",
      "Norm of the params: 1.8489486\n",
      "                Loss: fixed   8 labels. Loss 0.50516. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5269519\n",
      "Train loss (w/o reg) on all data: 0.5268617\n",
      "Test loss (w/o reg) on all data: 0.47107357\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012224432\n",
      "Norm of the params: 1.3429931\n",
      "              Random: fixed   2 labels. Loss 0.47107. Accuracy 0.778.\n",
      "### Flips: 30, rs: 7, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [83] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47513902\n",
      "Train loss (w/o reg) on all data: 0.47501785\n",
      "Test loss (w/o reg) on all data: 0.44460866\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011716418\n",
      "Norm of the params: 1.5568058\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.44461. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [140] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4059681\n",
      "Train loss (w/o reg) on all data: 0.40578735\n",
      "Test loss (w/o reg) on all data: 0.5036206\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010006821\n",
      "Norm of the params: 1.9013915\n",
      "                Loss: fixed  12 labels. Loss 0.50362. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [286] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5145387\n",
      "Train loss (w/o reg) on all data: 0.5144459\n",
      "Test loss (w/o reg) on all data: 0.47585404\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007895781\n",
      "Norm of the params: 1.3625959\n",
      "              Random: fixed   4 labels. Loss 0.47585. Accuracy 0.800.\n",
      "### Flips: 30, rs: 7, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [172] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4586352\n",
      "Train loss (w/o reg) on all data: 0.45848414\n",
      "Test loss (w/o reg) on all data: 0.4293161\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022708417\n",
      "Norm of the params: 1.7382504\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.42932. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36201295\n",
      "Train loss (w/o reg) on all data: 0.36178392\n",
      "Test loss (w/o reg) on all data: 0.49500105\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0046581123\n",
      "Norm of the params: 2.1401813\n",
      "                Loss: fixed  17 labels. Loss 0.49500. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [306] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5217997\n",
      "Train loss (w/o reg) on all data: 0.52171785\n",
      "Test loss (w/o reg) on all data: 0.4678222\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011114955\n",
      "Norm of the params: 1.2793621\n",
      "              Random: fixed   6 labels. Loss 0.46782. Accuracy 0.822.\n",
      "### Flips: 30, rs: 7, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4394703\n",
      "Train loss (w/o reg) on all data: 0.43930438\n",
      "Test loss (w/o reg) on all data: 0.4264728\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0029412792\n",
      "Norm of the params: 1.8216711\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.42647. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34711605\n",
      "Train loss (w/o reg) on all data: 0.34684056\n",
      "Test loss (w/o reg) on all data: 0.47082302\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.005715638\n",
      "Norm of the params: 2.347301\n",
      "                Loss: fixed  19 labels. Loss 0.47082. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52180094\n",
      "Train loss (w/o reg) on all data: 0.52171904\n",
      "Test loss (w/o reg) on all data: 0.46789432\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0021787495\n",
      "Norm of the params: 1.2800086\n",
      "              Random: fixed   6 labels. Loss 0.46789. Accuracy 0.822.\n",
      "### Flips: 30, rs: 7, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [190] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4053078\n",
      "Train loss (w/o reg) on all data: 0.40510792\n",
      "Test loss (w/o reg) on all data: 0.44208655\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0014945717\n",
      "Norm of the params: 1.9993919\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.44209. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [189] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3379127\n",
      "Train loss (w/o reg) on all data: 0.3376116\n",
      "Test loss (w/o reg) on all data: 0.45728293\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00083654665\n",
      "Norm of the params: 2.4541268\n",
      "                Loss: fixed  21 labels. Loss 0.45728. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5192442\n",
      "Train loss (w/o reg) on all data: 0.5191608\n",
      "Test loss (w/o reg) on all data: 0.4621909\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00093494344\n",
      "Norm of the params: 1.2915624\n",
      "              Random: fixed   7 labels. Loss 0.46219. Accuracy 0.800.\n",
      "### Flips: 30, rs: 7, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [339] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3700303\n",
      "Train loss (w/o reg) on all data: 0.36977458\n",
      "Test loss (w/o reg) on all data: 0.4635473\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0033041735\n",
      "Norm of the params: 2.2615073\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.46355. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33409122\n",
      "Train loss (w/o reg) on all data: 0.3337524\n",
      "Test loss (w/o reg) on all data: 0.4636393\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00058002124\n",
      "Norm of the params: 2.6031759\n",
      "                Loss: fixed  22 labels. Loss 0.46364. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [308] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51818603\n",
      "Train loss (w/o reg) on all data: 0.5180957\n",
      "Test loss (w/o reg) on all data: 0.4618497\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00046625818\n",
      "Norm of the params: 1.3441247\n",
      "              Random: fixed   9 labels. Loss 0.46185. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52236676\n",
      "Train loss (w/o reg) on all data: 0.5222788\n",
      "Test loss (w/o reg) on all data: 0.38541743\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00058667193\n",
      "Norm of the params: 1.3266149\n",
      "Flipped loss: 0.38542. Accuracy: 0.822\n",
      "### Flips: 30, rs: 8, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [128] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4973012\n",
      "Train loss (w/o reg) on all data: 0.49718878\n",
      "Test loss (w/o reg) on all data: 0.37897965\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0033710576\n",
      "Norm of the params: 1.499487\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.37898. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.461498\n",
      "Train loss (w/o reg) on all data: 0.46134427\n",
      "Test loss (w/o reg) on all data: 0.36874092\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00041265917\n",
      "Norm of the params: 1.7534091\n",
      "                Loss: fixed   6 labels. Loss 0.36874. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5094238\n",
      "Train loss (w/o reg) on all data: 0.50931585\n",
      "Test loss (w/o reg) on all data: 0.37196493\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017843683\n",
      "Norm of the params: 1.4693537\n",
      "              Random: fixed   2 labels. Loss 0.37196. Accuracy 0.822.\n",
      "### Flips: 30, rs: 8, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [193] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4681188\n",
      "Train loss (w/o reg) on all data: 0.46797445\n",
      "Test loss (w/o reg) on all data: 0.3753878\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013859695\n",
      "Norm of the params: 1.6990006\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.37539. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40961063\n",
      "Train loss (w/o reg) on all data: 0.40937263\n",
      "Test loss (w/o reg) on all data: 0.40173548\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012570114\n",
      "Norm of the params: 2.1816905\n",
      "                Loss: fixed  12 labels. Loss 0.40174. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [169] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50362515\n",
      "Train loss (w/o reg) on all data: 0.5035167\n",
      "Test loss (w/o reg) on all data: 0.37986276\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010548132\n",
      "Norm of the params: 1.4730829\n",
      "              Random: fixed   3 labels. Loss 0.37986. Accuracy 0.800.\n",
      "### Flips: 30, rs: 8, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [165] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4360108\n",
      "Train loss (w/o reg) on all data: 0.43581742\n",
      "Test loss (w/o reg) on all data: 0.37195176\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015696165\n",
      "Norm of the params: 1.9666039\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.37195. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36394426\n",
      "Train loss (w/o reg) on all data: 0.3635844\n",
      "Test loss (w/o reg) on all data: 0.38620728\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00087539275\n",
      "Norm of the params: 2.6827347\n",
      "                Loss: fixed  18 labels. Loss 0.38621. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [278] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5036252\n",
      "Train loss (w/o reg) on all data: 0.503515\n",
      "Test loss (w/o reg) on all data: 0.37964004\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014202653\n",
      "Norm of the params: 1.4845412\n",
      "              Random: fixed   3 labels. Loss 0.37964. Accuracy 0.800.\n",
      "### Flips: 30, rs: 8, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [153] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43101856\n",
      "Train loss (w/o reg) on all data: 0.43082097\n",
      "Test loss (w/o reg) on all data: 0.37112498\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017124863\n",
      "Norm of the params: 1.9878368\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.37112. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [144] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3600646\n",
      "Train loss (w/o reg) on all data: 0.35971987\n",
      "Test loss (w/o reg) on all data: 0.3978118\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0066877045\n",
      "Norm of the params: 2.625732\n",
      "                Loss: fixed  20 labels. Loss 0.39781. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [198] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5035901\n",
      "Train loss (w/o reg) on all data: 0.5034846\n",
      "Test loss (w/o reg) on all data: 0.39221781\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0024331864\n",
      "Norm of the params: 1.4526998\n",
      "              Random: fixed   5 labels. Loss 0.39222. Accuracy 0.822.\n",
      "### Flips: 30, rs: 8, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4014423\n",
      "Train loss (w/o reg) on all data: 0.40123683\n",
      "Test loss (w/o reg) on all data: 0.40457183\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011716234\n",
      "Norm of the params: 2.0271356\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.40457. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35142207\n",
      "Train loss (w/o reg) on all data: 0.35103068\n",
      "Test loss (w/o reg) on all data: 0.39107895\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00067837443\n",
      "Norm of the params: 2.797802\n",
      "                Loss: fixed  24 labels. Loss 0.39108. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5028282\n",
      "Train loss (w/o reg) on all data: 0.50271964\n",
      "Test loss (w/o reg) on all data: 0.39830077\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0045613945\n",
      "Norm of the params: 1.4734378\n",
      "              Random: fixed   6 labels. Loss 0.39830. Accuracy 0.800.\n",
      "### Flips: 30, rs: 8, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36388466\n",
      "Train loss (w/o reg) on all data: 0.3635811\n",
      "Test loss (w/o reg) on all data: 0.39240032\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002096501\n",
      "Norm of the params: 2.4639726\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.39240. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35115898\n",
      "Train loss (w/o reg) on all data: 0.35074946\n",
      "Test loss (w/o reg) on all data: 0.3907307\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0052833823\n",
      "Norm of the params: 2.8618257\n",
      "                Loss: fixed  25 labels. Loss 0.39073. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [335] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5016532\n",
      "Train loss (w/o reg) on all data: 0.50154644\n",
      "Test loss (w/o reg) on all data: 0.3933805\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010379292\n",
      "Norm of the params: 1.4613317\n",
      "              Random: fixed   8 labels. Loss 0.39338. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [286] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5319808\n",
      "Train loss (w/o reg) on all data: 0.5318396\n",
      "Test loss (w/o reg) on all data: 0.3950713\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0024084372\n",
      "Norm of the params: 1.6803458\n",
      "Flipped loss: 0.39507. Accuracy: 0.800\n",
      "### Flips: 30, rs: 9, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [152] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49133906\n",
      "Train loss (w/o reg) on all data: 0.49117106\n",
      "Test loss (w/o reg) on all data: 0.33780447\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.009374277\n",
      "Norm of the params: 1.8330442\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.33780. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43765315\n",
      "Train loss (w/o reg) on all data: 0.4373854\n",
      "Test loss (w/o reg) on all data: 0.36437464\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0031077755\n",
      "Norm of the params: 2.3140042\n",
      "                Loss: fixed   8 labels. Loss 0.36437. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5187525\n",
      "Train loss (w/o reg) on all data: 0.5185854\n",
      "Test loss (w/o reg) on all data: 0.40229362\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0018810102\n",
      "Norm of the params: 1.828297\n",
      "              Random: fixed   2 labels. Loss 0.40229. Accuracy 0.756.\n",
      "### Flips: 30, rs: 9, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [159] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45333636\n",
      "Train loss (w/o reg) on all data: 0.45315194\n",
      "Test loss (w/o reg) on all data: 0.3439685\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0005880965\n",
      "Norm of the params: 1.9205145\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.34397. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40973544\n",
      "Train loss (w/o reg) on all data: 0.40944797\n",
      "Test loss (w/o reg) on all data: 0.3495032\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0051750573\n",
      "Norm of the params: 2.3978622\n",
      "                Loss: fixed  11 labels. Loss 0.34950. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5149317\n",
      "Train loss (w/o reg) on all data: 0.5147731\n",
      "Test loss (w/o reg) on all data: 0.41078034\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0036955127\n",
      "Norm of the params: 1.7809569\n",
      "              Random: fixed   3 labels. Loss 0.41078. Accuracy 0.756.\n",
      "### Flips: 30, rs: 9, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [289] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41519156\n",
      "Train loss (w/o reg) on all data: 0.41493705\n",
      "Test loss (w/o reg) on all data: 0.35138738\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020513963\n",
      "Norm of the params: 2.2562191\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.35139. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3826121\n",
      "Train loss (w/o reg) on all data: 0.38230097\n",
      "Test loss (w/o reg) on all data: 0.36890408\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006470741\n",
      "Norm of the params: 2.4945142\n",
      "                Loss: fixed  15 labels. Loss 0.36890. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5149268\n",
      "Train loss (w/o reg) on all data: 0.51476806\n",
      "Test loss (w/o reg) on all data: 0.40983242\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0010193402\n",
      "Norm of the params: 1.7818724\n",
      "              Random: fixed   3 labels. Loss 0.40983. Accuracy 0.756.\n",
      "### Flips: 30, rs: 9, checks: 40\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [390] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39294603\n",
      "Train loss (w/o reg) on all data: 0.3926287\n",
      "Test loss (w/o reg) on all data: 0.3632913\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010636846\n",
      "Norm of the params: 2.519307\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.36329. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.365208\n",
      "Train loss (w/o reg) on all data: 0.36488572\n",
      "Test loss (w/o reg) on all data: 0.37445247\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0007560261\n",
      "Norm of the params: 2.5387723\n",
      "                Loss: fixed  18 labels. Loss 0.37445. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [330] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5169331\n",
      "Train loss (w/o reg) on all data: 0.51677114\n",
      "Test loss (w/o reg) on all data: 0.4147383\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0013486902\n",
      "Norm of the params: 1.7997766\n",
      "              Random: fixed   4 labels. Loss 0.41474. Accuracy 0.778.\n",
      "### Flips: 30, rs: 9, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [325] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38617375\n",
      "Train loss (w/o reg) on all data: 0.38587573\n",
      "Test loss (w/o reg) on all data: 0.36558974\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0018848577\n",
      "Norm of the params: 2.441397\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.36559. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [291] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3548826\n",
      "Train loss (w/o reg) on all data: 0.35453868\n",
      "Test loss (w/o reg) on all data: 0.37771234\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.006848084\n",
      "Norm of the params: 2.6227057\n",
      "                Loss: fixed  22 labels. Loss 0.37771. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49557042\n",
      "Train loss (w/o reg) on all data: 0.4954056\n",
      "Test loss (w/o reg) on all data: 0.39923164\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0011141425\n",
      "Norm of the params: 1.8155855\n",
      "              Random: fixed   7 labels. Loss 0.39923. Accuracy 0.756.\n",
      "### Flips: 30, rs: 9, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [222] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3861785\n",
      "Train loss (w/o reg) on all data: 0.385878\n",
      "Test loss (w/o reg) on all data: 0.36486226\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015051959\n",
      "Norm of the params: 2.451502\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.36486. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [171] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35259336\n",
      "Train loss (w/o reg) on all data: 0.3522391\n",
      "Test loss (w/o reg) on all data: 0.38238266\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0060645775\n",
      "Norm of the params: 2.6617846\n",
      "                Loss: fixed  24 labels. Loss 0.38238. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [288] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49090275\n",
      "Train loss (w/o reg) on all data: 0.49074\n",
      "Test loss (w/o reg) on all data: 0.40780342\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011491886\n",
      "Norm of the params: 1.8041892\n",
      "              Random: fixed   8 labels. Loss 0.40780. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [285] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55484015\n",
      "Train loss (w/o reg) on all data: 0.5547727\n",
      "Test loss (w/o reg) on all data: 0.44607025\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0031388612\n",
      "Norm of the params: 1.1614985\n",
      "Flipped loss: 0.44607. Accuracy: 0.800\n",
      "### Flips: 30, rs: 10, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50080395\n",
      "Train loss (w/o reg) on all data: 0.50072014\n",
      "Test loss (w/o reg) on all data: 0.42456144\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00091774616\n",
      "Norm of the params: 1.2948031\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.42456. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [117] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45953164\n",
      "Train loss (w/o reg) on all data: 0.45940557\n",
      "Test loss (w/o reg) on all data: 0.41150916\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007217464\n",
      "Norm of the params: 1.5879166\n",
      "                Loss: fixed   8 labels. Loss 0.41151. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.556144\n",
      "Train loss (w/o reg) on all data: 0.55606925\n",
      "Test loss (w/o reg) on all data: 0.45472446\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0012799785\n",
      "Norm of the params: 1.2226562\n",
      "              Random: fixed   1 labels. Loss 0.45472. Accuracy 0.756.\n",
      "### Flips: 30, rs: 10, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [198] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4584328\n",
      "Train loss (w/o reg) on all data: 0.45829144\n",
      "Test loss (w/o reg) on all data: 0.4208946\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0020133047\n",
      "Norm of the params: 1.6814616\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.42089. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [258] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39622566\n",
      "Train loss (w/o reg) on all data: 0.39597535\n",
      "Test loss (w/o reg) on all data: 0.42191508\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00076592923\n",
      "Norm of the params: 2.2374823\n",
      "                Loss: fixed  14 labels. Loss 0.42192. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [320] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55186445\n",
      "Train loss (w/o reg) on all data: 0.5517846\n",
      "Test loss (w/o reg) on all data: 0.44600073\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0012163103\n",
      "Norm of the params: 1.2638888\n",
      "              Random: fixed   2 labels. Loss 0.44600. Accuracy 0.756.\n",
      "### Flips: 30, rs: 10, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [142] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43387198\n",
      "Train loss (w/o reg) on all data: 0.43371758\n",
      "Test loss (w/o reg) on all data: 0.41542053\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020851449\n",
      "Norm of the params: 1.7572641\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.41542. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [380] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36551085\n",
      "Train loss (w/o reg) on all data: 0.36517715\n",
      "Test loss (w/o reg) on all data: 0.42581746\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017638042\n",
      "Norm of the params: 2.5833771\n",
      "                Loss: fixed  17 labels. Loss 0.42582. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5463338\n",
      "Train loss (w/o reg) on all data: 0.54624844\n",
      "Test loss (w/o reg) on all data: 0.43228078\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002717597\n",
      "Norm of the params: 1.3066086\n",
      "              Random: fixed   3 labels. Loss 0.43228. Accuracy 0.778.\n",
      "### Flips: 30, rs: 10, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [319] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38110706\n",
      "Train loss (w/o reg) on all data: 0.38081652\n",
      "Test loss (w/o reg) on all data: 0.40987203\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015700615\n",
      "Norm of the params: 2.41063\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.40987. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss (w reg) on all data: 0.34675786\n",
      "Train loss (w/o reg) on all data: 0.34639624\n",
      "Test loss (w/o reg) on all data: 0.40861422\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00039546163\n",
      "Norm of the params: 2.6892989\n",
      "                Loss: fixed  21 labels. Loss 0.40861. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [406] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53637064\n",
      "Train loss (w/o reg) on all data: 0.5362673\n",
      "Test loss (w/o reg) on all data: 0.42022505\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.008042086\n",
      "Norm of the params: 1.437782\n",
      "              Random: fixed   5 labels. Loss 0.42023. Accuracy 0.778.\n",
      "### Flips: 30, rs: 10, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [258] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38110408\n",
      "Train loss (w/o reg) on all data: 0.38081342\n",
      "Test loss (w/o reg) on all data: 0.40986905\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00087879575\n",
      "Norm of the params: 2.4110208\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.40987. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [285] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33987096\n",
      "Train loss (w/o reg) on all data: 0.33945805\n",
      "Test loss (w/o reg) on all data: 0.42144823\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008283371\n",
      "Norm of the params: 2.8737442\n",
      "                Loss: fixed  22 labels. Loss 0.42145. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5363716\n",
      "Train loss (w/o reg) on all data: 0.53626835\n",
      "Test loss (w/o reg) on all data: 0.42013824\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0025889655\n",
      "Norm of the params: 1.4368446\n",
      "              Random: fixed   5 labels. Loss 0.42014. Accuracy 0.778.\n",
      "### Flips: 30, rs: 10, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3656243\n",
      "Train loss (w/o reg) on all data: 0.36530668\n",
      "Test loss (w/o reg) on all data: 0.41794187\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013546267\n",
      "Norm of the params: 2.5204906\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.41794. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [122] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34038743\n",
      "Train loss (w/o reg) on all data: 0.3400201\n",
      "Test loss (w/o reg) on all data: 0.4237953\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.008928457\n",
      "Norm of the params: 2.7105088\n",
      "                Loss: fixed  23 labels. Loss 0.42380. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [211] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51510715\n",
      "Train loss (w/o reg) on all data: 0.51499003\n",
      "Test loss (w/o reg) on all data: 0.40836522\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010438311\n",
      "Norm of the params: 1.5306526\n",
      "              Random: fixed   7 labels. Loss 0.40837. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [222] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53180057\n",
      "Train loss (w/o reg) on all data: 0.5317028\n",
      "Test loss (w/o reg) on all data: 0.41791886\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011255905\n",
      "Norm of the params: 1.3980747\n",
      "Flipped loss: 0.41792. Accuracy: 0.822\n",
      "### Flips: 30, rs: 11, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [131] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4914305\n",
      "Train loss (w/o reg) on all data: 0.4912999\n",
      "Test loss (w/o reg) on all data: 0.39147416\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006074984\n",
      "Norm of the params: 1.6161915\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.39147. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45273975\n",
      "Train loss (w/o reg) on all data: 0.4525299\n",
      "Test loss (w/o reg) on all data: 0.3998052\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019685277\n",
      "Norm of the params: 2.0486102\n",
      "                Loss: fixed   7 labels. Loss 0.39981. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5318016\n",
      "Train loss (w/o reg) on all data: 0.53170407\n",
      "Test loss (w/o reg) on all data: 0.41770962\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008852279\n",
      "Norm of the params: 1.3965275\n",
      "              Random: fixed   0 labels. Loss 0.41771. Accuracy 0.822.\n",
      "### Flips: 30, rs: 11, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4522449\n",
      "Train loss (w/o reg) on all data: 0.45207244\n",
      "Test loss (w/o reg) on all data: 0.40165824\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002345869\n",
      "Norm of the params: 1.8572216\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.40166. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [187] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40138596\n",
      "Train loss (w/o reg) on all data: 0.40108556\n",
      "Test loss (w/o reg) on all data: 0.4066669\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00049801916\n",
      "Norm of the params: 2.4511967\n",
      "                Loss: fixed  13 labels. Loss 0.40667. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5274496\n",
      "Train loss (w/o reg) on all data: 0.52735394\n",
      "Test loss (w/o reg) on all data: 0.4079686\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004066302\n",
      "Norm of the params: 1.3832\n",
      "              Random: fixed   2 labels. Loss 0.40797. Accuracy 0.822.\n",
      "### Flips: 30, rs: 11, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [341] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43175483\n",
      "Train loss (w/o reg) on all data: 0.43152812\n",
      "Test loss (w/o reg) on all data: 0.42094228\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.011960076\n",
      "Norm of the params: 2.1293948\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.42094. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [158] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38207066\n",
      "Train loss (w/o reg) on all data: 0.38173583\n",
      "Test loss (w/o reg) on all data: 0.41194937\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0056839995\n",
      "Norm of the params: 2.5877688\n",
      "                Loss: fixed  16 labels. Loss 0.41195. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [177] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52341413\n",
      "Train loss (w/o reg) on all data: 0.523331\n",
      "Test loss (w/o reg) on all data: 0.40265787\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.010660794\n",
      "Norm of the params: 1.289384\n",
      "              Random: fixed   5 labels. Loss 0.40266. Accuracy 0.844.\n",
      "### Flips: 30, rs: 11, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41021052\n",
      "Train loss (w/o reg) on all data: 0.40995908\n",
      "Test loss (w/o reg) on all data: 0.3777563\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0025735397\n",
      "Norm of the params: 2.242572\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.37776. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3716472\n",
      "Train loss (w/o reg) on all data: 0.37129614\n",
      "Test loss (w/o reg) on all data: 0.42825887\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0008534478\n",
      "Norm of the params: 2.649838\n",
      "                Loss: fixed  18 labels. Loss 0.42826. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5207995\n",
      "Train loss (w/o reg) on all data: 0.5207181\n",
      "Test loss (w/o reg) on all data: 0.40790486\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00081205554\n",
      "Norm of the params: 1.2761806\n",
      "              Random: fixed   6 labels. Loss 0.40790. Accuracy 0.822.\n",
      "### Flips: 30, rs: 11, checks: 50\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [318] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3968345\n",
      "Train loss (w/o reg) on all data: 0.39657682\n",
      "Test loss (w/o reg) on all data: 0.40138897\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.003247806\n",
      "Norm of the params: 2.2701795\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.40139. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35940772\n",
      "Train loss (w/o reg) on all data: 0.35902792\n",
      "Test loss (w/o reg) on all data: 0.4358455\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0026446597\n",
      "Norm of the params: 2.7561405\n",
      "                Loss: fixed  23 labels. Loss 0.43585. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51429826\n",
      "Train loss (w/o reg) on all data: 0.51421136\n",
      "Test loss (w/o reg) on all data: 0.3964913\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0012221631\n",
      "Norm of the params: 1.3181384\n",
      "              Random: fixed   7 labels. Loss 0.39649. Accuracy 0.844.\n",
      "### Flips: 30, rs: 11, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [324] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37927172\n",
      "Train loss (w/o reg) on all data: 0.37897053\n",
      "Test loss (w/o reg) on all data: 0.4123429\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0012710952\n",
      "Norm of the params: 2.454298\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.41234. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [152] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35339853\n",
      "Train loss (w/o reg) on all data: 0.3530138\n",
      "Test loss (w/o reg) on all data: 0.43107936\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0031500137\n",
      "Norm of the params: 2.7738955\n",
      "                Loss: fixed  26 labels. Loss 0.43108. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49929765\n",
      "Train loss (w/o reg) on all data: 0.49920243\n",
      "Test loss (w/o reg) on all data: 0.39772838\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019185689\n",
      "Norm of the params: 1.3800653\n",
      "              Random: fixed   9 labels. Loss 0.39773. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [101] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54398173\n",
      "Train loss (w/o reg) on all data: 0.5438696\n",
      "Test loss (w/o reg) on all data: 0.3956096\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0032539214\n",
      "Norm of the params: 1.4972547\n",
      "Flipped loss: 0.39561. Accuracy: 0.822\n",
      "### Flips: 30, rs: 12, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [307] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48083088\n",
      "Train loss (w/o reg) on all data: 0.48059937\n",
      "Test loss (w/o reg) on all data: 0.38931236\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00180411\n",
      "Norm of the params: 2.1518133\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.38931. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [136] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46092147\n",
      "Train loss (w/o reg) on all data: 0.46063647\n",
      "Test loss (w/o reg) on all data: 0.40519214\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006024221\n",
      "Norm of the params: 2.3874547\n",
      "                Loss: fixed   8 labels. Loss 0.40519. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53272927\n",
      "Train loss (w/o reg) on all data: 0.5326147\n",
      "Test loss (w/o reg) on all data: 0.40926328\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016062931\n",
      "Norm of the params: 1.5138472\n",
      "              Random: fixed   2 labels. Loss 0.40926. Accuracy 0.822.\n",
      "### Flips: 30, rs: 12, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4598183\n",
      "Train loss (w/o reg) on all data: 0.45958033\n",
      "Test loss (w/o reg) on all data: 0.39292446\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00058598194\n",
      "Norm of the params: 2.1815763\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.39292. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [325] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40705222\n",
      "Train loss (w/o reg) on all data: 0.40670475\n",
      "Test loss (w/o reg) on all data: 0.36624295\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00047706268\n",
      "Norm of the params: 2.6361556\n",
      "                Loss: fixed  14 labels. Loss 0.36624. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5327292\n",
      "Train loss (w/o reg) on all data: 0.5326147\n",
      "Test loss (w/o reg) on all data: 0.4091557\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004012585\n",
      "Norm of the params: 1.5132846\n",
      "              Random: fixed   2 labels. Loss 0.40916. Accuracy 0.822.\n",
      "### Flips: 30, rs: 12, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [278] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42991588\n",
      "Train loss (w/o reg) on all data: 0.4296643\n",
      "Test loss (w/o reg) on all data: 0.3726875\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0048800204\n",
      "Norm of the params: 2.2430532\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.37269. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3747155\n",
      "Train loss (w/o reg) on all data: 0.37424067\n",
      "Test loss (w/o reg) on all data: 0.39320967\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014435158\n",
      "Norm of the params: 3.0817327\n",
      "                Loss: fixed  18 labels. Loss 0.39321. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [322] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53682506\n",
      "Train loss (w/o reg) on all data: 0.53671616\n",
      "Test loss (w/o reg) on all data: 0.4131062\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017612488\n",
      "Norm of the params: 1.4756908\n",
      "              Random: fixed   3 labels. Loss 0.41311. Accuracy 0.822.\n",
      "### Flips: 30, rs: 12, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41405284\n",
      "Train loss (w/o reg) on all data: 0.41377836\n",
      "Test loss (w/o reg) on all data: 0.37594545\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0031569817\n",
      "Norm of the params: 2.3429952\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.37595. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [211] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3644562\n",
      "Train loss (w/o reg) on all data: 0.36399934\n",
      "Test loss (w/o reg) on all data: 0.39870808\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014944124\n",
      "Norm of the params: 3.02283\n",
      "                Loss: fixed  20 labels. Loss 0.39871. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53366107\n",
      "Train loss (w/o reg) on all data: 0.53354686\n",
      "Test loss (w/o reg) on all data: 0.4044476\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0030321747\n",
      "Norm of the params: 1.5112602\n",
      "              Random: fixed   5 labels. Loss 0.40445. Accuracy 0.800.\n",
      "### Flips: 30, rs: 12, checks: 50\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39254013\n",
      "Train loss (w/o reg) on all data: 0.39222097\n",
      "Test loss (w/o reg) on all data: 0.36860058\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00078422186\n",
      "Norm of the params: 2.5264301\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.36860. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [179] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34609106\n",
      "Train loss (w/o reg) on all data: 0.34559208\n",
      "Test loss (w/o reg) on all data: 0.4396218\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0038449173\n",
      "Norm of the params: 3.1590633\n",
      "                Loss: fixed  23 labels. Loss 0.43962. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [393] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5169584\n",
      "Train loss (w/o reg) on all data: 0.51679397\n",
      "Test loss (w/o reg) on all data: 0.40063608\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011672415\n",
      "Norm of the params: 1.8135179\n",
      "              Random: fixed   8 labels. Loss 0.40064. Accuracy 0.800.\n",
      "### Flips: 30, rs: 12, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [286] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3815271\n",
      "Train loss (w/o reg) on all data: 0.38118532\n",
      "Test loss (w/o reg) on all data: 0.3690616\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014499613\n",
      "Norm of the params: 2.6144798\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.36906. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [129] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34299678\n",
      "Train loss (w/o reg) on all data: 0.34258074\n",
      "Test loss (w/o reg) on all data: 0.4362395\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00086982624\n",
      "Norm of the params: 2.8846204\n",
      "                Loss: fixed  26 labels. Loss 0.43624. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [332] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5033691\n",
      "Train loss (w/o reg) on all data: 0.50320995\n",
      "Test loss (w/o reg) on all data: 0.39788926\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0029209787\n",
      "Norm of the params: 1.7841291\n",
      "              Random: fixed  10 labels. Loss 0.39789. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46354568\n",
      "Train loss (w/o reg) on all data: 0.46328673\n",
      "Test loss (w/o reg) on all data: 0.381411\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007725755\n",
      "Norm of the params: 2.2758076\n",
      "Flipped loss: 0.38141. Accuracy: 0.822\n",
      "### Flips: 30, rs: 13, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [117] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41180927\n",
      "Train loss (w/o reg) on all data: 0.41146126\n",
      "Test loss (w/o reg) on all data: 0.3764885\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0013453653\n",
      "Norm of the params: 2.638133\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.37649. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39373598\n",
      "Train loss (w/o reg) on all data: 0.39334625\n",
      "Test loss (w/o reg) on all data: 0.38579512\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003168443\n",
      "Norm of the params: 2.7918816\n",
      "                Loss: fixed   6 labels. Loss 0.38580. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46083236\n",
      "Train loss (w/o reg) on all data: 0.4605866\n",
      "Test loss (w/o reg) on all data: 0.38132244\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0018152082\n",
      "Norm of the params: 2.2169352\n",
      "              Random: fixed   1 labels. Loss 0.38132. Accuracy 0.844.\n",
      "### Flips: 30, rs: 13, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38916624\n",
      "Train loss (w/o reg) on all data: 0.3888272\n",
      "Test loss (w/o reg) on all data: 0.37699908\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0041783196\n",
      "Norm of the params: 2.6039634\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.37700. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34662393\n",
      "Train loss (w/o reg) on all data: 0.346138\n",
      "Test loss (w/o reg) on all data: 0.41942576\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006830485\n",
      "Norm of the params: 3.1174386\n",
      "                Loss: fixed  11 labels. Loss 0.41943. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [313] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46680546\n",
      "Train loss (w/o reg) on all data: 0.46656793\n",
      "Test loss (w/o reg) on all data: 0.38413975\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0066105057\n",
      "Norm of the params: 2.179542\n",
      "              Random: fixed   2 labels. Loss 0.38414. Accuracy 0.822.\n",
      "### Flips: 30, rs: 13, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37605616\n",
      "Train loss (w/o reg) on all data: 0.3757024\n",
      "Test loss (w/o reg) on all data: 0.35746714\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0016837537\n",
      "Norm of the params: 2.6599166\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.35747. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [309] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32596865\n",
      "Train loss (w/o reg) on all data: 0.32547674\n",
      "Test loss (w/o reg) on all data: 0.43251613\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010916558\n",
      "Norm of the params: 3.1366074\n",
      "                Loss: fixed  14 labels. Loss 0.43252. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [280] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44967392\n",
      "Train loss (w/o reg) on all data: 0.4494259\n",
      "Test loss (w/o reg) on all data: 0.38625404\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018814146\n",
      "Norm of the params: 2.2271395\n",
      "              Random: fixed   5 labels. Loss 0.38625. Accuracy 0.822.\n",
      "### Flips: 30, rs: 13, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3644033\n",
      "Train loss (w/o reg) on all data: 0.36402008\n",
      "Test loss (w/o reg) on all data: 0.38240132\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010089541\n",
      "Norm of the params: 2.7684977\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.38240. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [301] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3050151\n",
      "Train loss (w/o reg) on all data: 0.3044715\n",
      "Test loss (w/o reg) on all data: 0.44179583\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019594443\n",
      "Norm of the params: 3.29727\n",
      "                Loss: fixed  19 labels. Loss 0.44180. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [338] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44247052\n",
      "Train loss (w/o reg) on all data: 0.442226\n",
      "Test loss (w/o reg) on all data: 0.39045385\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00085928704\n",
      "Norm of the params: 2.21142\n",
      "              Random: fixed   6 labels. Loss 0.39045. Accuracy 0.822.\n",
      "### Flips: 30, rs: 13, checks: 50\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34615138\n",
      "Train loss (w/o reg) on all data: 0.34572205\n",
      "Test loss (w/o reg) on all data: 0.39460602\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0021991888\n",
      "Norm of the params: 2.9303422\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.39461. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.29311213\n",
      "Train loss (w/o reg) on all data: 0.29250824\n",
      "Test loss (w/o reg) on all data: 0.4523495\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0029455626\n",
      "Norm of the params: 3.475259\n",
      "                Loss: fixed  21 labels. Loss 0.45235. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44247177\n",
      "Train loss (w/o reg) on all data: 0.4422261\n",
      "Test loss (w/o reg) on all data: 0.38995785\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0033222483\n",
      "Norm of the params: 2.2165585\n",
      "              Random: fixed   6 labels. Loss 0.38996. Accuracy 0.822.\n",
      "### Flips: 30, rs: 13, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33187452\n",
      "Train loss (w/o reg) on all data: 0.33139506\n",
      "Test loss (w/o reg) on all data: 0.3971135\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00067266345\n",
      "Norm of the params: 3.0966022\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.39711. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.29311478\n",
      "Train loss (w/o reg) on all data: 0.29251257\n",
      "Test loss (w/o reg) on all data: 0.4522637\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015496769\n",
      "Norm of the params: 3.4704561\n",
      "                Loss: fixed  21 labels. Loss 0.45226. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [371] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43985668\n",
      "Train loss (w/o reg) on all data: 0.4396194\n",
      "Test loss (w/o reg) on all data: 0.37716663\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0037910722\n",
      "Norm of the params: 2.1784108\n",
      "              Random: fixed   8 labels. Loss 0.37717. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [264] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55019647\n",
      "Train loss (w/o reg) on all data: 0.5501285\n",
      "Test loss (w/o reg) on all data: 0.35782802\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0040391614\n",
      "Norm of the params: 1.1658437\n",
      "Flipped loss: 0.35783. Accuracy: 0.867\n",
      "### Flips: 30, rs: 14, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [264] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5164152\n",
      "Train loss (w/o reg) on all data: 0.51630044\n",
      "Test loss (w/o reg) on all data: 0.32738894\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0032458515\n",
      "Norm of the params: 1.5149894\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.32739. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46073222\n",
      "Train loss (w/o reg) on all data: 0.4605679\n",
      "Test loss (w/o reg) on all data: 0.32899576\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0006893858\n",
      "Norm of the params: 1.8129365\n",
      "                Loss: fixed   8 labels. Loss 0.32900. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5492081\n",
      "Train loss (w/o reg) on all data: 0.54913896\n",
      "Test loss (w/o reg) on all data: 0.3619421\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010553476\n",
      "Norm of the params: 1.1761291\n",
      "              Random: fixed   1 labels. Loss 0.36194. Accuracy 0.844.\n",
      "### Flips: 30, rs: 14, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47973692\n",
      "Train loss (w/o reg) on all data: 0.4795808\n",
      "Test loss (w/o reg) on all data: 0.30907926\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0013842726\n",
      "Norm of the params: 1.767123\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.30908. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [190] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39928204\n",
      "Train loss (w/o reg) on all data: 0.3989684\n",
      "Test loss (w/o reg) on all data: 0.33222795\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004444696\n",
      "Norm of the params: 2.504546\n",
      "                Loss: fixed  14 labels. Loss 0.33223. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5378719\n",
      "Train loss (w/o reg) on all data: 0.5377987\n",
      "Test loss (w/o reg) on all data: 0.36061525\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0024530992\n",
      "Norm of the params: 1.2101028\n",
      "              Random: fixed   4 labels. Loss 0.36062. Accuracy 0.867.\n",
      "### Flips: 30, rs: 14, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44538042\n",
      "Train loss (w/o reg) on all data: 0.4451651\n",
      "Test loss (w/o reg) on all data: 0.31005797\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.006642107\n",
      "Norm of the params: 2.075199\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.31006. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36381564\n",
      "Train loss (w/o reg) on all data: 0.363464\n",
      "Test loss (w/o reg) on all data: 0.369851\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00082655007\n",
      "Norm of the params: 2.651938\n",
      "                Loss: fixed  20 labels. Loss 0.36985. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [349] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5272818\n",
      "Train loss (w/o reg) on all data: 0.52719843\n",
      "Test loss (w/o reg) on all data: 0.35310677\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0032238693\n",
      "Norm of the params: 1.2915809\n",
      "              Random: fixed   5 labels. Loss 0.35311. Accuracy 0.867.\n",
      "### Flips: 30, rs: 14, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44221812\n",
      "Train loss (w/o reg) on all data: 0.44202152\n",
      "Test loss (w/o reg) on all data: 0.31103387\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0019031775\n",
      "Norm of the params: 1.9829801\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.31103. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34105182\n",
      "Train loss (w/o reg) on all data: 0.34061885\n",
      "Test loss (w/o reg) on all data: 0.39863357\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0030544668\n",
      "Norm of the params: 2.9426916\n",
      "                Loss: fixed  23 labels. Loss 0.39863. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [314] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.510274\n",
      "Train loss (w/o reg) on all data: 0.5101794\n",
      "Test loss (w/o reg) on all data: 0.35250437\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0014573579\n",
      "Norm of the params: 1.375264\n",
      "              Random: fixed   7 labels. Loss 0.35250. Accuracy 0.844.\n",
      "### Flips: 30, rs: 14, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40099403\n",
      "Train loss (w/o reg) on all data: 0.40067747\n",
      "Test loss (w/o reg) on all data: 0.33488828\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00053927844\n",
      "Norm of the params: 2.516181\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.33489. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.339686\n",
      "Train loss (w/o reg) on all data: 0.33930928\n",
      "Test loss (w/o reg) on all data: 0.3887758\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0022241462\n",
      "Norm of the params: 2.744937\n",
      "                Loss: fixed  26 labels. Loss 0.38878. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [316] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49940336\n",
      "Train loss (w/o reg) on all data: 0.4992986\n",
      "Test loss (w/o reg) on all data: 0.34780163\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0014258545\n",
      "Norm of the params: 1.447544\n",
      "              Random: fixed   8 labels. Loss 0.34780. Accuracy 0.867.\n",
      "### Flips: 30, rs: 14, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [288] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38485906\n",
      "Train loss (w/o reg) on all data: 0.384514\n",
      "Test loss (w/o reg) on all data: 0.34233677\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00115299\n",
      "Norm of the params: 2.626974\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.34234. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [264] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33968484\n",
      "Train loss (w/o reg) on all data: 0.33930725\n",
      "Test loss (w/o reg) on all data: 0.38920975\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00026557365\n",
      "Norm of the params: 2.7480364\n",
      "                Loss: fixed  26 labels. Loss 0.38921. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49553156\n",
      "Train loss (w/o reg) on all data: 0.49542797\n",
      "Test loss (w/o reg) on all data: 0.35304397\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00561963\n",
      "Norm of the params: 1.4394656\n",
      "              Random: fixed   9 labels. Loss 0.35304. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [340] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5479077\n",
      "Train loss (w/o reg) on all data: 0.54778814\n",
      "Test loss (w/o reg) on all data: 0.44195658\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010152393\n",
      "Norm of the params: 1.5464426\n",
      "Flipped loss: 0.44196. Accuracy: 0.800\n",
      "### Flips: 30, rs: 15, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49546093\n",
      "Train loss (w/o reg) on all data: 0.4952625\n",
      "Test loss (w/o reg) on all data: 0.42097312\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007644444\n",
      "Norm of the params: 1.9920559\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.42097. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [161] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44057247\n",
      "Train loss (w/o reg) on all data: 0.4403095\n",
      "Test loss (w/o reg) on all data: 0.4212455\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00083241117\n",
      "Norm of the params: 2.2934115\n",
      "                Loss: fixed   9 labels. Loss 0.42125. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [266] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5411366\n",
      "Train loss (w/o reg) on all data: 0.5410174\n",
      "Test loss (w/o reg) on all data: 0.4324971\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.005402679\n",
      "Norm of the params: 1.5439163\n",
      "              Random: fixed   1 labels. Loss 0.43250. Accuracy 0.822.\n",
      "### Flips: 30, rs: 15, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4728249\n",
      "Train loss (w/o reg) on all data: 0.47260717\n",
      "Test loss (w/o reg) on all data: 0.42631835\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010107824\n",
      "Norm of the params: 2.0868258\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.42632. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4241972\n",
      "Train loss (w/o reg) on all data: 0.4239152\n",
      "Test loss (w/o reg) on all data: 0.41759297\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003579022\n",
      "Norm of the params: 2.3747768\n",
      "                Loss: fixed  11 labels. Loss 0.41759. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [136] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53661364\n",
      "Train loss (w/o reg) on all data: 0.53648543\n",
      "Test loss (w/o reg) on all data: 0.43580818\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00056497584\n",
      "Norm of the params: 1.601245\n",
      "              Random: fixed   2 labels. Loss 0.43581. Accuracy 0.822.\n",
      "### Flips: 30, rs: 15, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44372368\n",
      "Train loss (w/o reg) on all data: 0.44342285\n",
      "Test loss (w/o reg) on all data: 0.43638548\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.006922988\n",
      "Norm of the params: 2.4527977\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.43639. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39180157\n",
      "Train loss (w/o reg) on all data: 0.39143023\n",
      "Test loss (w/o reg) on all data: 0.43585515\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009846478\n",
      "Norm of the params: 2.7252557\n",
      "                Loss: fixed  16 labels. Loss 0.43586. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5245398\n",
      "Train loss (w/o reg) on all data: 0.52441525\n",
      "Test loss (w/o reg) on all data: 0.43118304\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009228527\n",
      "Norm of the params: 1.5783479\n",
      "              Random: fixed   5 labels. Loss 0.43118. Accuracy 0.822.\n",
      "### Flips: 30, rs: 15, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41037104\n",
      "Train loss (w/o reg) on all data: 0.41001812\n",
      "Test loss (w/o reg) on all data: 0.42522782\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0045411447\n",
      "Norm of the params: 2.6568017\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.42523. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37019998\n",
      "Train loss (w/o reg) on all data: 0.3697047\n",
      "Test loss (w/o reg) on all data: 0.46492314\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00040704283\n",
      "Norm of the params: 3.1472938\n",
      "                Loss: fixed  21 labels. Loss 0.46492. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [328] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5245415\n",
      "Train loss (w/o reg) on all data: 0.5244165\n",
      "Test loss (w/o reg) on all data: 0.43128377\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0055768564\n",
      "Norm of the params: 1.5809671\n",
      "              Random: fixed   5 labels. Loss 0.43128. Accuracy 0.822.\n",
      "### Flips: 30, rs: 15, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [279] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3906629\n",
      "Train loss (w/o reg) on all data: 0.39027473\n",
      "Test loss (w/o reg) on all data: 0.42092794\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00059721636\n",
      "Norm of the params: 2.7862546\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.42093. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [330] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3589981\n",
      "Train loss (w/o reg) on all data: 0.35848635\n",
      "Test loss (w/o reg) on all data: 0.45247734\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018672657\n",
      "Norm of the params: 3.1992083\n",
      "                Loss: fixed  23 labels. Loss 0.45248. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [289] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51725185\n",
      "Train loss (w/o reg) on all data: 0.51712024\n",
      "Test loss (w/o reg) on all data: 0.42051324\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008873237\n",
      "Norm of the params: 1.6224674\n",
      "              Random: fixed   6 labels. Loss 0.42051. Accuracy 0.822.\n",
      "### Flips: 30, rs: 15, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38157356\n",
      "Train loss (w/o reg) on all data: 0.38114506\n",
      "Test loss (w/o reg) on all data: 0.41947246\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00039475752\n",
      "Norm of the params: 2.9274986\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.41947. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3610029\n",
      "Train loss (w/o reg) on all data: 0.36050957\n",
      "Test loss (w/o reg) on all data: 0.44961968\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018947525\n",
      "Norm of the params: 3.141036\n",
      "                Loss: fixed  25 labels. Loss 0.44962. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5172522\n",
      "Train loss (w/o reg) on all data: 0.51712006\n",
      "Test loss (w/o reg) on all data: 0.4213851\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004599534\n",
      "Norm of the params: 1.6256334\n",
      "              Random: fixed   6 labels. Loss 0.42139. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5488997\n",
      "Train loss (w/o reg) on all data: 0.5488599\n",
      "Test loss (w/o reg) on all data: 0.45852098\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020235388\n",
      "Norm of the params: 0.8924931\n",
      "Flipped loss: 0.45852. Accuracy: 0.800\n",
      "### Flips: 30, rs: 16, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.507239\n",
      "Train loss (w/o reg) on all data: 0.5071601\n",
      "Test loss (w/o reg) on all data: 0.42243233\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0070083416\n",
      "Norm of the params: 1.2558067\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.42243. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [148] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4599929\n",
      "Train loss (w/o reg) on all data: 0.45989725\n",
      "Test loss (w/o reg) on all data: 0.425639\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0019156873\n",
      "Norm of the params: 1.3830456\n",
      "                Loss: fixed   8 labels. Loss 0.42564. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [129] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.535263\n",
      "Train loss (w/o reg) on all data: 0.53521895\n",
      "Test loss (w/o reg) on all data: 0.4413094\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0025358316\n",
      "Norm of the params: 0.93853575\n",
      "              Random: fixed   2 labels. Loss 0.44131. Accuracy 0.800.\n",
      "### Flips: 30, rs: 16, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [309] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46405834\n",
      "Train loss (w/o reg) on all data: 0.46394396\n",
      "Test loss (w/o reg) on all data: 0.41828147\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005373026\n",
      "Norm of the params: 1.5124226\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.41828. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41494182\n",
      "Train loss (w/o reg) on all data: 0.41480002\n",
      "Test loss (w/o reg) on all data: 0.45561633\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0018950326\n",
      "Norm of the params: 1.6839889\n",
      "                Loss: fixed  13 labels. Loss 0.45562. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [293] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52839726\n",
      "Train loss (w/o reg) on all data: 0.5283499\n",
      "Test loss (w/o reg) on all data: 0.4495646\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00424334\n",
      "Norm of the params: 0.9734482\n",
      "              Random: fixed   3 labels. Loss 0.44956. Accuracy 0.800.\n",
      "### Flips: 30, rs: 16, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [323] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43338966\n",
      "Train loss (w/o reg) on all data: 0.43324664\n",
      "Test loss (w/o reg) on all data: 0.40152195\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022338848\n",
      "Norm of the params: 1.6912948\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.40152. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [283] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38332415\n",
      "Train loss (w/o reg) on all data: 0.38314185\n",
      "Test loss (w/o reg) on all data: 0.437554\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009227072\n",
      "Norm of the params: 1.9093804\n",
      "                Loss: fixed  17 labels. Loss 0.43755. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [300] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5283991\n",
      "Train loss (w/o reg) on all data: 0.5283518\n",
      "Test loss (w/o reg) on all data: 0.44928795\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0028454643\n",
      "Norm of the params: 0.97276515\n",
      "              Random: fixed   3 labels. Loss 0.44929. Accuracy 0.800.\n",
      "### Flips: 30, rs: 16, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41149825\n",
      "Train loss (w/o reg) on all data: 0.4113102\n",
      "Test loss (w/o reg) on all data: 0.39896414\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012462772\n",
      "Norm of the params: 1.9392768\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.39896. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [142] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36367196\n",
      "Train loss (w/o reg) on all data: 0.36345083\n",
      "Test loss (w/o reg) on all data: 0.43393317\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00069632375\n",
      "Norm of the params: 2.10302\n",
      "                Loss: fixed  20 labels. Loss 0.43393. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [300] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51537275\n",
      "Train loss (w/o reg) on all data: 0.5153171\n",
      "Test loss (w/o reg) on all data: 0.42864466\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00089560746\n",
      "Norm of the params: 1.0553519\n",
      "              Random: fixed   5 labels. Loss 0.42864. Accuracy 0.800.\n",
      "### Flips: 30, rs: 16, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [185] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40418875\n",
      "Train loss (w/o reg) on all data: 0.4039946\n",
      "Test loss (w/o reg) on all data: 0.4058896\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011160171\n",
      "Norm of the params: 1.9706365\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.40589. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34692\n",
      "Train loss (w/o reg) on all data: 0.34663755\n",
      "Test loss (w/o reg) on all data: 0.42803746\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005682519\n",
      "Norm of the params: 2.376832\n",
      "                Loss: fixed  23 labels. Loss 0.42804. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4875341\n",
      "Train loss (w/o reg) on all data: 0.48745212\n",
      "Test loss (w/o reg) on all data: 0.4242168\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002455098\n",
      "Norm of the params: 1.2805108\n",
      "              Random: fixed   8 labels. Loss 0.42422. Accuracy 0.778.\n",
      "### Flips: 30, rs: 16, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4041938\n",
      "Train loss (w/o reg) on all data: 0.40399852\n",
      "Test loss (w/o reg) on all data: 0.40613744\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.005531241\n",
      "Norm of the params: 1.9761416\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.40614. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [152] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34811845\n",
      "Train loss (w/o reg) on all data: 0.3478555\n",
      "Test loss (w/o reg) on all data: 0.42941406\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009651447\n",
      "Norm of the params: 2.293245\n",
      "                Loss: fixed  24 labels. Loss 0.42941. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [352] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4879962\n",
      "Train loss (w/o reg) on all data: 0.48790035\n",
      "Test loss (w/o reg) on all data: 0.4271452\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0006712474\n",
      "Norm of the params: 1.3845744\n",
      "              Random: fixed   9 labels. Loss 0.42715. Accuracy 0.733.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5523357\n",
      "Train loss (w/o reg) on all data: 0.5522496\n",
      "Test loss (w/o reg) on all data: 0.44602916\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0038372655\n",
      "Norm of the params: 1.3119355\n",
      "Flipped loss: 0.44603. Accuracy: 0.822\n",
      "### Flips: 30, rs: 17, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [147] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51927567\n",
      "Train loss (w/o reg) on all data: 0.5191974\n",
      "Test loss (w/o reg) on all data: 0.4140924\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00080439565\n",
      "Norm of the params: 1.2510618\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.41409. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [300] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4882747\n",
      "Train loss (w/o reg) on all data: 0.48811978\n",
      "Test loss (w/o reg) on all data: 0.40173995\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00068307156\n",
      "Norm of the params: 1.7602576\n",
      "                Loss: fixed   6 labels. Loss 0.40174. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55003756\n",
      "Train loss (w/o reg) on all data: 0.54995704\n",
      "Test loss (w/o reg) on all data: 0.44854867\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0033461186\n",
      "Norm of the params: 1.2690135\n",
      "              Random: fixed   1 labels. Loss 0.44855. Accuracy 0.822.\n",
      "### Flips: 30, rs: 17, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50606334\n",
      "Train loss (w/o reg) on all data: 0.50597876\n",
      "Test loss (w/o reg) on all data: 0.40844312\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00075025106\n",
      "Norm of the params: 1.3003988\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.40844. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43268684\n",
      "Train loss (w/o reg) on all data: 0.43240204\n",
      "Test loss (w/o reg) on all data: 0.40256822\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005627198\n",
      "Norm of the params: 2.386627\n",
      "                Loss: fixed  12 labels. Loss 0.40257. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.550039\n",
      "Train loss (w/o reg) on all data: 0.54995835\n",
      "Test loss (w/o reg) on all data: 0.4479975\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0037872535\n",
      "Norm of the params: 1.2698921\n",
      "              Random: fixed   1 labels. Loss 0.44800. Accuracy 0.822.\n",
      "### Flips: 30, rs: 17, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [167] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.460896\n",
      "Train loss (w/o reg) on all data: 0.46073925\n",
      "Test loss (w/o reg) on all data: 0.4029539\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0015298558\n",
      "Norm of the params: 1.7704368\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.40295. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41793534\n",
      "Train loss (w/o reg) on all data: 0.41767296\n",
      "Test loss (w/o reg) on all data: 0.38752767\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019817066\n",
      "Norm of the params: 2.2907407\n",
      "                Loss: fixed  14 labels. Loss 0.38753. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [172] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5439011\n",
      "Train loss (w/o reg) on all data: 0.54380393\n",
      "Test loss (w/o reg) on all data: 0.4544904\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010395495\n",
      "Norm of the params: 1.3941613\n",
      "              Random: fixed   2 labels. Loss 0.45449. Accuracy 0.822.\n",
      "### Flips: 30, rs: 17, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [282] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4371625\n",
      "Train loss (w/o reg) on all data: 0.43697995\n",
      "Test loss (w/o reg) on all data: 0.39278445\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0004178505\n",
      "Norm of the params: 1.9107575\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.39278. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [161] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3882627\n",
      "Train loss (w/o reg) on all data: 0.38798457\n",
      "Test loss (w/o reg) on all data: 0.3923422\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013085649\n",
      "Norm of the params: 2.3584394\n",
      "                Loss: fixed  18 labels. Loss 0.39234. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [240] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5439047\n",
      "Train loss (w/o reg) on all data: 0.54380566\n",
      "Test loss (w/o reg) on all data: 0.45416752\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0038312483\n",
      "Norm of the params: 1.407477\n",
      "              Random: fixed   2 labels. Loss 0.45417. Accuracy 0.822.\n",
      "### Flips: 30, rs: 17, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [159] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40468866\n",
      "Train loss (w/o reg) on all data: 0.40447015\n",
      "Test loss (w/o reg) on all data: 0.38803977\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0030899686\n",
      "Norm of the params: 2.0904405\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.38804. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [136] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3776444\n",
      "Train loss (w/o reg) on all data: 0.3773754\n",
      "Test loss (w/o reg) on all data: 0.39537057\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014777203\n",
      "Norm of the params: 2.3194115\n",
      "                Loss: fixed  20 labels. Loss 0.39537. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [332] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54389995\n",
      "Train loss (w/o reg) on all data: 0.5438016\n",
      "Test loss (w/o reg) on all data: 0.4538879\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001554469\n",
      "Norm of the params: 1.4023265\n",
      "              Random: fixed   2 labels. Loss 0.45389. Accuracy 0.822.\n",
      "### Flips: 30, rs: 17, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39885214\n",
      "Train loss (w/o reg) on all data: 0.39864942\n",
      "Test loss (w/o reg) on all data: 0.38967723\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0037181177\n",
      "Norm of the params: 2.0135708\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.38968. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37532395\n",
      "Train loss (w/o reg) on all data: 0.3750408\n",
      "Test loss (w/o reg) on all data: 0.3925981\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00062379026\n",
      "Norm of the params: 2.3797252\n",
      "                Loss: fixed  22 labels. Loss 0.39260. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5438996\n",
      "Train loss (w/o reg) on all data: 0.5438013\n",
      "Test loss (w/o reg) on all data: 0.45457226\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006947505\n",
      "Norm of the params: 1.4021941\n",
      "              Random: fixed   2 labels. Loss 0.45457. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54372215\n",
      "Train loss (w/o reg) on all data: 0.543595\n",
      "Test loss (w/o reg) on all data: 0.4441727\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.007811256\n",
      "Norm of the params: 1.5945122\n",
      "Flipped loss: 0.44417. Accuracy: 0.756\n",
      "### Flips: 30, rs: 18, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49171573\n",
      "Train loss (w/o reg) on all data: 0.491519\n",
      "Test loss (w/o reg) on all data: 0.40333578\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.011638398\n",
      "Norm of the params: 1.9835178\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.40334. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [147] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43262452\n",
      "Train loss (w/o reg) on all data: 0.43227825\n",
      "Test loss (w/o reg) on all data: 0.40807685\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002407021\n",
      "Norm of the params: 2.6316526\n",
      "                Loss: fixed   9 labels. Loss 0.40808. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5437226\n",
      "Train loss (w/o reg) on all data: 0.5435939\n",
      "Test loss (w/o reg) on all data: 0.44431418\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.006680151\n",
      "Norm of the params: 1.6045569\n",
      "              Random: fixed   0 labels. Loss 0.44431. Accuracy 0.756.\n",
      "### Flips: 30, rs: 18, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44721895\n",
      "Train loss (w/o reg) on all data: 0.44693568\n",
      "Test loss (w/o reg) on all data: 0.41296083\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00627432\n",
      "Norm of the params: 2.3802092\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.41296. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38980162\n",
      "Train loss (w/o reg) on all data: 0.3893408\n",
      "Test loss (w/o reg) on all data: 0.4468371\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012845773\n",
      "Norm of the params: 3.0358891\n",
      "                Loss: fixed  14 labels. Loss 0.44684. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [377] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5437321\n",
      "Train loss (w/o reg) on all data: 0.5436003\n",
      "Test loss (w/o reg) on all data: 0.44469962\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0028943236\n",
      "Norm of the params: 1.6233339\n",
      "              Random: fixed   0 labels. Loss 0.44470. Accuracy 0.756.\n",
      "### Flips: 30, rs: 18, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [278] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43769062\n",
      "Train loss (w/o reg) on all data: 0.43740356\n",
      "Test loss (w/o reg) on all data: 0.4111795\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0033075244\n",
      "Norm of the params: 2.396079\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.41118. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [269] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37559307\n",
      "Train loss (w/o reg) on all data: 0.37510595\n",
      "Test loss (w/o reg) on all data: 0.46100116\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0042515146\n",
      "Norm of the params: 3.1212466\n",
      "                Loss: fixed  16 labels. Loss 0.46100. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [525] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53402954\n",
      "Train loss (w/o reg) on all data: 0.533878\n",
      "Test loss (w/o reg) on all data: 0.43946254\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.003070977\n",
      "Norm of the params: 1.7408553\n",
      "              Random: fixed   2 labels. Loss 0.43946. Accuracy 0.778.\n",
      "### Flips: 30, rs: 18, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41740498\n",
      "Train loss (w/o reg) on all data: 0.41716564\n",
      "Test loss (w/o reg) on all data: 0.367037\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.007868329\n",
      "Norm of the params: 2.187927\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.36704. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3575775\n",
      "Train loss (w/o reg) on all data: 0.3571026\n",
      "Test loss (w/o reg) on all data: 0.46341512\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010752036\n",
      "Norm of the params: 3.0818589\n",
      "                Loss: fixed  19 labels. Loss 0.46342. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [391] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5246864\n",
      "Train loss (w/o reg) on all data: 0.52454007\n",
      "Test loss (w/o reg) on all data: 0.43687442\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0021305804\n",
      "Norm of the params: 1.7107114\n",
      "              Random: fixed   4 labels. Loss 0.43687. Accuracy 0.756.\n",
      "### Flips: 30, rs: 18, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40909472\n",
      "Train loss (w/o reg) on all data: 0.40882963\n",
      "Test loss (w/o reg) on all data: 0.3725051\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006919412\n",
      "Norm of the params: 2.3025994\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.37251. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3553433\n",
      "Train loss (w/o reg) on all data: 0.35486722\n",
      "Test loss (w/o reg) on all data: 0.45707288\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007650889\n",
      "Norm of the params: 3.0857446\n",
      "                Loss: fixed  20 labels. Loss 0.45707. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [337] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5139936\n",
      "Train loss (w/o reg) on all data: 0.51383936\n",
      "Test loss (w/o reg) on all data: 0.4251763\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00075619196\n",
      "Norm of the params: 1.7566195\n",
      "              Random: fixed   5 labels. Loss 0.42518. Accuracy 0.756.\n",
      "### Flips: 30, rs: 18, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [353] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39189005\n",
      "Train loss (w/o reg) on all data: 0.39159897\n",
      "Test loss (w/o reg) on all data: 0.3807396\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0023941838\n",
      "Norm of the params: 2.412785\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.38074. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [407] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3499951\n",
      "Train loss (w/o reg) on all data: 0.34951887\n",
      "Test loss (w/o reg) on all data: 0.4786466\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0004553089\n",
      "Norm of the params: 3.08626\n",
      "                Loss: fixed  23 labels. Loss 0.47865. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [240] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49563354\n",
      "Train loss (w/o reg) on all data: 0.49545196\n",
      "Test loss (w/o reg) on all data: 0.40737563\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0013297021\n",
      "Norm of the params: 1.9057115\n",
      "              Random: fixed   7 labels. Loss 0.40738. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [376] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5339032\n",
      "Train loss (w/o reg) on all data: 0.5338408\n",
      "Test loss (w/o reg) on all data: 0.41242963\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0050461204\n",
      "Norm of the params: 1.1172985\n",
      "Flipped loss: 0.41243. Accuracy: 0.778\n",
      "### Flips: 30, rs: 19, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4738418\n",
      "Train loss (w/o reg) on all data: 0.4737261\n",
      "Test loss (w/o reg) on all data: 0.3765763\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0028654726\n",
      "Norm of the params: 1.5210408\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.37658. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [307] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4523734\n",
      "Train loss (w/o reg) on all data: 0.45224652\n",
      "Test loss (w/o reg) on all data: 0.40888426\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0018979859\n",
      "Norm of the params: 1.5929288\n",
      "                Loss: fixed   7 labels. Loss 0.40888. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5263119\n",
      "Train loss (w/o reg) on all data: 0.5262408\n",
      "Test loss (w/o reg) on all data: 0.41054207\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019751482\n",
      "Norm of the params: 1.19192\n",
      "              Random: fixed   3 labels. Loss 0.41054. Accuracy 0.800.\n",
      "### Flips: 30, rs: 19, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [134] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44928297\n",
      "Train loss (w/o reg) on all data: 0.4491487\n",
      "Test loss (w/o reg) on all data: 0.37323722\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016327379\n",
      "Norm of the params: 1.6387318\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.37324. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.389851\n",
      "Train loss (w/o reg) on all data: 0.38968155\n",
      "Test loss (w/o reg) on all data: 0.43470564\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011628127\n",
      "Norm of the params: 1.840967\n",
      "                Loss: fixed  14 labels. Loss 0.43471. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [146] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5262889\n",
      "Train loss (w/o reg) on all data: 0.52621293\n",
      "Test loss (w/o reg) on all data: 0.41239023\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001382862\n",
      "Norm of the params: 1.2329333\n",
      "              Random: fixed   4 labels. Loss 0.41239. Accuracy 0.800.\n",
      "### Flips: 30, rs: 19, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [226] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4181555\n",
      "Train loss (w/o reg) on all data: 0.41796345\n",
      "Test loss (w/o reg) on all data: 0.38593653\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007021879\n",
      "Norm of the params: 1.9598757\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.38594. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [358] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3497221\n",
      "Train loss (w/o reg) on all data: 0.34946334\n",
      "Test loss (w/o reg) on all data: 0.4272831\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00041578134\n",
      "Norm of the params: 2.2747755\n",
      "                Loss: fixed  19 labels. Loss 0.42728. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [288] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5262797\n",
      "Train loss (w/o reg) on all data: 0.5262033\n",
      "Test loss (w/o reg) on all data: 0.41137445\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0023531974\n",
      "Norm of the params: 1.2361538\n",
      "              Random: fixed   4 labels. Loss 0.41137. Accuracy 0.800.\n",
      "### Flips: 30, rs: 19, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4005078\n",
      "Train loss (w/o reg) on all data: 0.40026757\n",
      "Test loss (w/o reg) on all data: 0.39279935\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018678568\n",
      "Norm of the params: 2.1919603\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.39280. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32263327\n",
      "Train loss (w/o reg) on all data: 0.3222713\n",
      "Test loss (w/o reg) on all data: 0.4475346\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0064110896\n",
      "Norm of the params: 2.6906846\n",
      "                Loss: fixed  22 labels. Loss 0.44753. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.503767\n",
      "Train loss (w/o reg) on all data: 0.50366807\n",
      "Test loss (w/o reg) on all data: 0.4070244\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0032225896\n",
      "Norm of the params: 1.4069012\n",
      "              Random: fixed   8 labels. Loss 0.40702. Accuracy 0.822.\n",
      "### Flips: 30, rs: 19, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [328] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38106927\n",
      "Train loss (w/o reg) on all data: 0.38082194\n",
      "Test loss (w/o reg) on all data: 0.4137328\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017238627\n",
      "Norm of the params: 2.2240567\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.41373. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3169404\n",
      "Train loss (w/o reg) on all data: 0.3165385\n",
      "Test loss (w/o reg) on all data: 0.44551608\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010588794\n",
      "Norm of the params: 2.8351078\n",
      "                Loss: fixed  23 labels. Loss 0.44552. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [333] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49927384\n",
      "Train loss (w/o reg) on all data: 0.49917242\n",
      "Test loss (w/o reg) on all data: 0.4121936\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0026468812\n",
      "Norm of the params: 1.4242483\n",
      "              Random: fixed   9 labels. Loss 0.41219. Accuracy 0.800.\n",
      "### Flips: 30, rs: 19, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [341] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3503777\n",
      "Train loss (w/o reg) on all data: 0.3500628\n",
      "Test loss (w/o reg) on all data: 0.42265794\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0024062637\n",
      "Norm of the params: 2.509694\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.42266. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31377435\n",
      "Train loss (w/o reg) on all data: 0.31335136\n",
      "Test loss (w/o reg) on all data: 0.4496621\n",
      "Train acc on all data:  0.8820754716981132\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00096377893\n",
      "Norm of the params: 2.9085221\n",
      "                Loss: fixed  24 labels. Loss 0.44966. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [321] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4921707\n",
      "Train loss (w/o reg) on all data: 0.49206728\n",
      "Test loss (w/o reg) on all data: 0.41820398\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015382058\n",
      "Norm of the params: 1.4381108\n",
      "              Random: fixed  10 labels. Loss 0.41820. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [173] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5461956\n",
      "Train loss (w/o reg) on all data: 0.5460872\n",
      "Test loss (w/o reg) on all data: 0.5163441\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0019363675\n",
      "Norm of the params: 1.4726274\n",
      "Flipped loss: 0.51634. Accuracy: 0.711\n",
      "### Flips: 30, rs: 20, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49034333\n",
      "Train loss (w/o reg) on all data: 0.4902082\n",
      "Test loss (w/o reg) on all data: 0.49825367\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.00211197\n",
      "Norm of the params: 1.6439033\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.49825. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45828146\n",
      "Train loss (w/o reg) on all data: 0.45809296\n",
      "Test loss (w/o reg) on all data: 0.52429867\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0008087233\n",
      "Norm of the params: 1.9416184\n",
      "                Loss: fixed   7 labels. Loss 0.52430. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5367369\n",
      "Train loss (w/o reg) on all data: 0.53661233\n",
      "Test loss (w/o reg) on all data: 0.5217654\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.011734152\n",
      "Norm of the params: 1.5784789\n",
      "              Random: fixed   1 labels. Loss 0.52177. Accuracy 0.711.\n",
      "### Flips: 30, rs: 20, checks: 20\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [101] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48238406\n",
      "Train loss (w/o reg) on all data: 0.48223493\n",
      "Test loss (w/o reg) on all data: 0.49297315\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0027462381\n",
      "Norm of the params: 1.7269804\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.49297. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40102944\n",
      "Train loss (w/o reg) on all data: 0.40073967\n",
      "Test loss (w/o reg) on all data: 0.48051706\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0005504182\n",
      "Norm of the params: 2.407304\n",
      "                Loss: fixed  13 labels. Loss 0.48052. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [352] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53328335\n",
      "Train loss (w/o reg) on all data: 0.5331729\n",
      "Test loss (w/o reg) on all data: 0.5092146\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.003030808\n",
      "Norm of the params: 1.4860865\n",
      "              Random: fixed   2 labels. Loss 0.50921. Accuracy 0.711.\n",
      "### Flips: 30, rs: 20, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45769975\n",
      "Train loss (w/o reg) on all data: 0.45755994\n",
      "Test loss (w/o reg) on all data: 0.45931435\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.000584989\n",
      "Norm of the params: 1.6722268\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.45931. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3671982\n",
      "Train loss (w/o reg) on all data: 0.36691248\n",
      "Test loss (w/o reg) on all data: 0.47657844\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0043934947\n",
      "Norm of the params: 2.3904915\n",
      "                Loss: fixed  18 labels. Loss 0.47658. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [348] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5269489\n",
      "Train loss (w/o reg) on all data: 0.52683634\n",
      "Test loss (w/o reg) on all data: 0.49821866\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011669669\n",
      "Norm of the params: 1.5008093\n",
      "              Random: fixed   3 labels. Loss 0.49822. Accuracy 0.778.\n",
      "### Flips: 30, rs: 20, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41770053\n",
      "Train loss (w/o reg) on all data: 0.41752353\n",
      "Test loss (w/o reg) on all data: 0.45511585\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0026110285\n",
      "Norm of the params: 1.8814468\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.45512. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3531967\n",
      "Train loss (w/o reg) on all data: 0.35286593\n",
      "Test loss (w/o reg) on all data: 0.46420962\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012177543\n",
      "Norm of the params: 2.5721207\n",
      "                Loss: fixed  20 labels. Loss 0.46421. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52836716\n",
      "Train loss (w/o reg) on all data: 0.5282441\n",
      "Test loss (w/o reg) on all data: 0.50229126\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0011710095\n",
      "Norm of the params: 1.5690447\n",
      "              Random: fixed   5 labels. Loss 0.50229. Accuracy 0.733.\n",
      "### Flips: 30, rs: 20, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41639125\n",
      "Train loss (w/o reg) on all data: 0.4161963\n",
      "Test loss (w/o reg) on all data: 0.44055218\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.004456722\n",
      "Norm of the params: 1.9746532\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.44055. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [272] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35092562\n",
      "Train loss (w/o reg) on all data: 0.35062537\n",
      "Test loss (w/o reg) on all data: 0.45861024\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006501286\n",
      "Norm of the params: 2.4505794\n",
      "                Loss: fixed  21 labels. Loss 0.45861. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53108525\n",
      "Train loss (w/o reg) on all data: 0.53096575\n",
      "Test loss (w/o reg) on all data: 0.49560314\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0007030602\n",
      "Norm of the params: 1.5461798\n",
      "              Random: fixed   6 labels. Loss 0.49560. Accuracy 0.733.\n",
      "### Flips: 30, rs: 20, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [159] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39570853\n",
      "Train loss (w/o reg) on all data: 0.3954724\n",
      "Test loss (w/o reg) on all data: 0.44799635\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011249309\n",
      "Norm of the params: 2.173096\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.44800. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [343] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34472418\n",
      "Train loss (w/o reg) on all data: 0.34441808\n",
      "Test loss (w/o reg) on all data: 0.45093292\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0021623166\n",
      "Norm of the params: 2.4742558\n",
      "                Loss: fixed  23 labels. Loss 0.45093. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5226943\n",
      "Train loss (w/o reg) on all data: 0.5225684\n",
      "Test loss (w/o reg) on all data: 0.46533746\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0016804244\n",
      "Norm of the params: 1.5866462\n",
      "              Random: fixed   8 labels. Loss 0.46534. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49327806\n",
      "Train loss (w/o reg) on all data: 0.49315047\n",
      "Test loss (w/o reg) on all data: 0.37904266\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015880755\n",
      "Norm of the params: 1.5973284\n",
      "Flipped loss: 0.37904. Accuracy: 0.822\n",
      "### Flips: 30, rs: 21, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [153] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46709833\n",
      "Train loss (w/o reg) on all data: 0.4669514\n",
      "Test loss (w/o reg) on all data: 0.37642458\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013131157\n",
      "Norm of the params: 1.7142738\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.37642. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4237389\n",
      "Train loss (w/o reg) on all data: 0.42347637\n",
      "Test loss (w/o reg) on all data: 0.3709555\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0012241722\n",
      "Norm of the params: 2.2914472\n",
      "                Loss: fixed   5 labels. Loss 0.37096. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49504447\n",
      "Train loss (w/o reg) on all data: 0.49491507\n",
      "Test loss (w/o reg) on all data: 0.37464374\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00078453583\n",
      "Norm of the params: 1.6088165\n",
      "              Random: fixed   1 labels. Loss 0.37464. Accuracy 0.844.\n",
      "### Flips: 30, rs: 21, checks: 20\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [172] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4253528\n",
      "Train loss (w/o reg) on all data: 0.4251239\n",
      "Test loss (w/o reg) on all data: 0.36510158\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001837092\n",
      "Norm of the params: 2.13965\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.36510. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37404403\n",
      "Train loss (w/o reg) on all data: 0.37370315\n",
      "Test loss (w/o reg) on all data: 0.39101562\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0074244994\n",
      "Norm of the params: 2.611051\n",
      "                Loss: fixed  11 labels. Loss 0.39102. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47979334\n",
      "Train loss (w/o reg) on all data: 0.47964397\n",
      "Test loss (w/o reg) on all data: 0.3686069\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0027612438\n",
      "Norm of the params: 1.7283474\n",
      "              Random: fixed   3 labels. Loss 0.36861. Accuracy 0.867.\n",
      "### Flips: 30, rs: 21, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39418754\n",
      "Train loss (w/o reg) on all data: 0.39390308\n",
      "Test loss (w/o reg) on all data: 0.36876234\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00072995265\n",
      "Norm of the params: 2.3852\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.36876. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [167] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35946855\n",
      "Train loss (w/o reg) on all data: 0.35911393\n",
      "Test loss (w/o reg) on all data: 0.37797657\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007416513\n",
      "Norm of the params: 2.6630995\n",
      "                Loss: fixed  14 labels. Loss 0.37798. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46848994\n",
      "Train loss (w/o reg) on all data: 0.46834967\n",
      "Test loss (w/o reg) on all data: 0.37735906\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00085272803\n",
      "Norm of the params: 1.6750145\n",
      "              Random: fixed   6 labels. Loss 0.37736. Accuracy 0.822.\n",
      "### Flips: 30, rs: 21, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3940754\n",
      "Train loss (w/o reg) on all data: 0.39380607\n",
      "Test loss (w/o reg) on all data: 0.3700242\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00066367205\n",
      "Norm of the params: 2.3208923\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.37002. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33893892\n",
      "Train loss (w/o reg) on all data: 0.3385513\n",
      "Test loss (w/o reg) on all data: 0.39068955\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0019153841\n",
      "Norm of the params: 2.7843118\n",
      "                Loss: fixed  18 labels. Loss 0.39069. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [333] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4684831\n",
      "Train loss (w/o reg) on all data: 0.46834397\n",
      "Test loss (w/o reg) on all data: 0.37731773\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010493628\n",
      "Norm of the params: 1.6679944\n",
      "              Random: fixed   6 labels. Loss 0.37732. Accuracy 0.822.\n",
      "### Flips: 30, rs: 21, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38486436\n",
      "Train loss (w/o reg) on all data: 0.38457\n",
      "Test loss (w/o reg) on all data: 0.36125484\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.003928718\n",
      "Norm of the params: 2.426388\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.36125. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [104] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33455944\n",
      "Train loss (w/o reg) on all data: 0.3341854\n",
      "Test loss (w/o reg) on all data: 0.39496967\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0046122484\n",
      "Norm of the params: 2.7351856\n",
      "                Loss: fixed  20 labels. Loss 0.39497. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45669982\n",
      "Train loss (w/o reg) on all data: 0.4565362\n",
      "Test loss (w/o reg) on all data: 0.3796414\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013028306\n",
      "Norm of the params: 1.8089789\n",
      "              Random: fixed   8 labels. Loss 0.37964. Accuracy 0.822.\n",
      "### Flips: 30, rs: 21, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35798734\n",
      "Train loss (w/o reg) on all data: 0.3576529\n",
      "Test loss (w/o reg) on all data: 0.37421516\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006672374\n",
      "Norm of the params: 2.5862381\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.37422. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [139] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3334616\n",
      "Train loss (w/o reg) on all data: 0.3331184\n",
      "Test loss (w/o reg) on all data: 0.40465924\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00478721\n",
      "Norm of the params: 2.6199548\n",
      "                Loss: fixed  23 labels. Loss 0.40466. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [344] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44559354\n",
      "Train loss (w/o reg) on all data: 0.4454367\n",
      "Test loss (w/o reg) on all data: 0.37962183\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0014985844\n",
      "Norm of the params: 1.7711567\n",
      "              Random: fixed  10 labels. Loss 0.37962. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54114324\n",
      "Train loss (w/o reg) on all data: 0.5409976\n",
      "Test loss (w/o reg) on all data: 0.3839982\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.007059034\n",
      "Norm of the params: 1.706517\n",
      "Flipped loss: 0.38400. Accuracy: 0.844\n",
      "### Flips: 30, rs: 22, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [299] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5290527\n",
      "Train loss (w/o reg) on all data: 0.5289339\n",
      "Test loss (w/o reg) on all data: 0.36468595\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0012516318\n",
      "Norm of the params: 1.5415446\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.36469. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47046095\n",
      "Train loss (w/o reg) on all data: 0.47026062\n",
      "Test loss (w/o reg) on all data: 0.36445165\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006398511\n",
      "Norm of the params: 2.001662\n",
      "                Loss: fixed   7 labels. Loss 0.36445. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5301721\n",
      "Train loss (w/o reg) on all data: 0.5300194\n",
      "Test loss (w/o reg) on all data: 0.38148054\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0024228941\n",
      "Norm of the params: 1.747561\n",
      "              Random: fixed   2 labels. Loss 0.38148. Accuracy 0.844.\n",
      "### Flips: 30, rs: 22, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [291] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5112888\n",
      "Train loss (w/o reg) on all data: 0.51117665\n",
      "Test loss (w/o reg) on all data: 0.35241467\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0012666971\n",
      "Norm of the params: 1.4977578\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.35241. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.423089\n",
      "Train loss (w/o reg) on all data: 0.42285514\n",
      "Test loss (w/o reg) on all data: 0.3793549\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006254954\n",
      "Norm of the params: 2.162704\n",
      "                Loss: fixed  12 labels. Loss 0.37935. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [301] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5258297\n",
      "Train loss (w/o reg) on all data: 0.52567464\n",
      "Test loss (w/o reg) on all data: 0.3717616\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0052228016\n",
      "Norm of the params: 1.7607341\n",
      "              Random: fixed   6 labels. Loss 0.37176. Accuracy 0.867.\n",
      "### Flips: 30, rs: 22, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [280] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4853329\n",
      "Train loss (w/o reg) on all data: 0.48520157\n",
      "Test loss (w/o reg) on all data: 0.34790263\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0030995568\n",
      "Norm of the params: 1.6207746\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.34790. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [359] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3964667\n",
      "Train loss (w/o reg) on all data: 0.3961823\n",
      "Test loss (w/o reg) on all data: 0.37085643\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0033829901\n",
      "Norm of the params: 2.3850164\n",
      "                Loss: fixed  16 labels. Loss 0.37086. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [109] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52282035\n",
      "Train loss (w/o reg) on all data: 0.52265847\n",
      "Test loss (w/o reg) on all data: 0.37519675\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0018901055\n",
      "Norm of the params: 1.7993735\n",
      "              Random: fixed   8 labels. Loss 0.37520. Accuracy 0.844.\n",
      "### Flips: 30, rs: 22, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44978788\n",
      "Train loss (w/o reg) on all data: 0.4496091\n",
      "Test loss (w/o reg) on all data: 0.34089893\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.005880667\n",
      "Norm of the params: 1.8909838\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.34090. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37890774\n",
      "Train loss (w/o reg) on all data: 0.37861645\n",
      "Test loss (w/o reg) on all data: 0.37163848\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004171127\n",
      "Norm of the params: 2.4136353\n",
      "                Loss: fixed  18 labels. Loss 0.37164. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [114] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5206452\n",
      "Train loss (w/o reg) on all data: 0.520487\n",
      "Test loss (w/o reg) on all data: 0.3797263\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0022272458\n",
      "Norm of the params: 1.7786466\n",
      "              Random: fixed   9 labels. Loss 0.37973. Accuracy 0.867.\n",
      "### Flips: 30, rs: 22, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [330] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44642687\n",
      "Train loss (w/o reg) on all data: 0.4462554\n",
      "Test loss (w/o reg) on all data: 0.34460005\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0016849427\n",
      "Norm of the params: 1.8518596\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.34460. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36420134\n",
      "Train loss (w/o reg) on all data: 0.36391073\n",
      "Test loss (w/o reg) on all data: 0.38440514\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0035647023\n",
      "Norm of the params: 2.4108045\n",
      "                Loss: fixed  19 labels. Loss 0.38441. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [288] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52063584\n",
      "Train loss (w/o reg) on all data: 0.52047575\n",
      "Test loss (w/o reg) on all data: 0.3799284\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.003850105\n",
      "Norm of the params: 1.7894499\n",
      "              Random: fixed   9 labels. Loss 0.37993. Accuracy 0.867.\n",
      "### Flips: 30, rs: 22, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [318] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42704606\n",
      "Train loss (w/o reg) on all data: 0.42681545\n",
      "Test loss (w/o reg) on all data: 0.3623538\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0023959344\n",
      "Norm of the params: 2.1475463\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.36235. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [240] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3638878\n",
      "Train loss (w/o reg) on all data: 0.3635899\n",
      "Test loss (w/o reg) on all data: 0.38347858\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005772786\n",
      "Norm of the params: 2.4408305\n",
      "                Loss: fixed  20 labels. Loss 0.38348. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [347] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5140777\n",
      "Train loss (w/o reg) on all data: 0.51391643\n",
      "Test loss (w/o reg) on all data: 0.38238847\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00017404534\n",
      "Norm of the params: 1.7961255\n",
      "              Random: fixed  10 labels. Loss 0.38239. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49212122\n",
      "Train loss (w/o reg) on all data: 0.491982\n",
      "Test loss (w/o reg) on all data: 0.4093124\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0019395226\n",
      "Norm of the params: 1.6686289\n",
      "Flipped loss: 0.40931. Accuracy: 0.778\n",
      "### Flips: 30, rs: 23, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [179] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43863836\n",
      "Train loss (w/o reg) on all data: 0.4384667\n",
      "Test loss (w/o reg) on all data: 0.3923075\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.006143239\n",
      "Norm of the params: 1.8528291\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.39231. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4055958\n",
      "Train loss (w/o reg) on all data: 0.4053515\n",
      "Test loss (w/o reg) on all data: 0.4089463\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0056961323\n",
      "Norm of the params: 2.210467\n",
      "                Loss: fixed   7 labels. Loss 0.40895. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50021005\n",
      "Train loss (w/o reg) on all data: 0.50007606\n",
      "Test loss (w/o reg) on all data: 0.4062507\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012013968\n",
      "Norm of the params: 1.6369777\n",
      "              Random: fixed   2 labels. Loss 0.40625. Accuracy 0.778.\n",
      "### Flips: 30, rs: 23, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40893224\n",
      "Train loss (w/o reg) on all data: 0.40873274\n",
      "Test loss (w/o reg) on all data: 0.37412766\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0024546138\n",
      "Norm of the params: 1.9974443\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.37413. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [303] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36901695\n",
      "Train loss (w/o reg) on all data: 0.36872086\n",
      "Test loss (w/o reg) on all data: 0.40456307\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010277109\n",
      "Norm of the params: 2.4334662\n",
      "                Loss: fixed  11 labels. Loss 0.40456. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50021577\n",
      "Train loss (w/o reg) on all data: 0.5000799\n",
      "Test loss (w/o reg) on all data: 0.40603882\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012625135\n",
      "Norm of the params: 1.648773\n",
      "              Random: fixed   2 labels. Loss 0.40604. Accuracy 0.778.\n",
      "### Flips: 30, rs: 23, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [328] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38716108\n",
      "Train loss (w/o reg) on all data: 0.38688958\n",
      "Test loss (w/o reg) on all data: 0.38235575\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0014910012\n",
      "Norm of the params: 2.3302503\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.38236. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34658048\n",
      "Train loss (w/o reg) on all data: 0.34630457\n",
      "Test loss (w/o reg) on all data: 0.4230654\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022663537\n",
      "Norm of the params: 2.349107\n",
      "                Loss: fixed  15 labels. Loss 0.42307. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5002101\n",
      "Train loss (w/o reg) on all data: 0.50007725\n",
      "Test loss (w/o reg) on all data: 0.40673128\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.007573622\n",
      "Norm of the params: 1.6300523\n",
      "              Random: fixed   2 labels. Loss 0.40673. Accuracy 0.756.\n",
      "### Flips: 30, rs: 23, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [371] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3836679\n",
      "Train loss (w/o reg) on all data: 0.38341153\n",
      "Test loss (w/o reg) on all data: 0.38629916\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0018404827\n",
      "Norm of the params: 2.2643094\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.38630. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32355615\n",
      "Train loss (w/o reg) on all data: 0.3232162\n",
      "Test loss (w/o reg) on all data: 0.44246423\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015914568\n",
      "Norm of the params: 2.607526\n",
      "                Loss: fixed  19 labels. Loss 0.44246. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5002114\n",
      "Train loss (w/o reg) on all data: 0.5000773\n",
      "Test loss (w/o reg) on all data: 0.4060918\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00085958006\n",
      "Norm of the params: 1.6378943\n",
      "              Random: fixed   2 labels. Loss 0.40609. Accuracy 0.756.\n",
      "### Flips: 30, rs: 23, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3603019\n",
      "Train loss (w/o reg) on all data: 0.3600019\n",
      "Test loss (w/o reg) on all data: 0.41202518\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.007956428\n",
      "Norm of the params: 2.4496021\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.41203. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [195] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3111537\n",
      "Train loss (w/o reg) on all data: 0.31076643\n",
      "Test loss (w/o reg) on all data: 0.4485171\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.000913564\n",
      "Norm of the params: 2.7831135\n",
      "                Loss: fixed  23 labels. Loss 0.44852. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49084815\n",
      "Train loss (w/o reg) on all data: 0.49073294\n",
      "Test loss (w/o reg) on all data: 0.40659213\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.005481482\n",
      "Norm of the params: 1.5180206\n",
      "              Random: fixed   4 labels. Loss 0.40659. Accuracy 0.778.\n",
      "### Flips: 30, rs: 23, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3479384\n",
      "Train loss (w/o reg) on all data: 0.34763005\n",
      "Test loss (w/o reg) on all data: 0.4291082\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.009177188\n",
      "Norm of the params: 2.4832432\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.42911. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.30655596\n",
      "Train loss (w/o reg) on all data: 0.3061208\n",
      "Test loss (w/o reg) on all data: 0.44265816\n",
      "Train acc on all data:  0.8820754716981132\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0002850928\n",
      "Norm of the params: 2.950084\n",
      "                Loss: fixed  25 labels. Loss 0.44266. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49555916\n",
      "Train loss (w/o reg) on all data: 0.4954449\n",
      "Test loss (w/o reg) on all data: 0.40094617\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.008833367\n",
      "Norm of the params: 1.5117013\n",
      "              Random: fixed   6 labels. Loss 0.40095. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5371859\n",
      "Train loss (w/o reg) on all data: 0.53713125\n",
      "Test loss (w/o reg) on all data: 0.45660925\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011159778\n",
      "Norm of the params: 1.0456853\n",
      "Flipped loss: 0.45661. Accuracy: 0.778\n",
      "### Flips: 30, rs: 24, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.489001\n",
      "Train loss (w/o reg) on all data: 0.48890406\n",
      "Test loss (w/o reg) on all data: 0.43754268\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0026311255\n",
      "Norm of the params: 1.3925371\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.43754. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [148] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4495689\n",
      "Train loss (w/o reg) on all data: 0.44945678\n",
      "Test loss (w/o reg) on all data: 0.39979485\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0022272868\n",
      "Norm of the params: 1.4975191\n",
      "                Loss: fixed   8 labels. Loss 0.39979. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5205741\n",
      "Train loss (w/o reg) on all data: 0.520517\n",
      "Test loss (w/o reg) on all data: 0.4369345\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.012817462\n",
      "Norm of the params: 1.0686942\n",
      "              Random: fixed   2 labels. Loss 0.43693. Accuracy 0.800.\n",
      "### Flips: 30, rs: 24, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4611763\n",
      "Train loss (w/o reg) on all data: 0.46106923\n",
      "Test loss (w/o reg) on all data: 0.41608778\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003522389\n",
      "Norm of the params: 1.46333\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.41609. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [318] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4176536\n",
      "Train loss (w/o reg) on all data: 0.41749707\n",
      "Test loss (w/o reg) on all data: 0.41717258\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0027738195\n",
      "Norm of the params: 1.7693119\n",
      "                Loss: fixed  12 labels. Loss 0.41717. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5135883\n",
      "Train loss (w/o reg) on all data: 0.5135203\n",
      "Test loss (w/o reg) on all data: 0.43189728\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019086164\n",
      "Norm of the params: 1.1662331\n",
      "              Random: fixed   3 labels. Loss 0.43190. Accuracy 0.800.\n",
      "### Flips: 30, rs: 24, checks: 30\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [179] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42359513\n",
      "Train loss (w/o reg) on all data: 0.42345133\n",
      "Test loss (w/o reg) on all data: 0.41979095\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018048764\n",
      "Norm of the params: 1.695941\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.41979. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [153] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36948007\n",
      "Train loss (w/o reg) on all data: 0.3692456\n",
      "Test loss (w/o reg) on all data: 0.41095275\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.003504591\n",
      "Norm of the params: 2.1655684\n",
      "                Loss: fixed  19 labels. Loss 0.41095. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [310] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5117172\n",
      "Train loss (w/o reg) on all data: 0.5116446\n",
      "Test loss (w/o reg) on all data: 0.4371972\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011550541\n",
      "Norm of the params: 1.2047546\n",
      "              Random: fixed   5 labels. Loss 0.43720. Accuracy 0.800.\n",
      "### Flips: 30, rs: 24, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [327] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39666763\n",
      "Train loss (w/o reg) on all data: 0.39648616\n",
      "Test loss (w/o reg) on all data: 0.4340757\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002147419\n",
      "Norm of the params: 1.9050537\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.43408. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3580196\n",
      "Train loss (w/o reg) on all data: 0.3577439\n",
      "Test loss (w/o reg) on all data: 0.39915136\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0013921208\n",
      "Norm of the params: 2.3482027\n",
      "                Loss: fixed  21 labels. Loss 0.39915. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [309] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5106067\n",
      "Train loss (w/o reg) on all data: 0.51053125\n",
      "Test loss (w/o reg) on all data: 0.42969182\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007081256\n",
      "Norm of the params: 1.2284486\n",
      "              Random: fixed   8 labels. Loss 0.42969. Accuracy 0.822.\n",
      "### Flips: 30, rs: 24, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [288] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38574058\n",
      "Train loss (w/o reg) on all data: 0.38552734\n",
      "Test loss (w/o reg) on all data: 0.44773304\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0034449804\n",
      "Norm of the params: 2.065165\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.44773. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [372] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34959227\n",
      "Train loss (w/o reg) on all data: 0.3492739\n",
      "Test loss (w/o reg) on all data: 0.3855417\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0008754919\n",
      "Norm of the params: 2.52338\n",
      "                Loss: fixed  22 labels. Loss 0.38554. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [345] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5106097\n",
      "Train loss (w/o reg) on all data: 0.51053506\n",
      "Test loss (w/o reg) on all data: 0.43018505\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011435429\n",
      "Norm of the params: 1.2219018\n",
      "              Random: fixed   8 labels. Loss 0.43019. Accuracy 0.822.\n",
      "### Flips: 30, rs: 24, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3702336\n",
      "Train loss (w/o reg) on all data: 0.36998922\n",
      "Test loss (w/o reg) on all data: 0.44937196\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0009893281\n",
      "Norm of the params: 2.2107623\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.44937. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [222] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34844342\n",
      "Train loss (w/o reg) on all data: 0.34814924\n",
      "Test loss (w/o reg) on all data: 0.39191228\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002333831\n",
      "Norm of the params: 2.4255588\n",
      "                Loss: fixed  24 labels. Loss 0.39191. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [374] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5049442\n",
      "Train loss (w/o reg) on all data: 0.50485826\n",
      "Test loss (w/o reg) on all data: 0.41457707\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.005527958\n",
      "Norm of the params: 1.311214\n",
      "              Random: fixed   9 labels. Loss 0.41458. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [291] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5045929\n",
      "Train loss (w/o reg) on all data: 0.50445026\n",
      "Test loss (w/o reg) on all data: 0.4245265\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.000775604\n",
      "Norm of the params: 1.6888342\n",
      "Flipped loss: 0.42453. Accuracy: 0.867\n",
      "### Flips: 30, rs: 25, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [177] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47847885\n",
      "Train loss (w/o reg) on all data: 0.47829872\n",
      "Test loss (w/o reg) on all data: 0.38562042\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.003919616\n",
      "Norm of the params: 1.8979684\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.38562. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41854525\n",
      "Train loss (w/o reg) on all data: 0.41828272\n",
      "Test loss (w/o reg) on all data: 0.42301077\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006925254\n",
      "Norm of the params: 2.2913892\n",
      "                Loss: fixed   8 labels. Loss 0.42301. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [110] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46265972\n",
      "Train loss (w/o reg) on all data: 0.46248296\n",
      "Test loss (w/o reg) on all data: 0.42256248\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.000722632\n",
      "Norm of the params: 1.8802227\n",
      "              Random: fixed   5 labels. Loss 0.42256. Accuracy 0.867.\n",
      "### Flips: 30, rs: 25, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [127] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44533312\n",
      "Train loss (w/o reg) on all data: 0.4451195\n",
      "Test loss (w/o reg) on all data: 0.36709055\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.007050474\n",
      "Norm of the params: 2.0669267\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.36709. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [150] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3678518\n",
      "Train loss (w/o reg) on all data: 0.36750463\n",
      "Test loss (w/o reg) on all data: 0.44077644\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002976794\n",
      "Norm of the params: 2.6349926\n",
      "                Loss: fixed  13 labels. Loss 0.44078. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45900592\n",
      "Train loss (w/o reg) on all data: 0.45882082\n",
      "Test loss (w/o reg) on all data: 0.42548418\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015594307\n",
      "Norm of the params: 1.9240332\n",
      "              Random: fixed   6 labels. Loss 0.42548. Accuracy 0.822.\n",
      "### Flips: 30, rs: 25, checks: 30\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4129436\n",
      "Train loss (w/o reg) on all data: 0.41266316\n",
      "Test loss (w/o reg) on all data: 0.35844865\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005770734\n",
      "Norm of the params: 2.3682623\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.35845. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [266] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34979388\n",
      "Train loss (w/o reg) on all data: 0.34942037\n",
      "Test loss (w/o reg) on all data: 0.44132683\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019302255\n",
      "Norm of the params: 2.7331517\n",
      "                Loss: fixed  15 labels. Loss 0.44133. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46351826\n",
      "Train loss (w/o reg) on all data: 0.46335372\n",
      "Test loss (w/o reg) on all data: 0.43363973\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0023412113\n",
      "Norm of the params: 1.8141241\n",
      "              Random: fixed   8 labels. Loss 0.43364. Accuracy 0.822.\n",
      "### Flips: 30, rs: 25, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [258] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3734281\n",
      "Train loss (w/o reg) on all data: 0.37307292\n",
      "Test loss (w/o reg) on all data: 0.38247076\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017056138\n",
      "Norm of the params: 2.6652918\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.38247. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [290] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32695267\n",
      "Train loss (w/o reg) on all data: 0.32647347\n",
      "Test loss (w/o reg) on all data: 0.4176966\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018924634\n",
      "Norm of the params: 3.0957348\n",
      "                Loss: fixed  19 labels. Loss 0.41770. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [264] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4635179\n",
      "Train loss (w/o reg) on all data: 0.46335196\n",
      "Test loss (w/o reg) on all data: 0.43341184\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019663733\n",
      "Norm of the params: 1.8217311\n",
      "              Random: fixed   8 labels. Loss 0.43341. Accuracy 0.822.\n",
      "### Flips: 30, rs: 25, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3417668\n",
      "Train loss (w/o reg) on all data: 0.3413119\n",
      "Test loss (w/o reg) on all data: 0.4175643\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00326357\n",
      "Norm of the params: 3.0163372\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.41756. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [195] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31309998\n",
      "Train loss (w/o reg) on all data: 0.31260565\n",
      "Test loss (w/o reg) on all data: 0.42748755\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022808292\n",
      "Norm of the params: 3.1443233\n",
      "                Loss: fixed  21 labels. Loss 0.42749. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [309] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45014355\n",
      "Train loss (w/o reg) on all data: 0.44997793\n",
      "Test loss (w/o reg) on all data: 0.44927487\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019370115\n",
      "Norm of the params: 1.8199313\n",
      "              Random: fixed  10 labels. Loss 0.44927. Accuracy 0.822.\n",
      "### Flips: 30, rs: 25, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [331] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31345752\n",
      "Train loss (w/o reg) on all data: 0.31288594\n",
      "Test loss (w/o reg) on all data: 0.4289851\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0003021677\n",
      "Norm of the params: 3.3810408\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.42899. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [293] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31309736\n",
      "Train loss (w/o reg) on all data: 0.31260285\n",
      "Test loss (w/o reg) on all data: 0.42800796\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00087464065\n",
      "Norm of the params: 3.1448998\n",
      "                Loss: fixed  21 labels. Loss 0.42801. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45678815\n",
      "Train loss (w/o reg) on all data: 0.45662677\n",
      "Test loss (w/o reg) on all data: 0.44000775\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0053646667\n",
      "Norm of the params: 1.7965095\n",
      "              Random: fixed  13 labels. Loss 0.44001. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5572364\n",
      "Train loss (w/o reg) on all data: 0.5571755\n",
      "Test loss (w/o reg) on all data: 0.4004253\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.004041312\n",
      "Norm of the params: 1.1032087\n",
      "Flipped loss: 0.40043. Accuracy: 0.867\n",
      "### Flips: 30, rs: 26, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5183124\n",
      "Train loss (w/o reg) on all data: 0.51820993\n",
      "Test loss (w/o reg) on all data: 0.38550347\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00058357144\n",
      "Norm of the params: 1.4313686\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.38550. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4767856\n",
      "Train loss (w/o reg) on all data: 0.4766477\n",
      "Test loss (w/o reg) on all data: 0.36834654\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0009748372\n",
      "Norm of the params: 1.6607717\n",
      "                Loss: fixed   8 labels. Loss 0.36835. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [208] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5460455\n",
      "Train loss (w/o reg) on all data: 0.54597193\n",
      "Test loss (w/o reg) on all data: 0.4002107\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.006517875\n",
      "Norm of the params: 1.212624\n",
      "              Random: fixed   3 labels. Loss 0.40021. Accuracy 0.844.\n",
      "### Flips: 30, rs: 26, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [152] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4905502\n",
      "Train loss (w/o reg) on all data: 0.49041677\n",
      "Test loss (w/o reg) on all data: 0.36167955\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0012930728\n",
      "Norm of the params: 1.633566\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.36168. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [316] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39176098\n",
      "Train loss (w/o reg) on all data: 0.39142492\n",
      "Test loss (w/o reg) on all data: 0.35199487\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0044552903\n",
      "Norm of the params: 2.5924928\n",
      "                Loss: fixed  16 labels. Loss 0.35199. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [301] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52988726\n",
      "Train loss (w/o reg) on all data: 0.5298062\n",
      "Test loss (w/o reg) on all data: 0.40423632\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010284394\n",
      "Norm of the params: 1.273366\n",
      "              Random: fixed   5 labels. Loss 0.40424. Accuracy 0.844.\n",
      "### Flips: 30, rs: 26, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [112] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4781005\n",
      "Train loss (w/o reg) on all data: 0.47795364\n",
      "Test loss (w/o reg) on all data: 0.36321428\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011149733\n",
      "Norm of the params: 1.713897\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.36321. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [353] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35347995\n",
      "Train loss (w/o reg) on all data: 0.35309643\n",
      "Test loss (w/o reg) on all data: 0.37366617\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008860785\n",
      "Norm of the params: 2.7696068\n",
      "                Loss: fixed  21 labels. Loss 0.37367. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52362376\n",
      "Train loss (w/o reg) on all data: 0.5235335\n",
      "Test loss (w/o reg) on all data: 0.39672554\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.007504562\n",
      "Norm of the params: 1.3436368\n",
      "              Random: fixed   6 labels. Loss 0.39673. Accuracy 0.844.\n",
      "### Flips: 30, rs: 26, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45957196\n",
      "Train loss (w/o reg) on all data: 0.45940608\n",
      "Test loss (w/o reg) on all data: 0.34942967\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0006773481\n",
      "Norm of the params: 1.821496\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.34943. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33583617\n",
      "Train loss (w/o reg) on all data: 0.33542863\n",
      "Test loss (w/o reg) on all data: 0.4016187\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017303952\n",
      "Norm of the params: 2.8549404\n",
      "                Loss: fixed  23 labels. Loss 0.40162. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51788646\n",
      "Train loss (w/o reg) on all data: 0.51779115\n",
      "Test loss (w/o reg) on all data: 0.40303093\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004551658\n",
      "Norm of the params: 1.3808028\n",
      "              Random: fixed   7 labels. Loss 0.40303. Accuracy 0.844.\n",
      "### Flips: 30, rs: 26, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [416] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40027517\n",
      "Train loss (w/o reg) on all data: 0.40000433\n",
      "Test loss (w/o reg) on all data: 0.36114806\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001039898\n",
      "Norm of the params: 2.3274179\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.36115. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33583054\n",
      "Train loss (w/o reg) on all data: 0.3354224\n",
      "Test loss (w/o reg) on all data: 0.4016792\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003642624\n",
      "Norm of the params: 2.857098\n",
      "                Loss: fixed  23 labels. Loss 0.40168. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [285] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48850796\n",
      "Train loss (w/o reg) on all data: 0.48837847\n",
      "Test loss (w/o reg) on all data: 0.38938907\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00066729577\n",
      "Norm of the params: 1.6093193\n",
      "              Random: fixed  10 labels. Loss 0.38939. Accuracy 0.844.\n",
      "### Flips: 30, rs: 26, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [341] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3700627\n",
      "Train loss (w/o reg) on all data: 0.36970907\n",
      "Test loss (w/o reg) on all data: 0.37907037\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001222421\n",
      "Norm of the params: 2.6595044\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.37907. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33583483\n",
      "Train loss (w/o reg) on all data: 0.33542994\n",
      "Test loss (w/o reg) on all data: 0.4017092\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0040675397\n",
      "Norm of the params: 2.8457055\n",
      "                Loss: fixed  23 labels. Loss 0.40171. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [280] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48645923\n",
      "Train loss (w/o reg) on all data: 0.48632902\n",
      "Test loss (w/o reg) on all data: 0.40181017\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0008115027\n",
      "Norm of the params: 1.6137153\n",
      "              Random: fixed  11 labels. Loss 0.40181. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.555765\n",
      "Train loss (w/o reg) on all data: 0.55567807\n",
      "Test loss (w/o reg) on all data: 0.4305259\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013875661\n",
      "Norm of the params: 1.3182087\n",
      "Flipped loss: 0.43053. Accuracy: 0.822\n",
      "### Flips: 30, rs: 27, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [91] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5186046\n",
      "Train loss (w/o reg) on all data: 0.5184806\n",
      "Test loss (w/o reg) on all data: 0.4002729\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00070837437\n",
      "Norm of the params: 1.5745494\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.40027. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47175574\n",
      "Train loss (w/o reg) on all data: 0.47155842\n",
      "Test loss (w/o reg) on all data: 0.3853326\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00068957213\n",
      "Norm of the params: 1.9864887\n",
      "                Loss: fixed   8 labels. Loss 0.38533. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55211496\n",
      "Train loss (w/o reg) on all data: 0.55203044\n",
      "Test loss (w/o reg) on all data: 0.4297642\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006168544\n",
      "Norm of the params: 1.30013\n",
      "              Random: fixed   1 labels. Loss 0.42976. Accuracy 0.822.\n",
      "### Flips: 30, rs: 27, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [128] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49032402\n",
      "Train loss (w/o reg) on all data: 0.49014515\n",
      "Test loss (w/o reg) on all data: 0.39483514\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.010085471\n",
      "Norm of the params: 1.8914319\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.39484. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44551244\n",
      "Train loss (w/o reg) on all data: 0.44526342\n",
      "Test loss (w/o reg) on all data: 0.35940886\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0023307519\n",
      "Norm of the params: 2.2317424\n",
      "                Loss: fixed  11 labels. Loss 0.35941. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [342] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55187637\n",
      "Train loss (w/o reg) on all data: 0.55180204\n",
      "Test loss (w/o reg) on all data: 0.42071825\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011504424\n",
      "Norm of the params: 1.2191502\n",
      "              Random: fixed   2 labels. Loss 0.42072. Accuracy 0.844.\n",
      "### Flips: 30, rs: 27, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [199] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47893322\n",
      "Train loss (w/o reg) on all data: 0.4787426\n",
      "Test loss (w/o reg) on all data: 0.38266543\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0028799584\n",
      "Norm of the params: 1.9524767\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.38267. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [130] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.424248\n",
      "Train loss (w/o reg) on all data: 0.42397365\n",
      "Test loss (w/o reg) on all data: 0.35249662\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0012601505\n",
      "Norm of the params: 2.3425212\n",
      "                Loss: fixed  14 labels. Loss 0.35250. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [293] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52767974\n",
      "Train loss (w/o reg) on all data: 0.52758324\n",
      "Test loss (w/o reg) on all data: 0.40496784\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0071396525\n",
      "Norm of the params: 1.3892777\n",
      "              Random: fixed   5 labels. Loss 0.40497. Accuracy 0.867.\n",
      "### Flips: 30, rs: 27, checks: 40\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4550688\n",
      "Train loss (w/o reg) on all data: 0.4548288\n",
      "Test loss (w/o reg) on all data: 0.37022623\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0020215902\n",
      "Norm of the params: 2.1909382\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.37023. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3947917\n",
      "Train loss (w/o reg) on all data: 0.39450008\n",
      "Test loss (w/o reg) on all data: 0.37927297\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00080230087\n",
      "Norm of the params: 2.4150472\n",
      "                Loss: fixed  19 labels. Loss 0.37927. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5276787\n",
      "Train loss (w/o reg) on all data: 0.52758133\n",
      "Test loss (w/o reg) on all data: 0.40486455\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0008556623\n",
      "Norm of the params: 1.3955564\n",
      "              Random: fixed   5 labels. Loss 0.40486. Accuracy 0.867.\n",
      "### Flips: 30, rs: 27, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4465926\n",
      "Train loss (w/o reg) on all data: 0.446352\n",
      "Test loss (w/o reg) on all data: 0.377391\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015410885\n",
      "Norm of the params: 2.1936154\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.37739. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [187] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.367186\n",
      "Train loss (w/o reg) on all data: 0.3668575\n",
      "Test loss (w/o reg) on all data: 0.38844326\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013723715\n",
      "Norm of the params: 2.5632124\n",
      "                Loss: fixed  23 labels. Loss 0.38844. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5269036\n",
      "Train loss (w/o reg) on all data: 0.5268028\n",
      "Test loss (w/o reg) on all data: 0.4017541\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0067249383\n",
      "Norm of the params: 1.4201666\n",
      "              Random: fixed   6 labels. Loss 0.40175. Accuracy 0.867.\n",
      "### Flips: 30, rs: 27, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [201] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4143123\n",
      "Train loss (w/o reg) on all data: 0.41400412\n",
      "Test loss (w/o reg) on all data: 0.36093408\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017247283\n",
      "Norm of the params: 2.4827259\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.36093. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [314] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35958046\n",
      "Train loss (w/o reg) on all data: 0.3591976\n",
      "Test loss (w/o reg) on all data: 0.3911607\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00041512708\n",
      "Norm of the params: 2.7672222\n",
      "                Loss: fixed  25 labels. Loss 0.39116. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [292] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52409434\n",
      "Train loss (w/o reg) on all data: 0.5239921\n",
      "Test loss (w/o reg) on all data: 0.3919128\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0027038031\n",
      "Norm of the params: 1.4299808\n",
      "              Random: fixed   7 labels. Loss 0.39191. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [135] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5684244\n",
      "Train loss (w/o reg) on all data: 0.5683408\n",
      "Test loss (w/o reg) on all data: 0.44455883\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017138476\n",
      "Norm of the params: 1.2930994\n",
      "Flipped loss: 0.44456. Accuracy: 0.800\n",
      "### Flips: 30, rs: 28, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53682846\n",
      "Train loss (w/o reg) on all data: 0.53671753\n",
      "Test loss (w/o reg) on all data: 0.41799352\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0016260942\n",
      "Norm of the params: 1.4893327\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.41799. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [123] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47630915\n",
      "Train loss (w/o reg) on all data: 0.4761379\n",
      "Test loss (w/o reg) on all data: 0.39277738\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0025641646\n",
      "Norm of the params: 1.8506622\n",
      "                Loss: fixed   8 labels. Loss 0.39278. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5669293\n",
      "Train loss (w/o reg) on all data: 0.56685036\n",
      "Test loss (w/o reg) on all data: 0.44789436\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00065984833\n",
      "Norm of the params: 1.2563779\n",
      "              Random: fixed   1 labels. Loss 0.44789. Accuracy 0.800.\n",
      "### Flips: 30, rs: 28, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [343] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45808527\n",
      "Train loss (w/o reg) on all data: 0.45788926\n",
      "Test loss (w/o reg) on all data: 0.38719288\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0016302952\n",
      "Norm of the params: 1.9799998\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.38719. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40813512\n",
      "Train loss (w/o reg) on all data: 0.40783757\n",
      "Test loss (w/o reg) on all data: 0.41516113\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.008757446\n",
      "Norm of the params: 2.4394364\n",
      "                Loss: fixed  16 labels. Loss 0.41516. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5597978\n",
      "Train loss (w/o reg) on all data: 0.55972564\n",
      "Test loss (w/o reg) on all data: 0.43216115\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0018296553\n",
      "Norm of the params: 1.2013774\n",
      "              Random: fixed   5 labels. Loss 0.43216. Accuracy 0.778.\n",
      "### Flips: 30, rs: 28, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [351] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42929265\n",
      "Train loss (w/o reg) on all data: 0.42904633\n",
      "Test loss (w/o reg) on all data: 0.3903205\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00060105487\n",
      "Norm of the params: 2.2194755\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.39032. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3641898\n",
      "Train loss (w/o reg) on all data: 0.3637909\n",
      "Test loss (w/o reg) on all data: 0.4331067\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0025344659\n",
      "Norm of the params: 2.8245077\n",
      "                Loss: fixed  21 labels. Loss 0.43311. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [269] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54588324\n",
      "Train loss (w/o reg) on all data: 0.54580027\n",
      "Test loss (w/o reg) on all data: 0.41480428\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.007595845\n",
      "Norm of the params: 1.2881352\n",
      "              Random: fixed   7 labels. Loss 0.41480. Accuracy 0.822.\n",
      "### Flips: 30, rs: 28, checks: 40\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4163738\n",
      "Train loss (w/o reg) on all data: 0.41610473\n",
      "Test loss (w/o reg) on all data: 0.38610235\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001386715\n",
      "Norm of the params: 2.3197258\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.38610. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35660863\n",
      "Train loss (w/o reg) on all data: 0.35619873\n",
      "Test loss (w/o reg) on all data: 0.43265903\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00053728744\n",
      "Norm of the params: 2.863222\n",
      "                Loss: fixed  23 labels. Loss 0.43266. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [447] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5375459\n",
      "Train loss (w/o reg) on all data: 0.5374595\n",
      "Test loss (w/o reg) on all data: 0.41346884\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011595172\n",
      "Norm of the params: 1.3149419\n",
      "              Random: fixed   8 labels. Loss 0.41347. Accuracy 0.822.\n",
      "### Flips: 30, rs: 28, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [282] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40518013\n",
      "Train loss (w/o reg) on all data: 0.4048866\n",
      "Test loss (w/o reg) on all data: 0.38110954\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00090866775\n",
      "Norm of the params: 2.4228714\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.38111. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35255456\n",
      "Train loss (w/o reg) on all data: 0.3521568\n",
      "Test loss (w/o reg) on all data: 0.4348834\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002386414\n",
      "Norm of the params: 2.8205073\n",
      "                Loss: fixed  25 labels. Loss 0.43488. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [389] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5321173\n",
      "Train loss (w/o reg) on all data: 0.532024\n",
      "Test loss (w/o reg) on all data: 0.41759518\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010387948\n",
      "Norm of the params: 1.3657404\n",
      "              Random: fixed  10 labels. Loss 0.41760. Accuracy 0.800.\n",
      "### Flips: 30, rs: 28, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39106268\n",
      "Train loss (w/o reg) on all data: 0.3907462\n",
      "Test loss (w/o reg) on all data: 0.3836283\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0009430628\n",
      "Norm of the params: 2.5158012\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.38363. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [332] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3510323\n",
      "Train loss (w/o reg) on all data: 0.3506256\n",
      "Test loss (w/o reg) on all data: 0.4319995\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018558513\n",
      "Norm of the params: 2.8519437\n",
      "                Loss: fixed  26 labels. Loss 0.43200. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [392] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52961206\n",
      "Train loss (w/o reg) on all data: 0.5295267\n",
      "Test loss (w/o reg) on all data: 0.40914565\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0047899997\n",
      "Norm of the params: 1.3063823\n",
      "              Random: fixed  12 labels. Loss 0.40915. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5058557\n",
      "Train loss (w/o reg) on all data: 0.505733\n",
      "Test loss (w/o reg) on all data: 0.44095942\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0025823421\n",
      "Norm of the params: 1.566264\n",
      "Flipped loss: 0.44096. Accuracy: 0.822\n",
      "### Flips: 30, rs: 29, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47737134\n",
      "Train loss (w/o reg) on all data: 0.47720596\n",
      "Test loss (w/o reg) on all data: 0.445211\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00081046484\n",
      "Norm of the params: 1.8186224\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.44521. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [145] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42480624\n",
      "Train loss (w/o reg) on all data: 0.4245765\n",
      "Test loss (w/o reg) on all data: 0.45989788\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009044852\n",
      "Norm of the params: 2.1436129\n",
      "                Loss: fixed   7 labels. Loss 0.45990. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47715887\n",
      "Train loss (w/o reg) on all data: 0.47700673\n",
      "Test loss (w/o reg) on all data: 0.4260682\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.010659973\n",
      "Norm of the params: 1.7443043\n",
      "              Random: fixed   4 labels. Loss 0.42607. Accuracy 0.822.\n",
      "### Flips: 30, rs: 29, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [184] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42290246\n",
      "Train loss (w/o reg) on all data: 0.42270336\n",
      "Test loss (w/o reg) on all data: 0.40732113\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012823931\n",
      "Norm of the params: 1.9955846\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.40732. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [332] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39042926\n",
      "Train loss (w/o reg) on all data: 0.39015645\n",
      "Test loss (w/o reg) on all data: 0.466831\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008002522\n",
      "Norm of the params: 2.335875\n",
      "                Loss: fixed  11 labels. Loss 0.46683. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [97] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46348643\n",
      "Train loss (w/o reg) on all data: 0.46331686\n",
      "Test loss (w/o reg) on all data: 0.4271708\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011948734\n",
      "Norm of the params: 1.8415406\n",
      "              Random: fixed   7 labels. Loss 0.42717. Accuracy 0.822.\n",
      "### Flips: 30, rs: 29, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [98] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41150615\n",
      "Train loss (w/o reg) on all data: 0.41128817\n",
      "Test loss (w/o reg) on all data: 0.40174472\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00085130596\n",
      "Norm of the params: 2.087883\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.40174. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33797792\n",
      "Train loss (w/o reg) on all data: 0.33759034\n",
      "Test loss (w/o reg) on all data: 0.5114479\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016123783\n",
      "Norm of the params: 2.7842038\n",
      "                Loss: fixed  18 labels. Loss 0.51145. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4634823\n",
      "Train loss (w/o reg) on all data: 0.46331394\n",
      "Test loss (w/o reg) on all data: 0.4268019\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00058003975\n",
      "Norm of the params: 1.8350347\n",
      "              Random: fixed   7 labels. Loss 0.42680. Accuracy 0.822.\n",
      "### Flips: 30, rs: 29, checks: 40\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39036512\n",
      "Train loss (w/o reg) on all data: 0.39006737\n",
      "Test loss (w/o reg) on all data: 0.41224992\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002719308\n",
      "Norm of the params: 2.4403255\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.41225. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [318] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33631092\n",
      "Train loss (w/o reg) on all data: 0.33593258\n",
      "Test loss (w/o reg) on all data: 0.48589295\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010790613\n",
      "Norm of the params: 2.7508342\n",
      "                Loss: fixed  19 labels. Loss 0.48589. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46116516\n",
      "Train loss (w/o reg) on all data: 0.46102363\n",
      "Test loss (w/o reg) on all data: 0.42157954\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00032381527\n",
      "Norm of the params: 1.6823969\n",
      "              Random: fixed   9 labels. Loss 0.42158. Accuracy 0.822.\n",
      "### Flips: 30, rs: 29, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [367] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39312416\n",
      "Train loss (w/o reg) on all data: 0.39283562\n",
      "Test loss (w/o reg) on all data: 0.40782097\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012115198\n",
      "Norm of the params: 2.4022343\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.40782. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32882333\n",
      "Train loss (w/o reg) on all data: 0.32846385\n",
      "Test loss (w/o reg) on all data: 0.46224913\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00029054674\n",
      "Norm of the params: 2.681286\n",
      "                Loss: fixed  22 labels. Loss 0.46225. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [179] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46144187\n",
      "Train loss (w/o reg) on all data: 0.46130168\n",
      "Test loss (w/o reg) on all data: 0.3851479\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0008370652\n",
      "Norm of the params: 1.674493\n",
      "              Random: fixed  12 labels. Loss 0.38515. Accuracy 0.844.\n",
      "### Flips: 30, rs: 29, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [310] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38057402\n",
      "Train loss (w/o reg) on all data: 0.3802464\n",
      "Test loss (w/o reg) on all data: 0.4210499\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005898812\n",
      "Norm of the params: 2.5597997\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.42105. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [306] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32882467\n",
      "Train loss (w/o reg) on all data: 0.32846677\n",
      "Test loss (w/o reg) on all data: 0.4620013\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009409182\n",
      "Norm of the params: 2.675443\n",
      "                Loss: fixed  22 labels. Loss 0.46200. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46144286\n",
      "Train loss (w/o reg) on all data: 0.4613029\n",
      "Test loss (w/o reg) on all data: 0.38494217\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0028679718\n",
      "Norm of the params: 1.6730385\n",
      "              Random: fixed  12 labels. Loss 0.38494. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [288] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53000754\n",
      "Train loss (w/o reg) on all data: 0.5299314\n",
      "Test loss (w/o reg) on all data: 0.44933853\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.007644618\n",
      "Norm of the params: 1.2340395\n",
      "Flipped loss: 0.44934. Accuracy: 0.733\n",
      "### Flips: 30, rs: 30, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [110] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4984821\n",
      "Train loss (w/o reg) on all data: 0.49840134\n",
      "Test loss (w/o reg) on all data: 0.3972115\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00061137247\n",
      "Norm of the params: 1.2709156\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.39721. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [117] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46271488\n",
      "Train loss (w/o reg) on all data: 0.4625798\n",
      "Test loss (w/o reg) on all data: 0.40010253\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0022741328\n",
      "Norm of the params: 1.6436487\n",
      "                Loss: fixed   7 labels. Loss 0.40010. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [165] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5277343\n",
      "Train loss (w/o reg) on all data: 0.5276576\n",
      "Test loss (w/o reg) on all data: 0.44003746\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.00090382533\n",
      "Norm of the params: 1.2383077\n",
      "              Random: fixed   1 labels. Loss 0.44004. Accuracy 0.733.\n",
      "### Flips: 30, rs: 30, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [299] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46207747\n",
      "Train loss (w/o reg) on all data: 0.4619564\n",
      "Test loss (w/o reg) on all data: 0.38295564\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013506964\n",
      "Norm of the params: 1.5559717\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.38296. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4176955\n",
      "Train loss (w/o reg) on all data: 0.41751024\n",
      "Test loss (w/o reg) on all data: 0.39429614\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0063407817\n",
      "Norm of the params: 1.9248648\n",
      "                Loss: fixed  12 labels. Loss 0.39430. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52772796\n",
      "Train loss (w/o reg) on all data: 0.52765137\n",
      "Test loss (w/o reg) on all data: 0.43937108\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.00092673686\n",
      "Norm of the params: 1.2377203\n",
      "              Random: fixed   1 labels. Loss 0.43937. Accuracy 0.733.\n",
      "### Flips: 30, rs: 30, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45030308\n",
      "Train loss (w/o reg) on all data: 0.4501712\n",
      "Test loss (w/o reg) on all data: 0.39799592\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00047510403\n",
      "Norm of the params: 1.6239783\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.39800. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4002069\n",
      "Train loss (w/o reg) on all data: 0.3999623\n",
      "Test loss (w/o reg) on all data: 0.41106927\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.004271353\n",
      "Norm of the params: 2.2117698\n",
      "                Loss: fixed  14 labels. Loss 0.41107. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5304128\n",
      "Train loss (w/o reg) on all data: 0.5303397\n",
      "Test loss (w/o reg) on all data: 0.42530414\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0018703634\n",
      "Norm of the params: 1.2088387\n",
      "              Random: fixed   2 labels. Loss 0.42530. Accuracy 0.756.\n",
      "### Flips: 30, rs: 30, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [350] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4252177\n",
      "Train loss (w/o reg) on all data: 0.42504188\n",
      "Test loss (w/o reg) on all data: 0.40596694\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0049430085\n",
      "Norm of the params: 1.8751806\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.40597. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [126] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39136347\n",
      "Train loss (w/o reg) on all data: 0.39112788\n",
      "Test loss (w/o reg) on all data: 0.42725536\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0008319327\n",
      "Norm of the params: 2.1707222\n",
      "                Loss: fixed  16 labels. Loss 0.42726. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50640136\n",
      "Train loss (w/o reg) on all data: 0.5062913\n",
      "Test loss (w/o reg) on all data: 0.42501178\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0032097239\n",
      "Norm of the params: 1.483848\n",
      "              Random: fixed   5 labels. Loss 0.42501. Accuracy 0.733.\n",
      "### Flips: 30, rs: 30, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [264] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40895817\n",
      "Train loss (w/o reg) on all data: 0.40876767\n",
      "Test loss (w/o reg) on all data: 0.41068116\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00074242847\n",
      "Norm of the params: 1.9519624\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.41068. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37386486\n",
      "Train loss (w/o reg) on all data: 0.37365615\n",
      "Test loss (w/o reg) on all data: 0.441493\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00039845053\n",
      "Norm of the params: 2.043102\n",
      "                Loss: fixed  20 labels. Loss 0.44149. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49970415\n",
      "Train loss (w/o reg) on all data: 0.49959552\n",
      "Test loss (w/o reg) on all data: 0.43068585\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0029496194\n",
      "Norm of the params: 1.4739907\n",
      "              Random: fixed   6 labels. Loss 0.43069. Accuracy 0.733.\n",
      "### Flips: 30, rs: 30, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3993262\n",
      "Train loss (w/o reg) on all data: 0.39910352\n",
      "Test loss (w/o reg) on all data: 0.4186601\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.002754043\n",
      "Norm of the params: 2.1104002\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.41866. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36756524\n",
      "Train loss (w/o reg) on all data: 0.36732802\n",
      "Test loss (w/o reg) on all data: 0.4471777\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00052212575\n",
      "Norm of the params: 2.1782277\n",
      "                Loss: fixed  21 labels. Loss 0.44718. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [350] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48878863\n",
      "Train loss (w/o reg) on all data: 0.48866704\n",
      "Test loss (w/o reg) on all data: 0.41129154\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001457695\n",
      "Norm of the params: 1.5594506\n",
      "              Random: fixed   8 labels. Loss 0.41129. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52539563\n",
      "Train loss (w/o reg) on all data: 0.5253333\n",
      "Test loss (w/o reg) on all data: 0.49974442\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019027536\n",
      "Norm of the params: 1.1164937\n",
      "Flipped loss: 0.49974. Accuracy: 0.800\n",
      "### Flips: 30, rs: 31, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [163] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48813114\n",
      "Train loss (w/o reg) on all data: 0.48798874\n",
      "Test loss (w/o reg) on all data: 0.51183885\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0008558366\n",
      "Norm of the params: 1.6875879\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.51184. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44599617\n",
      "Train loss (w/o reg) on all data: 0.4458752\n",
      "Test loss (w/o reg) on all data: 0.5130809\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019634136\n",
      "Norm of the params: 1.5554625\n",
      "                Loss: fixed   7 labels. Loss 0.51308. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [167] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5122646\n",
      "Train loss (w/o reg) on all data: 0.51219237\n",
      "Test loss (w/o reg) on all data: 0.5016769\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010426136\n",
      "Norm of the params: 1.2020323\n",
      "              Random: fixed   1 labels. Loss 0.50168. Accuracy 0.800.\n",
      "### Flips: 30, rs: 31, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43864015\n",
      "Train loss (w/o reg) on all data: 0.43851313\n",
      "Test loss (w/o reg) on all data: 0.49498415\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019364916\n",
      "Norm of the params: 1.5938219\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.49498. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [124] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41355905\n",
      "Train loss (w/o reg) on all data: 0.41342765\n",
      "Test loss (w/o reg) on all data: 0.49984834\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00057682703\n",
      "Norm of the params: 1.6210929\n",
      "                Loss: fixed  11 labels. Loss 0.49985. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [122] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51225775\n",
      "Train loss (w/o reg) on all data: 0.5121865\n",
      "Test loss (w/o reg) on all data: 0.50138444\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0032856613\n",
      "Norm of the params: 1.1937618\n",
      "              Random: fixed   1 labels. Loss 0.50138. Accuracy 0.800.\n",
      "### Flips: 30, rs: 31, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [140] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4242063\n",
      "Train loss (w/o reg) on all data: 0.42408234\n",
      "Test loss (w/o reg) on all data: 0.47508103\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011832564\n",
      "Norm of the params: 1.5745192\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.47508. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3841371\n",
      "Train loss (w/o reg) on all data: 0.38391942\n",
      "Test loss (w/o reg) on all data: 0.50644404\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.007970619\n",
      "Norm of the params: 2.0864992\n",
      "                Loss: fixed  15 labels. Loss 0.50644. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5143415\n",
      "Train loss (w/o reg) on all data: 0.51427037\n",
      "Test loss (w/o reg) on all data: 0.48182046\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014352084\n",
      "Norm of the params: 1.192575\n",
      "              Random: fixed   2 labels. Loss 0.48182. Accuracy 0.800.\n",
      "### Flips: 30, rs: 31, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3938802\n",
      "Train loss (w/o reg) on all data: 0.39370763\n",
      "Test loss (w/o reg) on all data: 0.44390184\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022594165\n",
      "Norm of the params: 1.8577939\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.44390. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36327586\n",
      "Train loss (w/o reg) on all data: 0.36303976\n",
      "Test loss (w/o reg) on all data: 0.5184691\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010350986\n",
      "Norm of the params: 2.1730528\n",
      "                Loss: fixed  19 labels. Loss 0.51847. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [313] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5009623\n",
      "Train loss (w/o reg) on all data: 0.50085604\n",
      "Test loss (w/o reg) on all data: 0.48229983\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0024086712\n",
      "Norm of the params: 1.4579022\n",
      "              Random: fixed   5 labels. Loss 0.48230. Accuracy 0.800.\n",
      "### Flips: 30, rs: 31, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37005877\n",
      "Train loss (w/o reg) on all data: 0.36984706\n",
      "Test loss (w/o reg) on all data: 0.45452246\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010560241\n",
      "Norm of the params: 2.0576828\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.45452. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3511576\n",
      "Train loss (w/o reg) on all data: 0.35089493\n",
      "Test loss (w/o reg) on all data: 0.49360487\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00035414225\n",
      "Norm of the params: 2.2920656\n",
      "                Loss: fixed  21 labels. Loss 0.49360. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48568404\n",
      "Train loss (w/o reg) on all data: 0.48557606\n",
      "Test loss (w/o reg) on all data: 0.48457417\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011643134\n",
      "Norm of the params: 1.4695292\n",
      "              Random: fixed   7 labels. Loss 0.48457. Accuracy 0.778.\n",
      "### Flips: 30, rs: 31, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37006024\n",
      "Train loss (w/o reg) on all data: 0.36984888\n",
      "Test loss (w/o reg) on all data: 0.45432833\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006123498\n",
      "Norm of the params: 2.055988\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.45433. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34789345\n",
      "Train loss (w/o reg) on all data: 0.34762898\n",
      "Test loss (w/o reg) on all data: 0.468171\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0028730384\n",
      "Norm of the params: 2.2999082\n",
      "                Loss: fixed  23 labels. Loss 0.46817. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45818695\n",
      "Train loss (w/o reg) on all data: 0.45804107\n",
      "Test loss (w/o reg) on all data: 0.50234574\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00058669806\n",
      "Norm of the params: 1.7081014\n",
      "              Random: fixed  10 labels. Loss 0.50235. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [291] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5205067\n",
      "Train loss (w/o reg) on all data: 0.5204034\n",
      "Test loss (w/o reg) on all data: 0.38583264\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013973564\n",
      "Norm of the params: 1.4373473\n",
      "Flipped loss: 0.38583. Accuracy: 0.822\n",
      "### Flips: 30, rs: 32, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [57] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4611109\n",
      "Train loss (w/o reg) on all data: 0.46094736\n",
      "Test loss (w/o reg) on all data: 0.35112488\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00297771\n",
      "Norm of the params: 1.8083798\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.35112. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [135] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40195775\n",
      "Train loss (w/o reg) on all data: 0.40165925\n",
      "Test loss (w/o reg) on all data: 0.3771901\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0032753733\n",
      "Norm of the params: 2.4433637\n",
      "                Loss: fixed   8 labels. Loss 0.37719. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5130012\n",
      "Train loss (w/o reg) on all data: 0.5128873\n",
      "Test loss (w/o reg) on all data: 0.37509266\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0021478508\n",
      "Norm of the params: 1.5093677\n",
      "              Random: fixed   1 labels. Loss 0.37509. Accuracy 0.844.\n",
      "### Flips: 30, rs: 32, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [326] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4054908\n",
      "Train loss (w/o reg) on all data: 0.40517423\n",
      "Test loss (w/o reg) on all data: 0.36509657\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0016147116\n",
      "Norm of the params: 2.5161963\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.36510. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [343] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33333614\n",
      "Train loss (w/o reg) on all data: 0.33284765\n",
      "Test loss (w/o reg) on all data: 0.43650734\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0003600205\n",
      "Norm of the params: 3.12566\n",
      "                Loss: fixed  14 labels. Loss 0.43651. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [378] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5148297\n",
      "Train loss (w/o reg) on all data: 0.5147144\n",
      "Test loss (w/o reg) on all data: 0.3727793\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0012163508\n",
      "Norm of the params: 1.5184852\n",
      "              Random: fixed   3 labels. Loss 0.37278. Accuracy 0.844.\n",
      "### Flips: 30, rs: 32, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39578646\n",
      "Train loss (w/o reg) on all data: 0.395466\n",
      "Test loss (w/o reg) on all data: 0.377957\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00046229167\n",
      "Norm of the params: 2.531612\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.37796. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [295] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3233017\n",
      "Train loss (w/o reg) on all data: 0.32277924\n",
      "Test loss (w/o reg) on all data: 0.4107113\n",
      "Train acc on all data:  0.8820754716981132\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010007323\n",
      "Norm of the params: 3.232551\n",
      "                Loss: fixed  16 labels. Loss 0.41071. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [388] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48661098\n",
      "Train loss (w/o reg) on all data: 0.4864466\n",
      "Test loss (w/o reg) on all data: 0.36324972\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0061689666\n",
      "Norm of the params: 1.8131937\n",
      "              Random: fixed   5 labels. Loss 0.36325. Accuracy 0.822.\n",
      "### Flips: 30, rs: 32, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [354] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38380918\n",
      "Train loss (w/o reg) on all data: 0.38349295\n",
      "Test loss (w/o reg) on all data: 0.3966938\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0033674096\n",
      "Norm of the params: 2.514872\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.39669. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [378] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3160164\n",
      "Train loss (w/o reg) on all data: 0.31546834\n",
      "Test loss (w/o reg) on all data: 0.42878887\n",
      "Train acc on all data:  0.8867924528301887\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002655736\n",
      "Norm of the params: 3.3108244\n",
      "                Loss: fixed  18 labels. Loss 0.42879. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [329] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4866092\n",
      "Train loss (w/o reg) on all data: 0.48644602\n",
      "Test loss (w/o reg) on all data: 0.3639429\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0023671559\n",
      "Norm of the params: 1.8064368\n",
      "              Random: fixed   5 labels. Loss 0.36394. Accuracy 0.822.\n",
      "### Flips: 30, rs: 32, checks: 50\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36930588\n",
      "Train loss (w/o reg) on all data: 0.36893028\n",
      "Test loss (w/o reg) on all data: 0.39008856\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00046536716\n",
      "Norm of the params: 2.7407866\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.39009. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [326] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3125637\n",
      "Train loss (w/o reg) on all data: 0.31208462\n",
      "Test loss (w/o reg) on all data: 0.42522338\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005370498\n",
      "Norm of the params: 3.0953836\n",
      "                Loss: fixed  20 labels. Loss 0.42522. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46855265\n",
      "Train loss (w/o reg) on all data: 0.46834582\n",
      "Test loss (w/o reg) on all data: 0.35513428\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0020328467\n",
      "Norm of the params: 2.0338142\n",
      "              Random: fixed   8 labels. Loss 0.35513. Accuracy 0.822.\n",
      "### Flips: 30, rs: 32, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34438556\n",
      "Train loss (w/o reg) on all data: 0.3439648\n",
      "Test loss (w/o reg) on all data: 0.42995185\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013974032\n",
      "Norm of the params: 2.9009943\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.42995. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31334266\n",
      "Train loss (w/o reg) on all data: 0.3128513\n",
      "Test loss (w/o reg) on all data: 0.4286178\n",
      "Train acc on all data:  0.8962264150943396\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0028018644\n",
      "Norm of the params: 3.1347754\n",
      "                Loss: fixed  23 labels. Loss 0.42862. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4643574\n",
      "Train loss (w/o reg) on all data: 0.464147\n",
      "Test loss (w/o reg) on all data: 0.35040125\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.000537946\n",
      "Norm of the params: 2.0514266\n",
      "              Random: fixed  11 labels. Loss 0.35040. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [280] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5519364\n",
      "Train loss (w/o reg) on all data: 0.55184865\n",
      "Test loss (w/o reg) on all data: 0.42821968\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0046638483\n",
      "Norm of the params: 1.3248724\n",
      "Flipped loss: 0.42822. Accuracy: 0.800\n",
      "### Flips: 30, rs: 33, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45399356\n",
      "Train loss (w/o reg) on all data: 0.45381758\n",
      "Test loss (w/o reg) on all data: 0.3872571\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006221345\n",
      "Norm of the params: 1.8760049\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.38726. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [125] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42180806\n",
      "Train loss (w/o reg) on all data: 0.42157757\n",
      "Test loss (w/o reg) on all data: 0.38275787\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.005890467\n",
      "Norm of the params: 2.1470547\n",
      "                Loss: fixed   9 labels. Loss 0.38276. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [146] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55344814\n",
      "Train loss (w/o reg) on all data: 0.5533654\n",
      "Test loss (w/o reg) on all data: 0.4328304\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0036615022\n",
      "Norm of the params: 1.286358\n",
      "              Random: fixed   1 labels. Loss 0.43283. Accuracy 0.778.\n",
      "### Flips: 30, rs: 33, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [297] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42350832\n",
      "Train loss (w/o reg) on all data: 0.4232634\n",
      "Test loss (w/o reg) on all data: 0.36376488\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0020828506\n",
      "Norm of the params: 2.2132058\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.36376. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [157] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3571387\n",
      "Train loss (w/o reg) on all data: 0.35676864\n",
      "Test loss (w/o reg) on all data: 0.38168564\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.000893157\n",
      "Norm of the params: 2.7205238\n",
      "                Loss: fixed  14 labels. Loss 0.38169. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [308] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5487558\n",
      "Train loss (w/o reg) on all data: 0.5486536\n",
      "Test loss (w/o reg) on all data: 0.4309094\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0029457097\n",
      "Norm of the params: 1.4297429\n",
      "              Random: fixed   3 labels. Loss 0.43091. Accuracy 0.800.\n",
      "### Flips: 30, rs: 33, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [323] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40067482\n",
      "Train loss (w/o reg) on all data: 0.40038785\n",
      "Test loss (w/o reg) on all data: 0.38786846\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00035377246\n",
      "Norm of the params: 2.395728\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.38787. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [264] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32976738\n",
      "Train loss (w/o reg) on all data: 0.32928553\n",
      "Test loss (w/o reg) on all data: 0.37671322\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0001817032\n",
      "Norm of the params: 3.1043115\n",
      "                Loss: fixed  17 labels. Loss 0.37671. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [258] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55085254\n",
      "Train loss (w/o reg) on all data: 0.5507618\n",
      "Test loss (w/o reg) on all data: 0.43015447\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012083623\n",
      "Norm of the params: 1.3471894\n",
      "              Random: fixed   4 labels. Loss 0.43015. Accuracy 0.800.\n",
      "### Flips: 30, rs: 33, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3804308\n",
      "Train loss (w/o reg) on all data: 0.38009405\n",
      "Test loss (w/o reg) on all data: 0.39033717\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008491262\n",
      "Norm of the params: 2.5951602\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.39034. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [291] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.30733657\n",
      "Train loss (w/o reg) on all data: 0.30679882\n",
      "Test loss (w/o reg) on all data: 0.3886797\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0020733683\n",
      "Norm of the params: 3.2794645\n",
      "                Loss: fixed  21 labels. Loss 0.38868. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [353] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5508513\n",
      "Train loss (w/o reg) on all data: 0.55076104\n",
      "Test loss (w/o reg) on all data: 0.43090397\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013979189\n",
      "Norm of the params: 1.343429\n",
      "              Random: fixed   4 labels. Loss 0.43090. Accuracy 0.800.\n",
      "### Flips: 30, rs: 33, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [416] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37796944\n",
      "Train loss (w/o reg) on all data: 0.37762845\n",
      "Test loss (w/o reg) on all data: 0.3888895\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012656157\n",
      "Norm of the params: 2.6114845\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.38889. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [315] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3073366\n",
      "Train loss (w/o reg) on all data: 0.30679983\n",
      "Test loss (w/o reg) on all data: 0.38865423\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0026966278\n",
      "Norm of the params: 3.2764802\n",
      "                Loss: fixed  21 labels. Loss 0.38865. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [397] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5185555\n",
      "Train loss (w/o reg) on all data: 0.5184369\n",
      "Test loss (w/o reg) on all data: 0.39563805\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0028524543\n",
      "Norm of the params: 1.5402808\n",
      "              Random: fixed   8 labels. Loss 0.39564. Accuracy 0.844.\n",
      "### Flips: 30, rs: 33, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [466] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35095286\n",
      "Train loss (w/o reg) on all data: 0.35054603\n",
      "Test loss (w/o reg) on all data: 0.38951764\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00087630615\n",
      "Norm of the params: 2.852499\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.38952. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [201] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.30269808\n",
      "Train loss (w/o reg) on all data: 0.3021761\n",
      "Test loss (w/o reg) on all data: 0.404068\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00034438082\n",
      "Norm of the params: 3.2310443\n",
      "                Loss: fixed  22 labels. Loss 0.40407. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [352] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5185545\n",
      "Train loss (w/o reg) on all data: 0.51843655\n",
      "Test loss (w/o reg) on all data: 0.39590105\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.010495815\n",
      "Norm of the params: 1.5359304\n",
      "              Random: fixed   8 labels. Loss 0.39590. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [226] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55545926\n",
      "Train loss (w/o reg) on all data: 0.5553743\n",
      "Test loss (w/o reg) on all data: 0.41212553\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0019169279\n",
      "Norm of the params: 1.3033336\n",
      "Flipped loss: 0.41213. Accuracy: 0.844\n",
      "### Flips: 30, rs: 34, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [135] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48150113\n",
      "Train loss (w/o reg) on all data: 0.4813682\n",
      "Test loss (w/o reg) on all data: 0.39061087\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0056798323\n",
      "Norm of the params: 1.6304374\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.39061. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4768885\n",
      "Train loss (w/o reg) on all data: 0.47673446\n",
      "Test loss (w/o reg) on all data: 0.37130126\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004183729\n",
      "Norm of the params: 1.7552851\n",
      "                Loss: fixed   7 labels. Loss 0.37130. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54404324\n",
      "Train loss (w/o reg) on all data: 0.54395217\n",
      "Test loss (w/o reg) on all data: 0.39651138\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00082859886\n",
      "Norm of the params: 1.3495104\n",
      "              Random: fixed   1 labels. Loss 0.39651. Accuracy 0.867.\n",
      "### Flips: 30, rs: 34, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44434384\n",
      "Train loss (w/o reg) on all data: 0.44417003\n",
      "Test loss (w/o reg) on all data: 0.38192004\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008962403\n",
      "Norm of the params: 1.8644485\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.38192. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42782173\n",
      "Train loss (w/o reg) on all data: 0.42759386\n",
      "Test loss (w/o reg) on all data: 0.36000642\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0088774\n",
      "Norm of the params: 2.134855\n",
      "                Loss: fixed  13 labels. Loss 0.36001. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [184] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5316767\n",
      "Train loss (w/o reg) on all data: 0.53155035\n",
      "Test loss (w/o reg) on all data: 0.40201828\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004128282\n",
      "Norm of the params: 1.5896928\n",
      "              Random: fixed   4 labels. Loss 0.40202. Accuracy 0.844.\n",
      "### Flips: 30, rs: 34, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41915774\n",
      "Train loss (w/o reg) on all data: 0.41893646\n",
      "Test loss (w/o reg) on all data: 0.37226135\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0007483348\n",
      "Norm of the params: 2.1036804\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.37226. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [173] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38410917\n",
      "Train loss (w/o reg) on all data: 0.38386592\n",
      "Test loss (w/o reg) on all data: 0.34291282\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00024220382\n",
      "Norm of the params: 2.2056077\n",
      "                Loss: fixed  18 labels. Loss 0.34291. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [114] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5316826\n",
      "Train loss (w/o reg) on all data: 0.531558\n",
      "Test loss (w/o reg) on all data: 0.4033157\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.008936475\n",
      "Norm of the params: 1.578774\n",
      "              Random: fixed   4 labels. Loss 0.40332. Accuracy 0.844.\n",
      "### Flips: 30, rs: 34, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [151] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41686764\n",
      "Train loss (w/o reg) on all data: 0.41663304\n",
      "Test loss (w/o reg) on all data: 0.37285155\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006585648\n",
      "Norm of the params: 2.166066\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.37285. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36204055\n",
      "Train loss (w/o reg) on all data: 0.36174077\n",
      "Test loss (w/o reg) on all data: 0.3533487\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002826239\n",
      "Norm of the params: 2.44861\n",
      "                Loss: fixed  22 labels. Loss 0.35335. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52846897\n",
      "Train loss (w/o reg) on all data: 0.52835524\n",
      "Test loss (w/o reg) on all data: 0.39322442\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00044434785\n",
      "Norm of the params: 1.5080283\n",
      "              Random: fixed   7 labels. Loss 0.39322. Accuracy 0.822.\n",
      "### Flips: 30, rs: 34, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3943713\n",
      "Train loss (w/o reg) on all data: 0.39414912\n",
      "Test loss (w/o reg) on all data: 0.37375188\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0008435807\n",
      "Norm of the params: 2.1080086\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.37375. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [290] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34957188\n",
      "Train loss (w/o reg) on all data: 0.34925115\n",
      "Test loss (w/o reg) on all data: 0.3955798\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016945297\n",
      "Norm of the params: 2.5327554\n",
      "                Loss: fixed  25 labels. Loss 0.39558. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [152] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5183774\n",
      "Train loss (w/o reg) on all data: 0.51826155\n",
      "Test loss (w/o reg) on all data: 0.3978447\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.010750618\n",
      "Norm of the params: 1.5222107\n",
      "              Random: fixed   8 labels. Loss 0.39784. Accuracy 0.822.\n",
      "### Flips: 30, rs: 34, checks: 60\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.381646\n",
      "Train loss (w/o reg) on all data: 0.3814153\n",
      "Test loss (w/o reg) on all data: 0.37936231\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011951434\n",
      "Norm of the params: 2.1479545\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.37936. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [305] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34513727\n",
      "Train loss (w/o reg) on all data: 0.34480545\n",
      "Test loss (w/o reg) on all data: 0.40128207\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001357209\n",
      "Norm of the params: 2.5761256\n",
      "                Loss: fixed  26 labels. Loss 0.40128. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [307] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5135043\n",
      "Train loss (w/o reg) on all data: 0.51337916\n",
      "Test loss (w/o reg) on all data: 0.3867337\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.008942585\n",
      "Norm of the params: 1.5820925\n",
      "              Random: fixed   9 labels. Loss 0.38673. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52034944\n",
      "Train loss (w/o reg) on all data: 0.5202211\n",
      "Test loss (w/o reg) on all data: 0.45053464\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001558172\n",
      "Norm of the params: 1.6021061\n",
      "Flipped loss: 0.45053. Accuracy: 0.778\n",
      "### Flips: 30, rs: 35, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47145778\n",
      "Train loss (w/o reg) on all data: 0.47124287\n",
      "Test loss (w/o reg) on all data: 0.47590956\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0018774264\n",
      "Norm of the params: 2.0731568\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.47591. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40635148\n",
      "Train loss (w/o reg) on all data: 0.40604147\n",
      "Test loss (w/o reg) on all data: 0.4735331\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0008065028\n",
      "Norm of the params: 2.4899487\n",
      "                Loss: fixed   9 labels. Loss 0.47353. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [280] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5105317\n",
      "Train loss (w/o reg) on all data: 0.5103873\n",
      "Test loss (w/o reg) on all data: 0.44795883\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00050047867\n",
      "Norm of the params: 1.6995246\n",
      "              Random: fixed   1 labels. Loss 0.44796. Accuracy 0.778.\n",
      "### Flips: 30, rs: 35, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4203636\n",
      "Train loss (w/o reg) on all data: 0.42010617\n",
      "Test loss (w/o reg) on all data: 0.43473196\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.00069522916\n",
      "Norm of the params: 2.2691238\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.43473. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [346] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35681367\n",
      "Train loss (w/o reg) on all data: 0.35639682\n",
      "Test loss (w/o reg) on all data: 0.4653851\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0007127793\n",
      "Norm of the params: 2.887341\n",
      "                Loss: fixed  13 labels. Loss 0.46539. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [341] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5038055\n",
      "Train loss (w/o reg) on all data: 0.5036605\n",
      "Test loss (w/o reg) on all data: 0.44002604\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.010852659\n",
      "Norm of the params: 1.7028965\n",
      "              Random: fixed   2 labels. Loss 0.44003. Accuracy 0.756.\n",
      "### Flips: 30, rs: 35, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37497738\n",
      "Train loss (w/o reg) on all data: 0.37462664\n",
      "Test loss (w/o reg) on all data: 0.45353073\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.002939837\n",
      "Norm of the params: 2.6486058\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.45353. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [290] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32972482\n",
      "Train loss (w/o reg) on all data: 0.32910413\n",
      "Test loss (w/o reg) on all data: 0.53499556\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00047954608\n",
      "Norm of the params: 3.5232913\n",
      "                Loss: fixed  16 labels. Loss 0.53500. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [293] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50362223\n",
      "Train loss (w/o reg) on all data: 0.503486\n",
      "Test loss (w/o reg) on all data: 0.43419865\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010906616\n",
      "Norm of the params: 1.6507065\n",
      "              Random: fixed   3 labels. Loss 0.43420. Accuracy 0.800.\n",
      "### Flips: 30, rs: 35, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [347] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36990106\n",
      "Train loss (w/o reg) on all data: 0.36955178\n",
      "Test loss (w/o reg) on all data: 0.43802944\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00033592497\n",
      "Norm of the params: 2.643032\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.43803. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [341] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3297244\n",
      "Train loss (w/o reg) on all data: 0.32910696\n",
      "Test loss (w/o reg) on all data: 0.5344748\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0007847429\n",
      "Norm of the params: 3.514059\n",
      "                Loss: fixed  16 labels. Loss 0.53447. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [339] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5058741\n",
      "Train loss (w/o reg) on all data: 0.5057583\n",
      "Test loss (w/o reg) on all data: 0.4322327\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004695689\n",
      "Norm of the params: 1.522098\n",
      "              Random: fixed   5 labels. Loss 0.43223. Accuracy 0.800.\n",
      "### Flips: 30, rs: 35, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [324] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3301075\n",
      "Train loss (w/o reg) on all data: 0.32961166\n",
      "Test loss (w/o reg) on all data: 0.46817932\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0039067827\n",
      "Norm of the params: 3.1491332\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.46818. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31828678\n",
      "Train loss (w/o reg) on all data: 0.3177402\n",
      "Test loss (w/o reg) on all data: 0.5230514\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.001456732\n",
      "Norm of the params: 3.3062654\n",
      "                Loss: fixed  20 labels. Loss 0.52305. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [337] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5220615\n",
      "Train loss (w/o reg) on all data: 0.5219701\n",
      "Test loss (w/o reg) on all data: 0.42428094\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00071734807\n",
      "Norm of the params: 1.352495\n",
      "              Random: fixed   8 labels. Loss 0.42428. Accuracy 0.822.\n",
      "### Flips: 30, rs: 35, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [401] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31249946\n",
      "Train loss (w/o reg) on all data: 0.3119347\n",
      "Test loss (w/o reg) on all data: 0.49521866\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0009845325\n",
      "Norm of the params: 3.3607805\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.49522. Accuracy 0.756.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3165938\n",
      "Train loss (w/o reg) on all data: 0.31605005\n",
      "Test loss (w/o reg) on all data: 0.5191174\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0004184195\n",
      "Norm of the params: 3.2977471\n",
      "                Loss: fixed  21 labels. Loss 0.51912. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [307] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48000026\n",
      "Train loss (w/o reg) on all data: 0.47985983\n",
      "Test loss (w/o reg) on all data: 0.40161422\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0038387873\n",
      "Norm of the params: 1.6759261\n",
      "              Random: fixed  12 labels. Loss 0.40161. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5121179\n",
      "Train loss (w/o reg) on all data: 0.51198417\n",
      "Test loss (w/o reg) on all data: 0.3856624\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0031524594\n",
      "Norm of the params: 1.6357001\n",
      "Flipped loss: 0.38566. Accuracy: 0.822\n",
      "### Flips: 30, rs: 36, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4733909\n",
      "Train loss (w/o reg) on all data: 0.47320268\n",
      "Test loss (w/o reg) on all data: 0.37688515\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00061995187\n",
      "Norm of the params: 1.9401909\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.37689. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40818515\n",
      "Train loss (w/o reg) on all data: 0.40791443\n",
      "Test loss (w/o reg) on all data: 0.3698411\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00319598\n",
      "Norm of the params: 2.3268626\n",
      "                Loss: fixed   8 labels. Loss 0.36984. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [136] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5010574\n",
      "Train loss (w/o reg) on all data: 0.5009228\n",
      "Test loss (w/o reg) on all data: 0.38719136\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016261274\n",
      "Norm of the params: 1.6407397\n",
      "              Random: fixed   1 labels. Loss 0.38719. Accuracy 0.822.\n",
      "### Flips: 30, rs: 36, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [151] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4348963\n",
      "Train loss (w/o reg) on all data: 0.4346581\n",
      "Test loss (w/o reg) on all data: 0.36678636\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0070145503\n",
      "Norm of the params: 2.182547\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.36679. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [331] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3587585\n",
      "Train loss (w/o reg) on all data: 0.35832012\n",
      "Test loss (w/o reg) on all data: 0.3944956\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0021822343\n",
      "Norm of the params: 2.961036\n",
      "                Loss: fixed  13 labels. Loss 0.39450. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [379] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49268824\n",
      "Train loss (w/o reg) on all data: 0.4925461\n",
      "Test loss (w/o reg) on all data: 0.3809906\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007413597\n",
      "Norm of the params: 1.6859645\n",
      "              Random: fixed   2 labels. Loss 0.38099. Accuracy 0.822.\n",
      "### Flips: 30, rs: 36, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41291505\n",
      "Train loss (w/o reg) on all data: 0.412652\n",
      "Test loss (w/o reg) on all data: 0.35168692\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006149676\n",
      "Norm of the params: 2.293776\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.35169. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3373362\n",
      "Train loss (w/o reg) on all data: 0.33688095\n",
      "Test loss (w/o reg) on all data: 0.41117766\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013108958\n",
      "Norm of the params: 3.0175312\n",
      "                Loss: fixed  17 labels. Loss 0.41118. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [314] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5013811\n",
      "Train loss (w/o reg) on all data: 0.5012411\n",
      "Test loss (w/o reg) on all data: 0.37925202\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0020264522\n",
      "Norm of the params: 1.6733392\n",
      "              Random: fixed   3 labels. Loss 0.37925. Accuracy 0.822.\n",
      "### Flips: 30, rs: 36, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38845888\n",
      "Train loss (w/o reg) on all data: 0.38816372\n",
      "Test loss (w/o reg) on all data: 0.34986395\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015425795\n",
      "Norm of the params: 2.429639\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.34986. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [346] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3357762\n",
      "Train loss (w/o reg) on all data: 0.33531946\n",
      "Test loss (w/o reg) on all data: 0.41808125\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013606452\n",
      "Norm of the params: 3.022407\n",
      "                Loss: fixed  18 labels. Loss 0.41808. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [355] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48961574\n",
      "Train loss (w/o reg) on all data: 0.48945838\n",
      "Test loss (w/o reg) on all data: 0.38962707\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0062990123\n",
      "Norm of the params: 1.7740837\n",
      "              Random: fixed   6 labels. Loss 0.38963. Accuracy 0.822.\n",
      "### Flips: 30, rs: 36, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [208] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38845837\n",
      "Train loss (w/o reg) on all data: 0.3881626\n",
      "Test loss (w/o reg) on all data: 0.3498489\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008309524\n",
      "Norm of the params: 2.4321089\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.34985. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [371] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3232647\n",
      "Train loss (w/o reg) on all data: 0.32280523\n",
      "Test loss (w/o reg) on all data: 0.4153858\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00083699054\n",
      "Norm of the params: 3.0314243\n",
      "                Loss: fixed  21 labels. Loss 0.41539. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [328] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47236046\n",
      "Train loss (w/o reg) on all data: 0.472199\n",
      "Test loss (w/o reg) on all data: 0.3818294\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008394819\n",
      "Norm of the params: 1.7970457\n",
      "              Random: fixed   9 labels. Loss 0.38183. Accuracy 0.822.\n",
      "### Flips: 30, rs: 36, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3765909\n",
      "Train loss (w/o reg) on all data: 0.37625647\n",
      "Test loss (w/o reg) on all data: 0.3434243\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0007332946\n",
      "Norm of the params: 2.5862496\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.34342. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31804055\n",
      "Train loss (w/o reg) on all data: 0.31760192\n",
      "Test loss (w/o reg) on all data: 0.41922167\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00020167216\n",
      "Norm of the params: 2.9618137\n",
      "                Loss: fixed  25 labels. Loss 0.41922. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4680897\n",
      "Train loss (w/o reg) on all data: 0.4679277\n",
      "Test loss (w/o reg) on all data: 0.3754121\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0023938438\n",
      "Norm of the params: 1.8000778\n",
      "              Random: fixed  10 labels. Loss 0.37541. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5315771\n",
      "Train loss (w/o reg) on all data: 0.5314859\n",
      "Test loss (w/o reg) on all data: 0.38590157\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.003964971\n",
      "Norm of the params: 1.3505322\n",
      "Flipped loss: 0.38590. Accuracy: 0.844\n",
      "### Flips: 30, rs: 37, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [154] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48933837\n",
      "Train loss (w/o reg) on all data: 0.4892132\n",
      "Test loss (w/o reg) on all data: 0.3787953\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0025219708\n",
      "Norm of the params: 1.5821923\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.37880. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [173] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4115228\n",
      "Train loss (w/o reg) on all data: 0.41123456\n",
      "Test loss (w/o reg) on all data: 0.35176525\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005165731\n",
      "Norm of the params: 2.401075\n",
      "                Loss: fixed   9 labels. Loss 0.35177. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [151] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52215385\n",
      "Train loss (w/o reg) on all data: 0.5220478\n",
      "Test loss (w/o reg) on all data: 0.37948817\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004339166\n",
      "Norm of the params: 1.4561725\n",
      "              Random: fixed   1 labels. Loss 0.37949. Accuracy 0.822.\n",
      "### Flips: 30, rs: 37, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [240] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42156395\n",
      "Train loss (w/o reg) on all data: 0.42135453\n",
      "Test loss (w/o reg) on all data: 0.38193554\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0022633227\n",
      "Norm of the params: 2.0465548\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.38194. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [202] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34519857\n",
      "Train loss (w/o reg) on all data: 0.34472337\n",
      "Test loss (w/o reg) on all data: 0.4019841\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0004480001\n",
      "Norm of the params: 3.0828707\n",
      "                Loss: fixed  15 labels. Loss 0.40198. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5238398\n",
      "Train loss (w/o reg) on all data: 0.5237231\n",
      "Test loss (w/o reg) on all data: 0.38094202\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00093955634\n",
      "Norm of the params: 1.5272008\n",
      "              Random: fixed   3 labels. Loss 0.38094. Accuracy 0.822.\n",
      "### Flips: 30, rs: 37, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [382] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38245848\n",
      "Train loss (w/o reg) on all data: 0.3821614\n",
      "Test loss (w/o reg) on all data: 0.3896701\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00054142834\n",
      "Norm of the params: 2.437477\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.38967. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [185] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33943588\n",
      "Train loss (w/o reg) on all data: 0.33895573\n",
      "Test loss (w/o reg) on all data: 0.41250974\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0016895114\n",
      "Norm of the params: 3.0988712\n",
      "                Loss: fixed  16 labels. Loss 0.41251. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5205214\n",
      "Train loss (w/o reg) on all data: 0.5204055\n",
      "Test loss (w/o reg) on all data: 0.3869934\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00087562634\n",
      "Norm of the params: 1.5225486\n",
      "              Random: fixed   4 labels. Loss 0.38699. Accuracy 0.822.\n",
      "### Flips: 30, rs: 37, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [326] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36599568\n",
      "Train loss (w/o reg) on all data: 0.36565074\n",
      "Test loss (w/o reg) on all data: 0.39140016\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0006471682\n",
      "Norm of the params: 2.6265674\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.39140. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.30949312\n",
      "Train loss (w/o reg) on all data: 0.30901203\n",
      "Test loss (w/o reg) on all data: 0.4452953\n",
      "Train acc on all data:  0.8820754716981132\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00039154768\n",
      "Norm of the params: 3.1018982\n",
      "                Loss: fixed  21 labels. Loss 0.44530. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [291] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5082364\n",
      "Train loss (w/o reg) on all data: 0.5081044\n",
      "Test loss (w/o reg) on all data: 0.38510764\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016566342\n",
      "Norm of the params: 1.624878\n",
      "              Random: fixed   5 labels. Loss 0.38511. Accuracy 0.822.\n",
      "### Flips: 30, rs: 37, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3403504\n",
      "Train loss (w/o reg) on all data: 0.33992353\n",
      "Test loss (w/o reg) on all data: 0.3972455\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0016399387\n",
      "Norm of the params: 2.9218855\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.39725. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3037036\n",
      "Train loss (w/o reg) on all data: 0.3031629\n",
      "Test loss (w/o reg) on all data: 0.4453908\n",
      "Train acc on all data:  0.8962264150943396\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0021151835\n",
      "Norm of the params: 3.2884734\n",
      "                Loss: fixed  23 labels. Loss 0.44539. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [306] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46956903\n",
      "Train loss (w/o reg) on all data: 0.4693487\n",
      "Test loss (w/o reg) on all data: 0.35772568\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0021022998\n",
      "Norm of the params: 2.0992239\n",
      "              Random: fixed   8 labels. Loss 0.35773. Accuracy 0.844.\n",
      "### Flips: 30, rs: 37, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [338] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3249987\n",
      "Train loss (w/o reg) on all data: 0.32453236\n",
      "Test loss (w/o reg) on all data: 0.42305687\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00038087292\n",
      "Norm of the params: 3.0540278\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.42306. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.30370253\n",
      "Train loss (w/o reg) on all data: 0.3031631\n",
      "Test loss (w/o reg) on all data: 0.44553533\n",
      "Train acc on all data:  0.8962264150943396\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006697693\n",
      "Norm of the params: 3.284599\n",
      "                Loss: fixed  23 labels. Loss 0.44554. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46956995\n",
      "Train loss (w/o reg) on all data: 0.46935248\n",
      "Test loss (w/o reg) on all data: 0.3577177\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0013482308\n",
      "Norm of the params: 2.0854745\n",
      "              Random: fixed   8 labels. Loss 0.35772. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [293] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5297453\n",
      "Train loss (w/o reg) on all data: 0.52961946\n",
      "Test loss (w/o reg) on all data: 0.37376374\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0018635793\n",
      "Norm of the params: 1.5864446\n",
      "Flipped loss: 0.37376. Accuracy: 0.867\n",
      "### Flips: 30, rs: 38, checks: 10\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [121] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47730544\n",
      "Train loss (w/o reg) on all data: 0.4771636\n",
      "Test loss (w/o reg) on all data: 0.35691935\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0019606175\n",
      "Norm of the params: 1.6841569\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.35692. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [81] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4689021\n",
      "Train loss (w/o reg) on all data: 0.4687126\n",
      "Test loss (w/o reg) on all data: 0.34262568\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0018730572\n",
      "Norm of the params: 1.9468873\n",
      "                Loss: fixed   6 labels. Loss 0.34263. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [55] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5297466\n",
      "Train loss (w/o reg) on all data: 0.52961874\n",
      "Test loss (w/o reg) on all data: 0.374746\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0047488017\n",
      "Norm of the params: 1.5989398\n",
      "              Random: fixed   0 labels. Loss 0.37475. Accuracy 0.867.\n",
      "### Flips: 30, rs: 38, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [285] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4449497\n",
      "Train loss (w/o reg) on all data: 0.44471106\n",
      "Test loss (w/o reg) on all data: 0.3731006\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011120986\n",
      "Norm of the params: 2.1846428\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.37310. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [84] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40509796\n",
      "Train loss (w/o reg) on all data: 0.40483516\n",
      "Test loss (w/o reg) on all data: 0.3665549\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0026030191\n",
      "Norm of the params: 2.2925231\n",
      "                Loss: fixed  13 labels. Loss 0.36655. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [374] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5283992\n",
      "Train loss (w/o reg) on all data: 0.52826923\n",
      "Test loss (w/o reg) on all data: 0.37481257\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.001129827\n",
      "Norm of the params: 1.6123642\n",
      "              Random: fixed   2 labels. Loss 0.37481. Accuracy 0.867.\n",
      "### Flips: 30, rs: 38, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4188132\n",
      "Train loss (w/o reg) on all data: 0.4186048\n",
      "Test loss (w/o reg) on all data: 0.34731382\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00070509204\n",
      "Norm of the params: 2.0415578\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.34731. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38636014\n",
      "Train loss (w/o reg) on all data: 0.38608554\n",
      "Test loss (w/o reg) on all data: 0.37169027\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0036839112\n",
      "Norm of the params: 2.3434827\n",
      "                Loss: fixed  15 labels. Loss 0.37169. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52201515\n",
      "Train loss (w/o reg) on all data: 0.5218918\n",
      "Test loss (w/o reg) on all data: 0.369554\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0006195951\n",
      "Norm of the params: 1.5710337\n",
      "              Random: fixed   4 labels. Loss 0.36955. Accuracy 0.867.\n",
      "### Flips: 30, rs: 38, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40700853\n",
      "Train loss (w/o reg) on all data: 0.40680298\n",
      "Test loss (w/o reg) on all data: 0.3425847\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0024051953\n",
      "Norm of the params: 2.027567\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.34258. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36892095\n",
      "Train loss (w/o reg) on all data: 0.36864668\n",
      "Test loss (w/o reg) on all data: 0.38071218\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007649383\n",
      "Norm of the params: 2.342082\n",
      "                Loss: fixed  20 labels. Loss 0.38071. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [397] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51631933\n",
      "Train loss (w/o reg) on all data: 0.51618254\n",
      "Test loss (w/o reg) on all data: 0.37044045\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0009936701\n",
      "Norm of the params: 1.6540936\n",
      "              Random: fixed   5 labels. Loss 0.37044. Accuracy 0.867.\n",
      "### Flips: 30, rs: 38, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [198] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38868433\n",
      "Train loss (w/o reg) on all data: 0.38843983\n",
      "Test loss (w/o reg) on all data: 0.36328128\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0058910316\n",
      "Norm of the params: 2.2113101\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.36328. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3524033\n",
      "Train loss (w/o reg) on all data: 0.35213146\n",
      "Test loss (w/o reg) on all data: 0.37336493\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0039153206\n",
      "Norm of the params: 2.3317826\n",
      "                Loss: fixed  24 labels. Loss 0.37336. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51632744\n",
      "Train loss (w/o reg) on all data: 0.5161926\n",
      "Test loss (w/o reg) on all data: 0.37039176\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.004857959\n",
      "Norm of the params: 1.6420709\n",
      "              Random: fixed   5 labels. Loss 0.37039. Accuracy 0.867.\n",
      "### Flips: 30, rs: 38, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [168] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37571317\n",
      "Train loss (w/o reg) on all data: 0.37544268\n",
      "Test loss (w/o reg) on all data: 0.3893827\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0013448605\n",
      "Norm of the params: 2.325854\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.38938. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [67] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3482913\n",
      "Train loss (w/o reg) on all data: 0.34799093\n",
      "Test loss (w/o reg) on all data: 0.39790234\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0058974815\n",
      "Norm of the params: 2.4510686\n",
      "                Loss: fixed  26 labels. Loss 0.39790. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.503148\n",
      "Train loss (w/o reg) on all data: 0.5030008\n",
      "Test loss (w/o reg) on all data: 0.37706786\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004909903\n",
      "Norm of the params: 1.7161171\n",
      "              Random: fixed   7 labels. Loss 0.37707. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [321] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5501511\n",
      "Train loss (w/o reg) on all data: 0.55005306\n",
      "Test loss (w/o reg) on all data: 0.40842912\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010248112\n",
      "Norm of the params: 1.400304\n",
      "Flipped loss: 0.40843. Accuracy: 0.822\n",
      "### Flips: 30, rs: 39, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [89] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5095058\n",
      "Train loss (w/o reg) on all data: 0.50937104\n",
      "Test loss (w/o reg) on all data: 0.39422333\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018591231\n",
      "Norm of the params: 1.6416543\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.39422. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [157] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47865382\n",
      "Train loss (w/o reg) on all data: 0.47845063\n",
      "Test loss (w/o reg) on all data: 0.39956516\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00078885915\n",
      "Norm of the params: 2.015854\n",
      "                Loss: fixed   7 labels. Loss 0.39957. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5501521\n",
      "Train loss (w/o reg) on all data: 0.55005366\n",
      "Test loss (w/o reg) on all data: 0.4079583\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008028359\n",
      "Norm of the params: 1.4031895\n",
      "              Random: fixed   0 labels. Loss 0.40796. Accuracy 0.822.\n",
      "### Flips: 30, rs: 39, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [132] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4878901\n",
      "Train loss (w/o reg) on all data: 0.48773748\n",
      "Test loss (w/o reg) on all data: 0.37664977\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.008481745\n",
      "Norm of the params: 1.7470838\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.37665. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [283] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4476203\n",
      "Train loss (w/o reg) on all data: 0.447386\n",
      "Test loss (w/o reg) on all data: 0.40167108\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007901301\n",
      "Norm of the params: 2.164771\n",
      "                Loss: fixed  11 labels. Loss 0.40167. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [201] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52352244\n",
      "Train loss (w/o reg) on all data: 0.5233965\n",
      "Test loss (w/o reg) on all data: 0.3994939\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0022624512\n",
      "Norm of the params: 1.5870153\n",
      "              Random: fixed   3 labels. Loss 0.39949. Accuracy 0.822.\n",
      "### Flips: 30, rs: 39, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45915917\n",
      "Train loss (w/o reg) on all data: 0.4589978\n",
      "Test loss (w/o reg) on all data: 0.36520183\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0012177115\n",
      "Norm of the params: 1.7965412\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.36520. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41544375\n",
      "Train loss (w/o reg) on all data: 0.41519445\n",
      "Test loss (w/o reg) on all data: 0.3925712\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004102251\n",
      "Norm of the params: 2.2329752\n",
      "                Loss: fixed  15 labels. Loss 0.39257. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [278] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.520252\n",
      "Train loss (w/o reg) on all data: 0.5201364\n",
      "Test loss (w/o reg) on all data: 0.38883802\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0022266016\n",
      "Norm of the params: 1.5202492\n",
      "              Random: fixed   5 labels. Loss 0.38884. Accuracy 0.844.\n",
      "### Flips: 30, rs: 39, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4456901\n",
      "Train loss (w/o reg) on all data: 0.44551128\n",
      "Test loss (w/o reg) on all data: 0.34983045\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00077988923\n",
      "Norm of the params: 1.8910604\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.34983. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39351138\n",
      "Train loss (w/o reg) on all data: 0.3932323\n",
      "Test loss (w/o reg) on all data: 0.41140926\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011507269\n",
      "Norm of the params: 2.3625576\n",
      "                Loss: fixed  19 labels. Loss 0.41141. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [305] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5202524\n",
      "Train loss (w/o reg) on all data: 0.52013755\n",
      "Test loss (w/o reg) on all data: 0.3886948\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010056364\n",
      "Norm of the params: 1.5156724\n",
      "              Random: fixed   5 labels. Loss 0.38869. Accuracy 0.844.\n",
      "### Flips: 30, rs: 39, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43434355\n",
      "Train loss (w/o reg) on all data: 0.43412247\n",
      "Test loss (w/o reg) on all data: 0.36456493\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002886935\n",
      "Norm of the params: 2.1026924\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.36456. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3737128\n",
      "Train loss (w/o reg) on all data: 0.37338918\n",
      "Test loss (w/o reg) on all data: 0.41212326\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0037719226\n",
      "Norm of the params: 2.544098\n",
      "                Loss: fixed  22 labels. Loss 0.41212. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [321] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5202511\n",
      "Train loss (w/o reg) on all data: 0.5201365\n",
      "Test loss (w/o reg) on all data: 0.38872617\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.017470246\n",
      "Norm of the params: 1.5140408\n",
      "              Random: fixed   5 labels. Loss 0.38873. Accuracy 0.844.\n",
      "### Flips: 30, rs: 39, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41831496\n",
      "Train loss (w/o reg) on all data: 0.41805375\n",
      "Test loss (w/o reg) on all data: 0.375852\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0056391656\n",
      "Norm of the params: 2.2856648\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.37585. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [177] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36448368\n",
      "Train loss (w/o reg) on all data: 0.36415157\n",
      "Test loss (w/o reg) on all data: 0.39247927\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0042197886\n",
      "Norm of the params: 2.577285\n",
      "                Loss: fixed  25 labels. Loss 0.39248. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [295] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5189205\n",
      "Train loss (w/o reg) on all data: 0.51881135\n",
      "Test loss (w/o reg) on all data: 0.39078373\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004794126\n",
      "Norm of the params: 1.477459\n",
      "              Random: fixed   7 labels. Loss 0.39078. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53224045\n",
      "Train loss (w/o reg) on all data: 0.53213525\n",
      "Test loss (w/o reg) on all data: 0.47172248\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016474194\n",
      "Norm of the params: 1.4506499\n",
      "Flipped loss: 0.47172. Accuracy: 0.800\n",
      "### Flips: 40, rs: 0, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4881617\n",
      "Train loss (w/o reg) on all data: 0.4880324\n",
      "Test loss (w/o reg) on all data: 0.44075212\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00032320977\n",
      "Norm of the params: 1.608153\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.44075. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4593255\n",
      "Train loss (w/o reg) on all data: 0.45915344\n",
      "Test loss (w/o reg) on all data: 0.43267924\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011344199\n",
      "Norm of the params: 1.8550153\n",
      "                Loss: fixed   7 labels. Loss 0.43268. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [131] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52781814\n",
      "Train loss (w/o reg) on all data: 0.5276985\n",
      "Test loss (w/o reg) on all data: 0.46923196\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022726639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of the params: 1.5465957\n",
      "              Random: fixed   2 labels. Loss 0.46923. Accuracy 0.800.\n",
      "### Flips: 40, rs: 0, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [127] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47248\n",
      "Train loss (w/o reg) on all data: 0.47231582\n",
      "Test loss (w/o reg) on all data: 0.41946524\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.013565295\n",
      "Norm of the params: 1.8120539\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.41947. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39056438\n",
      "Train loss (w/o reg) on all data: 0.3903207\n",
      "Test loss (w/o reg) on all data: 0.427847\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015000724\n",
      "Norm of the params: 2.2076385\n",
      "                Loss: fixed  14 labels. Loss 0.42785. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5262349\n",
      "Train loss (w/o reg) on all data: 0.5261182\n",
      "Test loss (w/o reg) on all data: 0.47251672\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0081269685\n",
      "Norm of the params: 1.5279526\n",
      "              Random: fixed   3 labels. Loss 0.47252. Accuracy 0.778.\n",
      "### Flips: 40, rs: 0, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4305576\n",
      "Train loss (w/o reg) on all data: 0.4303462\n",
      "Test loss (w/o reg) on all data: 0.4196752\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00033708493\n",
      "Norm of the params: 2.0562298\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.41968. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35047942\n",
      "Train loss (w/o reg) on all data: 0.35011446\n",
      "Test loss (w/o reg) on all data: 0.44265363\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0037476874\n",
      "Norm of the params: 2.701749\n",
      "                Loss: fixed  19 labels. Loss 0.44265. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5176215\n",
      "Train loss (w/o reg) on all data: 0.5174917\n",
      "Test loss (w/o reg) on all data: 0.47525805\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0021236648\n",
      "Norm of the params: 1.6114593\n",
      "              Random: fixed   4 labels. Loss 0.47526. Accuracy 0.778.\n",
      "### Flips: 40, rs: 0, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3855248\n",
      "Train loss (w/o reg) on all data: 0.38523537\n",
      "Test loss (w/o reg) on all data: 0.41088998\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0046386807\n",
      "Norm of the params: 2.4059887\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.41089. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33182964\n",
      "Train loss (w/o reg) on all data: 0.33142492\n",
      "Test loss (w/o reg) on all data: 0.4536999\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0017251095\n",
      "Norm of the params: 2.8450387\n",
      "                Loss: fixed  23 labels. Loss 0.45370. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.503377\n",
      "Train loss (w/o reg) on all data: 0.5032287\n",
      "Test loss (w/o reg) on all data: 0.45807865\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00082367024\n",
      "Norm of the params: 1.7221707\n",
      "              Random: fixed   5 labels. Loss 0.45808. Accuracy 0.778.\n",
      "### Flips: 40, rs: 0, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37018958\n",
      "Train loss (w/o reg) on all data: 0.36989537\n",
      "Test loss (w/o reg) on all data: 0.43042555\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0019330747\n",
      "Norm of the params: 2.4256783\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.43043. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3318251\n",
      "Train loss (w/o reg) on all data: 0.33141896\n",
      "Test loss (w/o reg) on all data: 0.45322037\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0002919633\n",
      "Norm of the params: 2.8500836\n",
      "                Loss: fixed  23 labels. Loss 0.45322. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49548972\n",
      "Train loss (w/o reg) on all data: 0.49532843\n",
      "Test loss (w/o reg) on all data: 0.44391203\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0032955229\n",
      "Norm of the params: 1.7959996\n",
      "              Random: fixed   6 labels. Loss 0.44391. Accuracy 0.800.\n",
      "### Flips: 40, rs: 0, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [354] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3480536\n",
      "Train loss (w/o reg) on all data: 0.34771976\n",
      "Test loss (w/o reg) on all data: 0.43880707\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0009824678\n",
      "Norm of the params: 2.5839794\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.43881. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [128] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3107356\n",
      "Train loss (w/o reg) on all data: 0.310319\n",
      "Test loss (w/o reg) on all data: 0.48705032\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00094402157\n",
      "Norm of the params: 2.88656\n",
      "                Loss: fixed  27 labels. Loss 0.48705. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [343] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48145157\n",
      "Train loss (w/o reg) on all data: 0.48129526\n",
      "Test loss (w/o reg) on all data: 0.43531197\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002391546\n",
      "Norm of the params: 1.7681781\n",
      "              Random: fixed  10 labels. Loss 0.43531. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [177] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57057345\n",
      "Train loss (w/o reg) on all data: 0.5704792\n",
      "Test loss (w/o reg) on all data: 0.453122\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.007265898\n",
      "Norm of the params: 1.3727609\n",
      "Flipped loss: 0.45312. Accuracy: 0.778\n",
      "### Flips: 40, rs: 1, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [153] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54331005\n",
      "Train loss (w/o reg) on all data: 0.54323345\n",
      "Test loss (w/o reg) on all data: 0.42393395\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001356963\n",
      "Norm of the params: 1.2377934\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.42393. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49918917\n",
      "Train loss (w/o reg) on all data: 0.49904358\n",
      "Test loss (w/o reg) on all data: 0.41382968\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0035042271\n",
      "Norm of the params: 1.7063915\n",
      "                Loss: fixed   7 labels. Loss 0.41383. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [150] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5620631\n",
      "Train loss (w/o reg) on all data: 0.561964\n",
      "Test loss (w/o reg) on all data: 0.45241314\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015851526\n",
      "Norm of the params: 1.408144\n",
      "              Random: fixed   1 labels. Loss 0.45241. Accuracy 0.778.\n",
      "### Flips: 40, rs: 1, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [136] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5294963\n",
      "Train loss (w/o reg) on all data: 0.5294175\n",
      "Test loss (w/o reg) on all data: 0.4077244\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.007002608\n",
      "Norm of the params: 1.2552421\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.40772. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [394] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44544375\n",
      "Train loss (w/o reg) on all data: 0.4452416\n",
      "Test loss (w/o reg) on all data: 0.38858205\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0025774988\n",
      "Norm of the params: 2.010712\n",
      "                Loss: fixed  13 labels. Loss 0.38858. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5536671\n",
      "Train loss (w/o reg) on all data: 0.5535703\n",
      "Test loss (w/o reg) on all data: 0.44088304\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0061806603\n",
      "Norm of the params: 1.3917307\n",
      "              Random: fixed   3 labels. Loss 0.44088. Accuracy 0.800.\n",
      "### Flips: 40, rs: 1, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.492123\n",
      "Train loss (w/o reg) on all data: 0.49201772\n",
      "Test loss (w/o reg) on all data: 0.39520127\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00041382265\n",
      "Norm of the params: 1.451074\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.39520. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39524272\n",
      "Train loss (w/o reg) on all data: 0.39500448\n",
      "Test loss (w/o reg) on all data: 0.40348294\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0021220725\n",
      "Norm of the params: 2.1827762\n",
      "                Loss: fixed  19 labels. Loss 0.40348. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55595225\n",
      "Train loss (w/o reg) on all data: 0.55585355\n",
      "Test loss (w/o reg) on all data: 0.4346636\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0034493569\n",
      "Norm of the params: 1.4052255\n",
      "              Random: fixed   4 labels. Loss 0.43466. Accuracy 0.800.\n",
      "### Flips: 40, rs: 1, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46947348\n",
      "Train loss (w/o reg) on all data: 0.4693689\n",
      "Test loss (w/o reg) on all data: 0.37127432\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006013548\n",
      "Norm of the params: 1.4461198\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.37127. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [314] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36534268\n",
      "Train loss (w/o reg) on all data: 0.3650642\n",
      "Test loss (w/o reg) on all data: 0.39424136\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0014328719\n",
      "Norm of the params: 2.3599772\n",
      "                Loss: fixed  23 labels. Loss 0.39424. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [332] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5536165\n",
      "Train loss (w/o reg) on all data: 0.5535157\n",
      "Test loss (w/o reg) on all data: 0.43729132\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012584565\n",
      "Norm of the params: 1.4200785\n",
      "              Random: fixed   6 labels. Loss 0.43729. Accuracy 0.778.\n",
      "### Flips: 40, rs: 1, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4428048\n",
      "Train loss (w/o reg) on all data: 0.44267362\n",
      "Test loss (w/o reg) on all data: 0.360386\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0026253778\n",
      "Norm of the params: 1.61983\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.36039. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [318] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35013163\n",
      "Train loss (w/o reg) on all data: 0.34986454\n",
      "Test loss (w/o reg) on all data: 0.4077811\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005546682\n",
      "Norm of the params: 2.3112705\n",
      "                Loss: fixed  27 labels. Loss 0.40778. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [266] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5461216\n",
      "Train loss (w/o reg) on all data: 0.546022\n",
      "Test loss (w/o reg) on all data: 0.4262125\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003356188\n",
      "Norm of the params: 1.4115858\n",
      "              Random: fixed   8 labels. Loss 0.42621. Accuracy 0.800.\n",
      "### Flips: 40, rs: 1, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41784996\n",
      "Train loss (w/o reg) on all data: 0.4176708\n",
      "Test loss (w/o reg) on all data: 0.35817358\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0025286698\n",
      "Norm of the params: 1.893063\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.35817. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34238556\n",
      "Train loss (w/o reg) on all data: 0.3420742\n",
      "Test loss (w/o reg) on all data: 0.40273857\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0036235887\n",
      "Norm of the params: 2.4954872\n",
      "                Loss: fixed  30 labels. Loss 0.40274. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5364963\n",
      "Train loss (w/o reg) on all data: 0.5364148\n",
      "Test loss (w/o reg) on all data: 0.4195329\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00143789\n",
      "Norm of the params: 1.2763958\n",
      "              Random: fixed  10 labels. Loss 0.41953. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5880876\n",
      "Train loss (w/o reg) on all data: 0.588027\n",
      "Test loss (w/o reg) on all data: 0.45720375\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010168557\n",
      "Norm of the params: 1.1012605\n",
      "Flipped loss: 0.45720. Accuracy: 0.778\n",
      "### Flips: 40, rs: 2, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5522958\n",
      "Train loss (w/o reg) on all data: 0.55221045\n",
      "Test loss (w/o reg) on all data: 0.41742447\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0027191325\n",
      "Norm of the params: 1.3065982\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.41742. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51682913\n",
      "Train loss (w/o reg) on all data: 0.5166875\n",
      "Test loss (w/o reg) on all data: 0.39912626\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00073854736\n",
      "Norm of the params: 1.6828619\n",
      "                Loss: fixed   8 labels. Loss 0.39913. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5823797\n",
      "Train loss (w/o reg) on all data: 0.58232117\n",
      "Test loss (w/o reg) on all data: 0.45480356\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0021338144\n",
      "Norm of the params: 1.0816941\n",
      "              Random: fixed   1 labels. Loss 0.45480. Accuracy 0.778.\n",
      "### Flips: 40, rs: 2, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51142263\n",
      "Train loss (w/o reg) on all data: 0.5112901\n",
      "Test loss (w/o reg) on all data: 0.36177126\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0049889754\n",
      "Norm of the params: 1.6280981\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.36177. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45600528\n",
      "Train loss (w/o reg) on all data: 0.45580634\n",
      "Test loss (w/o reg) on all data: 0.38113362\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00023140422\n",
      "Norm of the params: 1.9945812\n",
      "                Loss: fixed  15 labels. Loss 0.38113. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58143634\n",
      "Train loss (w/o reg) on all data: 0.58137935\n",
      "Test loss (w/o reg) on all data: 0.4522675\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004059219\n",
      "Norm of the params: 1.0677794\n",
      "              Random: fixed   2 labels. Loss 0.45227. Accuracy 0.800.\n",
      "### Flips: 40, rs: 2, checks: 30\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5033249\n",
      "Train loss (w/o reg) on all data: 0.50319284\n",
      "Test loss (w/o reg) on all data: 0.36161622\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00100083\n",
      "Norm of the params: 1.6254923\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.36162. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3918116\n",
      "Train loss (w/o reg) on all data: 0.39150035\n",
      "Test loss (w/o reg) on all data: 0.37593126\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.005497739\n",
      "Norm of the params: 2.4949813\n",
      "                Loss: fixed  21 labels. Loss 0.37593. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5717383\n",
      "Train loss (w/o reg) on all data: 0.5716768\n",
      "Test loss (w/o reg) on all data: 0.4383987\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.007274674\n",
      "Norm of the params: 1.1093869\n",
      "              Random: fixed   3 labels. Loss 0.43840. Accuracy 0.800.\n",
      "### Flips: 40, rs: 2, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4934479\n",
      "Train loss (w/o reg) on all data: 0.49330673\n",
      "Test loss (w/o reg) on all data: 0.35889322\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0049033035\n",
      "Norm of the params: 1.6802871\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.35889. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [282] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3564925\n",
      "Train loss (w/o reg) on all data: 0.35617304\n",
      "Test loss (w/o reg) on all data: 0.38223788\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00070850214\n",
      "Norm of the params: 2.5276892\n",
      "                Loss: fixed  27 labels. Loss 0.38224. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5653083\n",
      "Train loss (w/o reg) on all data: 0.5652376\n",
      "Test loss (w/o reg) on all data: 0.43420783\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018775829\n",
      "Norm of the params: 1.1891252\n",
      "              Random: fixed   4 labels. Loss 0.43421. Accuracy 0.800.\n",
      "### Flips: 40, rs: 2, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [146] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48249686\n",
      "Train loss (w/o reg) on all data: 0.4823498\n",
      "Test loss (w/o reg) on all data: 0.35619357\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016420106\n",
      "Norm of the params: 1.7148281\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.35619. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [355] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34780878\n",
      "Train loss (w/o reg) on all data: 0.34746435\n",
      "Test loss (w/o reg) on all data: 0.4044262\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00068181264\n",
      "Norm of the params: 2.624562\n",
      "                Loss: fixed  29 labels. Loss 0.40443. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5628797\n",
      "Train loss (w/o reg) on all data: 0.5628231\n",
      "Test loss (w/o reg) on all data: 0.4348744\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00058789237\n",
      "Norm of the params: 1.063377\n",
      "              Random: fixed   6 labels. Loss 0.43487. Accuracy 0.800.\n",
      "### Flips: 40, rs: 2, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [163] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4409832\n",
      "Train loss (w/o reg) on all data: 0.44076955\n",
      "Test loss (w/o reg) on all data: 0.3764591\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003276197\n",
      "Norm of the params: 2.0671732\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.37646. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34422022\n",
      "Train loss (w/o reg) on all data: 0.34386057\n",
      "Test loss (w/o reg) on all data: 0.4060045\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014119977\n",
      "Norm of the params: 2.6820447\n",
      "                Loss: fixed  30 labels. Loss 0.40600. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [289] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55631495\n",
      "Train loss (w/o reg) on all data: 0.556245\n",
      "Test loss (w/o reg) on all data: 0.4327554\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003380125\n",
      "Norm of the params: 1.1827585\n",
      "              Random: fixed   8 labels. Loss 0.43276. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5677527\n",
      "Train loss (w/o reg) on all data: 0.56764966\n",
      "Test loss (w/o reg) on all data: 0.42310792\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004229298\n",
      "Norm of the params: 1.4356335\n",
      "Flipped loss: 0.42311. Accuracy: 0.822\n",
      "### Flips: 40, rs: 3, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5426003\n",
      "Train loss (w/o reg) on all data: 0.5424794\n",
      "Test loss (w/o reg) on all data: 0.40995133\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013345014\n",
      "Norm of the params: 1.5547568\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.40995. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47760203\n",
      "Train loss (w/o reg) on all data: 0.47737294\n",
      "Test loss (w/o reg) on all data: 0.4062781\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00072063046\n",
      "Norm of the params: 2.1404624\n",
      "                Loss: fixed   9 labels. Loss 0.40628. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.567748\n",
      "Train loss (w/o reg) on all data: 0.56764454\n",
      "Test loss (w/o reg) on all data: 0.42363822\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002050004\n",
      "Norm of the params: 1.4386774\n",
      "              Random: fixed   0 labels. Loss 0.42364. Accuracy 0.822.\n",
      "### Flips: 40, rs: 3, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [208] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51434696\n",
      "Train loss (w/o reg) on all data: 0.5141856\n",
      "Test loss (w/o reg) on all data: 0.41763708\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0028598758\n",
      "Norm of the params: 1.7963948\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41764. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43173966\n",
      "Train loss (w/o reg) on all data: 0.43146476\n",
      "Test loss (w/o reg) on all data: 0.4117559\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018125544\n",
      "Norm of the params: 2.3448124\n",
      "                Loss: fixed  14 labels. Loss 0.41176. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5555316\n",
      "Train loss (w/o reg) on all data: 0.55542487\n",
      "Test loss (w/o reg) on all data: 0.40603817\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0017012622\n",
      "Norm of the params: 1.4611447\n",
      "              Random: fixed   3 labels. Loss 0.40604. Accuracy 0.867.\n",
      "### Flips: 40, rs: 3, checks: 30\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48287928\n",
      "Train loss (w/o reg) on all data: 0.48266444\n",
      "Test loss (w/o reg) on all data: 0.38616773\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00509231\n",
      "Norm of the params: 2.0728252\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.38617. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [301] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4058938\n",
      "Train loss (w/o reg) on all data: 0.40559366\n",
      "Test loss (w/o reg) on all data: 0.43607932\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0030578247\n",
      "Norm of the params: 2.450044\n",
      "                Loss: fixed  17 labels. Loss 0.43608. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [353] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5451227\n",
      "Train loss (w/o reg) on all data: 0.5450082\n",
      "Test loss (w/o reg) on all data: 0.39170268\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0011299952\n",
      "Norm of the params: 1.5131842\n",
      "              Random: fixed   6 labels. Loss 0.39170. Accuracy 0.867.\n",
      "### Flips: 40, rs: 3, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46184987\n",
      "Train loss (w/o reg) on all data: 0.46163243\n",
      "Test loss (w/o reg) on all data: 0.38605064\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008587452\n",
      "Norm of the params: 2.0853872\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.38605. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [362] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3750692\n",
      "Train loss (w/o reg) on all data: 0.37467802\n",
      "Test loss (w/o reg) on all data: 0.4050017\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001194579\n",
      "Norm of the params: 2.7971337\n",
      "                Loss: fixed  23 labels. Loss 0.40500. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [308] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5358779\n",
      "Train loss (w/o reg) on all data: 0.5357579\n",
      "Test loss (w/o reg) on all data: 0.38907552\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0059702625\n",
      "Norm of the params: 1.5489178\n",
      "              Random: fixed   8 labels. Loss 0.38908. Accuracy 0.867.\n",
      "### Flips: 40, rs: 3, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [113] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4481337\n",
      "Train loss (w/o reg) on all data: 0.4479185\n",
      "Test loss (w/o reg) on all data: 0.37654516\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014566164\n",
      "Norm of the params: 2.0746439\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.37655. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [315] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35127017\n",
      "Train loss (w/o reg) on all data: 0.35076195\n",
      "Test loss (w/o reg) on all data: 0.40356785\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.000670546\n",
      "Norm of the params: 3.1881168\n",
      "                Loss: fixed  28 labels. Loss 0.40357. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5061396\n",
      "Train loss (w/o reg) on all data: 0.50597304\n",
      "Test loss (w/o reg) on all data: 0.37638667\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0074289874\n",
      "Norm of the params: 1.8249191\n",
      "              Random: fixed  13 labels. Loss 0.37639. Accuracy 0.867.\n",
      "### Flips: 40, rs: 3, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4344303\n",
      "Train loss (w/o reg) on all data: 0.43417543\n",
      "Test loss (w/o reg) on all data: 0.37586406\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010792013\n",
      "Norm of the params: 2.257805\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.37586. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [401] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3406723\n",
      "Train loss (w/o reg) on all data: 0.34017807\n",
      "Test loss (w/o reg) on all data: 0.4259249\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007963006\n",
      "Norm of the params: 3.1440303\n",
      "                Loss: fixed  31 labels. Loss 0.42592. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49157107\n",
      "Train loss (w/o reg) on all data: 0.49137723\n",
      "Test loss (w/o reg) on all data: 0.3796419\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0015869561\n",
      "Norm of the params: 1.9688706\n",
      "              Random: fixed  15 labels. Loss 0.37964. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53369117\n",
      "Train loss (w/o reg) on all data: 0.53361416\n",
      "Test loss (w/o reg) on all data: 0.3934812\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0049820235\n",
      "Norm of the params: 1.2411106\n",
      "Flipped loss: 0.39348. Accuracy: 0.822\n",
      "### Flips: 40, rs: 4, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [119] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49305558\n",
      "Train loss (w/o reg) on all data: 0.49296293\n",
      "Test loss (w/o reg) on all data: 0.35098833\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0028262604\n",
      "Norm of the params: 1.3612239\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.35099. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [284] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46245378\n",
      "Train loss (w/o reg) on all data: 0.46233547\n",
      "Test loss (w/o reg) on all data: 0.35797638\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005571585\n",
      "Norm of the params: 1.5383111\n",
      "                Loss: fixed   6 labels. Loss 0.35798. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [312] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52796954\n",
      "Train loss (w/o reg) on all data: 0.52789444\n",
      "Test loss (w/o reg) on all data: 0.38763335\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008077023\n",
      "Norm of the params: 1.2253428\n",
      "              Random: fixed   1 labels. Loss 0.38763. Accuracy 0.800.\n",
      "### Flips: 40, rs: 4, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [226] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4782801\n",
      "Train loss (w/o reg) on all data: 0.47813946\n",
      "Test loss (w/o reg) on all data: 0.34444618\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0033315353\n",
      "Norm of the params: 1.677063\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.34445. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40680736\n",
      "Train loss (w/o reg) on all data: 0.4065825\n",
      "Test loss (w/o reg) on all data: 0.34751564\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001159763\n",
      "Norm of the params: 2.1205904\n",
      "                Loss: fixed  12 labels. Loss 0.34752. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5190076\n",
      "Train loss (w/o reg) on all data: 0.5189245\n",
      "Test loss (w/o reg) on all data: 0.40080413\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011674885\n",
      "Norm of the params: 1.2896427\n",
      "              Random: fixed   5 labels. Loss 0.40080. Accuracy 0.800.\n",
      "### Flips: 40, rs: 4, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42193922\n",
      "Train loss (w/o reg) on all data: 0.4217235\n",
      "Test loss (w/o reg) on all data: 0.34665272\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0026837345\n",
      "Norm of the params: 2.0771055\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.34665. Accuracy 0.889.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37129557\n",
      "Train loss (w/o reg) on all data: 0.3710591\n",
      "Test loss (w/o reg) on all data: 0.3578612\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0024323899\n",
      "Norm of the params: 2.174738\n",
      "                Loss: fixed  17 labels. Loss 0.35786. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51705295\n",
      "Train loss (w/o reg) on all data: 0.51695865\n",
      "Test loss (w/o reg) on all data: 0.39946544\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.007976078\n",
      "Norm of the params: 1.3733718\n",
      "              Random: fixed   6 labels. Loss 0.39947. Accuracy 0.800.\n",
      "### Flips: 40, rs: 4, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [155] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40588167\n",
      "Train loss (w/o reg) on all data: 0.40564576\n",
      "Test loss (w/o reg) on all data: 0.3342643\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0022568607\n",
      "Norm of the params: 2.1722279\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.33426. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [348] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35228643\n",
      "Train loss (w/o reg) on all data: 0.35201493\n",
      "Test loss (w/o reg) on all data: 0.38191855\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.000370523\n",
      "Norm of the params: 2.3302176\n",
      "                Loss: fixed  21 labels. Loss 0.38192. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5170488\n",
      "Train loss (w/o reg) on all data: 0.5169546\n",
      "Test loss (w/o reg) on all data: 0.40018633\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0023688045\n",
      "Norm of the params: 1.3725344\n",
      "              Random: fixed   6 labels. Loss 0.40019. Accuracy 0.800.\n",
      "### Flips: 40, rs: 4, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [157] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38850012\n",
      "Train loss (w/o reg) on all data: 0.38823065\n",
      "Test loss (w/o reg) on all data: 0.33753964\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0016171827\n",
      "Norm of the params: 2.3215303\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.33754. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [118] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31627956\n",
      "Train loss (w/o reg) on all data: 0.31594554\n",
      "Test loss (w/o reg) on all data: 0.4034437\n",
      "Train acc on all data:  0.8820754716981132\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002131425\n",
      "Norm of the params: 2.5846636\n",
      "                Loss: fixed  26 labels. Loss 0.40344. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [226] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5095508\n",
      "Train loss (w/o reg) on all data: 0.5094643\n",
      "Test loss (w/o reg) on all data: 0.4027005\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006678031\n",
      "Norm of the params: 1.3150398\n",
      "              Random: fixed   9 labels. Loss 0.40270. Accuracy 0.800.\n",
      "### Flips: 40, rs: 4, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35667595\n",
      "Train loss (w/o reg) on all data: 0.35636866\n",
      "Test loss (w/o reg) on all data: 0.36198574\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00063671096\n",
      "Norm of the params: 2.4791124\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.36199. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.30752188\n",
      "Train loss (w/o reg) on all data: 0.3071608\n",
      "Test loss (w/o reg) on all data: 0.41425622\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00034038658\n",
      "Norm of the params: 2.687317\n",
      "                Loss: fixed  27 labels. Loss 0.41426. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [313] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49471432\n",
      "Train loss (w/o reg) on all data: 0.49461636\n",
      "Test loss (w/o reg) on all data: 0.394857\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017943566\n",
      "Norm of the params: 1.3998064\n",
      "              Random: fixed  10 labels. Loss 0.39486. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54404306\n",
      "Train loss (w/o reg) on all data: 0.5438737\n",
      "Test loss (w/o reg) on all data: 0.4632742\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0037960235\n",
      "Norm of the params: 1.8403516\n",
      "Flipped loss: 0.46327. Accuracy: 0.800\n",
      "### Flips: 40, rs: 5, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [153] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50599813\n",
      "Train loss (w/o reg) on all data: 0.5057698\n",
      "Test loss (w/o reg) on all data: 0.48985985\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.004713503\n",
      "Norm of the params: 2.136941\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.48986. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44491902\n",
      "Train loss (w/o reg) on all data: 0.44454768\n",
      "Test loss (w/o reg) on all data: 0.45378467\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00058472663\n",
      "Norm of the params: 2.7251933\n",
      "                Loss: fixed  10 labels. Loss 0.45378. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [154] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53348356\n",
      "Train loss (w/o reg) on all data: 0.5333037\n",
      "Test loss (w/o reg) on all data: 0.43599096\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013165585\n",
      "Norm of the params: 1.8966712\n",
      "              Random: fixed   3 labels. Loss 0.43599. Accuracy 0.800.\n",
      "### Flips: 40, rs: 5, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [179] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48251346\n",
      "Train loss (w/o reg) on all data: 0.482261\n",
      "Test loss (w/o reg) on all data: 0.44532487\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002552132\n",
      "Norm of the params: 2.246985\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.44532. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40512905\n",
      "Train loss (w/o reg) on all data: 0.40471983\n",
      "Test loss (w/o reg) on all data: 0.46540302\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.000903487\n",
      "Norm of the params: 2.8607917\n",
      "                Loss: fixed  14 labels. Loss 0.46540. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [308] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50652534\n",
      "Train loss (w/o reg) on all data: 0.50631773\n",
      "Test loss (w/o reg) on all data: 0.43963492\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0065809707\n",
      "Norm of the params: 2.0375528\n",
      "              Random: fixed   8 labels. Loss 0.43963. Accuracy 0.800.\n",
      "### Flips: 40, rs: 5, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [102] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43526176\n",
      "Train loss (w/o reg) on all data: 0.4349546\n",
      "Test loss (w/o reg) on all data: 0.44739515\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0051480564\n",
      "Norm of the params: 2.4784606\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.44740. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [329] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3718668\n",
      "Train loss (w/o reg) on all data: 0.37137902\n",
      "Test loss (w/o reg) on all data: 0.4595872\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0004977731\n",
      "Norm of the params: 3.12341\n",
      "                Loss: fixed  18 labels. Loss 0.45959. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [358] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49551183\n",
      "Train loss (w/o reg) on all data: 0.4952979\n",
      "Test loss (w/o reg) on all data: 0.42328233\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0046183025\n",
      "Norm of the params: 2.0684805\n",
      "              Random: fixed  10 labels. Loss 0.42328. Accuracy 0.800.\n",
      "### Flips: 40, rs: 5, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [291] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40667826\n",
      "Train loss (w/o reg) on all data: 0.40628916\n",
      "Test loss (w/o reg) on all data: 0.45799938\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0065724677\n",
      "Norm of the params: 2.78962\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.45800. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35685113\n",
      "Train loss (w/o reg) on all data: 0.35631227\n",
      "Test loss (w/o reg) on all data: 0.4949617\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011718808\n",
      "Norm of the params: 3.2828293\n",
      "                Loss: fixed  20 labels. Loss 0.49496. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [310] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49853614\n",
      "Train loss (w/o reg) on all data: 0.49835518\n",
      "Test loss (w/o reg) on all data: 0.4012637\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013330344\n",
      "Norm of the params: 1.9024532\n",
      "              Random: fixed  13 labels. Loss 0.40126. Accuracy 0.800.\n",
      "### Flips: 40, rs: 5, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39843163\n",
      "Train loss (w/o reg) on all data: 0.39801472\n",
      "Test loss (w/o reg) on all data: 0.45484343\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020686153\n",
      "Norm of the params: 2.8875792\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.45484. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33390686\n",
      "Train loss (w/o reg) on all data: 0.33329517\n",
      "Test loss (w/o reg) on all data: 0.5212391\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00077114353\n",
      "Norm of the params: 3.497715\n",
      "                Loss: fixed  23 labels. Loss 0.52124. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [336] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49353293\n",
      "Train loss (w/o reg) on all data: 0.49334437\n",
      "Test loss (w/o reg) on all data: 0.38648674\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0045012683\n",
      "Norm of the params: 1.9419684\n",
      "              Random: fixed  15 labels. Loss 0.38649. Accuracy 0.800.\n",
      "### Flips: 40, rs: 5, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36427787\n",
      "Train loss (w/o reg) on all data: 0.3638132\n",
      "Test loss (w/o reg) on all data: 0.43767267\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0019058925\n",
      "Norm of the params: 3.048506\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.43767. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [362] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3152553\n",
      "Train loss (w/o reg) on all data: 0.31466395\n",
      "Test loss (w/o reg) on all data: 0.501219\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00048135826\n",
      "Norm of the params: 3.4390988\n",
      "                Loss: fixed  27 labels. Loss 0.50122. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [402] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49352995\n",
      "Train loss (w/o reg) on all data: 0.49334046\n",
      "Test loss (w/o reg) on all data: 0.38621974\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007451416\n",
      "Norm of the params: 1.9466994\n",
      "              Random: fixed  15 labels. Loss 0.38622. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [318] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5813352\n",
      "Train loss (w/o reg) on all data: 0.5812725\n",
      "Test loss (w/o reg) on all data: 0.4552301\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008658409\n",
      "Norm of the params: 1.1198089\n",
      "Flipped loss: 0.45523. Accuracy: 0.822\n",
      "### Flips: 40, rs: 6, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [123] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5258752\n",
      "Train loss (w/o reg) on all data: 0.5257658\n",
      "Test loss (w/o reg) on all data: 0.40960872\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011915583\n",
      "Norm of the params: 1.4794414\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.40961. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46239167\n",
      "Train loss (w/o reg) on all data: 0.46217722\n",
      "Test loss (w/o reg) on all data: 0.438937\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017415058\n",
      "Norm of the params: 2.071037\n",
      "                Loss: fixed  10 labels. Loss 0.43894. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [379] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57968235\n",
      "Train loss (w/o reg) on all data: 0.57962096\n",
      "Test loss (w/o reg) on all data: 0.45526597\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009459016\n",
      "Norm of the params: 1.1079493\n",
      "              Random: fixed   2 labels. Loss 0.45527. Accuracy 0.822.\n",
      "### Flips: 40, rs: 6, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [123] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5020994\n",
      "Train loss (w/o reg) on all data: 0.50196433\n",
      "Test loss (w/o reg) on all data: 0.4146101\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002025559\n",
      "Norm of the params: 1.6436743\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41461. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [333] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40602356\n",
      "Train loss (w/o reg) on all data: 0.40570673\n",
      "Test loss (w/o reg) on all data: 0.44781908\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.005075825\n",
      "Norm of the params: 2.5172157\n",
      "                Loss: fixed  15 labels. Loss 0.44782. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [313] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56765634\n",
      "Train loss (w/o reg) on all data: 0.56758785\n",
      "Test loss (w/o reg) on all data: 0.45519498\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003194939\n",
      "Norm of the params: 1.1705894\n",
      "              Random: fixed   4 labels. Loss 0.45519. Accuracy 0.800.\n",
      "### Flips: 40, rs: 6, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46450606\n",
      "Train loss (w/o reg) on all data: 0.46436194\n",
      "Test loss (w/o reg) on all data: 0.39476034\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0002517417\n",
      "Norm of the params: 1.697852\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.39476. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [310] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.370565\n",
      "Train loss (w/o reg) on all data: 0.37018824\n",
      "Test loss (w/o reg) on all data: 0.45131636\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009371041\n",
      "Norm of the params: 2.7450159\n",
      "                Loss: fixed  20 labels. Loss 0.45132. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [379] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57143974\n",
      "Train loss (w/o reg) on all data: 0.57138544\n",
      "Test loss (w/o reg) on all data: 0.44708365\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00079362834\n",
      "Norm of the params: 1.0422692\n",
      "              Random: fixed   6 labels. Loss 0.44708. Accuracy 0.800.\n",
      "### Flips: 40, rs: 6, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42649832\n",
      "Train loss (w/o reg) on all data: 0.42628494\n",
      "Test loss (w/o reg) on all data: 0.4065607\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002256381\n",
      "Norm of the params: 2.0658655\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.40656. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34673813\n",
      "Train loss (w/o reg) on all data: 0.34625527\n",
      "Test loss (w/o reg) on all data: 0.4646479\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017298819\n",
      "Norm of the params: 3.107577\n",
      "                Loss: fixed  23 labels. Loss 0.46465. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [343] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56538326\n",
      "Train loss (w/o reg) on all data: 0.5653267\n",
      "Test loss (w/o reg) on all data: 0.43304852\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013051063\n",
      "Norm of the params: 1.0636545\n",
      "              Random: fixed   8 labels. Loss 0.43305. Accuracy 0.822.\n",
      "### Flips: 40, rs: 6, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4139824\n",
      "Train loss (w/o reg) on all data: 0.41375574\n",
      "Test loss (w/o reg) on all data: 0.40635538\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0023040953\n",
      "Norm of the params: 2.1290805\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.40636. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [334] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33363682\n",
      "Train loss (w/o reg) on all data: 0.3331574\n",
      "Test loss (w/o reg) on all data: 0.44902563\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019370785\n",
      "Norm of the params: 3.096561\n",
      "                Loss: fixed  26 labels. Loss 0.44903. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [426] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55882925\n",
      "Train loss (w/o reg) on all data: 0.5587755\n",
      "Test loss (w/o reg) on all data: 0.41411933\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0107138315\n",
      "Norm of the params: 1.0371922\n",
      "              Random: fixed  10 labels. Loss 0.41412. Accuracy 0.822.\n",
      "### Flips: 40, rs: 6, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39276528\n",
      "Train loss (w/o reg) on all data: 0.3925109\n",
      "Test loss (w/o reg) on all data: 0.39121416\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00038787074\n",
      "Norm of the params: 2.255689\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.39121. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [334] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32301804\n",
      "Train loss (w/o reg) on all data: 0.32253718\n",
      "Test loss (w/o reg) on all data: 0.45024875\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0017583863\n",
      "Norm of the params: 3.1011288\n",
      "                Loss: fixed  29 labels. Loss 0.45025. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [373] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52456146\n",
      "Train loss (w/o reg) on all data: 0.52446616\n",
      "Test loss (w/o reg) on all data: 0.41538587\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006333038\n",
      "Norm of the params: 1.3806262\n",
      "              Random: fixed  14 labels. Loss 0.41539. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [127] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60274506\n",
      "Train loss (w/o reg) on all data: 0.60269654\n",
      "Test loss (w/o reg) on all data: 0.4457845\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.016429706\n",
      "Norm of the params: 0.98505497\n",
      "Flipped loss: 0.44578. Accuracy: 0.822\n",
      "### Flips: 40, rs: 7, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [123] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54431033\n",
      "Train loss (w/o reg) on all data: 0.54423493\n",
      "Test loss (w/o reg) on all data: 0.39717704\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011365351\n",
      "Norm of the params: 1.2281674\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.39718. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [134] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5115233\n",
      "Train loss (w/o reg) on all data: 0.5113758\n",
      "Test loss (w/o reg) on all data: 0.38122556\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0048742215\n",
      "Norm of the params: 1.7178534\n",
      "                Loss: fixed   9 labels. Loss 0.38123. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [353] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5836659\n",
      "Train loss (w/o reg) on all data: 0.58361804\n",
      "Test loss (w/o reg) on all data: 0.42599338\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010744083\n",
      "Norm of the params: 0.97817683\n",
      "              Random: fixed   2 labels. Loss 0.42599. Accuracy 0.844.\n",
      "### Flips: 40, rs: 7, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [131] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53596467\n",
      "Train loss (w/o reg) on all data: 0.5358586\n",
      "Test loss (w/o reg) on all data: 0.3982887\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0046148486\n",
      "Norm of the params: 1.4566556\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.39829. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4171315\n",
      "Train loss (w/o reg) on all data: 0.4168452\n",
      "Test loss (w/o reg) on all data: 0.36085713\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013974205\n",
      "Norm of the params: 2.3929014\n",
      "                Loss: fixed  18 labels. Loss 0.36086. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.558138\n",
      "Train loss (w/o reg) on all data: 0.55808455\n",
      "Test loss (w/o reg) on all data: 0.40776545\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012814244\n",
      "Norm of the params: 1.0338649\n",
      "              Random: fixed   6 labels. Loss 0.40777. Accuracy 0.822.\n",
      "### Flips: 40, rs: 7, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [299] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4969065\n",
      "Train loss (w/o reg) on all data: 0.49676204\n",
      "Test loss (w/o reg) on all data: 0.3815184\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010549028\n",
      "Norm of the params: 1.6996484\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.38152. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38204104\n",
      "Train loss (w/o reg) on all data: 0.38173595\n",
      "Test loss (w/o reg) on all data: 0.38657185\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00092671806\n",
      "Norm of the params: 2.4702246\n",
      "                Loss: fixed  22 labels. Loss 0.38657. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss (w reg) on all data: 0.5577929\n",
      "Train loss (w/o reg) on all data: 0.5577389\n",
      "Test loss (w/o reg) on all data: 0.40991178\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013035389\n",
      "Norm of the params: 1.0389742\n",
      "              Random: fixed   8 labels. Loss 0.40991. Accuracy 0.800.\n",
      "### Flips: 40, rs: 7, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [84] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46900502\n",
      "Train loss (w/o reg) on all data: 0.46880987\n",
      "Test loss (w/o reg) on all data: 0.366649\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00134641\n",
      "Norm of the params: 1.9756187\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.36665. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34910068\n",
      "Train loss (w/o reg) on all data: 0.34866837\n",
      "Test loss (w/o reg) on all data: 0.38918135\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017971438\n",
      "Norm of the params: 2.9404497\n",
      "                Loss: fixed  27 labels. Loss 0.38918. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [358] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55013144\n",
      "Train loss (w/o reg) on all data: 0.55007046\n",
      "Test loss (w/o reg) on all data: 0.40579012\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001594113\n",
      "Norm of the params: 1.1045653\n",
      "              Random: fixed   9 labels. Loss 0.40579. Accuracy 0.800.\n",
      "### Flips: 40, rs: 7, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43002218\n",
      "Train loss (w/o reg) on all data: 0.4297583\n",
      "Test loss (w/o reg) on all data: 0.34776914\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.000841762\n",
      "Norm of the params: 2.2972565\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.34777. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [323] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33586833\n",
      "Train loss (w/o reg) on all data: 0.33546397\n",
      "Test loss (w/o reg) on all data: 0.3984584\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.005656173\n",
      "Norm of the params: 2.843752\n",
      "                Loss: fixed  29 labels. Loss 0.39846. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53045046\n",
      "Train loss (w/o reg) on all data: 0.53038466\n",
      "Test loss (w/o reg) on all data: 0.40125906\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014158408\n",
      "Norm of the params: 1.1469791\n",
      "              Random: fixed  12 labels. Loss 0.40126. Accuracy 0.800.\n",
      "### Flips: 40, rs: 7, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42212915\n",
      "Train loss (w/o reg) on all data: 0.42189386\n",
      "Test loss (w/o reg) on all data: 0.34699583\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0021354696\n",
      "Norm of the params: 2.1693416\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.34700. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [279] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3323402\n",
      "Train loss (w/o reg) on all data: 0.3319305\n",
      "Test loss (w/o reg) on all data: 0.40096283\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00043168946\n",
      "Norm of the params: 2.862587\n",
      "                Loss: fixed  31 labels. Loss 0.40096. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53045315\n",
      "Train loss (w/o reg) on all data: 0.5303871\n",
      "Test loss (w/o reg) on all data: 0.40133345\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002847046\n",
      "Norm of the params: 1.149393\n",
      "              Random: fixed  12 labels. Loss 0.40133. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [136] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5496864\n",
      "Train loss (w/o reg) on all data: 0.5495734\n",
      "Test loss (w/o reg) on all data: 0.52796674\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0012921832\n",
      "Norm of the params: 1.5031177\n",
      "Flipped loss: 0.52797. Accuracy: 0.733\n",
      "### Flips: 40, rs: 8, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49553677\n",
      "Train loss (w/o reg) on all data: 0.49536532\n",
      "Test loss (w/o reg) on all data: 0.49106637\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.012028933\n",
      "Norm of the params: 1.8517429\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.49107. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [186] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44596776\n",
      "Train loss (w/o reg) on all data: 0.44572294\n",
      "Test loss (w/o reg) on all data: 0.52888703\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.00066030165\n",
      "Norm of the params: 2.2127662\n",
      "                Loss: fixed   9 labels. Loss 0.52889. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [187] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55011886\n",
      "Train loss (w/o reg) on all data: 0.55000955\n",
      "Test loss (w/o reg) on all data: 0.5176216\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0057654474\n",
      "Norm of the params: 1.4786102\n",
      "              Random: fixed   1 labels. Loss 0.51762. Accuracy 0.733.\n",
      "### Flips: 40, rs: 8, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4602743\n",
      "Train loss (w/o reg) on all data: 0.46007884\n",
      "Test loss (w/o reg) on all data: 0.49305877\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0022556295\n",
      "Norm of the params: 1.977267\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.49306. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [115] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39062542\n",
      "Train loss (w/o reg) on all data: 0.39030975\n",
      "Test loss (w/o reg) on all data: 0.5214786\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.005590893\n",
      "Norm of the params: 2.512656\n",
      "                Loss: fixed  14 labels. Loss 0.52148. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [331] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5501161\n",
      "Train loss (w/o reg) on all data: 0.55000645\n",
      "Test loss (w/o reg) on all data: 0.51767004\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0060037957\n",
      "Norm of the params: 1.4808767\n",
      "              Random: fixed   1 labels. Loss 0.51767. Accuracy 0.733.\n",
      "### Flips: 40, rs: 8, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [154] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40942338\n",
      "Train loss (w/o reg) on all data: 0.40916187\n",
      "Test loss (w/o reg) on all data: 0.5148073\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0061149313\n",
      "Norm of the params: 2.2869408\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.51481. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [299] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35122824\n",
      "Train loss (w/o reg) on all data: 0.35082093\n",
      "Test loss (w/o reg) on all data: 0.5718068\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0006100595\n",
      "Norm of the params: 2.8541536\n",
      "                Loss: fixed  19 labels. Loss 0.57181. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [297] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55591524\n",
      "Train loss (w/o reg) on all data: 0.5558107\n",
      "Test loss (w/o reg) on all data: 0.5162526\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0028596548\n",
      "Norm of the params: 1.4458637\n",
      "              Random: fixed   2 labels. Loss 0.51625. Accuracy 0.733.\n",
      "### Flips: 40, rs: 8, checks: 40\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [303] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3878945\n",
      "Train loss (w/o reg) on all data: 0.38758242\n",
      "Test loss (w/o reg) on all data: 0.54663473\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00078191503\n",
      "Norm of the params: 2.4983175\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.54663. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [335] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32979855\n",
      "Train loss (w/o reg) on all data: 0.32930648\n",
      "Test loss (w/o reg) on all data: 0.570663\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017502861\n",
      "Norm of the params: 3.1371303\n",
      "                Loss: fixed  22 labels. Loss 0.57066. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [353] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5556412\n",
      "Train loss (w/o reg) on all data: 0.5555494\n",
      "Test loss (w/o reg) on all data: 0.50552934\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.003024943\n",
      "Norm of the params: 1.3549004\n",
      "              Random: fixed   4 labels. Loss 0.50553. Accuracy 0.733.\n",
      "### Flips: 40, rs: 8, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [471] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37939176\n",
      "Train loss (w/o reg) on all data: 0.37905645\n",
      "Test loss (w/o reg) on all data: 0.5543031\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0006291494\n",
      "Norm of the params: 2.589561\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.55430. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [322] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32150772\n",
      "Train loss (w/o reg) on all data: 0.3209834\n",
      "Test loss (w/o reg) on all data: 0.5770006\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013733824\n",
      "Norm of the params: 3.238217\n",
      "                Loss: fixed  24 labels. Loss 0.57700. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [387] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5546032\n",
      "Train loss (w/o reg) on all data: 0.55452335\n",
      "Test loss (w/o reg) on all data: 0.49106738\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00092791766\n",
      "Norm of the params: 1.2638397\n",
      "              Random: fixed   7 labels. Loss 0.49107. Accuracy 0.756.\n",
      "### Flips: 40, rs: 8, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [340] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35286224\n",
      "Train loss (w/o reg) on all data: 0.35247183\n",
      "Test loss (w/o reg) on all data: 0.56380063\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0028220387\n",
      "Norm of the params: 2.7942786\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.56380. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3215081\n",
      "Train loss (w/o reg) on all data: 0.3209837\n",
      "Test loss (w/o reg) on all data: 0.5772613\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007832656\n",
      "Norm of the params: 3.238554\n",
      "                Loss: fixed  24 labels. Loss 0.57726. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [388] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54095745\n",
      "Train loss (w/o reg) on all data: 0.5408705\n",
      "Test loss (w/o reg) on all data: 0.46843347\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0029539117\n",
      "Norm of the params: 1.3188204\n",
      "              Random: fixed   9 labels. Loss 0.46843. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [328] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59975106\n",
      "Train loss (w/o reg) on all data: 0.5996847\n",
      "Test loss (w/o reg) on all data: 0.4229818\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.003028441\n",
      "Norm of the params: 1.1517463\n",
      "Flipped loss: 0.42298. Accuracy: 0.844\n",
      "### Flips: 40, rs: 9, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55903864\n",
      "Train loss (w/o reg) on all data: 0.5589441\n",
      "Test loss (w/o reg) on all data: 0.3655824\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0007931198\n",
      "Norm of the params: 1.3750218\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.36558. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [130] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.516352\n",
      "Train loss (w/o reg) on all data: 0.5162299\n",
      "Test loss (w/o reg) on all data: 0.34996215\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0010441439\n",
      "Norm of the params: 1.5625612\n",
      "                Loss: fixed   9 labels. Loss 0.34996. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5914666\n",
      "Train loss (w/o reg) on all data: 0.5913936\n",
      "Test loss (w/o reg) on all data: 0.4180636\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0031262804\n",
      "Norm of the params: 1.2085054\n",
      "              Random: fixed   2 labels. Loss 0.41806. Accuracy 0.822.\n",
      "### Flips: 40, rs: 9, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53232276\n",
      "Train loss (w/o reg) on all data: 0.5321974\n",
      "Test loss (w/o reg) on all data: 0.3504371\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0041782963\n",
      "Norm of the params: 1.5833013\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.35044. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4487291\n",
      "Train loss (w/o reg) on all data: 0.4485317\n",
      "Test loss (w/o reg) on all data: 0.33603853\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0018810419\n",
      "Norm of the params: 1.9870684\n",
      "                Loss: fixed  16 labels. Loss 0.33604. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5918945\n",
      "Train loss (w/o reg) on all data: 0.5918223\n",
      "Test loss (w/o reg) on all data: 0.4208581\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0030548852\n",
      "Norm of the params: 1.2013663\n",
      "              Random: fixed   3 labels. Loss 0.42086. Accuracy 0.822.\n",
      "### Flips: 40, rs: 9, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [155] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50928086\n",
      "Train loss (w/o reg) on all data: 0.5091464\n",
      "Test loss (w/o reg) on all data: 0.3392917\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0013776605\n",
      "Norm of the params: 1.63983\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.33929. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40137175\n",
      "Train loss (w/o reg) on all data: 0.40110376\n",
      "Test loss (w/o reg) on all data: 0.3433308\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0005210866\n",
      "Norm of the params: 2.3150697\n",
      "                Loss: fixed  22 labels. Loss 0.34333. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [347] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58780456\n",
      "Train loss (w/o reg) on all data: 0.58773136\n",
      "Test loss (w/o reg) on all data: 0.4270373\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0026883935\n",
      "Norm of the params: 1.2098141\n",
      "              Random: fixed   4 labels. Loss 0.42704. Accuracy 0.822.\n",
      "### Flips: 40, rs: 9, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [190] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48258176\n",
      "Train loss (w/o reg) on all data: 0.48241377\n",
      "Test loss (w/o reg) on all data: 0.33607033\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001556728\n",
      "Norm of the params: 1.8330535\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.33607. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37953916\n",
      "Train loss (w/o reg) on all data: 0.37925875\n",
      "Test loss (w/o reg) on all data: 0.3737607\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0031368053\n",
      "Norm of the params: 2.368214\n",
      "                Loss: fixed  25 labels. Loss 0.37376. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5868243\n",
      "Train loss (w/o reg) on all data: 0.5867488\n",
      "Test loss (w/o reg) on all data: 0.4361483\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019875043\n",
      "Norm of the params: 1.2287453\n",
      "              Random: fixed   5 labels. Loss 0.43615. Accuracy 0.800.\n",
      "### Flips: 40, rs: 9, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [266] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47759852\n",
      "Train loss (w/o reg) on all data: 0.4774553\n",
      "Test loss (w/o reg) on all data: 0.3347616\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00053337193\n",
      "Norm of the params: 1.6925036\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.33476. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [158] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36149198\n",
      "Train loss (w/o reg) on all data: 0.36113948\n",
      "Test loss (w/o reg) on all data: 0.36416423\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001226083\n",
      "Norm of the params: 2.6551962\n",
      "                Loss: fixed  29 labels. Loss 0.36416. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [315] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57484376\n",
      "Train loss (w/o reg) on all data: 0.57476556\n",
      "Test loss (w/o reg) on all data: 0.43466762\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0044912156\n",
      "Norm of the params: 1.250636\n",
      "              Random: fixed   7 labels. Loss 0.43467. Accuracy 0.822.\n",
      "### Flips: 40, rs: 9, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46119934\n",
      "Train loss (w/o reg) on all data: 0.46101704\n",
      "Test loss (w/o reg) on all data: 0.3223023\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00091908744\n",
      "Norm of the params: 1.9095275\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.32230. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [283] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33921167\n",
      "Train loss (w/o reg) on all data: 0.33877492\n",
      "Test loss (w/o reg) on all data: 0.37789032\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.000477165\n",
      "Norm of the params: 2.9555151\n",
      "                Loss: fixed  32 labels. Loss 0.37789. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [349] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.574843\n",
      "Train loss (w/o reg) on all data: 0.5747649\n",
      "Test loss (w/o reg) on all data: 0.4348822\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003281461\n",
      "Norm of the params: 1.2494396\n",
      "              Random: fixed   7 labels. Loss 0.43488. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5709409\n",
      "Train loss (w/o reg) on all data: 0.57087815\n",
      "Test loss (w/o reg) on all data: 0.44737798\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002553622\n",
      "Norm of the params: 1.1203749\n",
      "Flipped loss: 0.44738. Accuracy: 0.800\n",
      "### Flips: 40, rs: 10, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5108387\n",
      "Train loss (w/o reg) on all data: 0.51074046\n",
      "Test loss (w/o reg) on all data: 0.42452765\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004669984\n",
      "Norm of the params: 1.4016004\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.42453. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [198] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47739628\n",
      "Train loss (w/o reg) on all data: 0.47723642\n",
      "Test loss (w/o reg) on all data: 0.43312907\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019268534\n",
      "Norm of the params: 1.7880384\n",
      "                Loss: fixed   8 labels. Loss 0.43313. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [126] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56873727\n",
      "Train loss (w/o reg) on all data: 0.5686701\n",
      "Test loss (w/o reg) on all data: 0.45976257\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002656319\n",
      "Norm of the params: 1.1589254\n",
      "              Random: fixed   2 labels. Loss 0.45976. Accuracy 0.800.\n",
      "### Flips: 40, rs: 10, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4811634\n",
      "Train loss (w/o reg) on all data: 0.48104447\n",
      "Test loss (w/o reg) on all data: 0.4073312\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006735489\n",
      "Norm of the params: 1.5423026\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.40733. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3895798\n",
      "Train loss (w/o reg) on all data: 0.38926104\n",
      "Test loss (w/o reg) on all data: 0.42956114\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017295309\n",
      "Norm of the params: 2.5249112\n",
      "                Loss: fixed  16 labels. Loss 0.42956. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.554209\n",
      "Train loss (w/o reg) on all data: 0.5541279\n",
      "Test loss (w/o reg) on all data: 0.45302925\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0023178216\n",
      "Norm of the params: 1.2738178\n",
      "              Random: fixed   5 labels. Loss 0.45303. Accuracy 0.778.\n",
      "### Flips: 40, rs: 10, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [314] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45919675\n",
      "Train loss (w/o reg) on all data: 0.45905948\n",
      "Test loss (w/o reg) on all data: 0.38411\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.000976136\n",
      "Norm of the params: 1.656936\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.38411. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [396] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.344412\n",
      "Train loss (w/o reg) on all data: 0.3439437\n",
      "Test loss (w/o reg) on all data: 0.42899767\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011975919\n",
      "Norm of the params: 3.0604186\n",
      "                Loss: fixed  20 labels. Loss 0.42900. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [177] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5569981\n",
      "Train loss (w/o reg) on all data: 0.5569199\n",
      "Test loss (w/o reg) on all data: 0.45900726\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00086855557\n",
      "Norm of the params: 1.2508019\n",
      "              Random: fixed   6 labels. Loss 0.45901. Accuracy 0.778.\n",
      "### Flips: 40, rs: 10, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39876023\n",
      "Train loss (w/o reg) on all data: 0.3984933\n",
      "Test loss (w/o reg) on all data: 0.3893589\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.003444917\n",
      "Norm of the params: 2.3105443\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.38936. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [308] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32569703\n",
      "Train loss (w/o reg) on all data: 0.32523957\n",
      "Test loss (w/o reg) on all data: 0.45775694\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.000498123\n",
      "Norm of the params: 3.024826\n",
      "                Loss: fixed  25 labels. Loss 0.45776. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [295] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55126965\n",
      "Train loss (w/o reg) on all data: 0.5511789\n",
      "Test loss (w/o reg) on all data: 0.45093867\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004445186\n",
      "Norm of the params: 1.3475893\n",
      "              Random: fixed   7 labels. Loss 0.45094. Accuracy 0.800.\n",
      "### Flips: 40, rs: 10, checks: 50\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36957598\n",
      "Train loss (w/o reg) on all data: 0.3692267\n",
      "Test loss (w/o reg) on all data: 0.43139485\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008585725\n",
      "Norm of the params: 2.642985\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.43139. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.30842486\n",
      "Train loss (w/o reg) on all data: 0.30787492\n",
      "Test loss (w/o reg) on all data: 0.46146956\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0023027463\n",
      "Norm of the params: 3.316446\n",
      "                Loss: fixed  28 labels. Loss 0.46147. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [371] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5512676\n",
      "Train loss (w/o reg) on all data: 0.5511769\n",
      "Test loss (w/o reg) on all data: 0.45064515\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015058714\n",
      "Norm of the params: 1.3468683\n",
      "              Random: fixed   7 labels. Loss 0.45065. Accuracy 0.800.\n",
      "### Flips: 40, rs: 10, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [171] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34417552\n",
      "Train loss (w/o reg) on all data: 0.3437471\n",
      "Test loss (w/o reg) on all data: 0.44961742\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00653789\n",
      "Norm of the params: 2.9271789\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.44962. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [366] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.30607688\n",
      "Train loss (w/o reg) on all data: 0.30552465\n",
      "Test loss (w/o reg) on all data: 0.45605987\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00090765004\n",
      "Norm of the params: 3.323325\n",
      "                Loss: fixed  30 labels. Loss 0.45606. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [340] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5318001\n",
      "Train loss (w/o reg) on all data: 0.53168505\n",
      "Test loss (w/o reg) on all data: 0.4607198\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.004787293\n",
      "Norm of the params: 1.5166389\n",
      "              Random: fixed  11 labels. Loss 0.46072. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [314] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5113622\n",
      "Train loss (w/o reg) on all data: 0.51125133\n",
      "Test loss (w/o reg) on all data: 0.34634364\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0021863796\n",
      "Norm of the params: 1.4889772\n",
      "Flipped loss: 0.34634. Accuracy: 0.822\n",
      "### Flips: 40, rs: 11, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43866795\n",
      "Train loss (w/o reg) on all data: 0.43851796\n",
      "Test loss (w/o reg) on all data: 0.30771458\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0028952577\n",
      "Norm of the params: 1.7320051\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.30771. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43841845\n",
      "Train loss (w/o reg) on all data: 0.43822867\n",
      "Test loss (w/o reg) on all data: 0.32913944\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0017180823\n",
      "Norm of the params: 1.9482709\n",
      "                Loss: fixed   6 labels. Loss 0.32914. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [211] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5039695\n",
      "Train loss (w/o reg) on all data: 0.5038455\n",
      "Test loss (w/o reg) on all data: 0.35253865\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006684093\n",
      "Norm of the params: 1.5745877\n",
      "              Random: fixed   1 labels. Loss 0.35254. Accuracy 0.822.\n",
      "### Flips: 40, rs: 11, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [199] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39488968\n",
      "Train loss (w/o reg) on all data: 0.3946355\n",
      "Test loss (w/o reg) on all data: 0.30271146\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.000837345\n",
      "Norm of the params: 2.2547452\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.30271. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35553876\n",
      "Train loss (w/o reg) on all data: 0.35518697\n",
      "Test loss (w/o reg) on all data: 0.34258303\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00049089006\n",
      "Norm of the params: 2.6525378\n",
      "                Loss: fixed  16 labels. Loss 0.34258. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47537082\n",
      "Train loss (w/o reg) on all data: 0.4752347\n",
      "Test loss (w/o reg) on all data: 0.3507174\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015978954\n",
      "Norm of the params: 1.6501008\n",
      "              Random: fixed   5 labels. Loss 0.35072. Accuracy 0.822.\n",
      "### Flips: 40, rs: 11, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [258] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3665466\n",
      "Train loss (w/o reg) on all data: 0.36621517\n",
      "Test loss (w/o reg) on all data: 0.3131662\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0018303118\n",
      "Norm of the params: 2.5745804\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.31317. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33885273\n",
      "Train loss (w/o reg) on all data: 0.33847103\n",
      "Test loss (w/o reg) on all data: 0.34918246\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.001776453\n",
      "Norm of the params: 2.7629886\n",
      "                Loss: fixed  18 labels. Loss 0.34918. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [143] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47617158\n",
      "Train loss (w/o reg) on all data: 0.47603765\n",
      "Test loss (w/o reg) on all data: 0.34823495\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014505424\n",
      "Norm of the params: 1.6367395\n",
      "              Random: fixed   6 labels. Loss 0.34823. Accuracy 0.822.\n",
      "### Flips: 40, rs: 11, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37132412\n",
      "Train loss (w/o reg) on all data: 0.37099946\n",
      "Test loss (w/o reg) on all data: 0.31877166\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0013228656\n",
      "Norm of the params: 2.5481935\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.31877. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3147673\n",
      "Train loss (w/o reg) on all data: 0.31440225\n",
      "Test loss (w/o reg) on all data: 0.37702006\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0013139526\n",
      "Norm of the params: 2.7019768\n",
      "                Loss: fixed  22 labels. Loss 0.37702. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4650142\n",
      "Train loss (w/o reg) on all data: 0.4648739\n",
      "Test loss (w/o reg) on all data: 0.3464311\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006292079\n",
      "Norm of the params: 1.67498\n",
      "              Random: fixed   7 labels. Loss 0.34643. Accuracy 0.844.\n",
      "### Flips: 40, rs: 11, checks: 50\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34520775\n",
      "Train loss (w/o reg) on all data: 0.3447927\n",
      "Test loss (w/o reg) on all data: 0.35351273\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001558247\n",
      "Norm of the params: 2.881175\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.35351. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [339] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.29917586\n",
      "Train loss (w/o reg) on all data: 0.29876554\n",
      "Test loss (w/o reg) on all data: 0.40003508\n",
      "Train acc on all data:  0.8820754716981132\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00051660766\n",
      "Norm of the params: 2.8647022\n",
      "                Loss: fixed  27 labels. Loss 0.40004. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [293] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46899262\n",
      "Train loss (w/o reg) on all data: 0.4688625\n",
      "Test loss (w/o reg) on all data: 0.35366848\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019042013\n",
      "Norm of the params: 1.6131853\n",
      "              Random: fixed   8 labels. Loss 0.35367. Accuracy 0.822.\n",
      "### Flips: 40, rs: 11, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32153717\n",
      "Train loss (w/o reg) on all data: 0.32105052\n",
      "Test loss (w/o reg) on all data: 0.38159397\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00038889234\n",
      "Norm of the params: 3.1197245\n",
      "     Influence (LOO): fixed  28 labels. Loss 0.38159. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [315] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.2999591\n",
      "Train loss (w/o reg) on all data: 0.2994925\n",
      "Test loss (w/o reg) on all data: 0.40188736\n",
      "Train acc on all data:  0.8820754716981132\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.000620771\n",
      "Norm of the params: 3.0547993\n",
      "                Loss: fixed  29 labels. Loss 0.40189. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46899062\n",
      "Train loss (w/o reg) on all data: 0.4688599\n",
      "Test loss (w/o reg) on all data: 0.3536359\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008072458\n",
      "Norm of the params: 1.616869\n",
      "              Random: fixed   8 labels. Loss 0.35364. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5759679\n",
      "Train loss (w/o reg) on all data: 0.5758789\n",
      "Test loss (w/o reg) on all data: 0.4466149\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011350004\n",
      "Norm of the params: 1.3340435\n",
      "Flipped loss: 0.44661. Accuracy: 0.844\n",
      "### Flips: 40, rs: 12, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [146] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54372466\n",
      "Train loss (w/o reg) on all data: 0.5435654\n",
      "Test loss (w/o reg) on all data: 0.42657483\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0014779246\n",
      "Norm of the params: 1.7845731\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.42657. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [110] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48600447\n",
      "Train loss (w/o reg) on all data: 0.48579112\n",
      "Test loss (w/o reg) on all data: 0.3877153\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0012010105\n",
      "Norm of the params: 2.0656295\n",
      "                Loss: fixed   7 labels. Loss 0.38772. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [189] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55796814\n",
      "Train loss (w/o reg) on all data: 0.55785966\n",
      "Test loss (w/o reg) on all data: 0.44003454\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002918139\n",
      "Norm of the params: 1.4727925\n",
      "              Random: fixed   3 labels. Loss 0.44003. Accuracy 0.844.\n",
      "### Flips: 40, rs: 12, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52217126\n",
      "Train loss (w/o reg) on all data: 0.5220057\n",
      "Test loss (w/o reg) on all data: 0.4173553\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0014378638\n",
      "Norm of the params: 1.8198745\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.41736. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4362482\n",
      "Train loss (w/o reg) on all data: 0.43596756\n",
      "Test loss (w/o reg) on all data: 0.38327235\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0003801445\n",
      "Norm of the params: 2.369116\n",
      "                Loss: fixed  13 labels. Loss 0.38327. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [165] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5581334\n",
      "Train loss (w/o reg) on all data: 0.55802435\n",
      "Test loss (w/o reg) on all data: 0.43724027\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0030931407\n",
      "Norm of the params: 1.4769073\n",
      "              Random: fixed   5 labels. Loss 0.43724. Accuracy 0.867.\n",
      "### Flips: 40, rs: 12, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48554936\n",
      "Train loss (w/o reg) on all data: 0.48533806\n",
      "Test loss (w/o reg) on all data: 0.38467705\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0023660471\n",
      "Norm of the params: 2.0556662\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.38468. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [291] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40897512\n",
      "Train loss (w/o reg) on all data: 0.40862557\n",
      "Test loss (w/o reg) on all data: 0.39258608\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00067490456\n",
      "Norm of the params: 2.6440701\n",
      "                Loss: fixed  16 labels. Loss 0.39259. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54920465\n",
      "Train loss (w/o reg) on all data: 0.54909104\n",
      "Test loss (w/o reg) on all data: 0.42787716\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00048129377\n",
      "Norm of the params: 1.5073053\n",
      "              Random: fixed   6 labels. Loss 0.42788. Accuracy 0.844.\n",
      "### Flips: 40, rs: 12, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4626863\n",
      "Train loss (w/o reg) on all data: 0.46247274\n",
      "Test loss (w/o reg) on all data: 0.3805002\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.002886504\n",
      "Norm of the params: 2.0666761\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.38050. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38093388\n",
      "Train loss (w/o reg) on all data: 0.38056803\n",
      "Test loss (w/o reg) on all data: 0.42396995\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0035353964\n",
      "Norm of the params: 2.7050588\n",
      "                Loss: fixed  22 labels. Loss 0.42397. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54807687\n",
      "Train loss (w/o reg) on all data: 0.5479717\n",
      "Test loss (w/o reg) on all data: 0.41966328\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0022458804\n",
      "Norm of the params: 1.4500183\n",
      "              Random: fixed   8 labels. Loss 0.41966. Accuracy 0.844.\n",
      "### Flips: 40, rs: 12, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44331437\n",
      "Train loss (w/o reg) on all data: 0.44311962\n",
      "Test loss (w/o reg) on all data: 0.35789904\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0023210489\n",
      "Norm of the params: 1.9735963\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.35790. Accuracy 0.889.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35351697\n",
      "Train loss (w/o reg) on all data: 0.3531106\n",
      "Test loss (w/o reg) on all data: 0.45166555\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001210479\n",
      "Norm of the params: 2.8508046\n",
      "                Loss: fixed  26 labels. Loss 0.45167. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [421] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5480714\n",
      "Train loss (w/o reg) on all data: 0.54796636\n",
      "Test loss (w/o reg) on all data: 0.41972244\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0013469926\n",
      "Norm of the params: 1.4492778\n",
      "              Random: fixed   8 labels. Loss 0.41972. Accuracy 0.844.\n",
      "### Flips: 40, rs: 12, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38514775\n",
      "Train loss (w/o reg) on all data: 0.3848305\n",
      "Test loss (w/o reg) on all data: 0.34635633\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010295755\n",
      "Norm of the params: 2.5189104\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.34636. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [328] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34129474\n",
      "Train loss (w/o reg) on all data: 0.34085494\n",
      "Test loss (w/o reg) on all data: 0.45775917\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0005737539\n",
      "Norm of the params: 2.9657779\n",
      "                Loss: fixed  29 labels. Loss 0.45776. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.542196\n",
      "Train loss (w/o reg) on all data: 0.5420969\n",
      "Test loss (w/o reg) on all data: 0.41429096\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0054731886\n",
      "Norm of the params: 1.4077669\n",
      "              Random: fixed  11 labels. Loss 0.41429. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [272] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57790124\n",
      "Train loss (w/o reg) on all data: 0.57784456\n",
      "Test loss (w/o reg) on all data: 0.44405755\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.008782394\n",
      "Norm of the params: 1.0649873\n",
      "Flipped loss: 0.44406. Accuracy: 0.800\n",
      "### Flips: 40, rs: 13, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [71] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5427732\n",
      "Train loss (w/o reg) on all data: 0.54268813\n",
      "Test loss (w/o reg) on all data: 0.4154672\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0037801692\n",
      "Norm of the params: 1.3042003\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.41547. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4898958\n",
      "Train loss (w/o reg) on all data: 0.4897733\n",
      "Test loss (w/o reg) on all data: 0.41516674\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00029506162\n",
      "Norm of the params: 1.5651138\n",
      "                Loss: fixed   9 labels. Loss 0.41517. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [126] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56932545\n",
      "Train loss (w/o reg) on all data: 0.5692634\n",
      "Test loss (w/o reg) on all data: 0.45825696\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00700532\n",
      "Norm of the params: 1.1137522\n",
      "              Random: fixed   2 labels. Loss 0.45826. Accuracy 0.800.\n",
      "### Flips: 40, rs: 13, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50595134\n",
      "Train loss (w/o reg) on all data: 0.50581837\n",
      "Test loss (w/o reg) on all data: 0.395772\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.010637675\n",
      "Norm of the params: 1.6306983\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.39577. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43712127\n",
      "Train loss (w/o reg) on all data: 0.43693224\n",
      "Test loss (w/o reg) on all data: 0.38874212\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.000716149\n",
      "Norm of the params: 1.9443343\n",
      "                Loss: fixed  15 labels. Loss 0.38874. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [113] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56933427\n",
      "Train loss (w/o reg) on all data: 0.5692707\n",
      "Test loss (w/o reg) on all data: 0.45905948\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017797718\n",
      "Norm of the params: 1.1276736\n",
      "              Random: fixed   2 labels. Loss 0.45906. Accuracy 0.800.\n",
      "### Flips: 40, rs: 13, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [101] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48263928\n",
      "Train loss (w/o reg) on all data: 0.48249117\n",
      "Test loss (w/o reg) on all data: 0.38873264\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0019010206\n",
      "Norm of the params: 1.7212209\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.38873. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3811861\n",
      "Train loss (w/o reg) on all data: 0.38086808\n",
      "Test loss (w/o reg) on all data: 0.37871972\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0021320486\n",
      "Norm of the params: 2.5219936\n",
      "                Loss: fixed  22 labels. Loss 0.37872. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5621956\n",
      "Train loss (w/o reg) on all data: 0.5621313\n",
      "Test loss (w/o reg) on all data: 0.45028904\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00069199986\n",
      "Norm of the params: 1.1343682\n",
      "              Random: fixed   3 labels. Loss 0.45029. Accuracy 0.800.\n",
      "### Flips: 40, rs: 13, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [286] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46176043\n",
      "Train loss (w/o reg) on all data: 0.46157545\n",
      "Test loss (w/o reg) on all data: 0.38326886\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022390631\n",
      "Norm of the params: 1.9234695\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.38327. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3696307\n",
      "Train loss (w/o reg) on all data: 0.3692989\n",
      "Test loss (w/o reg) on all data: 0.3806269\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011963714\n",
      "Norm of the params: 2.5760489\n",
      "                Loss: fixed  24 labels. Loss 0.38063. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [364] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5573591\n",
      "Train loss (w/o reg) on all data: 0.55727667\n",
      "Test loss (w/o reg) on all data: 0.4539408\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0022659432\n",
      "Norm of the params: 1.2839489\n",
      "              Random: fixed   6 labels. Loss 0.45394. Accuracy 0.778.\n",
      "### Flips: 40, rs: 13, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [347] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4306209\n",
      "Train loss (w/o reg) on all data: 0.43036273\n",
      "Test loss (w/o reg) on all data: 0.37155265\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0023488665\n",
      "Norm of the params: 2.2722824\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.37155. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3462002\n",
      "Train loss (w/o reg) on all data: 0.34574944\n",
      "Test loss (w/o reg) on all data: 0.4320999\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010877561\n",
      "Norm of the params: 3.002545\n",
      "                Loss: fixed  28 labels. Loss 0.43210. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [313] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56147146\n",
      "Train loss (w/o reg) on all data: 0.56139725\n",
      "Test loss (w/o reg) on all data: 0.45234036\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0026157121\n",
      "Norm of the params: 1.2182871\n",
      "              Random: fixed   7 labels. Loss 0.45234. Accuracy 0.800.\n",
      "### Flips: 40, rs: 13, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4194512\n",
      "Train loss (w/o reg) on all data: 0.41916844\n",
      "Test loss (w/o reg) on all data: 0.39325887\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0031165413\n",
      "Norm of the params: 2.378069\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.39326. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [290] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3271434\n",
      "Train loss (w/o reg) on all data: 0.32665873\n",
      "Test loss (w/o reg) on all data: 0.43060735\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.005087058\n",
      "Norm of the params: 3.1134765\n",
      "                Loss: fixed  32 labels. Loss 0.43061. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [388] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55978143\n",
      "Train loss (w/o reg) on all data: 0.5597089\n",
      "Test loss (w/o reg) on all data: 0.44174016\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009318774\n",
      "Norm of the params: 1.2046859\n",
      "              Random: fixed   8 labels. Loss 0.44174. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [139] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5970058\n",
      "Train loss (w/o reg) on all data: 0.59695923\n",
      "Test loss (w/o reg) on all data: 0.44032788\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0034892948\n",
      "Norm of the params: 0.9647788\n",
      "Flipped loss: 0.44033. Accuracy: 0.800\n",
      "### Flips: 40, rs: 14, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54031986\n",
      "Train loss (w/o reg) on all data: 0.54025155\n",
      "Test loss (w/o reg) on all data: 0.37764168\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0021633934\n",
      "Norm of the params: 1.1688832\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.37764. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51514703\n",
      "Train loss (w/o reg) on all data: 0.5150407\n",
      "Test loss (w/o reg) on all data: 0.37393156\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0023406425\n",
      "Norm of the params: 1.4582916\n",
      "                Loss: fixed   9 labels. Loss 0.37393. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.579258\n",
      "Train loss (w/o reg) on all data: 0.5792058\n",
      "Test loss (w/o reg) on all data: 0.44688085\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0053743045\n",
      "Norm of the params: 1.0219978\n",
      "              Random: fixed   3 labels. Loss 0.44688. Accuracy 0.756.\n",
      "### Flips: 40, rs: 14, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49767375\n",
      "Train loss (w/o reg) on all data: 0.49758393\n",
      "Test loss (w/o reg) on all data: 0.36151946\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0029154837\n",
      "Norm of the params: 1.340233\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.36152. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43066582\n",
      "Train loss (w/o reg) on all data: 0.43043646\n",
      "Test loss (w/o reg) on all data: 0.35137314\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013192191\n",
      "Norm of the params: 2.1418376\n",
      "                Loss: fixed  17 labels. Loss 0.35137. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [347] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5818054\n",
      "Train loss (w/o reg) on all data: 0.5817529\n",
      "Test loss (w/o reg) on all data: 0.4452595\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0012633485\n",
      "Norm of the params: 1.0250766\n",
      "              Random: fixed   4 labels. Loss 0.44526. Accuracy 0.756.\n",
      "### Flips: 40, rs: 14, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [149] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4904451\n",
      "Train loss (w/o reg) on all data: 0.49032938\n",
      "Test loss (w/o reg) on all data: 0.37700108\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009417019\n",
      "Norm of the params: 1.5213226\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.37700. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39242664\n",
      "Train loss (w/o reg) on all data: 0.3921838\n",
      "Test loss (w/o reg) on all data: 0.36406806\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003898692\n",
      "Norm of the params: 2.2037802\n",
      "                Loss: fixed  22 labels. Loss 0.36407. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [285] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5780515\n",
      "Train loss (w/o reg) on all data: 0.57799727\n",
      "Test loss (w/o reg) on all data: 0.4485572\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00091247563\n",
      "Norm of the params: 1.0416667\n",
      "              Random: fixed   5 labels. Loss 0.44856. Accuracy 0.756.\n",
      "### Flips: 40, rs: 14, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [118] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47995108\n",
      "Train loss (w/o reg) on all data: 0.47981784\n",
      "Test loss (w/o reg) on all data: 0.37885052\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004411867\n",
      "Norm of the params: 1.6325476\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.37885. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [186] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35599974\n",
      "Train loss (w/o reg) on all data: 0.35574\n",
      "Test loss (w/o reg) on all data: 0.36612222\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0032175512\n",
      "Norm of the params: 2.2791002\n",
      "                Loss: fixed  27 labels. Loss 0.36612. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5771987\n",
      "Train loss (w/o reg) on all data: 0.57713723\n",
      "Test loss (w/o reg) on all data: 0.4567829\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0029440725\n",
      "Norm of the params: 1.1088308\n",
      "              Random: fixed   8 labels. Loss 0.45678. Accuracy 0.756.\n",
      "### Flips: 40, rs: 14, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [126] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44740203\n",
      "Train loss (w/o reg) on all data: 0.4472354\n",
      "Test loss (w/o reg) on all data: 0.35289335\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.009181407\n",
      "Norm of the params: 1.8255584\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.35289. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34229597\n",
      "Train loss (w/o reg) on all data: 0.3420301\n",
      "Test loss (w/o reg) on all data: 0.39006996\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002124142\n",
      "Norm of the params: 2.3059745\n",
      "                Loss: fixed  29 labels. Loss 0.39007. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [377] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57719785\n",
      "Train loss (w/o reg) on all data: 0.5771366\n",
      "Test loss (w/o reg) on all data: 0.45692214\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0008636892\n",
      "Norm of the params: 1.1071748\n",
      "              Random: fixed   8 labels. Loss 0.45692. Accuracy 0.756.\n",
      "### Flips: 40, rs: 14, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4242186\n",
      "Train loss (w/o reg) on all data: 0.42399475\n",
      "Test loss (w/o reg) on all data: 0.35699886\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002571495\n",
      "Norm of the params: 2.1159089\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.35700. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [272] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33481595\n",
      "Train loss (w/o reg) on all data: 0.3345423\n",
      "Test loss (w/o reg) on all data: 0.40635884\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0059702033\n",
      "Norm of the params: 2.3394022\n",
      "                Loss: fixed  30 labels. Loss 0.40636. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [323] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57163054\n",
      "Train loss (w/o reg) on all data: 0.5715647\n",
      "Test loss (w/o reg) on all data: 0.44564933\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0028775427\n",
      "Norm of the params: 1.147694\n",
      "              Random: fixed   9 labels. Loss 0.44565. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6010187\n",
      "Train loss (w/o reg) on all data: 0.6009801\n",
      "Test loss (w/o reg) on all data: 0.469733\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014044658\n",
      "Norm of the params: 0.8788338\n",
      "Flipped loss: 0.46973. Accuracy: 0.800\n",
      "### Flips: 40, rs: 15, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5675336\n",
      "Train loss (w/o reg) on all data: 0.56746846\n",
      "Test loss (w/o reg) on all data: 0.42748514\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.012795596\n",
      "Norm of the params: 1.1412365\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.42749. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [101] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5221065\n",
      "Train loss (w/o reg) on all data: 0.5220312\n",
      "Test loss (w/o reg) on all data: 0.4032489\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.009264394\n",
      "Norm of the params: 1.2277379\n",
      "                Loss: fixed   9 labels. Loss 0.40325. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [128] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59200996\n",
      "Train loss (w/o reg) on all data: 0.591961\n",
      "Test loss (w/o reg) on all data: 0.46061847\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007392338\n",
      "Norm of the params: 0.989114\n",
      "              Random: fixed   2 labels. Loss 0.46062. Accuracy 0.822.\n",
      "### Flips: 40, rs: 15, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5571111\n",
      "Train loss (w/o reg) on all data: 0.5570365\n",
      "Test loss (w/o reg) on all data: 0.40959385\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012754312\n",
      "Norm of the params: 1.2212332\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.40959. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44656205\n",
      "Train loss (w/o reg) on all data: 0.44641724\n",
      "Test loss (w/o reg) on all data: 0.37512887\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004399515\n",
      "Norm of the params: 1.7017827\n",
      "                Loss: fixed  17 labels. Loss 0.37513. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [97] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58802646\n",
      "Train loss (w/o reg) on all data: 0.5879661\n",
      "Test loss (w/o reg) on all data: 0.45719892\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0044515757\n",
      "Norm of the params: 1.0989894\n",
      "              Random: fixed   4 labels. Loss 0.45720. Accuracy 0.800.\n",
      "### Flips: 40, rs: 15, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.506793\n",
      "Train loss (w/o reg) on all data: 0.5067024\n",
      "Test loss (w/o reg) on all data: 0.38318723\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004362146\n",
      "Norm of the params: 1.3458991\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.38319. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [395] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42048514\n",
      "Train loss (w/o reg) on all data: 0.42030936\n",
      "Test loss (w/o reg) on all data: 0.3667458\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0046030735\n",
      "Norm of the params: 1.8749235\n",
      "                Loss: fixed  20 labels. Loss 0.36675. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5798581\n",
      "Train loss (w/o reg) on all data: 0.5797868\n",
      "Test loss (w/o reg) on all data: 0.45362812\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0073672435\n",
      "Norm of the params: 1.1943944\n",
      "              Random: fixed   6 labels. Loss 0.45363. Accuracy 0.778.\n",
      "### Flips: 40, rs: 15, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48127377\n",
      "Train loss (w/o reg) on all data: 0.4811451\n",
      "Test loss (w/o reg) on all data: 0.36624667\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00069734256\n",
      "Norm of the params: 1.6041453\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.36625. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [364] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37596548\n",
      "Train loss (w/o reg) on all data: 0.375656\n",
      "Test loss (w/o reg) on all data: 0.36253738\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00038557136\n",
      "Norm of the params: 2.4878933\n",
      "                Loss: fixed  25 labels. Loss 0.36254. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5757268\n",
      "Train loss (w/o reg) on all data: 0.57565063\n",
      "Test loss (w/o reg) on all data: 0.44900492\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018381701\n",
      "Norm of the params: 1.2342427\n",
      "              Random: fixed   7 labels. Loss 0.44900. Accuracy 0.800.\n",
      "### Flips: 40, rs: 15, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [189] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4368856\n",
      "Train loss (w/o reg) on all data: 0.4366946\n",
      "Test loss (w/o reg) on all data: 0.36126158\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006710171\n",
      "Norm of the params: 1.9544454\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.36126. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35590866\n",
      "Train loss (w/o reg) on all data: 0.35565338\n",
      "Test loss (w/o reg) on all data: 0.37056965\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014514494\n",
      "Norm of the params: 2.2595706\n",
      "                Loss: fixed  29 labels. Loss 0.37057. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56276774\n",
      "Train loss (w/o reg) on all data: 0.5626882\n",
      "Test loss (w/o reg) on all data: 0.43689173\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0021659306\n",
      "Norm of the params: 1.2613357\n",
      "              Random: fixed  10 labels. Loss 0.43689. Accuracy 0.800.\n",
      "### Flips: 40, rs: 15, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41245362\n",
      "Train loss (w/o reg) on all data: 0.4122336\n",
      "Test loss (w/o reg) on all data: 0.35705093\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0048721586\n",
      "Norm of the params: 2.0977955\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.35705. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34712213\n",
      "Train loss (w/o reg) on all data: 0.34683976\n",
      "Test loss (w/o reg) on all data: 0.36298862\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002234998\n",
      "Norm of the params: 2.3764\n",
      "                Loss: fixed  30 labels. Loss 0.36299. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [179] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5627753\n",
      "Train loss (w/o reg) on all data: 0.56269675\n",
      "Test loss (w/o reg) on all data: 0.43712133\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00802762\n",
      "Norm of the params: 1.2534857\n",
      "              Random: fixed  10 labels. Loss 0.43712. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5462576\n",
      "Train loss (w/o reg) on all data: 0.54612684\n",
      "Test loss (w/o reg) on all data: 0.4320301\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015065325\n",
      "Norm of the params: 1.617083\n",
      "Flipped loss: 0.43203. Accuracy: 0.800\n",
      "### Flips: 40, rs: 16, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46555957\n",
      "Train loss (w/o reg) on all data: 0.46536392\n",
      "Test loss (w/o reg) on all data: 0.38880783\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0026771654\n",
      "Norm of the params: 1.9781169\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.38881. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [111] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4555085\n",
      "Train loss (w/o reg) on all data: 0.45524162\n",
      "Test loss (w/o reg) on all data: 0.43467844\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018899847\n",
      "Norm of the params: 2.310262\n",
      "                Loss: fixed   8 labels. Loss 0.43468. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5249102\n",
      "Train loss (w/o reg) on all data: 0.5247514\n",
      "Test loss (w/o reg) on all data: 0.43600503\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012848033\n",
      "Norm of the params: 1.781932\n",
      "              Random: fixed   3 labels. Loss 0.43601. Accuracy 0.800.\n",
      "### Flips: 40, rs: 16, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [342] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44121936\n",
      "Train loss (w/o reg) on all data: 0.44099706\n",
      "Test loss (w/o reg) on all data: 0.4031817\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0052111396\n",
      "Norm of the params: 2.1084743\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.40318. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [319] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39485252\n",
      "Train loss (w/o reg) on all data: 0.39446855\n",
      "Test loss (w/o reg) on all data: 0.43363377\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0031953899\n",
      "Norm of the params: 2.7712045\n",
      "                Loss: fixed  14 labels. Loss 0.43363. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5154369\n",
      "Train loss (w/o reg) on all data: 0.51526177\n",
      "Test loss (w/o reg) on all data: 0.4161196\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019913067\n",
      "Norm of the params: 1.8715488\n",
      "              Random: fixed   5 labels. Loss 0.41612. Accuracy 0.800.\n",
      "### Flips: 40, rs: 16, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [73] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41931632\n",
      "Train loss (w/o reg) on all data: 0.41907236\n",
      "Test loss (w/o reg) on all data: 0.4094139\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0018482945\n",
      "Norm of the params: 2.2088802\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.40941. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [179] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3614971\n",
      "Train loss (w/o reg) on all data: 0.3610059\n",
      "Test loss (w/o reg) on all data: 0.44566363\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0034783029\n",
      "Norm of the params: 3.134312\n",
      "                Loss: fixed  19 labels. Loss 0.44566. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [341] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50835425\n",
      "Train loss (w/o reg) on all data: 0.50816965\n",
      "Test loss (w/o reg) on all data: 0.4039688\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004770343\n",
      "Norm of the params: 1.9214293\n",
      "              Random: fixed   8 labels. Loss 0.40397. Accuracy 0.822.\n",
      "### Flips: 40, rs: 16, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4021318\n",
      "Train loss (w/o reg) on all data: 0.4018642\n",
      "Test loss (w/o reg) on all data: 0.40732792\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0036007632\n",
      "Norm of the params: 2.3133726\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.40733. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [283] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34901232\n",
      "Train loss (w/o reg) on all data: 0.34847915\n",
      "Test loss (w/o reg) on all data: 0.46070752\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017614596\n",
      "Norm of the params: 3.265439\n",
      "                Loss: fixed  21 labels. Loss 0.46071. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [400] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50015515\n",
      "Train loss (w/o reg) on all data: 0.4999764\n",
      "Test loss (w/o reg) on all data: 0.39179933\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00046857056\n",
      "Norm of the params: 1.8907568\n",
      "              Random: fixed  10 labels. Loss 0.39180. Accuracy 0.800.\n",
      "### Flips: 40, rs: 16, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [346] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37459934\n",
      "Train loss (w/o reg) on all data: 0.37427658\n",
      "Test loss (w/o reg) on all data: 0.3786092\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00036535505\n",
      "Norm of the params: 2.5407639\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.37861. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3264597\n",
      "Train loss (w/o reg) on all data: 0.32586864\n",
      "Test loss (w/o reg) on all data: 0.42810223\n",
      "Train acc on all data:  0.8820754716981132\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011226677\n",
      "Norm of the params: 3.438201\n",
      "                Loss: fixed  25 labels. Loss 0.42810. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [346] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49662897\n",
      "Train loss (w/o reg) on all data: 0.49645558\n",
      "Test loss (w/o reg) on all data: 0.39232144\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001309105\n",
      "Norm of the params: 1.8621931\n",
      "              Random: fixed  11 labels. Loss 0.39232. Accuracy 0.800.\n",
      "### Flips: 40, rs: 16, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [319] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36501756\n",
      "Train loss (w/o reg) on all data: 0.36464962\n",
      "Test loss (w/o reg) on all data: 0.38218418\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002469223\n",
      "Norm of the params: 2.7127624\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.38218. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32014707\n",
      "Train loss (w/o reg) on all data: 0.3195983\n",
      "Test loss (w/o reg) on all data: 0.43254805\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0028234185\n",
      "Norm of the params: 3.3129225\n",
      "                Loss: fixed  28 labels. Loss 0.43255. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [240] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss (w reg) on all data: 0.49663478\n",
      "Train loss (w/o reg) on all data: 0.49645853\n",
      "Test loss (w/o reg) on all data: 0.393288\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.010907083\n",
      "Norm of the params: 1.8775257\n",
      "              Random: fixed  11 labels. Loss 0.39329. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5544822\n",
      "Train loss (w/o reg) on all data: 0.5543969\n",
      "Test loss (w/o reg) on all data: 0.49648574\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0023513252\n",
      "Norm of the params: 1.3059998\n",
      "Flipped loss: 0.49649. Accuracy: 0.778\n",
      "### Flips: 40, rs: 17, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [109] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5063231\n",
      "Train loss (w/o reg) on all data: 0.5061717\n",
      "Test loss (w/o reg) on all data: 0.47569054\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0030101356\n",
      "Norm of the params: 1.7399925\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.47569. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [226] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48010287\n",
      "Train loss (w/o reg) on all data: 0.47991192\n",
      "Test loss (w/o reg) on all data: 0.4760671\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001305363\n",
      "Norm of the params: 1.9542081\n",
      "                Loss: fixed   7 labels. Loss 0.47607. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [152] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55059487\n",
      "Train loss (w/o reg) on all data: 0.55051345\n",
      "Test loss (w/o reg) on all data: 0.49506035\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0038609325\n",
      "Norm of the params: 1.2758695\n",
      "              Random: fixed   1 labels. Loss 0.49506. Accuracy 0.778.\n",
      "### Flips: 40, rs: 17, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4676534\n",
      "Train loss (w/o reg) on all data: 0.46747157\n",
      "Test loss (w/o reg) on all data: 0.46137255\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0059818267\n",
      "Norm of the params: 1.9069338\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.46137. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [266] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.412788\n",
      "Train loss (w/o reg) on all data: 0.41241425\n",
      "Test loss (w/o reg) on all data: 0.4913318\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0020760288\n",
      "Norm of the params: 2.7340674\n",
      "                Loss: fixed  15 labels. Loss 0.49133. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.546619\n",
      "Train loss (w/o reg) on all data: 0.5465392\n",
      "Test loss (w/o reg) on all data: 0.472542\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.000846967\n",
      "Norm of the params: 1.2635622\n",
      "              Random: fixed   2 labels. Loss 0.47254. Accuracy 0.800.\n",
      "### Flips: 40, rs: 17, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44064593\n",
      "Train loss (w/o reg) on all data: 0.44041437\n",
      "Test loss (w/o reg) on all data: 0.4294804\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014625778\n",
      "Norm of the params: 2.1520016\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.42948. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36035767\n",
      "Train loss (w/o reg) on all data: 0.35992014\n",
      "Test loss (w/o reg) on all data: 0.49832582\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00066889724\n",
      "Norm of the params: 2.9580956\n",
      "                Loss: fixed  22 labels. Loss 0.49833. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53387594\n",
      "Train loss (w/o reg) on all data: 0.53378516\n",
      "Test loss (w/o reg) on all data: 0.46879074\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018258175\n",
      "Norm of the params: 1.347531\n",
      "              Random: fixed   4 labels. Loss 0.46879. Accuracy 0.822.\n",
      "### Flips: 40, rs: 17, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41132128\n",
      "Train loss (w/o reg) on all data: 0.4110245\n",
      "Test loss (w/o reg) on all data: 0.45349213\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.000981005\n",
      "Norm of the params: 2.4362812\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.45349. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [153] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3436687\n",
      "Train loss (w/o reg) on all data: 0.34321514\n",
      "Test loss (w/o reg) on all data: 0.5019461\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006994869\n",
      "Norm of the params: 3.0118537\n",
      "                Loss: fixed  25 labels. Loss 0.50195. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [187] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5374136\n",
      "Train loss (w/o reg) on all data: 0.53733283\n",
      "Test loss (w/o reg) on all data: 0.45498967\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010604251\n",
      "Norm of the params: 1.270796\n",
      "              Random: fixed   6 labels. Loss 0.45499. Accuracy 0.844.\n",
      "### Flips: 40, rs: 17, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38718182\n",
      "Train loss (w/o reg) on all data: 0.3868327\n",
      "Test loss (w/o reg) on all data: 0.44858944\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003026007\n",
      "Norm of the params: 2.6423697\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.44859. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32971248\n",
      "Train loss (w/o reg) on all data: 0.3292694\n",
      "Test loss (w/o reg) on all data: 0.5025085\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003538303\n",
      "Norm of the params: 2.976845\n",
      "                Loss: fixed  28 labels. Loss 0.50251. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [334] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53169817\n",
      "Train loss (w/o reg) on all data: 0.5316043\n",
      "Test loss (w/o reg) on all data: 0.44834086\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0033527233\n",
      "Norm of the params: 1.3701433\n",
      "              Random: fixed   8 labels. Loss 0.44834. Accuracy 0.822.\n",
      "### Flips: 40, rs: 17, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36429864\n",
      "Train loss (w/o reg) on all data: 0.3638533\n",
      "Test loss (w/o reg) on all data: 0.4557166\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00043641112\n",
      "Norm of the params: 2.9844565\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.45572. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.325903\n",
      "Train loss (w/o reg) on all data: 0.32543024\n",
      "Test loss (w/o reg) on all data: 0.5096805\n",
      "Train acc on all data:  0.8915094339622641\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010668429\n",
      "Norm of the params: 3.074928\n",
      "                Loss: fixed  29 labels. Loss 0.50968. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [351] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss (w reg) on all data: 0.51817507\n",
      "Train loss (w/o reg) on all data: 0.5180675\n",
      "Test loss (w/o reg) on all data: 0.45348158\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00078680046\n",
      "Norm of the params: 1.4667431\n",
      "              Random: fixed  12 labels. Loss 0.45348. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [185] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.559112\n",
      "Train loss (w/o reg) on all data: 0.5590457\n",
      "Test loss (w/o reg) on all data: 0.4495184\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0050254283\n",
      "Norm of the params: 1.1516254\n",
      "Flipped loss: 0.44952. Accuracy: 0.778\n",
      "### Flips: 40, rs: 18, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5212243\n",
      "Train loss (w/o reg) on all data: 0.52112985\n",
      "Test loss (w/o reg) on all data: 0.4401737\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0008007433\n",
      "Norm of the params: 1.3745693\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.44017. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47091976\n",
      "Train loss (w/o reg) on all data: 0.47076872\n",
      "Test loss (w/o reg) on all data: 0.43026322\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0014985902\n",
      "Norm of the params: 1.73799\n",
      "                Loss: fixed   7 labels. Loss 0.43026. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5591119\n",
      "Train loss (w/o reg) on all data: 0.55904585\n",
      "Test loss (w/o reg) on all data: 0.4501356\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012535925\n",
      "Norm of the params: 1.1492208\n",
      "              Random: fixed   0 labels. Loss 0.45014. Accuracy 0.778.\n",
      "### Flips: 40, rs: 18, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47132304\n",
      "Train loss (w/o reg) on all data: 0.47117075\n",
      "Test loss (w/o reg) on all data: 0.43704414\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00088661717\n",
      "Norm of the params: 1.7452501\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.43704. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4029689\n",
      "Train loss (w/o reg) on all data: 0.40274328\n",
      "Test loss (w/o reg) on all data: 0.4872525\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0013240912\n",
      "Norm of the params: 2.1242988\n",
      "                Loss: fixed  15 labels. Loss 0.48725. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [149] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54300326\n",
      "Train loss (w/o reg) on all data: 0.54292023\n",
      "Test loss (w/o reg) on all data: 0.4374779\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0026864104\n",
      "Norm of the params: 1.2885692\n",
      "              Random: fixed   2 labels. Loss 0.43748. Accuracy 0.778.\n",
      "### Flips: 40, rs: 18, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4243751\n",
      "Train loss (w/o reg) on all data: 0.42414314\n",
      "Test loss (w/o reg) on all data: 0.43940914\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00040991014\n",
      "Norm of the params: 2.1537783\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.43941. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3414836\n",
      "Train loss (w/o reg) on all data: 0.3411156\n",
      "Test loss (w/o reg) on all data: 0.48297226\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0024126829\n",
      "Norm of the params: 2.7129633\n",
      "                Loss: fixed  22 labels. Loss 0.48297. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54678035\n",
      "Train loss (w/o reg) on all data: 0.5466849\n",
      "Test loss (w/o reg) on all data: 0.45851645\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.003341656\n",
      "Norm of the params: 1.3814312\n",
      "              Random: fixed   4 labels. Loss 0.45852. Accuracy 0.733.\n",
      "### Flips: 40, rs: 18, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40840378\n",
      "Train loss (w/o reg) on all data: 0.40814465\n",
      "Test loss (w/o reg) on all data: 0.4806449\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0006129718\n",
      "Norm of the params: 2.2764819\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.48064. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3270587\n",
      "Train loss (w/o reg) on all data: 0.32669464\n",
      "Test loss (w/o reg) on all data: 0.49755597\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00072248385\n",
      "Norm of the params: 2.6983945\n",
      "                Loss: fixed  26 labels. Loss 0.49756. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5418518\n",
      "Train loss (w/o reg) on all data: 0.5417443\n",
      "Test loss (w/o reg) on all data: 0.4520586\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.008499929\n",
      "Norm of the params: 1.4666216\n",
      "              Random: fixed   5 labels. Loss 0.45206. Accuracy 0.733.\n",
      "### Flips: 40, rs: 18, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [303] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40568656\n",
      "Train loss (w/o reg) on all data: 0.4054062\n",
      "Test loss (w/o reg) on all data: 0.47761753\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.004434731\n",
      "Norm of the params: 2.3679245\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.47762. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [306] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3108559\n",
      "Train loss (w/o reg) on all data: 0.31043625\n",
      "Test loss (w/o reg) on all data: 0.4594593\n",
      "Train acc on all data:  0.8915094339622641\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010845304\n",
      "Norm of the params: 2.8970637\n",
      "                Loss: fixed  31 labels. Loss 0.45946. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [342] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5387948\n",
      "Train loss (w/o reg) on all data: 0.53868717\n",
      "Test loss (w/o reg) on all data: 0.42041808\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.001784581\n",
      "Norm of the params: 1.4671437\n",
      "              Random: fixed   7 labels. Loss 0.42042. Accuracy 0.756.\n",
      "### Flips: 40, rs: 18, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [307] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38390818\n",
      "Train loss (w/o reg) on all data: 0.38362643\n",
      "Test loss (w/o reg) on all data: 0.47219685\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0002447247\n",
      "Norm of the params: 2.3738654\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.47220. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [316] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.30888408\n",
      "Train loss (w/o reg) on all data: 0.30842847\n",
      "Test loss (w/o reg) on all data: 0.45378387\n",
      "Train acc on all data:  0.8867924528301887\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012736359\n",
      "Norm of the params: 3.0186706\n",
      "                Loss: fixed  32 labels. Loss 0.45378. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [282] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5391635\n",
      "Train loss (w/o reg) on all data: 0.5390549\n",
      "Test loss (w/o reg) on all data: 0.41915956\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0046726638\n",
      "Norm of the params: 1.4737848\n",
      "              Random: fixed   8 labels. Loss 0.41916. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57970566\n",
      "Train loss (w/o reg) on all data: 0.5796484\n",
      "Test loss (w/o reg) on all data: 0.44838572\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001506436\n",
      "Norm of the params: 1.0701088\n",
      "Flipped loss: 0.44839. Accuracy: 0.800\n",
      "### Flips: 40, rs: 19, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [112] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54982346\n",
      "Train loss (w/o reg) on all data: 0.54973996\n",
      "Test loss (w/o reg) on all data: 0.4397609\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0031181353\n",
      "Norm of the params: 1.2922312\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.43976. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [137] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51360387\n",
      "Train loss (w/o reg) on all data: 0.51350236\n",
      "Test loss (w/o reg) on all data: 0.42798096\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00054742064\n",
      "Norm of the params: 1.4250275\n",
      "                Loss: fixed   7 labels. Loss 0.42798. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [128] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5767372\n",
      "Train loss (w/o reg) on all data: 0.576682\n",
      "Test loss (w/o reg) on all data: 0.4563151\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.005158277\n",
      "Norm of the params: 1.0511228\n",
      "              Random: fixed   2 labels. Loss 0.45632. Accuracy 0.778.\n",
      "### Flips: 40, rs: 19, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52822375\n",
      "Train loss (w/o reg) on all data: 0.52812964\n",
      "Test loss (w/o reg) on all data: 0.42671612\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020796678\n",
      "Norm of the params: 1.3718449\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.42672. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45143807\n",
      "Train loss (w/o reg) on all data: 0.45122686\n",
      "Test loss (w/o reg) on all data: 0.40861845\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00042756327\n",
      "Norm of the params: 2.05524\n",
      "                Loss: fixed  15 labels. Loss 0.40862. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [151] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5518606\n",
      "Train loss (w/o reg) on all data: 0.55179155\n",
      "Test loss (w/o reg) on all data: 0.43633825\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.010081734\n",
      "Norm of the params: 1.1748163\n",
      "              Random: fixed   5 labels. Loss 0.43634. Accuracy 0.800.\n",
      "### Flips: 40, rs: 19, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [143] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50304437\n",
      "Train loss (w/o reg) on all data: 0.5029446\n",
      "Test loss (w/o reg) on all data: 0.4151778\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0005596431\n",
      "Norm of the params: 1.412855\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.41518. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40415123\n",
      "Train loss (w/o reg) on all data: 0.40388006\n",
      "Test loss (w/o reg) on all data: 0.42260197\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0037130183\n",
      "Norm of the params: 2.3288827\n",
      "                Loss: fixed  21 labels. Loss 0.42260. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [135] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.544681\n",
      "Train loss (w/o reg) on all data: 0.544604\n",
      "Test loss (w/o reg) on all data: 0.42719814\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010379617\n",
      "Norm of the params: 1.2411016\n",
      "              Random: fixed   6 labels. Loss 0.42720. Accuracy 0.800.\n",
      "### Flips: 40, rs: 19, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [189] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49874666\n",
      "Train loss (w/o reg) on all data: 0.49865425\n",
      "Test loss (w/o reg) on all data: 0.4166568\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0052831247\n",
      "Norm of the params: 1.3595549\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.41666. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3812736\n",
      "Train loss (w/o reg) on all data: 0.38095585\n",
      "Test loss (w/o reg) on all data: 0.42025587\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004106146\n",
      "Norm of the params: 2.5209477\n",
      "                Loss: fixed  24 labels. Loss 0.42026. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5394661\n",
      "Train loss (w/o reg) on all data: 0.53938776\n",
      "Test loss (w/o reg) on all data: 0.4294745\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002315645\n",
      "Norm of the params: 1.2515678\n",
      "              Random: fixed   7 labels. Loss 0.42947. Accuracy 0.800.\n",
      "### Flips: 40, rs: 19, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [161] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47686735\n",
      "Train loss (w/o reg) on all data: 0.4767392\n",
      "Test loss (w/o reg) on all data: 0.40027398\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0016543929\n",
      "Norm of the params: 1.6009862\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.40027. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [289] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37474373\n",
      "Train loss (w/o reg) on all data: 0.3744452\n",
      "Test loss (w/o reg) on all data: 0.4312935\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019246452\n",
      "Norm of the params: 2.4434545\n",
      "                Loss: fixed  27 labels. Loss 0.43129. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [364] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5323583\n",
      "Train loss (w/o reg) on all data: 0.5322801\n",
      "Test loss (w/o reg) on all data: 0.4263691\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0004983635\n",
      "Norm of the params: 1.250485\n",
      "              Random: fixed   9 labels. Loss 0.42637. Accuracy 0.800.\n",
      "### Flips: 40, rs: 19, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43970606\n",
      "Train loss (w/o reg) on all data: 0.43950394\n",
      "Test loss (w/o reg) on all data: 0.3763014\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017618254\n",
      "Norm of the params: 2.0105202\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.37630. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [112] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37177688\n",
      "Train loss (w/o reg) on all data: 0.37146783\n",
      "Test loss (w/o reg) on all data: 0.4312753\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003207694\n",
      "Norm of the params: 2.486135\n",
      "                Loss: fixed  30 labels. Loss 0.43128. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52648014\n",
      "Train loss (w/o reg) on all data: 0.52640396\n",
      "Test loss (w/o reg) on all data: 0.44075736\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0007303835\n",
      "Norm of the params: 1.2343875\n",
      "              Random: fixed  11 labels. Loss 0.44076. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57945615\n",
      "Train loss (w/o reg) on all data: 0.57942134\n",
      "Test loss (w/o reg) on all data: 0.5415995\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0020474275\n",
      "Norm of the params: 0.8342624\n",
      "Flipped loss: 0.54160. Accuracy: 0.778\n",
      "### Flips: 40, rs: 20, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53680485\n",
      "Train loss (w/o reg) on all data: 0.5367188\n",
      "Test loss (w/o reg) on all data: 0.5176996\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0049029933\n",
      "Norm of the params: 1.3121964\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.51770. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49530306\n",
      "Train loss (w/o reg) on all data: 0.49520677\n",
      "Test loss (w/o reg) on all data: 0.5155522\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0012483663\n",
      "Norm of the params: 1.3877829\n",
      "                Loss: fixed   9 labels. Loss 0.51555. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57291263\n",
      "Train loss (w/o reg) on all data: 0.5728774\n",
      "Test loss (w/o reg) on all data: 0.5421795\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0012274276\n",
      "Norm of the params: 0.83935314\n",
      "              Random: fixed   2 labels. Loss 0.54218. Accuracy 0.733.\n",
      "### Flips: 40, rs: 20, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51187634\n",
      "Train loss (w/o reg) on all data: 0.51178414\n",
      "Test loss (w/o reg) on all data: 0.44857597\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001908961\n",
      "Norm of the params: 1.3581395\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.44858. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42475194\n",
      "Train loss (w/o reg) on all data: 0.42458817\n",
      "Test loss (w/o reg) on all data: 0.50090706\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.005290484\n",
      "Norm of the params: 1.8097405\n",
      "                Loss: fixed  16 labels. Loss 0.50091. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56560755\n",
      "Train loss (w/o reg) on all data: 0.5655669\n",
      "Test loss (w/o reg) on all data: 0.5441391\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.009014657\n",
      "Norm of the params: 0.90145546\n",
      "              Random: fixed   3 labels. Loss 0.54414. Accuracy 0.778.\n",
      "### Flips: 40, rs: 20, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [198] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48081526\n",
      "Train loss (w/o reg) on all data: 0.48069656\n",
      "Test loss (w/o reg) on all data: 0.4301501\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001621594\n",
      "Norm of the params: 1.540771\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.43015. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3774976\n",
      "Train loss (w/o reg) on all data: 0.377285\n",
      "Test loss (w/o reg) on all data: 0.5048212\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0012801541\n",
      "Norm of the params: 2.0620258\n",
      "                Loss: fixed  22 labels. Loss 0.50482. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [299] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55033004\n",
      "Train loss (w/o reg) on all data: 0.5502816\n",
      "Test loss (w/o reg) on all data: 0.5289579\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0029596253\n",
      "Norm of the params: 0.9845855\n",
      "              Random: fixed   5 labels. Loss 0.52896. Accuracy 0.778.\n",
      "### Flips: 40, rs: 20, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4367274\n",
      "Train loss (w/o reg) on all data: 0.43657368\n",
      "Test loss (w/o reg) on all data: 0.4048706\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005171174\n",
      "Norm of the params: 1.7534249\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.40487. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3587264\n",
      "Train loss (w/o reg) on all data: 0.35846522\n",
      "Test loss (w/o reg) on all data: 0.5347012\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0008046614\n",
      "Norm of the params: 2.2856174\n",
      "                Loss: fixed  24 labels. Loss 0.53470. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [363] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5570019\n",
      "Train loss (w/o reg) on all data: 0.55694985\n",
      "Test loss (w/o reg) on all data: 0.49435854\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0025351306\n",
      "Norm of the params: 1.0201674\n",
      "              Random: fixed   8 labels. Loss 0.49436. Accuracy 0.822.\n",
      "### Flips: 40, rs: 20, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [97] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41763335\n",
      "Train loss (w/o reg) on all data: 0.41744164\n",
      "Test loss (w/o reg) on all data: 0.39959285\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011149325\n",
      "Norm of the params: 1.95823\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.39959. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [333] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3419066\n",
      "Train loss (w/o reg) on all data: 0.34160742\n",
      "Test loss (w/o reg) on all data: 0.5376173\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.002693426\n",
      "Norm of the params: 2.4461734\n",
      "                Loss: fixed  27 labels. Loss 0.53762. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [356] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5532474\n",
      "Train loss (w/o reg) on all data: 0.5531908\n",
      "Test loss (w/o reg) on all data: 0.4819384\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00236464\n",
      "Norm of the params: 1.0633557\n",
      "              Random: fixed  10 labels. Loss 0.48194. Accuracy 0.800.\n",
      "### Flips: 40, rs: 20, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [148] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.392462\n",
      "Train loss (w/o reg) on all data: 0.39222524\n",
      "Test loss (w/o reg) on all data: 0.42027074\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008990259\n",
      "Norm of the params: 2.1760552\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.42027. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [326] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33290783\n",
      "Train loss (w/o reg) on all data: 0.33257818\n",
      "Test loss (w/o reg) on all data: 0.53048706\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0018914455\n",
      "Norm of the params: 2.567674\n",
      "                Loss: fixed  31 labels. Loss 0.53049. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5279705\n",
      "Train loss (w/o reg) on all data: 0.52790785\n",
      "Test loss (w/o reg) on all data: 0.5021622\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0011252807\n",
      "Norm of the params: 1.1193153\n",
      "              Random: fixed  14 labels. Loss 0.50216. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [201] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56535524\n",
      "Train loss (w/o reg) on all data: 0.56529874\n",
      "Test loss (w/o reg) on all data: 0.497427\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0049016583\n",
      "Norm of the params: 1.0628922\n",
      "Flipped loss: 0.49743. Accuracy: 0.822\n",
      "### Flips: 40, rs: 21, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [109] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5409182\n",
      "Train loss (w/o reg) on all data: 0.54084665\n",
      "Test loss (w/o reg) on all data: 0.48793274\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00038627483\n",
      "Norm of the params: 1.1961029\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.48793. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49108246\n",
      "Train loss (w/o reg) on all data: 0.49097666\n",
      "Test loss (w/o reg) on all data: 0.5007944\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0046480275\n",
      "Norm of the params: 1.4545878\n",
      "                Loss: fixed   8 labels. Loss 0.50079. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [175] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.558799\n",
      "Train loss (w/o reg) on all data: 0.55873996\n",
      "Test loss (w/o reg) on all data: 0.49530247\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011456421\n",
      "Norm of the params: 1.0866446\n",
      "              Random: fixed   2 labels. Loss 0.49530. Accuracy 0.822.\n",
      "### Flips: 40, rs: 21, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [138] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52319276\n",
      "Train loss (w/o reg) on all data: 0.52311045\n",
      "Test loss (w/o reg) on all data: 0.44487068\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0057151965\n",
      "Norm of the params: 1.2828444\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.44487. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40158272\n",
      "Train loss (w/o reg) on all data: 0.40132782\n",
      "Test loss (w/o reg) on all data: 0.4752593\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007209813\n",
      "Norm of the params: 2.2578285\n",
      "                Loss: fixed  17 labels. Loss 0.47526. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55439484\n",
      "Train loss (w/o reg) on all data: 0.55432904\n",
      "Test loss (w/o reg) on all data: 0.4647239\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002030747\n",
      "Norm of the params: 1.1472423\n",
      "              Random: fixed   5 labels. Loss 0.46472. Accuracy 0.844.\n",
      "### Flips: 40, rs: 21, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [199] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4685628\n",
      "Train loss (w/o reg) on all data: 0.46844947\n",
      "Test loss (w/o reg) on all data: 0.4257951\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00076943764\n",
      "Norm of the params: 1.5054983\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.42580. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36897758\n",
      "Train loss (w/o reg) on all data: 0.36866933\n",
      "Test loss (w/o reg) on all data: 0.44199812\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.005510209\n",
      "Norm of the params: 2.4829297\n",
      "                Loss: fixed  20 labels. Loss 0.44200. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5398141\n",
      "Train loss (w/o reg) on all data: 0.5397437\n",
      "Test loss (w/o reg) on all data: 0.45254016\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0015446313\n",
      "Norm of the params: 1.1864846\n",
      "              Random: fixed   7 labels. Loss 0.45254. Accuracy 0.844.\n",
      "### Flips: 40, rs: 21, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4445056\n",
      "Train loss (w/o reg) on all data: 0.4443589\n",
      "Test loss (w/o reg) on all data: 0.4216358\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014427565\n",
      "Norm of the params: 1.7130046\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.42164. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [290] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35503095\n",
      "Train loss (w/o reg) on all data: 0.3547269\n",
      "Test loss (w/o reg) on all data: 0.44735256\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00036658204\n",
      "Norm of the params: 2.4659784\n",
      "                Loss: fixed  23 labels. Loss 0.44735. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5329406\n",
      "Train loss (w/o reg) on all data: 0.5328588\n",
      "Test loss (w/o reg) on all data: 0.4478212\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001377523\n",
      "Norm of the params: 1.2792329\n",
      "              Random: fixed   8 labels. Loss 0.44782. Accuracy 0.822.\n",
      "### Flips: 40, rs: 21, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [148] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41512576\n",
      "Train loss (w/o reg) on all data: 0.4149242\n",
      "Test loss (w/o reg) on all data: 0.425015\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00054477045\n",
      "Norm of the params: 2.0077727\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.42502. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3418514\n",
      "Train loss (w/o reg) on all data: 0.3415567\n",
      "Test loss (w/o reg) on all data: 0.4413216\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00042050655\n",
      "Norm of the params: 2.4277651\n",
      "                Loss: fixed  27 labels. Loss 0.44132. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [222] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.537236\n",
      "Train loss (w/o reg) on all data: 0.5371566\n",
      "Test loss (w/o reg) on all data: 0.4383023\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013434414\n",
      "Norm of the params: 1.260029\n",
      "              Random: fixed  11 labels. Loss 0.43830. Accuracy 0.822.\n",
      "### Flips: 40, rs: 21, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.386388\n",
      "Train loss (w/o reg) on all data: 0.3861302\n",
      "Test loss (w/o reg) on all data: 0.41735557\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012429659\n",
      "Norm of the params: 2.270631\n",
      "     Influence (LOO): fixed  28 labels. Loss 0.41736. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3351503\n",
      "Train loss (w/o reg) on all data: 0.3348195\n",
      "Test loss (w/o reg) on all data: 0.43662083\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0004041706\n",
      "Norm of the params: 2.5722172\n",
      "                Loss: fixed  29 labels. Loss 0.43662. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54129153\n",
      "Train loss (w/o reg) on all data: 0.54122084\n",
      "Test loss (w/o reg) on all data: 0.4443301\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015231555\n",
      "Norm of the params: 1.1888387\n",
      "              Random: fixed  13 labels. Loss 0.44433. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [118] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5793932\n",
      "Train loss (w/o reg) on all data: 0.57932985\n",
      "Test loss (w/o reg) on all data: 0.48819652\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010823249\n",
      "Norm of the params: 1.1256309\n",
      "Flipped loss: 0.48820. Accuracy: 0.800\n",
      "### Flips: 40, rs: 22, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5303888\n",
      "Train loss (w/o reg) on all data: 0.5302796\n",
      "Test loss (w/o reg) on all data: 0.4651361\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.007978523\n",
      "Norm of the params: 1.4778917\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.46514. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [136] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5010379\n",
      "Train loss (w/o reg) on all data: 0.5008982\n",
      "Test loss (w/o reg) on all data: 0.46321923\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001691142\n",
      "Norm of the params: 1.671655\n",
      "                Loss: fixed   8 labels. Loss 0.46322. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [92] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5744606\n",
      "Train loss (w/o reg) on all data: 0.5743925\n",
      "Test loss (w/o reg) on all data: 0.5031155\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0033909094\n",
      "Norm of the params: 1.167153\n",
      "              Random: fixed   3 labels. Loss 0.50312. Accuracy 0.800.\n",
      "### Flips: 40, rs: 22, checks: 20\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49576235\n",
      "Train loss (w/o reg) on all data: 0.49564862\n",
      "Test loss (w/o reg) on all data: 0.4297033\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.005782514\n",
      "Norm of the params: 1.5081023\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.42970. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44856635\n",
      "Train loss (w/o reg) on all data: 0.44837946\n",
      "Test loss (w/o reg) on all data: 0.4591148\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013491788\n",
      "Norm of the params: 1.9334006\n",
      "                Loss: fixed  13 labels. Loss 0.45911. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5697913\n",
      "Train loss (w/o reg) on all data: 0.5697144\n",
      "Test loss (w/o reg) on all data: 0.5036749\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0078089726\n",
      "Norm of the params: 1.2402248\n",
      "              Random: fixed   5 labels. Loss 0.50367. Accuracy 0.800.\n",
      "### Flips: 40, rs: 22, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46549648\n",
      "Train loss (w/o reg) on all data: 0.46535343\n",
      "Test loss (w/o reg) on all data: 0.4205181\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.010670644\n",
      "Norm of the params: 1.6913956\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.42052. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4042852\n",
      "Train loss (w/o reg) on all data: 0.40406585\n",
      "Test loss (w/o reg) on all data: 0.4445562\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0030496307\n",
      "Norm of the params: 2.0944746\n",
      "                Loss: fixed  19 labels. Loss 0.44456. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5697935\n",
      "Train loss (w/o reg) on all data: 0.5697183\n",
      "Test loss (w/o reg) on all data: 0.50257134\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0050935782\n",
      "Norm of the params: 1.2265078\n",
      "              Random: fixed   5 labels. Loss 0.50257. Accuracy 0.800.\n",
      "### Flips: 40, rs: 22, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [171] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46549398\n",
      "Train loss (w/o reg) on all data: 0.4653513\n",
      "Test loss (w/o reg) on all data: 0.42047498\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.007386756\n",
      "Norm of the params: 1.6891385\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.42047. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38332248\n",
      "Train loss (w/o reg) on all data: 0.38307852\n",
      "Test loss (w/o reg) on all data: 0.432191\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0031104733\n",
      "Norm of the params: 2.2088895\n",
      "                Loss: fixed  22 labels. Loss 0.43219. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [110] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56023633\n",
      "Train loss (w/o reg) on all data: 0.5601505\n",
      "Test loss (w/o reg) on all data: 0.49662542\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00079909095\n",
      "Norm of the params: 1.3103116\n",
      "              Random: fixed   8 labels. Loss 0.49663. Accuracy 0.800.\n",
      "### Flips: 40, rs: 22, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4398574\n",
      "Train loss (w/o reg) on all data: 0.43969756\n",
      "Test loss (w/o reg) on all data: 0.40769878\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0035417078\n",
      "Norm of the params: 1.7878565\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.40770. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36328095\n",
      "Train loss (w/o reg) on all data: 0.3630073\n",
      "Test loss (w/o reg) on all data: 0.4428318\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00081517576\n",
      "Norm of the params: 2.339468\n",
      "                Loss: fixed  25 labels. Loss 0.44283. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54196376\n",
      "Train loss (w/o reg) on all data: 0.5418892\n",
      "Test loss (w/o reg) on all data: 0.46722656\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013106967\n",
      "Norm of the params: 1.2213677\n",
      "              Random: fixed  12 labels. Loss 0.46723. Accuracy 0.800.\n",
      "### Flips: 40, rs: 22, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [280] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41023487\n",
      "Train loss (w/o reg) on all data: 0.4100338\n",
      "Test loss (w/o reg) on all data: 0.39925838\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.008411203\n",
      "Norm of the params: 2.0053718\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.39926. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [320] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34149823\n",
      "Train loss (w/o reg) on all data: 0.34120467\n",
      "Test loss (w/o reg) on all data: 0.4371343\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00058306055\n",
      "Norm of the params: 2.423044\n",
      "                Loss: fixed  30 labels. Loss 0.43713. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53571343\n",
      "Train loss (w/o reg) on all data: 0.5356327\n",
      "Test loss (w/o reg) on all data: 0.47175235\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012014884\n",
      "Norm of the params: 1.2702575\n",
      "              Random: fixed  13 labels. Loss 0.47175. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55182993\n",
      "Train loss (w/o reg) on all data: 0.5517281\n",
      "Test loss (w/o reg) on all data: 0.42592612\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012171256\n",
      "Norm of the params: 1.4271159\n",
      "Flipped loss: 0.42593. Accuracy: 0.778\n",
      "### Flips: 40, rs: 23, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50816983\n",
      "Train loss (w/o reg) on all data: 0.50799835\n",
      "Test loss (w/o reg) on all data: 0.41599092\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.007837919\n",
      "Norm of the params: 1.8518561\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.41599. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46885216\n",
      "Train loss (w/o reg) on all data: 0.4686614\n",
      "Test loss (w/o reg) on all data: 0.3706148\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0034502575\n",
      "Norm of the params: 1.953267\n",
      "                Loss: fixed   8 labels. Loss 0.37061. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [285] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5375142\n",
      "Train loss (w/o reg) on all data: 0.5373741\n",
      "Test loss (w/o reg) on all data: 0.4207102\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018774869\n",
      "Norm of the params: 1.6739681\n",
      "              Random: fixed   3 labels. Loss 0.42071. Accuracy 0.800.\n",
      "### Flips: 40, rs: 23, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [122] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4856298\n",
      "Train loss (w/o reg) on all data: 0.4854485\n",
      "Test loss (w/o reg) on all data: 0.40243146\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012294658\n",
      "Norm of the params: 1.9040895\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.40243. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [285] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40157887\n",
      "Train loss (w/o reg) on all data: 0.40123335\n",
      "Test loss (w/o reg) on all data: 0.3605685\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0008217496\n",
      "Norm of the params: 2.6287432\n",
      "                Loss: fixed  15 labels. Loss 0.36057. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53065366\n",
      "Train loss (w/o reg) on all data: 0.5305017\n",
      "Test loss (w/o reg) on all data: 0.41232252\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00531897\n",
      "Norm of the params: 1.7431943\n",
      "              Random: fixed   4 labels. Loss 0.41232. Accuracy 0.800.\n",
      "### Flips: 40, rs: 23, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [160] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45059267\n",
      "Train loss (w/o reg) on all data: 0.4503441\n",
      "Test loss (w/o reg) on all data: 0.3965643\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00059397996\n",
      "Norm of the params: 2.229733\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.39656. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36138314\n",
      "Train loss (w/o reg) on all data: 0.3609884\n",
      "Test loss (w/o reg) on all data: 0.40228403\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016825554\n",
      "Norm of the params: 2.8097293\n",
      "                Loss: fixed  20 labels. Loss 0.40228. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5271997\n",
      "Train loss (w/o reg) on all data: 0.52703965\n",
      "Test loss (w/o reg) on all data: 0.41116315\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017664069\n",
      "Norm of the params: 1.7889322\n",
      "              Random: fixed   5 labels. Loss 0.41116. Accuracy 0.800.\n",
      "### Flips: 40, rs: 23, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41897824\n",
      "Train loss (w/o reg) on all data: 0.41867268\n",
      "Test loss (w/o reg) on all data: 0.3711885\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013707456\n",
      "Norm of the params: 2.472154\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.37119. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34237745\n",
      "Train loss (w/o reg) on all data: 0.3419604\n",
      "Test loss (w/o reg) on all data: 0.40832862\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009798015\n",
      "Norm of the params: 2.8881333\n",
      "                Loss: fixed  23 labels. Loss 0.40833. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50950503\n",
      "Train loss (w/o reg) on all data: 0.5093181\n",
      "Test loss (w/o reg) on all data: 0.4042458\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0018421824\n",
      "Norm of the params: 1.9333628\n",
      "              Random: fixed   8 labels. Loss 0.40425. Accuracy 0.778.\n",
      "### Flips: 40, rs: 23, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39924794\n",
      "Train loss (w/o reg) on all data: 0.39893585\n",
      "Test loss (w/o reg) on all data: 0.36058867\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020757802\n",
      "Norm of the params: 2.49841\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.36059. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3255602\n",
      "Train loss (w/o reg) on all data: 0.32511416\n",
      "Test loss (w/o reg) on all data: 0.42382684\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014326927\n",
      "Norm of the params: 2.9868338\n",
      "                Loss: fixed  27 labels. Loss 0.42383. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [340] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5022824\n",
      "Train loss (w/o reg) on all data: 0.5021044\n",
      "Test loss (w/o reg) on all data: 0.39286855\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0061913454\n",
      "Norm of the params: 1.8867704\n",
      "              Random: fixed  11 labels. Loss 0.39287. Accuracy 0.778.\n",
      "### Flips: 40, rs: 23, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [167] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39449647\n",
      "Train loss (w/o reg) on all data: 0.39420056\n",
      "Test loss (w/o reg) on all data: 0.36499268\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.009315611\n",
      "Norm of the params: 2.4327834\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.36499. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32556248\n",
      "Train loss (w/o reg) on all data: 0.3251171\n",
      "Test loss (w/o reg) on all data: 0.4239579\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008758102\n",
      "Norm of the params: 2.9844832\n",
      "                Loss: fixed  27 labels. Loss 0.42396. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4800428\n",
      "Train loss (w/o reg) on all data: 0.47983205\n",
      "Test loss (w/o reg) on all data: 0.37359747\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.005158009\n",
      "Norm of the params: 2.0530062\n",
      "              Random: fixed  13 labels. Loss 0.37360. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [336] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5866956\n",
      "Train loss (w/o reg) on all data: 0.5866023\n",
      "Test loss (w/o reg) on all data: 0.4807579\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0022072988\n",
      "Norm of the params: 1.3664497\n",
      "Flipped loss: 0.48076. Accuracy: 0.822\n",
      "### Flips: 40, rs: 24, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5455231\n",
      "Train loss (w/o reg) on all data: 0.5454129\n",
      "Test loss (w/o reg) on all data: 0.43933922\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0031567796\n",
      "Norm of the params: 1.4847513\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.43934. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [333] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4870316\n",
      "Train loss (w/o reg) on all data: 0.4867549\n",
      "Test loss (w/o reg) on all data: 0.4661596\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0008747774\n",
      "Norm of the params: 2.3524814\n",
      "                Loss: fixed   9 labels. Loss 0.46616. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5831078\n",
      "Train loss (w/o reg) on all data: 0.583028\n",
      "Test loss (w/o reg) on all data: 0.48439285\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010640706\n",
      "Norm of the params: 1.2632247\n",
      "              Random: fixed   3 labels. Loss 0.48439. Accuracy 0.822.\n",
      "### Flips: 40, rs: 24, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [139] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52657187\n",
      "Train loss (w/o reg) on all data: 0.5264324\n",
      "Test loss (w/o reg) on all data: 0.43976763\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011298968\n",
      "Norm of the params: 1.6701505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Influence (LOO): fixed  10 labels. Loss 0.43977. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [351] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42053464\n",
      "Train loss (w/o reg) on all data: 0.4200759\n",
      "Test loss (w/o reg) on all data: 0.4419554\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019191388\n",
      "Norm of the params: 3.0290627\n",
      "                Loss: fixed  16 labels. Loss 0.44196. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [345] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56569177\n",
      "Train loss (w/o reg) on all data: 0.56559616\n",
      "Test loss (w/o reg) on all data: 0.4732257\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.005821784\n",
      "Norm of the params: 1.3827083\n",
      "              Random: fixed   6 labels. Loss 0.47323. Accuracy 0.822.\n",
      "### Flips: 40, rs: 24, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [172] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49821526\n",
      "Train loss (w/o reg) on all data: 0.498026\n",
      "Test loss (w/o reg) on all data: 0.4378582\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012083097\n",
      "Norm of the params: 1.9454798\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.43786. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3826202\n",
      "Train loss (w/o reg) on all data: 0.3821029\n",
      "Test loss (w/o reg) on all data: 0.4685766\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015457702\n",
      "Norm of the params: 3.216446\n",
      "                Loss: fixed  21 labels. Loss 0.46858. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [445] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5548902\n",
      "Train loss (w/o reg) on all data: 0.5547823\n",
      "Test loss (w/o reg) on all data: 0.46633542\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00043612332\n",
      "Norm of the params: 1.4691513\n",
      "              Random: fixed   7 labels. Loss 0.46634. Accuracy 0.822.\n",
      "### Flips: 40, rs: 24, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [314] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43650904\n",
      "Train loss (w/o reg) on all data: 0.43622535\n",
      "Test loss (w/o reg) on all data: 0.4026687\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0005760156\n",
      "Norm of the params: 2.3819492\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.40267. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [186] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3660048\n",
      "Train loss (w/o reg) on all data: 0.36551327\n",
      "Test loss (w/o reg) on all data: 0.47198507\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0039351806\n",
      "Norm of the params: 3.135338\n",
      "                Loss: fixed  24 labels. Loss 0.47199. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [368] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5632869\n",
      "Train loss (w/o reg) on all data: 0.56319296\n",
      "Test loss (w/o reg) on all data: 0.4596967\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012524861\n",
      "Norm of the params: 1.3704884\n",
      "              Random: fixed   8 labels. Loss 0.45970. Accuracy 0.800.\n",
      "### Flips: 40, rs: 24, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39509064\n",
      "Train loss (w/o reg) on all data: 0.39468744\n",
      "Test loss (w/o reg) on all data: 0.398395\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0049456246\n",
      "Norm of the params: 2.8397102\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.39840. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [161] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3494318\n",
      "Train loss (w/o reg) on all data: 0.34895486\n",
      "Test loss (w/o reg) on all data: 0.45485017\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00044489847\n",
      "Norm of the params: 3.0885563\n",
      "                Loss: fixed  27 labels. Loss 0.45485. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [365] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5616254\n",
      "Train loss (w/o reg) on all data: 0.56153387\n",
      "Test loss (w/o reg) on all data: 0.4582155\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00254115\n",
      "Norm of the params: 1.3530796\n",
      "              Random: fixed  10 labels. Loss 0.45822. Accuracy 0.800.\n",
      "### Flips: 40, rs: 24, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [299] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39623475\n",
      "Train loss (w/o reg) on all data: 0.39588735\n",
      "Test loss (w/o reg) on all data: 0.39526376\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005463601\n",
      "Norm of the params: 2.63597\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.39526. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [211] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33826602\n",
      "Train loss (w/o reg) on all data: 0.33776602\n",
      "Test loss (w/o reg) on all data: 0.46182021\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0013599667\n",
      "Norm of the params: 3.1622276\n",
      "                Loss: fixed  30 labels. Loss 0.46182. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52605754\n",
      "Train loss (w/o reg) on all data: 0.5259539\n",
      "Test loss (w/o reg) on all data: 0.4509893\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013896236\n",
      "Norm of the params: 1.4396402\n",
      "              Random: fixed  15 labels. Loss 0.45099. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [91] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55933285\n",
      "Train loss (w/o reg) on all data: 0.5592511\n",
      "Test loss (w/o reg) on all data: 0.4267085\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017084642\n",
      "Norm of the params: 1.2785764\n",
      "Flipped loss: 0.42671. Accuracy: 0.822\n",
      "### Flips: 40, rs: 25, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50712144\n",
      "Train loss (w/o reg) on all data: 0.50700676\n",
      "Test loss (w/o reg) on all data: 0.4056094\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00046051983\n",
      "Norm of the params: 1.5143712\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.40561. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [142] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44975594\n",
      "Train loss (w/o reg) on all data: 0.44958198\n",
      "Test loss (w/o reg) on all data: 0.4320213\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008788445\n",
      "Norm of the params: 1.865268\n",
      "                Loss: fixed   9 labels. Loss 0.43202. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56198376\n",
      "Train loss (w/o reg) on all data: 0.56190807\n",
      "Test loss (w/o reg) on all data: 0.42395592\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015466044\n",
      "Norm of the params: 1.2306559\n",
      "              Random: fixed   1 labels. Loss 0.42396. Accuracy 0.800.\n",
      "### Flips: 40, rs: 25, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [167] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46482542\n",
      "Train loss (w/o reg) on all data: 0.46463588\n",
      "Test loss (w/o reg) on all data: 0.39230677\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0024918087\n",
      "Norm of the params: 1.9470812\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.39231. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37930638\n",
      "Train loss (w/o reg) on all data: 0.37899774\n",
      "Test loss (w/o reg) on all data: 0.49012595\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0014552377\n",
      "Norm of the params: 2.484492\n",
      "                Loss: fixed  17 labels. Loss 0.49013. Accuracy 0.756.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5679674\n",
      "Train loss (w/o reg) on all data: 0.5679002\n",
      "Test loss (w/o reg) on all data: 0.42727247\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.002097136\n",
      "Norm of the params: 1.1597794\n",
      "              Random: fixed   7 labels. Loss 0.42727. Accuracy 0.756.\n",
      "### Flips: 40, rs: 25, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43812716\n",
      "Train loss (w/o reg) on all data: 0.43788925\n",
      "Test loss (w/o reg) on all data: 0.3963346\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0026960117\n",
      "Norm of the params: 2.1812882\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.39633. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3451142\n",
      "Train loss (w/o reg) on all data: 0.34471983\n",
      "Test loss (w/o reg) on all data: 0.48071975\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00018426306\n",
      "Norm of the params: 2.8084788\n",
      "                Loss: fixed  22 labels. Loss 0.48072. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [297] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5631233\n",
      "Train loss (w/o reg) on all data: 0.5630465\n",
      "Test loss (w/o reg) on all data: 0.43568248\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.002208802\n",
      "Norm of the params: 1.2389424\n",
      "              Random: fixed   8 labels. Loss 0.43568. Accuracy 0.756.\n",
      "### Flips: 40, rs: 25, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [258] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43796477\n",
      "Train loss (w/o reg) on all data: 0.4377573\n",
      "Test loss (w/o reg) on all data: 0.38735914\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0071331197\n",
      "Norm of the params: 2.0369813\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.38736. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [462] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3194875\n",
      "Train loss (w/o reg) on all data: 0.31901756\n",
      "Test loss (w/o reg) on all data: 0.48085928\n",
      "Train acc on all data:  0.8820754716981132\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015434499\n",
      "Norm of the params: 3.0658352\n",
      "                Loss: fixed  27 labels. Loss 0.48086. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [297] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5495645\n",
      "Train loss (w/o reg) on all data: 0.5494767\n",
      "Test loss (w/o reg) on all data: 0.41238314\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002505742\n",
      "Norm of the params: 1.3250098\n",
      "              Random: fixed  10 labels. Loss 0.41238. Accuracy 0.800.\n",
      "### Flips: 40, rs: 25, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [327] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41956505\n",
      "Train loss (w/o reg) on all data: 0.41931966\n",
      "Test loss (w/o reg) on all data: 0.38427973\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0024441173\n",
      "Norm of the params: 2.2154279\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.38428. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [326] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3141652\n",
      "Train loss (w/o reg) on all data: 0.31371084\n",
      "Test loss (w/o reg) on all data: 0.48036164\n",
      "Train acc on all data:  0.8915094339622641\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011843592\n",
      "Norm of the params: 3.0145311\n",
      "                Loss: fixed  28 labels. Loss 0.48036. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55066043\n",
      "Train loss (w/o reg) on all data: 0.5505825\n",
      "Test loss (w/o reg) on all data: 0.41090143\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0025722706\n",
      "Norm of the params: 1.2484086\n",
      "              Random: fixed  11 labels. Loss 0.41090. Accuracy 0.800.\n",
      "### Flips: 40, rs: 25, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [288] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3895363\n",
      "Train loss (w/o reg) on all data: 0.38919723\n",
      "Test loss (w/o reg) on all data: 0.3921563\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00073627906\n",
      "Norm of the params: 2.6041114\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.39216. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [368] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31416276\n",
      "Train loss (w/o reg) on all data: 0.31370875\n",
      "Test loss (w/o reg) on all data: 0.48003367\n",
      "Train acc on all data:  0.8915094339622641\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00072823965\n",
      "Norm of the params: 3.0133076\n",
      "                Loss: fixed  28 labels. Loss 0.48003. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54526216\n",
      "Train loss (w/o reg) on all data: 0.5451831\n",
      "Test loss (w/o reg) on all data: 0.41600737\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.008156745\n",
      "Norm of the params: 1.2572521\n",
      "              Random: fixed  12 labels. Loss 0.41601. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [208] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53830063\n",
      "Train loss (w/o reg) on all data: 0.5382061\n",
      "Test loss (w/o reg) on all data: 0.4819626\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.000567873\n",
      "Norm of the params: 1.3750348\n",
      "Flipped loss: 0.48196. Accuracy: 0.756\n",
      "### Flips: 40, rs: 26, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48300317\n",
      "Train loss (w/o reg) on all data: 0.48284703\n",
      "Test loss (w/o reg) on all data: 0.47237495\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0023276175\n",
      "Norm of the params: 1.7671787\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.47237. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [106] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46691668\n",
      "Train loss (w/o reg) on all data: 0.46673942\n",
      "Test loss (w/o reg) on all data: 0.46693903\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012002336\n",
      "Norm of the params: 1.8829168\n",
      "                Loss: fixed   6 labels. Loss 0.46694. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5286734\n",
      "Train loss (w/o reg) on all data: 0.528588\n",
      "Test loss (w/o reg) on all data: 0.4633948\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0012860318\n",
      "Norm of the params: 1.307217\n",
      "              Random: fixed   3 labels. Loss 0.46339. Accuracy 0.756.\n",
      "### Flips: 40, rs: 26, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46827295\n",
      "Train loss (w/o reg) on all data: 0.4680928\n",
      "Test loss (w/o reg) on all data: 0.5017394\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0036487319\n",
      "Norm of the params: 1.8981705\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.50174. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [310] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41577893\n",
      "Train loss (w/o reg) on all data: 0.41550028\n",
      "Test loss (w/o reg) on all data: 0.5043798\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0012127867\n",
      "Norm of the params: 2.3606775\n",
      "                Loss: fixed  12 labels. Loss 0.50438. Accuracy 0.756.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5257101\n",
      "Train loss (w/o reg) on all data: 0.5256251\n",
      "Test loss (w/o reg) on all data: 0.44788438\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00086414575\n",
      "Norm of the params: 1.3036965\n",
      "              Random: fixed   4 labels. Loss 0.44788. Accuracy 0.756.\n",
      "### Flips: 40, rs: 26, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4596481\n",
      "Train loss (w/o reg) on all data: 0.45948026\n",
      "Test loss (w/o reg) on all data: 0.47818187\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0008401422\n",
      "Norm of the params: 1.8321893\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.47818. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [376] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.367821\n",
      "Train loss (w/o reg) on all data: 0.367485\n",
      "Test loss (w/o reg) on all data: 0.50656897\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0008649811\n",
      "Norm of the params: 2.5924208\n",
      "                Loss: fixed  19 labels. Loss 0.50657. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52296317\n",
      "Train loss (w/o reg) on all data: 0.5228806\n",
      "Test loss (w/o reg) on all data: 0.45580927\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0013949876\n",
      "Norm of the params: 1.2847346\n",
      "              Random: fixed   5 labels. Loss 0.45581. Accuracy 0.756.\n",
      "### Flips: 40, rs: 26, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4215463\n",
      "Train loss (w/o reg) on all data: 0.4213522\n",
      "Test loss (w/o reg) on all data: 0.48258483\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0010206865\n",
      "Norm of the params: 1.9702626\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.48258. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34667605\n",
      "Train loss (w/o reg) on all data: 0.34629732\n",
      "Test loss (w/o reg) on all data: 0.51263887\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0013092171\n",
      "Norm of the params: 2.7521825\n",
      "                Loss: fixed  24 labels. Loss 0.51264. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [193] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5095378\n",
      "Train loss (w/o reg) on all data: 0.5094466\n",
      "Test loss (w/o reg) on all data: 0.4362992\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001517749\n",
      "Norm of the params: 1.3504932\n",
      "              Random: fixed   8 labels. Loss 0.43630. Accuracy 0.778.\n",
      "### Flips: 40, rs: 26, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [289] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39364427\n",
      "Train loss (w/o reg) on all data: 0.39339122\n",
      "Test loss (w/o reg) on all data: 0.48275948\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0011357774\n",
      "Norm of the params: 2.249718\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.48276. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3277799\n",
      "Train loss (w/o reg) on all data: 0.32727122\n",
      "Test loss (w/o reg) on all data: 0.5226765\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0038306955\n",
      "Norm of the params: 3.189589\n",
      "                Loss: fixed  28 labels. Loss 0.52268. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [300] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48649526\n",
      "Train loss (w/o reg) on all data: 0.48638484\n",
      "Test loss (w/o reg) on all data: 0.4295067\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0034271935\n",
      "Norm of the params: 1.4860004\n",
      "              Random: fixed  10 labels. Loss 0.42951. Accuracy 0.800.\n",
      "### Flips: 40, rs: 26, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38383666\n",
      "Train loss (w/o reg) on all data: 0.38356167\n",
      "Test loss (w/o reg) on all data: 0.49285418\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0008592559\n",
      "Norm of the params: 2.3451958\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.49285. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [269] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32959324\n",
      "Train loss (w/o reg) on all data: 0.32909083\n",
      "Test loss (w/o reg) on all data: 0.49954504\n",
      "Train acc on all data:  0.8820754716981132\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.002176676\n",
      "Norm of the params: 3.169839\n",
      "                Loss: fixed  30 labels. Loss 0.49955. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [332] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47187597\n",
      "Train loss (w/o reg) on all data: 0.4717463\n",
      "Test loss (w/o reg) on all data: 0.42466733\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009702352\n",
      "Norm of the params: 1.610353\n",
      "              Random: fixed  12 labels. Loss 0.42467. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55195993\n",
      "Train loss (w/o reg) on all data: 0.55190367\n",
      "Test loss (w/o reg) on all data: 0.45859206\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002014546\n",
      "Norm of the params: 1.061002\n",
      "Flipped loss: 0.45859. Accuracy: 0.778\n",
      "### Flips: 40, rs: 27, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [92] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5349941\n",
      "Train loss (w/o reg) on all data: 0.5349259\n",
      "Test loss (w/o reg) on all data: 0.44881567\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0008798017\n",
      "Norm of the params: 1.1683387\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.44882. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [190] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46253526\n",
      "Train loss (w/o reg) on all data: 0.4624191\n",
      "Test loss (w/o reg) on all data: 0.44924057\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00028299342\n",
      "Norm of the params: 1.5243171\n",
      "                Loss: fixed   9 labels. Loss 0.44924. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [185] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5449452\n",
      "Train loss (w/o reg) on all data: 0.54488266\n",
      "Test loss (w/o reg) on all data: 0.44962394\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0067855795\n",
      "Norm of the params: 1.1183039\n",
      "              Random: fixed   1 labels. Loss 0.44962. Accuracy 0.778.\n",
      "### Flips: 40, rs: 27, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [118] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4858073\n",
      "Train loss (w/o reg) on all data: 0.48568657\n",
      "Test loss (w/o reg) on all data: 0.44014612\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0005269032\n",
      "Norm of the params: 1.5539018\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.44015. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [293] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40107837\n",
      "Train loss (w/o reg) on all data: 0.40089306\n",
      "Test loss (w/o reg) on all data: 0.4575258\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0009891199\n",
      "Norm of the params: 1.9251473\n",
      "                Loss: fixed  15 labels. Loss 0.45753. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [166] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5375597\n",
      "Train loss (w/o reg) on all data: 0.5374866\n",
      "Test loss (w/o reg) on all data: 0.45393023\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015481133\n",
      "Norm of the params: 1.2088306\n",
      "              Random: fixed   2 labels. Loss 0.45393. Accuracy 0.778.\n",
      "### Flips: 40, rs: 27, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [266] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46147594\n",
      "Train loss (w/o reg) on all data: 0.46134093\n",
      "Test loss (w/o reg) on all data: 0.4109232\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0026095577\n",
      "Norm of the params: 1.6432095\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.41092. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [322] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35566708\n",
      "Train loss (w/o reg) on all data: 0.35540217\n",
      "Test loss (w/o reg) on all data: 0.4844735\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00060667965\n",
      "Norm of the params: 2.3017387\n",
      "                Loss: fixed  21 labels. Loss 0.48447. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [326] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5381784\n",
      "Train loss (w/o reg) on all data: 0.5380932\n",
      "Test loss (w/o reg) on all data: 0.44947892\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.008666553\n",
      "Norm of the params: 1.305345\n",
      "              Random: fixed   4 labels. Loss 0.44948. Accuracy 0.800.\n",
      "### Flips: 40, rs: 27, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40245596\n",
      "Train loss (w/o reg) on all data: 0.40226594\n",
      "Test loss (w/o reg) on all data: 0.41135132\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015039596\n",
      "Norm of the params: 1.949439\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.41135. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32186458\n",
      "Train loss (w/o reg) on all data: 0.321561\n",
      "Test loss (w/o reg) on all data: 0.47752106\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00089761766\n",
      "Norm of the params: 2.4640045\n",
      "                Loss: fixed  26 labels. Loss 0.47752. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5295574\n",
      "Train loss (w/o reg) on all data: 0.529462\n",
      "Test loss (w/o reg) on all data: 0.44829226\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00173115\n",
      "Norm of the params: 1.3816391\n",
      "              Random: fixed   5 labels. Loss 0.44829. Accuracy 0.822.\n",
      "### Flips: 40, rs: 27, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.390319\n",
      "Train loss (w/o reg) on all data: 0.39008853\n",
      "Test loss (w/o reg) on all data: 0.41279033\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012339557\n",
      "Norm of the params: 2.1469223\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.41279. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [201] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3091173\n",
      "Train loss (w/o reg) on all data: 0.3087569\n",
      "Test loss (w/o reg) on all data: 0.46444076\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006356955\n",
      "Norm of the params: 2.6847925\n",
      "                Loss: fixed  29 labels. Loss 0.46444. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52096736\n",
      "Train loss (w/o reg) on all data: 0.5208641\n",
      "Test loss (w/o reg) on all data: 0.4429175\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018959189\n",
      "Norm of the params: 1.4367844\n",
      "              Random: fixed   8 labels. Loss 0.44292. Accuracy 0.822.\n",
      "### Flips: 40, rs: 27, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35556445\n",
      "Train loss (w/o reg) on all data: 0.3552914\n",
      "Test loss (w/o reg) on all data: 0.4396752\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0023772176\n",
      "Norm of the params: 2.3368375\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.43968. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.296023\n",
      "Train loss (w/o reg) on all data: 0.2956205\n",
      "Test loss (w/o reg) on all data: 0.48105976\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00086771237\n",
      "Norm of the params: 2.8372953\n",
      "                Loss: fixed  32 labels. Loss 0.48106. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [295] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50928617\n",
      "Train loss (w/o reg) on all data: 0.5091819\n",
      "Test loss (w/o reg) on all data: 0.4305235\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017596859\n",
      "Norm of the params: 1.4439805\n",
      "              Random: fixed  10 labels. Loss 0.43052. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60348094\n",
      "Train loss (w/o reg) on all data: 0.60342896\n",
      "Test loss (w/o reg) on all data: 0.44555154\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001846387\n",
      "Norm of the params: 1.0198076\n",
      "Flipped loss: 0.44555. Accuracy: 0.800\n",
      "### Flips: 40, rs: 28, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [94] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5801113\n",
      "Train loss (w/o reg) on all data: 0.580045\n",
      "Test loss (w/o reg) on all data: 0.4079117\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012519791\n",
      "Norm of the params: 1.1517633\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.40791. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [130] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5289256\n",
      "Train loss (w/o reg) on all data: 0.5288325\n",
      "Test loss (w/o reg) on all data: 0.3656134\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0070280684\n",
      "Norm of the params: 1.3645581\n",
      "                Loss: fixed   8 labels. Loss 0.36561. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [126] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5886249\n",
      "Train loss (w/o reg) on all data: 0.588565\n",
      "Test loss (w/o reg) on all data: 0.42840946\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0044569615\n",
      "Norm of the params: 1.0946223\n",
      "              Random: fixed   4 labels. Loss 0.42841. Accuracy 0.778.\n",
      "### Flips: 40, rs: 28, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [139] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5509309\n",
      "Train loss (w/o reg) on all data: 0.55083674\n",
      "Test loss (w/o reg) on all data: 0.3863647\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0036343113\n",
      "Norm of the params: 1.3722833\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.38636. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45548478\n",
      "Train loss (w/o reg) on all data: 0.45529693\n",
      "Test loss (w/o reg) on all data: 0.36976454\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010192764\n",
      "Norm of the params: 1.9383299\n",
      "                Loss: fixed  17 labels. Loss 0.36976. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [159] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5789083\n",
      "Train loss (w/o reg) on all data: 0.57884365\n",
      "Test loss (w/o reg) on all data: 0.41809997\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0153202405\n",
      "Norm of the params: 1.1375381\n",
      "              Random: fixed   6 labels. Loss 0.41810. Accuracy 0.800.\n",
      "### Flips: 40, rs: 28, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5322976\n",
      "Train loss (w/o reg) on all data: 0.5321675\n",
      "Test loss (w/o reg) on all data: 0.39535382\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010037877\n",
      "Norm of the params: 1.6132252\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.39535. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3902643\n",
      "Train loss (w/o reg) on all data: 0.38993764\n",
      "Test loss (w/o reg) on all data: 0.3867023\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0016533704\n",
      "Norm of the params: 2.5559702\n",
      "                Loss: fixed  24 labels. Loss 0.38670. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [302] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56558555\n",
      "Train loss (w/o reg) on all data: 0.56550515\n",
      "Test loss (w/o reg) on all data: 0.40930387\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012612427\n",
      "Norm of the params: 1.2681011\n",
      "              Random: fixed   9 labels. Loss 0.40930. Accuracy 0.800.\n",
      "### Flips: 40, rs: 28, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4930102\n",
      "Train loss (w/o reg) on all data: 0.49285924\n",
      "Test loss (w/o reg) on all data: 0.36808047\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.008040773\n",
      "Norm of the params: 1.7375928\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.36808. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [342] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3692389\n",
      "Train loss (w/o reg) on all data: 0.36891136\n",
      "Test loss (w/o reg) on all data: 0.417198\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0003439263\n",
      "Norm of the params: 2.559481\n",
      "                Loss: fixed  27 labels. Loss 0.41720. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [309] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5587548\n",
      "Train loss (w/o reg) on all data: 0.55867326\n",
      "Test loss (w/o reg) on all data: 0.41035193\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022913327\n",
      "Norm of the params: 1.2768371\n",
      "              Random: fixed  10 labels. Loss 0.41035. Accuracy 0.800.\n",
      "### Flips: 40, rs: 28, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [392] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47880292\n",
      "Train loss (w/o reg) on all data: 0.4786289\n",
      "Test loss (w/o reg) on all data: 0.36370987\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004355243\n",
      "Norm of the params: 1.865602\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.36371. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [388] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35007313\n",
      "Train loss (w/o reg) on all data: 0.3496671\n",
      "Test loss (w/o reg) on all data: 0.41761822\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0028775972\n",
      "Norm of the params: 2.8496647\n",
      "                Loss: fixed  32 labels. Loss 0.41762. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [172] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55709827\n",
      "Train loss (w/o reg) on all data: 0.55702823\n",
      "Test loss (w/o reg) on all data: 0.40209398\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002141848\n",
      "Norm of the params: 1.1833025\n",
      "              Random: fixed  12 labels. Loss 0.40209. Accuracy 0.800.\n",
      "### Flips: 40, rs: 28, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45889074\n",
      "Train loss (w/o reg) on all data: 0.4586863\n",
      "Test loss (w/o reg) on all data: 0.37301314\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012297698\n",
      "Norm of the params: 2.022096\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.37301. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34574047\n",
      "Train loss (w/o reg) on all data: 0.34532928\n",
      "Test loss (w/o reg) on all data: 0.41412002\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0058323536\n",
      "Norm of the params: 2.867672\n",
      "                Loss: fixed  34 labels. Loss 0.41412. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [312] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5326975\n",
      "Train loss (w/o reg) on all data: 0.532615\n",
      "Test loss (w/o reg) on all data: 0.37541708\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0013389289\n",
      "Norm of the params: 1.2843969\n",
      "              Random: fixed  14 labels. Loss 0.37542. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5525317\n",
      "Train loss (w/o reg) on all data: 0.5524743\n",
      "Test loss (w/o reg) on all data: 0.4390817\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018350435\n",
      "Norm of the params: 1.0712371\n",
      "Flipped loss: 0.43908. Accuracy: 0.822\n",
      "### Flips: 40, rs: 29, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [83] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.530296\n",
      "Train loss (w/o reg) on all data: 0.5302366\n",
      "Test loss (w/o reg) on all data: 0.4323407\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012401426\n",
      "Norm of the params: 1.0900167\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.43234. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [149] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46860415\n",
      "Train loss (w/o reg) on all data: 0.46849182\n",
      "Test loss (w/o reg) on all data: 0.44532165\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014569956\n",
      "Norm of the params: 1.4987595\n",
      "                Loss: fixed   9 labels. Loss 0.44532. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [125] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.542405\n",
      "Train loss (w/o reg) on all data: 0.54234153\n",
      "Test loss (w/o reg) on all data: 0.43747643\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016766123\n",
      "Norm of the params: 1.126523\n",
      "              Random: fixed   2 labels. Loss 0.43748. Accuracy 0.822.\n",
      "### Flips: 40, rs: 29, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5082162\n",
      "Train loss (w/o reg) on all data: 0.50816727\n",
      "Test loss (w/o reg) on all data: 0.43416283\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0044600475\n",
      "Norm of the params: 0.9892066\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.43416. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43541947\n",
      "Train loss (w/o reg) on all data: 0.435272\n",
      "Test loss (w/o reg) on all data: 0.44068918\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00057387527\n",
      "Norm of the params: 1.717323\n",
      "                Loss: fixed  13 labels. Loss 0.44069. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [138] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53486276\n",
      "Train loss (w/o reg) on all data: 0.5347901\n",
      "Test loss (w/o reg) on all data: 0.44273165\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.000593562\n",
      "Norm of the params: 1.2056775\n",
      "              Random: fixed   4 labels. Loss 0.44273. Accuracy 0.822.\n",
      "### Flips: 40, rs: 29, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [128] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49355233\n",
      "Train loss (w/o reg) on all data: 0.49348816\n",
      "Test loss (w/o reg) on all data: 0.41108844\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012801777\n",
      "Norm of the params: 1.1327783\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.41109. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.389428\n",
      "Train loss (w/o reg) on all data: 0.38918656\n",
      "Test loss (w/o reg) on all data: 0.43367597\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007404946\n",
      "Norm of the params: 2.1974475\n",
      "                Loss: fixed  18 labels. Loss 0.43368. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5191598\n",
      "Train loss (w/o reg) on all data: 0.51909125\n",
      "Test loss (w/o reg) on all data: 0.45282295\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0051754713\n",
      "Norm of the params: 1.1710224\n",
      "              Random: fixed   6 labels. Loss 0.45282. Accuracy 0.822.\n",
      "### Flips: 40, rs: 29, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46179017\n",
      "Train loss (w/o reg) on all data: 0.46169525\n",
      "Test loss (w/o reg) on all data: 0.3973961\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.006444531\n",
      "Norm of the params: 1.377883\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.39740. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36650446\n",
      "Train loss (w/o reg) on all data: 0.36624992\n",
      "Test loss (w/o reg) on all data: 0.47483867\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015903063\n",
      "Norm of the params: 2.2562943\n",
      "                Loss: fixed  23 labels. Loss 0.47484. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5121048\n",
      "Train loss (w/o reg) on all data: 0.5120274\n",
      "Test loss (w/o reg) on all data: 0.45863712\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0069505107\n",
      "Norm of the params: 1.2445396\n",
      "              Random: fixed   8 labels. Loss 0.45864. Accuracy 0.822.\n",
      "### Flips: 40, rs: 29, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [190] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42812693\n",
      "Train loss (w/o reg) on all data: 0.42799586\n",
      "Test loss (w/o reg) on all data: 0.43096492\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.006974981\n",
      "Norm of the params: 1.6190195\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.43096. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [361] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3472409\n",
      "Train loss (w/o reg) on all data: 0.34693426\n",
      "Test loss (w/o reg) on all data: 0.47788167\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0024959955\n",
      "Norm of the params: 2.4764907\n",
      "                Loss: fixed  27 labels. Loss 0.47788. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50244504\n",
      "Train loss (w/o reg) on all data: 0.5023583\n",
      "Test loss (w/o reg) on all data: 0.45949444\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002357035\n",
      "Norm of the params: 1.3172206\n",
      "              Random: fixed   9 labels. Loss 0.45949. Accuracy 0.822.\n",
      "### Flips: 40, rs: 29, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [428] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35589385\n",
      "Train loss (w/o reg) on all data: 0.35560438\n",
      "Test loss (w/o reg) on all data: 0.46536916\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0004819889\n",
      "Norm of the params: 2.406179\n",
      "     Influence (LOO): fixed  28 labels. Loss 0.46537. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [125] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32351395\n",
      "Train loss (w/o reg) on all data: 0.32314974\n",
      "Test loss (w/o reg) on all data: 0.4647165\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0033949492\n",
      "Norm of the params: 2.6989865\n",
      "                Loss: fixed  31 labels. Loss 0.46472. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [305] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49199298\n",
      "Train loss (w/o reg) on all data: 0.49190146\n",
      "Test loss (w/o reg) on all data: 0.4557326\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008135489\n",
      "Norm of the params: 1.3529966\n",
      "              Random: fixed  10 labels. Loss 0.45573. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56961966\n",
      "Train loss (w/o reg) on all data: 0.5695162\n",
      "Test loss (w/o reg) on all data: 0.47191337\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009648035\n",
      "Norm of the params: 1.4383833\n",
      "Flipped loss: 0.47191. Accuracy: 0.800\n",
      "### Flips: 40, rs: 30, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [155] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5159739\n",
      "Train loss (w/o reg) on all data: 0.51581246\n",
      "Test loss (w/o reg) on all data: 0.43282828\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0030459855\n",
      "Norm of the params: 1.797027\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.43283. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [320] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47236472\n",
      "Train loss (w/o reg) on all data: 0.47204885\n",
      "Test loss (w/o reg) on all data: 0.4315876\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015144378\n",
      "Norm of the params: 2.5134835\n",
      "                Loss: fixed   9 labels. Loss 0.43159. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [195] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55896914\n",
      "Train loss (w/o reg) on all data: 0.55884993\n",
      "Test loss (w/o reg) on all data: 0.4664124\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004919623\n",
      "Norm of the params: 1.5441546\n",
      "              Random: fixed   1 labels. Loss 0.46641. Accuracy 0.800.\n",
      "### Flips: 40, rs: 30, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [202] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4764073\n",
      "Train loss (w/o reg) on all data: 0.47616053\n",
      "Test loss (w/o reg) on all data: 0.42091087\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0009152031\n",
      "Norm of the params: 2.2215254\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.42091. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41053897\n",
      "Train loss (w/o reg) on all data: 0.41012117\n",
      "Test loss (w/o reg) on all data: 0.4271432\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.003148685\n",
      "Norm of the params: 2.8907163\n",
      "                Loss: fixed  16 labels. Loss 0.42714. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [150] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5439489\n",
      "Train loss (w/o reg) on all data: 0.5438166\n",
      "Test loss (w/o reg) on all data: 0.44540915\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.008910574\n",
      "Norm of the params: 1.6263041\n",
      "              Random: fixed   3 labels. Loss 0.44541. Accuracy 0.822.\n",
      "### Flips: 40, rs: 30, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4413412\n",
      "Train loss (w/o reg) on all data: 0.4410463\n",
      "Test loss (w/o reg) on all data: 0.41953713\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00056666246\n",
      "Norm of the params: 2.4285724\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.41954. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3906994\n",
      "Train loss (w/o reg) on all data: 0.39024782\n",
      "Test loss (w/o reg) on all data: 0.4431488\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016379474\n",
      "Norm of the params: 3.0052006\n",
      "                Loss: fixed  19 labels. Loss 0.44315. Accuracy 0.822.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [187] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53970987\n",
      "Train loss (w/o reg) on all data: 0.53957045\n",
      "Test loss (w/o reg) on all data: 0.43052584\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0090180775\n",
      "Norm of the params: 1.6699368\n",
      "              Random: fixed   6 labels. Loss 0.43053. Accuracy 0.822.\n",
      "### Flips: 40, rs: 30, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43069896\n",
      "Train loss (w/o reg) on all data: 0.43038055\n",
      "Test loss (w/o reg) on all data: 0.41563845\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.006743247\n",
      "Norm of the params: 2.5235288\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.41564. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36176348\n",
      "Train loss (w/o reg) on all data: 0.36127156\n",
      "Test loss (w/o reg) on all data: 0.41591763\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.005196069\n",
      "Norm of the params: 3.136597\n",
      "                Loss: fixed  24 labels. Loss 0.41592. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53970736\n",
      "Train loss (w/o reg) on all data: 0.5395678\n",
      "Test loss (w/o reg) on all data: 0.43000636\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013763872\n",
      "Norm of the params: 1.6704789\n",
      "              Random: fixed   6 labels. Loss 0.43001. Accuracy 0.822.\n",
      "### Flips: 40, rs: 30, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [288] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41228944\n",
      "Train loss (w/o reg) on all data: 0.41191524\n",
      "Test loss (w/o reg) on all data: 0.41184017\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00039852355\n",
      "Norm of the params: 2.735631\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.41184. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [240] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33922604\n",
      "Train loss (w/o reg) on all data: 0.33872154\n",
      "Test loss (w/o reg) on all data: 0.43494442\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004557332\n",
      "Norm of the params: 3.1764598\n",
      "                Loss: fixed  28 labels. Loss 0.43494. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [329] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5397059\n",
      "Train loss (w/o reg) on all data: 0.5395668\n",
      "Test loss (w/o reg) on all data: 0.42982098\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.000641959\n",
      "Norm of the params: 1.6677912\n",
      "              Random: fixed   6 labels. Loss 0.42982. Accuracy 0.822.\n",
      "### Flips: 40, rs: 30, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [302] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38473123\n",
      "Train loss (w/o reg) on all data: 0.38434276\n",
      "Test loss (w/o reg) on all data: 0.41392606\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0022661192\n",
      "Norm of the params: 2.7874067\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.41393. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31880572\n",
      "Train loss (w/o reg) on all data: 0.31828615\n",
      "Test loss (w/o reg) on all data: 0.43787155\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00053817703\n",
      "Norm of the params: 3.2235556\n",
      "                Loss: fixed  30 labels. Loss 0.43787. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [360] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53626\n",
      "Train loss (w/o reg) on all data: 0.53612435\n",
      "Test loss (w/o reg) on all data: 0.43060762\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007694331\n",
      "Norm of the params: 1.64724\n",
      "              Random: fixed   7 labels. Loss 0.43061. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [390] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52826023\n",
      "Train loss (w/o reg) on all data: 0.5281165\n",
      "Test loss (w/o reg) on all data: 0.46743652\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0020370248\n",
      "Norm of the params: 1.6954248\n",
      "Flipped loss: 0.46744. Accuracy: 0.756\n",
      "### Flips: 40, rs: 31, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [94] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4991107\n",
      "Train loss (w/o reg) on all data: 0.49896052\n",
      "Test loss (w/o reg) on all data: 0.44947112\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0021783682\n",
      "Norm of the params: 1.733048\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.44947. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [165] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43996608\n",
      "Train loss (w/o reg) on all data: 0.43969968\n",
      "Test loss (w/o reg) on all data: 0.43335193\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005105738\n",
      "Norm of the params: 2.3083172\n",
      "                Loss: fixed   8 labels. Loss 0.43335. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52413327\n",
      "Train loss (w/o reg) on all data: 0.52398384\n",
      "Test loss (w/o reg) on all data: 0.4559703\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010817839\n",
      "Norm of the params: 1.7287295\n",
      "              Random: fixed   1 labels. Loss 0.45597. Accuracy 0.778.\n",
      "### Flips: 40, rs: 31, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [163] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44427246\n",
      "Train loss (w/o reg) on all data: 0.44407046\n",
      "Test loss (w/o reg) on all data: 0.45898786\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.002585164\n",
      "Norm of the params: 2.0100167\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.45899. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41718018\n",
      "Train loss (w/o reg) on all data: 0.4168844\n",
      "Test loss (w/o reg) on all data: 0.44457227\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0042056767\n",
      "Norm of the params: 2.4322793\n",
      "                Loss: fixed  11 labels. Loss 0.44457. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [351] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5205925\n",
      "Train loss (w/o reg) on all data: 0.5204498\n",
      "Test loss (w/o reg) on all data: 0.4438352\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003791868\n",
      "Norm of the params: 1.6891673\n",
      "              Random: fixed   2 labels. Loss 0.44384. Accuracy 0.800.\n",
      "### Flips: 40, rs: 31, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [149] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39616674\n",
      "Train loss (w/o reg) on all data: 0.39585093\n",
      "Test loss (w/o reg) on all data: 0.45755115\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015595946\n",
      "Norm of the params: 2.513225\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.45755. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38258138\n",
      "Train loss (w/o reg) on all data: 0.38222885\n",
      "Test loss (w/o reg) on all data: 0.4301213\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007972897\n",
      "Norm of the params: 2.6552756\n",
      "                Loss: fixed  16 labels. Loss 0.43012. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51718867\n",
      "Train loss (w/o reg) on all data: 0.51705265\n",
      "Test loss (w/o reg) on all data: 0.4324718\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0053103324\n",
      "Norm of the params: 1.649373\n",
      "              Random: fixed   5 labels. Loss 0.43247. Accuracy 0.822.\n",
      "### Flips: 40, rs: 31, checks: 40\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [272] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38278872\n",
      "Train loss (w/o reg) on all data: 0.3824312\n",
      "Test loss (w/o reg) on all data: 0.4696042\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0020888888\n",
      "Norm of the params: 2.6739745\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.46960. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3605642\n",
      "Train loss (w/o reg) on all data: 0.36020416\n",
      "Test loss (w/o reg) on all data: 0.41699272\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007095062\n",
      "Norm of the params: 2.6834457\n",
      "                Loss: fixed  19 labels. Loss 0.41699. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [264] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5112294\n",
      "Train loss (w/o reg) on all data: 0.5110914\n",
      "Test loss (w/o reg) on all data: 0.42752525\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0032576788\n",
      "Norm of the params: 1.6613283\n",
      "              Random: fixed   6 labels. Loss 0.42753. Accuracy 0.822.\n",
      "### Flips: 40, rs: 31, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37098986\n",
      "Train loss (w/o reg) on all data: 0.3706966\n",
      "Test loss (w/o reg) on all data: 0.45887175\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0014112773\n",
      "Norm of the params: 2.4218314\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.45887. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33708864\n",
      "Train loss (w/o reg) on all data: 0.336645\n",
      "Test loss (w/o reg) on all data: 0.43141174\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00046979936\n",
      "Norm of the params: 2.978676\n",
      "                Loss: fixed  23 labels. Loss 0.43141. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [389] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50979125\n",
      "Train loss (w/o reg) on all data: 0.50965846\n",
      "Test loss (w/o reg) on all data: 0.4276849\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006349675\n",
      "Norm of the params: 1.6297171\n",
      "              Random: fixed   7 labels. Loss 0.42768. Accuracy 0.800.\n",
      "### Flips: 40, rs: 31, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [305] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3485677\n",
      "Train loss (w/o reg) on all data: 0.34823796\n",
      "Test loss (w/o reg) on all data: 0.43722266\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010971172\n",
      "Norm of the params: 2.568044\n",
      "     Influence (LOO): fixed  28 labels. Loss 0.43722. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33183235\n",
      "Train loss (w/o reg) on all data: 0.33137152\n",
      "Test loss (w/o reg) on all data: 0.44109967\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00095601333\n",
      "Norm of the params: 3.0358531\n",
      "                Loss: fixed  25 labels. Loss 0.44110. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [327] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5097912\n",
      "Train loss (w/o reg) on all data: 0.509658\n",
      "Test loss (w/o reg) on all data: 0.4276166\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011945696\n",
      "Norm of the params: 1.632157\n",
      "              Random: fixed   7 labels. Loss 0.42762. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55258566\n",
      "Train loss (w/o reg) on all data: 0.5524124\n",
      "Test loss (w/o reg) on all data: 0.4896804\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0036951988\n",
      "Norm of the params: 1.8614099\n",
      "Flipped loss: 0.48968. Accuracy: 0.778\n",
      "### Flips: 40, rs: 32, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51334685\n",
      "Train loss (w/o reg) on all data: 0.5131237\n",
      "Test loss (w/o reg) on all data: 0.44787952\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.007950206\n",
      "Norm of the params: 2.1125395\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.44788. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [199] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4791564\n",
      "Train loss (w/o reg) on all data: 0.4788173\n",
      "Test loss (w/o reg) on all data: 0.48236915\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00295012\n",
      "Norm of the params: 2.6041555\n",
      "                Loss: fixed   7 labels. Loss 0.48237. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [137] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54363847\n",
      "Train loss (w/o reg) on all data: 0.54345626\n",
      "Test loss (w/o reg) on all data: 0.47853678\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022417165\n",
      "Norm of the params: 1.9089241\n",
      "              Random: fixed   3 labels. Loss 0.47854. Accuracy 0.800.\n",
      "### Flips: 40, rs: 32, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [89] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5057189\n",
      "Train loss (w/o reg) on all data: 0.5055113\n",
      "Test loss (w/o reg) on all data: 0.42673984\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018880408\n",
      "Norm of the params: 2.037667\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.42674. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [264] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44107866\n",
      "Train loss (w/o reg) on all data: 0.44066423\n",
      "Test loss (w/o reg) on all data: 0.4860444\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012985745\n",
      "Norm of the params: 2.8789885\n",
      "                Loss: fixed  12 labels. Loss 0.48604. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [295] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5410074\n",
      "Train loss (w/o reg) on all data: 0.54082596\n",
      "Test loss (w/o reg) on all data: 0.4763289\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007687321\n",
      "Norm of the params: 1.9050548\n",
      "              Random: fixed   5 labels. Loss 0.47633. Accuracy 0.800.\n",
      "### Flips: 40, rs: 32, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4810806\n",
      "Train loss (w/o reg) on all data: 0.48086613\n",
      "Test loss (w/o reg) on all data: 0.4182254\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0053813695\n",
      "Norm of the params: 2.0710342\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.41823. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41813686\n",
      "Train loss (w/o reg) on all data: 0.41769862\n",
      "Test loss (w/o reg) on all data: 0.47051594\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0038722085\n",
      "Norm of the params: 2.960577\n",
      "                Loss: fixed  15 labels. Loss 0.47052. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5404129\n",
      "Train loss (w/o reg) on all data: 0.54025143\n",
      "Test loss (w/o reg) on all data: 0.45910865\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004926558\n",
      "Norm of the params: 1.7971821\n",
      "              Random: fixed   6 labels. Loss 0.45911. Accuracy 0.800.\n",
      "### Flips: 40, rs: 32, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4568016\n",
      "Train loss (w/o reg) on all data: 0.456521\n",
      "Test loss (w/o reg) on all data: 0.42578816\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0039560487\n",
      "Norm of the params: 2.368943\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.42579. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3903288\n",
      "Train loss (w/o reg) on all data: 0.38986227\n",
      "Test loss (w/o reg) on all data: 0.46805385\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012274688\n",
      "Norm of the params: 3.0545886\n",
      "                Loss: fixed  20 labels. Loss 0.46805. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [356] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53047776\n",
      "Train loss (w/o reg) on all data: 0.5303252\n",
      "Test loss (w/o reg) on all data: 0.4313913\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018737639\n",
      "Norm of the params: 1.7469877\n",
      "              Random: fixed   9 labels. Loss 0.43139. Accuracy 0.800.\n",
      "### Flips: 40, rs: 32, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43602088\n",
      "Train loss (w/o reg) on all data: 0.43572667\n",
      "Test loss (w/o reg) on all data: 0.4337115\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012119623\n",
      "Norm of the params: 2.42573\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.43371. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [269] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3903282\n",
      "Train loss (w/o reg) on all data: 0.38986307\n",
      "Test loss (w/o reg) on all data: 0.46821648\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020372886\n",
      "Norm of the params: 3.0499756\n",
      "                Loss: fixed  20 labels. Loss 0.46822. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [279] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52256626\n",
      "Train loss (w/o reg) on all data: 0.5223927\n",
      "Test loss (w/o reg) on all data: 0.4296319\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013769501\n",
      "Norm of the params: 1.8630344\n",
      "              Random: fixed  10 labels. Loss 0.42963. Accuracy 0.822.\n",
      "### Flips: 40, rs: 32, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40798748\n",
      "Train loss (w/o reg) on all data: 0.4076759\n",
      "Test loss (w/o reg) on all data: 0.42827606\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00026846927\n",
      "Norm of the params: 2.496346\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.42828. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [365] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35500574\n",
      "Train loss (w/o reg) on all data: 0.35448608\n",
      "Test loss (w/o reg) on all data: 0.4498568\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006237765\n",
      "Norm of the params: 3.2238135\n",
      "                Loss: fixed  25 labels. Loss 0.44986. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52257377\n",
      "Train loss (w/o reg) on all data: 0.52239996\n",
      "Test loss (w/o reg) on all data: 0.42959282\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014818321\n",
      "Norm of the params: 1.8645124\n",
      "              Random: fixed  10 labels. Loss 0.42959. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [306] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5992291\n",
      "Train loss (w/o reg) on all data: 0.5991953\n",
      "Test loss (w/o reg) on all data: 0.43820754\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00091602455\n",
      "Norm of the params: 0.82239133\n",
      "Flipped loss: 0.43821. Accuracy: 0.844\n",
      "### Flips: 40, rs: 33, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5667626\n",
      "Train loss (w/o reg) on all data: 0.5666981\n",
      "Test loss (w/o reg) on all data: 0.4199039\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002163949\n",
      "Norm of the params: 1.1364355\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.41990. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [75] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5271164\n",
      "Train loss (w/o reg) on all data: 0.5270507\n",
      "Test loss (w/o reg) on all data: 0.41318944\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011401781\n",
      "Norm of the params: 1.1466389\n",
      "                Loss: fixed   8 labels. Loss 0.41319. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [132] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5922091\n",
      "Train loss (w/o reg) on all data: 0.59217453\n",
      "Test loss (w/o reg) on all data: 0.44000015\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0036462734\n",
      "Norm of the params: 0.8314158\n",
      "              Random: fixed   1 labels. Loss 0.44000. Accuracy 0.822.\n",
      "### Flips: 40, rs: 33, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53069234\n",
      "Train loss (w/o reg) on all data: 0.53060776\n",
      "Test loss (w/o reg) on all data: 0.41605514\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.012935458\n",
      "Norm of the params: 1.3007458\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.41606. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46050432\n",
      "Train loss (w/o reg) on all data: 0.46037573\n",
      "Test loss (w/o reg) on all data: 0.40979508\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0019676608\n",
      "Norm of the params: 1.6036643\n",
      "                Loss: fixed  16 labels. Loss 0.40980. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [151] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59221035\n",
      "Train loss (w/o reg) on all data: 0.5921759\n",
      "Test loss (w/o reg) on all data: 0.44026995\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00238909\n",
      "Norm of the params: 0.82988924\n",
      "              Random: fixed   1 labels. Loss 0.44027. Accuracy 0.822.\n",
      "### Flips: 40, rs: 33, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [151] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5036052\n",
      "Train loss (w/o reg) on all data: 0.50348914\n",
      "Test loss (w/o reg) on all data: 0.4076198\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0019229214\n",
      "Norm of the params: 1.5235164\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.40762. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [222] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4034457\n",
      "Train loss (w/o reg) on all data: 0.40317896\n",
      "Test loss (w/o reg) on all data: 0.43349865\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0014654652\n",
      "Norm of the params: 2.309697\n",
      "                Loss: fixed  22 labels. Loss 0.43350. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [284] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5770034\n",
      "Train loss (w/o reg) on all data: 0.5769524\n",
      "Test loss (w/o reg) on all data: 0.42104608\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001581078\n",
      "Norm of the params: 1.0099967\n",
      "              Random: fixed   4 labels. Loss 0.42105. Accuracy 0.822.\n",
      "### Flips: 40, rs: 33, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [147] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45853174\n",
      "Train loss (w/o reg) on all data: 0.45837778\n",
      "Test loss (w/o reg) on all data: 0.40177074\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0045188633\n",
      "Norm of the params: 1.7548221\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.40177. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3513402\n",
      "Train loss (w/o reg) on all data: 0.3508914\n",
      "Test loss (w/o reg) on all data: 0.4622859\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0008041401\n",
      "Norm of the params: 2.9960093\n",
      "                Loss: fixed  29 labels. Loss 0.46229. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [356] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5645663\n",
      "Train loss (w/o reg) on all data: 0.56450194\n",
      "Test loss (w/o reg) on all data: 0.421095\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0042733466\n",
      "Norm of the params: 1.134803\n",
      "              Random: fixed   6 labels. Loss 0.42110. Accuracy 0.822.\n",
      "### Flips: 40, rs: 33, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [331] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45058307\n",
      "Train loss (w/o reg) on all data: 0.45042485\n",
      "Test loss (w/o reg) on all data: 0.39372742\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011988888\n",
      "Norm of the params: 1.7788082\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.39373. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [346] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33420146\n",
      "Train loss (w/o reg) on all data: 0.33380473\n",
      "Test loss (w/o reg) on all data: 0.45727983\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00077838235\n",
      "Norm of the params: 2.8168023\n",
      "                Loss: fixed  34 labels. Loss 0.45728. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55970967\n",
      "Train loss (w/o reg) on all data: 0.5596313\n",
      "Test loss (w/o reg) on all data: 0.42175546\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0058900155\n",
      "Norm of the params: 1.2521818\n",
      "              Random: fixed   7 labels. Loss 0.42176. Accuracy 0.822.\n",
      "### Flips: 40, rs: 33, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44823727\n",
      "Train loss (w/o reg) on all data: 0.4480704\n",
      "Test loss (w/o reg) on all data: 0.3951466\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004530733\n",
      "Norm of the params: 1.8268945\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.39515. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [322] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33420694\n",
      "Train loss (w/o reg) on all data: 0.33380693\n",
      "Test loss (w/o reg) on all data: 0.45722798\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0021909673\n",
      "Norm of the params: 2.8284426\n",
      "                Loss: fixed  34 labels. Loss 0.45723. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [185] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5617229\n",
      "Train loss (w/o reg) on all data: 0.56164426\n",
      "Test loss (w/o reg) on all data: 0.42781284\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001472136\n",
      "Norm of the params: 1.2537969\n",
      "              Random: fixed   8 labels. Loss 0.42781. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55785376\n",
      "Train loss (w/o reg) on all data: 0.55777234\n",
      "Test loss (w/o reg) on all data: 0.46702635\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0031431392\n",
      "Norm of the params: 1.2761946\n",
      "Flipped loss: 0.46703. Accuracy: 0.778\n",
      "### Flips: 40, rs: 34, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5428356\n",
      "Train loss (w/o reg) on all data: 0.5427471\n",
      "Test loss (w/o reg) on all data: 0.46252725\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0009110962\n",
      "Norm of the params: 1.3306959\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.46253. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4602375\n",
      "Train loss (w/o reg) on all data: 0.46005732\n",
      "Test loss (w/o reg) on all data: 0.46372592\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0051233512\n",
      "Norm of the params: 1.898322\n",
      "                Loss: fixed   9 labels. Loss 0.46373. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5593511\n",
      "Train loss (w/o reg) on all data: 0.5592725\n",
      "Test loss (w/o reg) on all data: 0.44654596\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.007487933\n",
      "Norm of the params: 1.2536427\n",
      "              Random: fixed   2 labels. Loss 0.44655. Accuracy 0.822.\n",
      "### Flips: 40, rs: 34, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [195] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4962113\n",
      "Train loss (w/o reg) on all data: 0.49607646\n",
      "Test loss (w/o reg) on all data: 0.4265663\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010269142\n",
      "Norm of the params: 1.6420615\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.42657. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41231704\n",
      "Train loss (w/o reg) on all data: 0.41208133\n",
      "Test loss (w/o reg) on all data: 0.4212853\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014485011\n",
      "Norm of the params: 2.1712167\n",
      "                Loss: fixed  14 labels. Loss 0.42129. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [165] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5517458\n",
      "Train loss (w/o reg) on all data: 0.551664\n",
      "Test loss (w/o reg) on all data: 0.4469643\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0037424988\n",
      "Norm of the params: 1.2790376\n",
      "              Random: fixed   3 labels. Loss 0.44696. Accuracy 0.822.\n",
      "### Flips: 40, rs: 34, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [166] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45028636\n",
      "Train loss (w/o reg) on all data: 0.45008448\n",
      "Test loss (w/o reg) on all data: 0.42281067\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00066422357\n",
      "Norm of the params: 2.009433\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.42281. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37289882\n",
      "Train loss (w/o reg) on all data: 0.37257022\n",
      "Test loss (w/o reg) on all data: 0.45840302\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.004144753\n",
      "Norm of the params: 2.563634\n",
      "                Loss: fixed  19 labels. Loss 0.45840. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [307] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5417918\n",
      "Train loss (w/o reg) on all data: 0.5417027\n",
      "Test loss (w/o reg) on all data: 0.43535656\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0020853153\n",
      "Norm of the params: 1.33506\n",
      "              Random: fixed   4 labels. Loss 0.43536. Accuracy 0.822.\n",
      "### Flips: 40, rs: 34, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4519596\n",
      "Train loss (w/o reg) on all data: 0.45177412\n",
      "Test loss (w/o reg) on all data: 0.401646\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002043163\n",
      "Norm of the params: 1.9260904\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.40165. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34776562\n",
      "Train loss (w/o reg) on all data: 0.3473641\n",
      "Test loss (w/o reg) on all data: 0.4647116\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0006512808\n",
      "Norm of the params: 2.8338335\n",
      "                Loss: fixed  23 labels. Loss 0.46471. Accuracy 0.756.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [269] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53176564\n",
      "Train loss (w/o reg) on all data: 0.531668\n",
      "Test loss (w/o reg) on all data: 0.43739682\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0070177205\n",
      "Norm of the params: 1.3974555\n",
      "              Random: fixed   5 labels. Loss 0.43740. Accuracy 0.822.\n",
      "### Flips: 40, rs: 34, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39986107\n",
      "Train loss (w/o reg) on all data: 0.3996341\n",
      "Test loss (w/o reg) on all data: 0.40006977\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0009415804\n",
      "Norm of the params: 2.130542\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.40007. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [96] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33981457\n",
      "Train loss (w/o reg) on all data: 0.3393815\n",
      "Test loss (w/o reg) on all data: 0.48015788\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0010820655\n",
      "Norm of the params: 2.943115\n",
      "                Loss: fixed  26 labels. Loss 0.48016. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5268703\n",
      "Train loss (w/o reg) on all data: 0.52678823\n",
      "Test loss (w/o reg) on all data: 0.44106555\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018019859\n",
      "Norm of the params: 1.2809973\n",
      "              Random: fixed   8 labels. Loss 0.44107. Accuracy 0.800.\n",
      "### Flips: 40, rs: 34, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [351] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3881796\n",
      "Train loss (w/o reg) on all data: 0.38793406\n",
      "Test loss (w/o reg) on all data: 0.39925414\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0014394228\n",
      "Norm of the params: 2.2160904\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.39925. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [240] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3293186\n",
      "Train loss (w/o reg) on all data: 0.32878524\n",
      "Test loss (w/o reg) on all data: 0.47028834\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0020121054\n",
      "Norm of the params: 3.266125\n",
      "                Loss: fixed  29 labels. Loss 0.47029. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5198722\n",
      "Train loss (w/o reg) on all data: 0.51978844\n",
      "Test loss (w/o reg) on all data: 0.43457863\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0050902516\n",
      "Norm of the params: 1.294367\n",
      "              Random: fixed   9 labels. Loss 0.43458. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5263494\n",
      "Train loss (w/o reg) on all data: 0.52627677\n",
      "Test loss (w/o reg) on all data: 0.445993\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004939492\n",
      "Norm of the params: 1.2056516\n",
      "Flipped loss: 0.44599. Accuracy: 0.800\n",
      "### Flips: 40, rs: 35, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [177] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46560395\n",
      "Train loss (w/o reg) on all data: 0.46547735\n",
      "Test loss (w/o reg) on all data: 0.40998298\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0037606708\n",
      "Norm of the params: 1.5912541\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.40998. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43865287\n",
      "Train loss (w/o reg) on all data: 0.43848604\n",
      "Test loss (w/o reg) on all data: 0.40038267\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0026502847\n",
      "Norm of the params: 1.8266965\n",
      "                Loss: fixed   7 labels. Loss 0.40038. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [133] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52669287\n",
      "Train loss (w/o reg) on all data: 0.5266099\n",
      "Test loss (w/o reg) on all data: 0.4413695\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00073367654\n",
      "Norm of the params: 1.2882855\n",
      "              Random: fixed   3 labels. Loss 0.44137. Accuracy 0.800.\n",
      "### Flips: 40, rs: 35, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [161] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4598338\n",
      "Train loss (w/o reg) on all data: 0.45969483\n",
      "Test loss (w/o reg) on all data: 0.40799904\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017156454\n",
      "Norm of the params: 1.6671095\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.40800. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [195] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41054577\n",
      "Train loss (w/o reg) on all data: 0.4103288\n",
      "Test loss (w/o reg) on all data: 0.41953522\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006157143\n",
      "Norm of the params: 2.083139\n",
      "                Loss: fixed  11 labels. Loss 0.41954. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [301] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53296375\n",
      "Train loss (w/o reg) on all data: 0.5328973\n",
      "Test loss (w/o reg) on all data: 0.44442174\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001934878\n",
      "Norm of the params: 1.1530277\n",
      "              Random: fixed   5 labels. Loss 0.44442. Accuracy 0.800.\n",
      "### Flips: 40, rs: 35, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43180245\n",
      "Train loss (w/o reg) on all data: 0.4316274\n",
      "Test loss (w/o reg) on all data: 0.39551905\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0021474964\n",
      "Norm of the params: 1.8711078\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.39552. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [299] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37577677\n",
      "Train loss (w/o reg) on all data: 0.37553757\n",
      "Test loss (w/o reg) on all data: 0.42245978\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007108509\n",
      "Norm of the params: 2.1872532\n",
      "                Loss: fixed  17 labels. Loss 0.42246. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [312] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5351024\n",
      "Train loss (w/o reg) on all data: 0.53503734\n",
      "Test loss (w/o reg) on all data: 0.44689006\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00043173358\n",
      "Norm of the params: 1.1410397\n",
      "              Random: fixed   6 labels. Loss 0.44689. Accuracy 0.800.\n",
      "### Flips: 40, rs: 35, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [123] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3916801\n",
      "Train loss (w/o reg) on all data: 0.3914323\n",
      "Test loss (w/o reg) on all data: 0.37117618\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0049099396\n",
      "Norm of the params: 2.2262661\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.37118. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [195] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34560275\n",
      "Train loss (w/o reg) on all data: 0.34532186\n",
      "Test loss (w/o reg) on all data: 0.40405685\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.008626328\n",
      "Norm of the params: 2.3701265\n",
      "                Loss: fixed  21 labels. Loss 0.40406. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51462626\n",
      "Train loss (w/o reg) on all data: 0.5145474\n",
      "Test loss (w/o reg) on all data: 0.42463395\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016248305\n",
      "Norm of the params: 1.2560381\n",
      "              Random: fixed  11 labels. Loss 0.42463. Accuracy 0.800.\n",
      "### Flips: 40, rs: 35, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [187] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3856732\n",
      "Train loss (w/o reg) on all data: 0.38543683\n",
      "Test loss (w/o reg) on all data: 0.39110658\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00093176466\n",
      "Norm of the params: 2.1742642\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.39111. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [141] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33941826\n",
      "Train loss (w/o reg) on all data: 0.33914962\n",
      "Test loss (w/o reg) on all data: 0.4102821\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00048019775\n",
      "Norm of the params: 2.317933\n",
      "                Loss: fixed  23 labels. Loss 0.41028. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [397] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5185137\n",
      "Train loss (w/o reg) on all data: 0.51844156\n",
      "Test loss (w/o reg) on all data: 0.41973227\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00043286267\n",
      "Norm of the params: 1.2010384\n",
      "              Random: fixed  14 labels. Loss 0.41973. Accuracy 0.822.\n",
      "### Flips: 40, rs: 35, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36234292\n",
      "Train loss (w/o reg) on all data: 0.3620561\n",
      "Test loss (w/o reg) on all data: 0.39561334\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0025688463\n",
      "Norm of the params: 2.395087\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.39561. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3265028\n",
      "Train loss (w/o reg) on all data: 0.32617712\n",
      "Test loss (w/o reg) on all data: 0.42144594\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00079897465\n",
      "Norm of the params: 2.5522304\n",
      "                Loss: fixed  28 labels. Loss 0.42145. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [319] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50929356\n",
      "Train loss (w/o reg) on all data: 0.50921357\n",
      "Test loss (w/o reg) on all data: 0.41971475\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0049228063\n",
      "Norm of the params: 1.2649925\n",
      "              Random: fixed  17 labels. Loss 0.41971. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [137] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52998513\n",
      "Train loss (w/o reg) on all data: 0.5298989\n",
      "Test loss (w/o reg) on all data: 0.46676186\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.010676795\n",
      "Norm of the params: 1.3133422\n",
      "Flipped loss: 0.46676. Accuracy: 0.822\n",
      "### Flips: 40, rs: 36, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [155] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4699525\n",
      "Train loss (w/o reg) on all data: 0.46981257\n",
      "Test loss (w/o reg) on all data: 0.43354836\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007242411\n",
      "Norm of the params: 1.6728001\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.43355. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43479443\n",
      "Train loss (w/o reg) on all data: 0.43462002\n",
      "Test loss (w/o reg) on all data: 0.43973652\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00090515695\n",
      "Norm of the params: 1.8676834\n",
      "                Loss: fixed   8 labels. Loss 0.43974. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.531896\n",
      "Train loss (w/o reg) on all data: 0.5318119\n",
      "Test loss (w/o reg) on all data: 0.44220182\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014816948\n",
      "Norm of the params: 1.2970991\n",
      "              Random: fixed   2 labels. Loss 0.44220. Accuracy 0.822.\n",
      "### Flips: 40, rs: 36, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40770307\n",
      "Train loss (w/o reg) on all data: 0.40747944\n",
      "Test loss (w/o reg) on all data: 0.4039725\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00439476\n",
      "Norm of the params: 2.1148243\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.40397. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [149] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37052283\n",
      "Train loss (w/o reg) on all data: 0.37025192\n",
      "Test loss (w/o reg) on all data: 0.41519162\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010529428\n",
      "Norm of the params: 2.327705\n",
      "                Loss: fixed  15 labels. Loss 0.41519. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [316] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51252544\n",
      "Train loss (w/o reg) on all data: 0.51241875\n",
      "Test loss (w/o reg) on all data: 0.42793813\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0031873249\n",
      "Norm of the params: 1.4607562\n",
      "              Random: fixed   5 labels. Loss 0.42794. Accuracy 0.844.\n",
      "### Flips: 40, rs: 36, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [368] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38402545\n",
      "Train loss (w/o reg) on all data: 0.38376805\n",
      "Test loss (w/o reg) on all data: 0.39805162\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005850441\n",
      "Norm of the params: 2.2688963\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.39805. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3176392\n",
      "Train loss (w/o reg) on all data: 0.3172982\n",
      "Test loss (w/o reg) on all data: 0.45400208\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00096218527\n",
      "Norm of the params: 2.6114774\n",
      "                Loss: fixed  22 labels. Loss 0.45400. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [341] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52234066\n",
      "Train loss (w/o reg) on all data: 0.522247\n",
      "Test loss (w/o reg) on all data: 0.41871342\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0019613954\n",
      "Norm of the params: 1.3684138\n",
      "              Random: fixed   7 labels. Loss 0.41871. Accuracy 0.844.\n",
      "### Flips: 40, rs: 36, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [325] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38059032\n",
      "Train loss (w/o reg) on all data: 0.38030967\n",
      "Test loss (w/o reg) on all data: 0.41136292\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012073086\n",
      "Norm of the params: 2.3691182\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.41136. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3009035\n",
      "Train loss (w/o reg) on all data: 0.30050072\n",
      "Test loss (w/o reg) on all data: 0.44504124\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00065281213\n",
      "Norm of the params: 2.8382328\n",
      "                Loss: fixed  25 labels. Loss 0.44504. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52593356\n",
      "Train loss (w/o reg) on all data: 0.52583575\n",
      "Test loss (w/o reg) on all data: 0.42135206\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001534771\n",
      "Norm of the params: 1.3987006\n",
      "              Random: fixed   8 labels. Loss 0.42135. Accuracy 0.844.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Flips: 40, rs: 36, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [329] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36915365\n",
      "Train loss (w/o reg) on all data: 0.3688819\n",
      "Test loss (w/o reg) on all data: 0.4340034\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012696945\n",
      "Norm of the params: 2.3312008\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.43400. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.29409862\n",
      "Train loss (w/o reg) on all data: 0.29365456\n",
      "Test loss (w/o reg) on all data: 0.46378815\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015150026\n",
      "Norm of the params: 2.980109\n",
      "                Loss: fixed  26 labels. Loss 0.46379. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50534904\n",
      "Train loss (w/o reg) on all data: 0.5052365\n",
      "Test loss (w/o reg) on all data: 0.41980314\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002982024\n",
      "Norm of the params: 1.5003045\n",
      "              Random: fixed  11 labels. Loss 0.41980. Accuracy 0.844.\n",
      "### Flips: 40, rs: 36, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36915255\n",
      "Train loss (w/o reg) on all data: 0.36888087\n",
      "Test loss (w/o reg) on all data: 0.43476334\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0004131501\n",
      "Norm of the params: 2.3310356\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.43476. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [282] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.28436553\n",
      "Train loss (w/o reg) on all data: 0.28385994\n",
      "Test loss (w/o reg) on all data: 0.46830449\n",
      "Train acc on all data:  0.8867924528301887\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00217244\n",
      "Norm of the params: 3.1798842\n",
      "                Loss: fixed  28 labels. Loss 0.46830. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [292] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.501195\n",
      "Train loss (w/o reg) on all data: 0.5010748\n",
      "Test loss (w/o reg) on all data: 0.42393032\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0024991287\n",
      "Norm of the params: 1.5506798\n",
      "              Random: fixed  12 labels. Loss 0.42393. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [75] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5298003\n",
      "Train loss (w/o reg) on all data: 0.5297041\n",
      "Test loss (w/o reg) on all data: 0.4070569\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0026913253\n",
      "Norm of the params: 1.386988\n",
      "Flipped loss: 0.40706. Accuracy: 0.867\n",
      "### Flips: 40, rs: 37, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46550652\n",
      "Train loss (w/o reg) on all data: 0.46532664\n",
      "Test loss (w/o reg) on all data: 0.4022996\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00033497775\n",
      "Norm of the params: 1.8968252\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.40230. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [149] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44686094\n",
      "Train loss (w/o reg) on all data: 0.44666782\n",
      "Test loss (w/o reg) on all data: 0.4152047\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00079460826\n",
      "Norm of the params: 1.9652466\n",
      "                Loss: fixed   7 labels. Loss 0.41520. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [149] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50088537\n",
      "Train loss (w/o reg) on all data: 0.500753\n",
      "Test loss (w/o reg) on all data: 0.40387085\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011906626\n",
      "Norm of the params: 1.6273172\n",
      "              Random: fixed   2 labels. Loss 0.40387. Accuracy 0.844.\n",
      "### Flips: 40, rs: 37, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [282] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45291862\n",
      "Train loss (w/o reg) on all data: 0.45273885\n",
      "Test loss (w/o reg) on all data: 0.40747103\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0029852204\n",
      "Norm of the params: 1.896181\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.40747. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3755064\n",
      "Train loss (w/o reg) on all data: 0.37521818\n",
      "Test loss (w/o reg) on all data: 0.41587064\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00054050254\n",
      "Norm of the params: 2.4008567\n",
      "                Loss: fixed  14 labels. Loss 0.41587. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [258] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50093377\n",
      "Train loss (w/o reg) on all data: 0.50080556\n",
      "Test loss (w/o reg) on all data: 0.40040478\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0028307922\n",
      "Norm of the params: 1.601243\n",
      "              Random: fixed   3 labels. Loss 0.40040. Accuracy 0.844.\n",
      "### Flips: 40, rs: 37, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [157] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42458352\n",
      "Train loss (w/o reg) on all data: 0.4243545\n",
      "Test loss (w/o reg) on all data: 0.39320332\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00319762\n",
      "Norm of the params: 2.1402683\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.39320. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [377] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.30442163\n",
      "Train loss (w/o reg) on all data: 0.30396843\n",
      "Test loss (w/o reg) on all data: 0.43942872\n",
      "Train acc on all data:  0.8915094339622641\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0028615887\n",
      "Norm of the params: 3.0106325\n",
      "                Loss: fixed  22 labels. Loss 0.43943. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [333] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5092928\n",
      "Train loss (w/o reg) on all data: 0.5091652\n",
      "Test loss (w/o reg) on all data: 0.39470756\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0015145831\n",
      "Norm of the params: 1.5971267\n",
      "              Random: fixed   5 labels. Loss 0.39471. Accuracy 0.844.\n",
      "### Flips: 40, rs: 37, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41016147\n",
      "Train loss (w/o reg) on all data: 0.40992498\n",
      "Test loss (w/o reg) on all data: 0.38775453\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011071686\n",
      "Norm of the params: 2.1747348\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.38775. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [318] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.2788757\n",
      "Train loss (w/o reg) on all data: 0.27843955\n",
      "Test loss (w/o reg) on all data: 0.48076427\n",
      "Train acc on all data:  0.9056603773584906\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001751764\n",
      "Norm of the params: 2.9534812\n",
      "                Loss: fixed  26 labels. Loss 0.48076. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50713587\n",
      "Train loss (w/o reg) on all data: 0.50699294\n",
      "Test loss (w/o reg) on all data: 0.4089465\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0008191095\n",
      "Norm of the params: 1.6907523\n",
      "              Random: fixed   8 labels. Loss 0.40895. Accuracy 0.844.\n",
      "### Flips: 40, rs: 37, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [301] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3917779\n",
      "Train loss (w/o reg) on all data: 0.39151937\n",
      "Test loss (w/o reg) on all data: 0.40427536\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0017287228\n",
      "Norm of the params: 2.2739012\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.40428. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [264] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.2788765\n",
      "Train loss (w/o reg) on all data: 0.27844226\n",
      "Test loss (w/o reg) on all data: 0.4802396\n",
      "Train acc on all data:  0.9056603773584906\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00038190023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of the params: 2.9470196\n",
      "                Loss: fixed  26 labels. Loss 0.48024. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [211] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5088784\n",
      "Train loss (w/o reg) on all data: 0.5087374\n",
      "Test loss (w/o reg) on all data: 0.4124042\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0013345842\n",
      "Norm of the params: 1.6793627\n",
      "              Random: fixed   9 labels. Loss 0.41240. Accuracy 0.844.\n",
      "### Flips: 40, rs: 37, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [338] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37643984\n",
      "Train loss (w/o reg) on all data: 0.37614483\n",
      "Test loss (w/o reg) on all data: 0.42785412\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0014894184\n",
      "Norm of the params: 2.4291065\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.42785. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [328] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.27447772\n",
      "Train loss (w/o reg) on all data: 0.27398014\n",
      "Test loss (w/o reg) on all data: 0.49080566\n",
      "Train acc on all data:  0.9056603773584906\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0016358624\n",
      "Norm of the params: 3.1546032\n",
      "                Loss: fixed  27 labels. Loss 0.49081. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48987216\n",
      "Train loss (w/o reg) on all data: 0.48969275\n",
      "Test loss (w/o reg) on all data: 0.43242744\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0026263157\n",
      "Norm of the params: 1.8942531\n",
      "              Random: fixed  12 labels. Loss 0.43243. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [286] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5471876\n",
      "Train loss (w/o reg) on all data: 0.54709387\n",
      "Test loss (w/o reg) on all data: 0.41467896\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001380942\n",
      "Norm of the params: 1.3692106\n",
      "Flipped loss: 0.41468. Accuracy: 0.844\n",
      "### Flips: 40, rs: 38, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5130211\n",
      "Train loss (w/o reg) on all data: 0.5128982\n",
      "Test loss (w/o reg) on all data: 0.38638696\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.001055826\n",
      "Norm of the params: 1.5679593\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.38639. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48723197\n",
      "Train loss (w/o reg) on all data: 0.4870756\n",
      "Test loss (w/o reg) on all data: 0.3798297\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0003123387\n",
      "Norm of the params: 1.7683896\n",
      "                Loss: fixed   6 labels. Loss 0.37983. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [92] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.523346\n",
      "Train loss (w/o reg) on all data: 0.523234\n",
      "Test loss (w/o reg) on all data: 0.41012204\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00094076357\n",
      "Norm of the params: 1.496447\n",
      "              Random: fixed   3 labels. Loss 0.41012. Accuracy 0.822.\n",
      "### Flips: 40, rs: 38, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4972397\n",
      "Train loss (w/o reg) on all data: 0.49709687\n",
      "Test loss (w/o reg) on all data: 0.37063527\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00084704225\n",
      "Norm of the params: 1.6902511\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.37064. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [142] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4183434\n",
      "Train loss (w/o reg) on all data: 0.41811183\n",
      "Test loss (w/o reg) on all data: 0.38127238\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00055995514\n",
      "Norm of the params: 2.1520834\n",
      "                Loss: fixed  14 labels. Loss 0.38127. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5246969\n",
      "Train loss (w/o reg) on all data: 0.5246102\n",
      "Test loss (w/o reg) on all data: 0.42460924\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0033179384\n",
      "Norm of the params: 1.3166118\n",
      "              Random: fixed   6 labels. Loss 0.42461. Accuracy 0.822.\n",
      "### Flips: 40, rs: 38, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [127] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46705815\n",
      "Train loss (w/o reg) on all data: 0.4668862\n",
      "Test loss (w/o reg) on all data: 0.3569472\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0004421738\n",
      "Norm of the params: 1.8545609\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.35695. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37319997\n",
      "Train loss (w/o reg) on all data: 0.37290055\n",
      "Test loss (w/o reg) on all data: 0.40384156\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019724602\n",
      "Norm of the params: 2.447158\n",
      "                Loss: fixed  20 labels. Loss 0.40384. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5130714\n",
      "Train loss (w/o reg) on all data: 0.512979\n",
      "Test loss (w/o reg) on all data: 0.42256695\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014660715\n",
      "Norm of the params: 1.3596805\n",
      "              Random: fixed   8 labels. Loss 0.42257. Accuracy 0.822.\n",
      "### Flips: 40, rs: 38, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [272] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4253069\n",
      "Train loss (w/o reg) on all data: 0.4250838\n",
      "Test loss (w/o reg) on all data: 0.38961962\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002576709\n",
      "Norm of the params: 2.112349\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.38962. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [154] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3513114\n",
      "Train loss (w/o reg) on all data: 0.35097006\n",
      "Test loss (w/o reg) on all data: 0.41359892\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005001479\n",
      "Norm of the params: 2.6127737\n",
      "                Loss: fixed  23 labels. Loss 0.41360. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51107347\n",
      "Train loss (w/o reg) on all data: 0.5109765\n",
      "Test loss (w/o reg) on all data: 0.4195951\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0050849672\n",
      "Norm of the params: 1.3925208\n",
      "              Random: fixed  10 labels. Loss 0.41960. Accuracy 0.844.\n",
      "### Flips: 40, rs: 38, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [431] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40657553\n",
      "Train loss (w/o reg) on all data: 0.4063379\n",
      "Test loss (w/o reg) on all data: 0.41789055\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014123474\n",
      "Norm of the params: 2.1801214\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.41789. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3368261\n",
      "Train loss (w/o reg) on all data: 0.33646446\n",
      "Test loss (w/o reg) on all data: 0.41024858\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0036178927\n",
      "Norm of the params: 2.6893585\n",
      "                Loss: fixed  25 labels. Loss 0.41025. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49929142\n",
      "Train loss (w/o reg) on all data: 0.4991895\n",
      "Test loss (w/o reg) on all data: 0.4117758\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011786366\n",
      "Norm of the params: 1.4278344\n",
      "              Random: fixed  12 labels. Loss 0.41178. Accuracy 0.844.\n",
      "### Flips: 40, rs: 38, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40207338\n",
      "Train loss (w/o reg) on all data: 0.40184698\n",
      "Test loss (w/o reg) on all data: 0.40200958\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.008562707\n",
      "Norm of the params: 2.1279373\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.40201. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32245693\n",
      "Train loss (w/o reg) on all data: 0.32203078\n",
      "Test loss (w/o reg) on all data: 0.4498549\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0038696514\n",
      "Norm of the params: 2.919355\n",
      "                Loss: fixed  28 labels. Loss 0.44985. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49927223\n",
      "Train loss (w/o reg) on all data: 0.49916863\n",
      "Test loss (w/o reg) on all data: 0.41246608\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004899844\n",
      "Norm of the params: 1.4394867\n",
      "              Random: fixed  12 labels. Loss 0.41247. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [190] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5423279\n",
      "Train loss (w/o reg) on all data: 0.5422571\n",
      "Test loss (w/o reg) on all data: 0.464871\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010250345\n",
      "Norm of the params: 1.189984\n",
      "Flipped loss: 0.46487. Accuracy: 0.800\n",
      "### Flips: 40, rs: 39, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [76] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48965198\n",
      "Train loss (w/o reg) on all data: 0.4895403\n",
      "Test loss (w/o reg) on all data: 0.43623945\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0027523548\n",
      "Norm of the params: 1.4943658\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.43624. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [110] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47196907\n",
      "Train loss (w/o reg) on all data: 0.4718372\n",
      "Test loss (w/o reg) on all data: 0.42526388\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0041721994\n",
      "Norm of the params: 1.624048\n",
      "                Loss: fixed   6 labels. Loss 0.42526. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [166] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5394585\n",
      "Train loss (w/o reg) on all data: 0.53939235\n",
      "Test loss (w/o reg) on all data: 0.46182802\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0051021655\n",
      "Norm of the params: 1.1504248\n",
      "              Random: fixed   1 labels. Loss 0.46183. Accuracy 0.800.\n",
      "### Flips: 40, rs: 39, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46612912\n",
      "Train loss (w/o reg) on all data: 0.46598944\n",
      "Test loss (w/o reg) on all data: 0.42079476\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0005805618\n",
      "Norm of the params: 1.6714585\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.42079. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41889098\n",
      "Train loss (w/o reg) on all data: 0.41871604\n",
      "Test loss (w/o reg) on all data: 0.43382704\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006241244\n",
      "Norm of the params: 1.8704765\n",
      "                Loss: fixed  12 labels. Loss 0.43383. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [112] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5394624\n",
      "Train loss (w/o reg) on all data: 0.53939605\n",
      "Test loss (w/o reg) on all data: 0.46254095\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0043424913\n",
      "Norm of the params: 1.1519889\n",
      "              Random: fixed   1 labels. Loss 0.46254. Accuracy 0.800.\n",
      "### Flips: 40, rs: 39, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42288086\n",
      "Train loss (w/o reg) on all data: 0.42270416\n",
      "Test loss (w/o reg) on all data: 0.41824338\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0035745013\n",
      "Norm of the params: 1.8798437\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.41824. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3598318\n",
      "Train loss (w/o reg) on all data: 0.35955054\n",
      "Test loss (w/o reg) on all data: 0.42950588\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0012455125\n",
      "Norm of the params: 2.3717566\n",
      "                Loss: fixed  19 labels. Loss 0.42951. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53986764\n",
      "Train loss (w/o reg) on all data: 0.5397918\n",
      "Test loss (w/o reg) on all data: 0.4645367\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0039782315\n",
      "Norm of the params: 1.2312376\n",
      "              Random: fixed   6 labels. Loss 0.46454. Accuracy 0.800.\n",
      "### Flips: 40, rs: 39, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41263244\n",
      "Train loss (w/o reg) on all data: 0.41244537\n",
      "Test loss (w/o reg) on all data: 0.41715503\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00029935333\n",
      "Norm of the params: 1.9342386\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.41716. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [360] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33611664\n",
      "Train loss (w/o reg) on all data: 0.33583343\n",
      "Test loss (w/o reg) on all data: 0.4492331\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0012159072\n",
      "Norm of the params: 2.3800159\n",
      "                Loss: fixed  24 labels. Loss 0.44923. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54046935\n",
      "Train loss (w/o reg) on all data: 0.5404023\n",
      "Test loss (w/o reg) on all data: 0.46456748\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008707575\n",
      "Norm of the params: 1.1581557\n",
      "              Random: fixed   7 labels. Loss 0.46457. Accuracy 0.822.\n",
      "### Flips: 40, rs: 39, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39231792\n",
      "Train loss (w/o reg) on all data: 0.39208424\n",
      "Test loss (w/o reg) on all data: 0.42586216\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0032846811\n",
      "Norm of the params: 2.161881\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.42586. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32595253\n",
      "Train loss (w/o reg) on all data: 0.32563195\n",
      "Test loss (w/o reg) on all data: 0.44538647\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0015704937\n",
      "Norm of the params: 2.532097\n",
      "                Loss: fixed  25 labels. Loss 0.44539. Accuracy 0.867.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [319] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5372047\n",
      "Train loss (w/o reg) on all data: 0.53713286\n",
      "Test loss (w/o reg) on all data: 0.46826223\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.000246962\n",
      "Norm of the params: 1.198284\n",
      "              Random: fixed   8 labels. Loss 0.46826. Accuracy 0.822.\n",
      "### Flips: 40, rs: 39, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3772788\n",
      "Train loss (w/o reg) on all data: 0.37700596\n",
      "Test loss (w/o reg) on all data: 0.43558413\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0003019347\n",
      "Norm of the params: 2.3360114\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.43558. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [114] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32639647\n",
      "Train loss (w/o reg) on all data: 0.32607147\n",
      "Test loss (w/o reg) on all data: 0.43865183\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0014737849\n",
      "Norm of the params: 2.5494833\n",
      "                Loss: fixed  26 labels. Loss 0.43865. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [322] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53597385\n",
      "Train loss (w/o reg) on all data: 0.5358922\n",
      "Test loss (w/o reg) on all data: 0.45897862\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011585716\n",
      "Norm of the params: 1.2780149\n",
      "              Random: fixed  10 labels. Loss 0.45898. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [126] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61629003\n",
      "Train loss (w/o reg) on all data: 0.61624795\n",
      "Test loss (w/o reg) on all data: 0.46776137\n",
      "Train acc on all data:  0.660377358490566\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0009417414\n",
      "Norm of the params: 0.91759056\n",
      "Flipped loss: 0.46776. Accuracy: 0.867\n",
      "### Flips: 50, rs: 0, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5840345\n",
      "Train loss (w/o reg) on all data: 0.5839636\n",
      "Test loss (w/o reg) on all data: 0.43835577\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0011277145\n",
      "Norm of the params: 1.1911544\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.43836. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [165] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53495806\n",
      "Train loss (w/o reg) on all data: 0.53484994\n",
      "Test loss (w/o reg) on all data: 0.409992\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.001066784\n",
      "Norm of the params: 1.4706191\n",
      "                Loss: fixed   8 labels. Loss 0.40999. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6008329\n",
      "Train loss (w/o reg) on all data: 0.600782\n",
      "Test loss (w/o reg) on all data: 0.441498\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0022201121\n",
      "Norm of the params: 1.0090729\n",
      "              Random: fixed   3 labels. Loss 0.44150. Accuracy 0.844.\n",
      "### Flips: 50, rs: 0, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.528115\n",
      "Train loss (w/o reg) on all data: 0.52800053\n",
      "Test loss (w/o reg) on all data: 0.38916177\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.9111111111111111\n",
      "Norm of the mean of gradients: 0.0024242809\n",
      "Norm of the params: 1.5130574\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.38916. Accuracy 0.911.\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46945423\n",
      "Train loss (w/o reg) on all data: 0.4692488\n",
      "Test loss (w/o reg) on all data: 0.35580724\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0031974046\n",
      "Norm of the params: 2.0269039\n",
      "                Loss: fixed  15 labels. Loss 0.35581. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [163] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6008481\n",
      "Train loss (w/o reg) on all data: 0.6007958\n",
      "Test loss (w/o reg) on all data: 0.44102123\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0054798853\n",
      "Norm of the params: 1.0223304\n",
      "              Random: fixed   3 labels. Loss 0.44102. Accuracy 0.844.\n",
      "### Flips: 50, rs: 0, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [325] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4988021\n",
      "Train loss (w/o reg) on all data: 0.49865192\n",
      "Test loss (w/o reg) on all data: 0.380534\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.9111111111111111\n",
      "Norm of the mean of gradients: 0.0009875117\n",
      "Norm of the params: 1.7331127\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.38053. Accuracy 0.911.\n",
      "Using normal model\n",
      "LBFGS training took [143] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4292127\n",
      "Train loss (w/o reg) on all data: 0.42895284\n",
      "Test loss (w/o reg) on all data: 0.3926265\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0007291592\n",
      "Norm of the params: 2.2796388\n",
      "                Loss: fixed  22 labels. Loss 0.39263. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [314] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.587756\n",
      "Train loss (w/o reg) on all data: 0.587696\n",
      "Test loss (w/o reg) on all data: 0.42365122\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0015480571\n",
      "Norm of the params: 1.0950403\n",
      "              Random: fixed   5 labels. Loss 0.42365. Accuracy 0.844.\n",
      "### Flips: 50, rs: 0, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [293] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49238792\n",
      "Train loss (w/o reg) on all data: 0.49222687\n",
      "Test loss (w/o reg) on all data: 0.36964852\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.004328608\n",
      "Norm of the params: 1.7947025\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.36965. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [165] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.398993\n",
      "Train loss (w/o reg) on all data: 0.39868078\n",
      "Test loss (w/o reg) on all data: 0.38688347\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0030241057\n",
      "Norm of the params: 2.4988394\n",
      "                Loss: fixed  27 labels. Loss 0.38688. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [278] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58399004\n",
      "Train loss (w/o reg) on all data: 0.5839292\n",
      "Test loss (w/o reg) on all data: 0.42521858\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0031494403\n",
      "Norm of the params: 1.1034857\n",
      "              Random: fixed   6 labels. Loss 0.42522. Accuracy 0.822.\n",
      "### Flips: 50, rs: 0, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47702608\n",
      "Train loss (w/o reg) on all data: 0.4768578\n",
      "Test loss (w/o reg) on all data: 0.35926405\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0005597494\n",
      "Norm of the params: 1.8344859\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.35926. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38249552\n",
      "Train loss (w/o reg) on all data: 0.3821399\n",
      "Test loss (w/o reg) on all data: 0.3964743\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.001109085\n",
      "Norm of the params: 2.6670039\n",
      "                Loss: fixed  30 labels. Loss 0.39647. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [283] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5842343\n",
      "Train loss (w/o reg) on all data: 0.5841785\n",
      "Test loss (w/o reg) on all data: 0.42406717\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0018260753\n",
      "Norm of the params: 1.0563072\n",
      "              Random: fixed   8 labels. Loss 0.42407. Accuracy 0.844.\n",
      "### Flips: 50, rs: 0, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47761852\n",
      "Train loss (w/o reg) on all data: 0.4774582\n",
      "Test loss (w/o reg) on all data: 0.35307103\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00084844255\n",
      "Norm of the params: 1.7906207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Influence (LOO): fixed  24 labels. Loss 0.35307. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [433] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37703657\n",
      "Train loss (w/o reg) on all data: 0.37665406\n",
      "Test loss (w/o reg) on all data: 0.40875635\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0008111558\n",
      "Norm of the params: 2.7659416\n",
      "                Loss: fixed  31 labels. Loss 0.40876. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [292] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5699183\n",
      "Train loss (w/o reg) on all data: 0.5698356\n",
      "Test loss (w/o reg) on all data: 0.41421527\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.00786178\n",
      "Norm of the params: 1.2859375\n",
      "              Random: fixed  11 labels. Loss 0.41422. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [158] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.62885386\n",
      "Train loss (w/o reg) on all data: 0.6288061\n",
      "Test loss (w/o reg) on all data: 0.49231118\n",
      "Train acc on all data:  0.6226415094339622\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006276275\n",
      "Norm of the params: 0.97714555\n",
      "Flipped loss: 0.49231. Accuracy: 0.822\n",
      "### Flips: 50, rs: 1, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58897936\n",
      "Train loss (w/o reg) on all data: 0.5888832\n",
      "Test loss (w/o reg) on all data: 0.46596143\n",
      "Train acc on all data:  0.6509433962264151\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.009168806\n",
      "Norm of the params: 1.3867984\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.46596. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54793787\n",
      "Train loss (w/o reg) on all data: 0.5478269\n",
      "Test loss (w/o reg) on all data: 0.42450255\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016316845\n",
      "Norm of the params: 1.489727\n",
      "                Loss: fixed  10 labels. Loss 0.42450. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [136] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6136086\n",
      "Train loss (w/o reg) on all data: 0.6135487\n",
      "Test loss (w/o reg) on all data: 0.4682412\n",
      "Train acc on all data:  0.6320754716981132\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018254169\n",
      "Norm of the params: 1.0946026\n",
      "              Random: fixed   3 labels. Loss 0.46824. Accuracy 0.822.\n",
      "### Flips: 50, rs: 1, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5679722\n",
      "Train loss (w/o reg) on all data: 0.567885\n",
      "Test loss (w/o reg) on all data: 0.45049617\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0030226612\n",
      "Norm of the params: 1.3206775\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.45050. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48378617\n",
      "Train loss (w/o reg) on all data: 0.48356903\n",
      "Test loss (w/o reg) on all data: 0.37116525\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0072838743\n",
      "Norm of the params: 2.083972\n",
      "                Loss: fixed  18 labels. Loss 0.37117. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59833926\n",
      "Train loss (w/o reg) on all data: 0.5982623\n",
      "Test loss (w/o reg) on all data: 0.4666692\n",
      "Train acc on all data:  0.6273584905660378\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0024907766\n",
      "Norm of the params: 1.2405602\n",
      "              Random: fixed   6 labels. Loss 0.46667. Accuracy 0.800.\n",
      "### Flips: 50, rs: 1, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [136] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5449227\n",
      "Train loss (w/o reg) on all data: 0.5448227\n",
      "Test loss (w/o reg) on all data: 0.4232186\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0029366072\n",
      "Norm of the params: 1.4142166\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.42322. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [344] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4433465\n",
      "Train loss (w/o reg) on all data: 0.44304225\n",
      "Test loss (w/o reg) on all data: 0.36370325\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0025542395\n",
      "Norm of the params: 2.4667633\n",
      "                Loss: fixed  23 labels. Loss 0.36370. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [346] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59833723\n",
      "Train loss (w/o reg) on all data: 0.59826034\n",
      "Test loss (w/o reg) on all data: 0.46615762\n",
      "Train acc on all data:  0.6273584905660378\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0026787748\n",
      "Norm of the params: 1.2398843\n",
      "              Random: fixed   6 labels. Loss 0.46616. Accuracy 0.800.\n",
      "### Flips: 50, rs: 1, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5199895\n",
      "Train loss (w/o reg) on all data: 0.5198578\n",
      "Test loss (w/o reg) on all data: 0.41880327\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0016535486\n",
      "Norm of the params: 1.622608\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.41880. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [278] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4235428\n",
      "Train loss (w/o reg) on all data: 0.42318714\n",
      "Test loss (w/o reg) on all data: 0.38456938\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0037455354\n",
      "Norm of the params: 2.6670246\n",
      "                Loss: fixed  27 labels. Loss 0.38457. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [333] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59833735\n",
      "Train loss (w/o reg) on all data: 0.59826136\n",
      "Test loss (w/o reg) on all data: 0.46609336\n",
      "Train acc on all data:  0.6273584905660378\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003150039\n",
      "Norm of the params: 1.2326132\n",
      "              Random: fixed   6 labels. Loss 0.46609. Accuracy 0.800.\n",
      "### Flips: 50, rs: 1, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5002225\n",
      "Train loss (w/o reg) on all data: 0.5000859\n",
      "Test loss (w/o reg) on all data: 0.4042405\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0021752135\n",
      "Norm of the params: 1.6529236\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.40424. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [341] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40102825\n",
      "Train loss (w/o reg) on all data: 0.40059346\n",
      "Test loss (w/o reg) on all data: 0.40443954\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0018634798\n",
      "Norm of the params: 2.9488556\n",
      "                Loss: fixed  31 labels. Loss 0.40444. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [460] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59161896\n",
      "Train loss (w/o reg) on all data: 0.5915303\n",
      "Test loss (w/o reg) on all data: 0.48260882\n",
      "Train acc on all data:  0.6415094339622641\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0026390003\n",
      "Norm of the params: 1.3313615\n",
      "              Random: fixed   9 labels. Loss 0.48261. Accuracy 0.778.\n",
      "### Flips: 50, rs: 1, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [356] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4878177\n",
      "Train loss (w/o reg) on all data: 0.48768342\n",
      "Test loss (w/o reg) on all data: 0.38901883\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012197578\n",
      "Norm of the params: 1.6387471\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.38902. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [325] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37534434\n",
      "Train loss (w/o reg) on all data: 0.3749519\n",
      "Test loss (w/o reg) on all data: 0.3716045\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001052823\n",
      "Norm of the params: 2.8015351\n",
      "                Loss: fixed  35 labels. Loss 0.37160. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [354] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5914964\n",
      "Train loss (w/o reg) on all data: 0.5914052\n",
      "Test loss (w/o reg) on all data: 0.4820134\n",
      "Train acc on all data:  0.6556603773584906\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0040117246\n",
      "Norm of the params: 1.350707\n",
      "              Random: fixed  11 labels. Loss 0.48201. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [312] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5841531\n",
      "Train loss (w/o reg) on all data: 0.5840394\n",
      "Test loss (w/o reg) on all data: 0.5532342\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0028045375\n",
      "Norm of the params: 1.5081096\n",
      "Flipped loss: 0.55323. Accuracy: 0.778\n",
      "### Flips: 50, rs: 2, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [79] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55149883\n",
      "Train loss (w/o reg) on all data: 0.5513631\n",
      "Test loss (w/o reg) on all data: 0.5245153\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0008846239\n",
      "Norm of the params: 1.6474022\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.52452. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [135] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5363302\n",
      "Train loss (w/o reg) on all data: 0.5361513\n",
      "Test loss (w/o reg) on all data: 0.5517871\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.00096516026\n",
      "Norm of the params: 1.8917862\n",
      "                Loss: fixed   6 labels. Loss 0.55179. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [189] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5841513\n",
      "Train loss (w/o reg) on all data: 0.5840357\n",
      "Test loss (w/o reg) on all data: 0.55333924\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0049820524\n",
      "Norm of the params: 1.520704\n",
      "              Random: fixed   0 labels. Loss 0.55334. Accuracy 0.778.\n",
      "### Flips: 50, rs: 2, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [187] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53811646\n",
      "Train loss (w/o reg) on all data: 0.5379815\n",
      "Test loss (w/o reg) on all data: 0.48910254\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0014582281\n",
      "Norm of the params: 1.642856\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.48910. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4927507\n",
      "Train loss (w/o reg) on all data: 0.49248868\n",
      "Test loss (w/o reg) on all data: 0.5470844\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0024804873\n",
      "Norm of the params: 2.2892148\n",
      "                Loss: fixed  12 labels. Loss 0.54708. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57080346\n",
      "Train loss (w/o reg) on all data: 0.57066333\n",
      "Test loss (w/o reg) on all data: 0.5513582\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00070613314\n",
      "Norm of the params: 1.6742367\n",
      "              Random: fixed   3 labels. Loss 0.55136. Accuracy 0.756.\n",
      "### Flips: 50, rs: 2, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [167] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5023815\n",
      "Train loss (w/o reg) on all data: 0.50219303\n",
      "Test loss (w/o reg) on all data: 0.48172873\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0037858493\n",
      "Norm of the params: 1.94154\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.48173. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43493918\n",
      "Train loss (w/o reg) on all data: 0.43463367\n",
      "Test loss (w/o reg) on all data: 0.5133621\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0038607048\n",
      "Norm of the params: 2.471829\n",
      "                Loss: fixed  20 labels. Loss 0.51336. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56636477\n",
      "Train loss (w/o reg) on all data: 0.5662362\n",
      "Test loss (w/o reg) on all data: 0.5373922\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0021513267\n",
      "Norm of the params: 1.6034749\n",
      "              Random: fixed   6 labels. Loss 0.53739. Accuracy 0.733.\n",
      "### Flips: 50, rs: 2, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49054933\n",
      "Train loss (w/o reg) on all data: 0.49037287\n",
      "Test loss (w/o reg) on all data: 0.43084905\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012397099\n",
      "Norm of the params: 1.8786564\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.43085. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [418] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38793996\n",
      "Train loss (w/o reg) on all data: 0.38757682\n",
      "Test loss (w/o reg) on all data: 0.49160638\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00052352005\n",
      "Norm of the params: 2.6950002\n",
      "                Loss: fixed  26 labels. Loss 0.49161. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [279] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5661441\n",
      "Train loss (w/o reg) on all data: 0.5660275\n",
      "Test loss (w/o reg) on all data: 0.52450246\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.010222797\n",
      "Norm of the params: 1.5271004\n",
      "              Random: fixed   8 labels. Loss 0.52450. Accuracy 0.733.\n",
      "### Flips: 50, rs: 2, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47114035\n",
      "Train loss (w/o reg) on all data: 0.4709316\n",
      "Test loss (w/o reg) on all data: 0.4203807\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0048113866\n",
      "Norm of the params: 2.043378\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.42038. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [279] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35974568\n",
      "Train loss (w/o reg) on all data: 0.3592687\n",
      "Test loss (w/o reg) on all data: 0.5070241\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00077240326\n",
      "Norm of the params: 3.0886304\n",
      "                Loss: fixed  31 labels. Loss 0.50702. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [285] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56025094\n",
      "Train loss (w/o reg) on all data: 0.5601291\n",
      "Test loss (w/o reg) on all data: 0.51944065\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0018813872\n",
      "Norm of the params: 1.5611302\n",
      "              Random: fixed   9 labels. Loss 0.51944. Accuracy 0.711.\n",
      "### Flips: 50, rs: 2, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [208] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4388777\n",
      "Train loss (w/o reg) on all data: 0.43857569\n",
      "Test loss (w/o reg) on all data: 0.4203157\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0018397802\n",
      "Norm of the params: 2.4576573\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.42032. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [279] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35338303\n",
      "Train loss (w/o reg) on all data: 0.35286155\n",
      "Test loss (w/o reg) on all data: 0.51330346\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0032052996\n",
      "Norm of the params: 3.229524\n",
      "                Loss: fixed  33 labels. Loss 0.51330. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [288] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5494515\n",
      "Train loss (w/o reg) on all data: 0.5493691\n",
      "Test loss (w/o reg) on all data: 0.51869124\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0018547511\n",
      "Norm of the params: 1.2833666\n",
      "              Random: fixed  15 labels. Loss 0.51869. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [278] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5833049\n",
      "Train loss (w/o reg) on all data: 0.5831754\n",
      "Test loss (w/o reg) on all data: 0.55400765\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0066869375\n",
      "Norm of the params: 1.6091363\n",
      "Flipped loss: 0.55401. Accuracy: 0.733\n",
      "### Flips: 50, rs: 3, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [141] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55616164\n",
      "Train loss (w/o reg) on all data: 0.5560125\n",
      "Test loss (w/o reg) on all data: 0.51064533\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0016654541\n",
      "Norm of the params: 1.7271327\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.51065. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4948675\n",
      "Train loss (w/o reg) on all data: 0.49459672\n",
      "Test loss (w/o reg) on all data: 0.5539868\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0006792497\n",
      "Norm of the params: 2.3271523\n",
      "                Loss: fixed   9 labels. Loss 0.55399. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57946163\n",
      "Train loss (w/o reg) on all data: 0.5793646\n",
      "Test loss (w/o reg) on all data: 0.5051222\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00057833066\n",
      "Norm of the params: 1.3929296\n",
      "              Random: fixed   4 labels. Loss 0.50512. Accuracy 0.756.\n",
      "### Flips: 50, rs: 3, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [154] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51878005\n",
      "Train loss (w/o reg) on all data: 0.5185687\n",
      "Test loss (w/o reg) on all data: 0.5056563\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0003474298\n",
      "Norm of the params: 2.055962\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.50566. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46694046\n",
      "Train loss (w/o reg) on all data: 0.46661314\n",
      "Test loss (w/o reg) on all data: 0.59582675\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0037131372\n",
      "Norm of the params: 2.558624\n",
      "                Loss: fixed  12 labels. Loss 0.59583. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [291] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5693985\n",
      "Train loss (w/o reg) on all data: 0.56931067\n",
      "Test loss (w/o reg) on all data: 0.47737917\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010205206\n",
      "Norm of the params: 1.3254148\n",
      "              Random: fixed   7 labels. Loss 0.47738. Accuracy 0.778.\n",
      "### Flips: 50, rs: 3, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4615463\n",
      "Train loss (w/o reg) on all data: 0.46122354\n",
      "Test loss (w/o reg) on all data: 0.5376804\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0016856528\n",
      "Norm of the params: 2.5407486\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.53768. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4066392\n",
      "Train loss (w/o reg) on all data: 0.40618047\n",
      "Test loss (w/o reg) on all data: 0.6078893\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.00046271263\n",
      "Norm of the params: 3.0289297\n",
      "                Loss: fixed  19 labels. Loss 0.60789. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56388783\n",
      "Train loss (w/o reg) on all data: 0.5637924\n",
      "Test loss (w/o reg) on all data: 0.46637988\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0020600124\n",
      "Norm of the params: 1.3815749\n",
      "              Random: fixed  10 labels. Loss 0.46638. Accuracy 0.778.\n",
      "### Flips: 50, rs: 3, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [370] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4513046\n",
      "Train loss (w/o reg) on all data: 0.45097068\n",
      "Test loss (w/o reg) on all data: 0.5360687\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0059803673\n",
      "Norm of the params: 2.5843365\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.53607. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37764683\n",
      "Train loss (w/o reg) on all data: 0.37714434\n",
      "Test loss (w/o reg) on all data: 0.6086817\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0004515016\n",
      "Norm of the params: 3.1701217\n",
      "                Loss: fixed  23 labels. Loss 0.60868. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [326] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5565961\n",
      "Train loss (w/o reg) on all data: 0.5564992\n",
      "Test loss (w/o reg) on all data: 0.45853555\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0060136477\n",
      "Norm of the params: 1.3921661\n",
      "              Random: fixed  11 labels. Loss 0.45854. Accuracy 0.778.\n",
      "### Flips: 50, rs: 3, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [278] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45124352\n",
      "Train loss (w/o reg) on all data: 0.45092914\n",
      "Test loss (w/o reg) on all data: 0.5040655\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001190148\n",
      "Norm of the params: 2.507502\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.50407. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [136] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37404633\n",
      "Train loss (w/o reg) on all data: 0.37353897\n",
      "Test loss (w/o reg) on all data: 0.5862587\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.00040665298\n",
      "Norm of the params: 3.185432\n",
      "                Loss: fixed  24 labels. Loss 0.58626. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [324] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5480086\n",
      "Train loss (w/o reg) on all data: 0.54792356\n",
      "Test loss (w/o reg) on all data: 0.4581386\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018136468\n",
      "Norm of the params: 1.3041706\n",
      "              Random: fixed  14 labels. Loss 0.45814. Accuracy 0.800.\n",
      "### Flips: 50, rs: 3, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45433024\n",
      "Train loss (w/o reg) on all data: 0.4540352\n",
      "Test loss (w/o reg) on all data: 0.494867\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0016741354\n",
      "Norm of the params: 2.4291084\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.49487. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [226] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3670385\n",
      "Train loss (w/o reg) on all data: 0.36649692\n",
      "Test loss (w/o reg) on all data: 0.5537279\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019224447\n",
      "Norm of the params: 3.2910953\n",
      "                Loss: fixed  26 labels. Loss 0.55373. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [384] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54529417\n",
      "Train loss (w/o reg) on all data: 0.5452108\n",
      "Test loss (w/o reg) on all data: 0.43910798\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0025468164\n",
      "Norm of the params: 1.2913707\n",
      "              Random: fixed  16 labels. Loss 0.43911. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60003793\n",
      "Train loss (w/o reg) on all data: 0.5999792\n",
      "Test loss (w/o reg) on all data: 0.45168355\n",
      "Train acc on all data:  0.6650943396226415\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013251911\n",
      "Norm of the params: 1.0836958\n",
      "Flipped loss: 0.45168. Accuracy: 0.800\n",
      "### Flips: 50, rs: 4, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [151] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5628045\n",
      "Train loss (w/o reg) on all data: 0.56272906\n",
      "Test loss (w/o reg) on all data: 0.455821\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001685673\n",
      "Norm of the params: 1.2286344\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.45582. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [292] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49761483\n",
      "Train loss (w/o reg) on all data: 0.49745688\n",
      "Test loss (w/o reg) on all data: 0.4180586\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012902103\n",
      "Norm of the params: 1.7773209\n",
      "                Loss: fixed  10 labels. Loss 0.41806. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.577839\n",
      "Train loss (w/o reg) on all data: 0.57775795\n",
      "Test loss (w/o reg) on all data: 0.451834\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006059549\n",
      "Norm of the params: 1.2732869\n",
      "              Random: fixed   4 labels. Loss 0.45183. Accuracy 0.844.\n",
      "### Flips: 50, rs: 4, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5410482\n",
      "Train loss (w/o reg) on all data: 0.5409413\n",
      "Test loss (w/o reg) on all data: 0.4376656\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006774044\n",
      "Norm of the params: 1.4623017\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.43767. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43383282\n",
      "Train loss (w/o reg) on all data: 0.43358102\n",
      "Test loss (w/o reg) on all data: 0.41186616\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007467089\n",
      "Norm of the params: 2.2440574\n",
      "                Loss: fixed  17 labels. Loss 0.41187. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5754537\n",
      "Train loss (w/o reg) on all data: 0.575364\n",
      "Test loss (w/o reg) on all data: 0.44947803\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0073269373\n",
      "Norm of the params: 1.3394637\n",
      "              Random: fixed   6 labels. Loss 0.44948. Accuracy 0.822.\n",
      "### Flips: 50, rs: 4, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [187] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50098515\n",
      "Train loss (w/o reg) on all data: 0.50085664\n",
      "Test loss (w/o reg) on all data: 0.42761722\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.019241009\n",
      "Norm of the params: 1.6030173\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.42762. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3763539\n",
      "Train loss (w/o reg) on all data: 0.37593985\n",
      "Test loss (w/o reg) on all data: 0.43160823\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012879235\n",
      "Norm of the params: 2.8776755\n",
      "                Loss: fixed  23 labels. Loss 0.43161. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57545245\n",
      "Train loss (w/o reg) on all data: 0.5753632\n",
      "Test loss (w/o reg) on all data: 0.450089\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012434399\n",
      "Norm of the params: 1.3357712\n",
      "              Random: fixed   6 labels. Loss 0.45009. Accuracy 0.822.\n",
      "### Flips: 50, rs: 4, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [146] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46551755\n",
      "Train loss (w/o reg) on all data: 0.46534285\n",
      "Test loss (w/o reg) on all data: 0.42450795\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.005234281\n",
      "Norm of the params: 1.8692414\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.42451. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [396] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3536493\n",
      "Train loss (w/o reg) on all data: 0.353175\n",
      "Test loss (w/o reg) on all data: 0.44243684\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011462701\n",
      "Norm of the params: 3.0798929\n",
      "                Loss: fixed  27 labels. Loss 0.44244. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [376] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5572971\n",
      "Train loss (w/o reg) on all data: 0.55719304\n",
      "Test loss (w/o reg) on all data: 0.4327284\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00086803193\n",
      "Norm of the params: 1.4428599\n",
      "              Random: fixed   8 labels. Loss 0.43273. Accuracy 0.822.\n",
      "### Flips: 50, rs: 4, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [107] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44705984\n",
      "Train loss (w/o reg) on all data: 0.4468566\n",
      "Test loss (w/o reg) on all data: 0.42727283\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017644265\n",
      "Norm of the params: 2.0162065\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.42727. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [373] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32939106\n",
      "Train loss (w/o reg) on all data: 0.32889622\n",
      "Test loss (w/o reg) on all data: 0.4572442\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0004208233\n",
      "Norm of the params: 3.1458805\n",
      "                Loss: fixed  30 labels. Loss 0.45724. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [322] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5607872\n",
      "Train loss (w/o reg) on all data: 0.5606924\n",
      "Test loss (w/o reg) on all data: 0.4292658\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0074970517\n",
      "Norm of the params: 1.3769305\n",
      "              Random: fixed  11 labels. Loss 0.42927. Accuracy 0.822.\n",
      "### Flips: 50, rs: 4, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [108] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4290611\n",
      "Train loss (w/o reg) on all data: 0.4288561\n",
      "Test loss (w/o reg) on all data: 0.40914133\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0037634005\n",
      "Norm of the params: 2.0248322\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.40914. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [351] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31430355\n",
      "Train loss (w/o reg) on all data: 0.3137068\n",
      "Test loss (w/o reg) on all data: 0.502994\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.005685173\n",
      "Norm of the params: 3.454755\n",
      "                Loss: fixed  34 labels. Loss 0.50299. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [290] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56091946\n",
      "Train loss (w/o reg) on all data: 0.5608323\n",
      "Test loss (w/o reg) on all data: 0.41452897\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00087103905\n",
      "Norm of the params: 1.3201232\n",
      "              Random: fixed  15 labels. Loss 0.41453. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61170226\n",
      "Train loss (w/o reg) on all data: 0.61164623\n",
      "Test loss (w/o reg) on all data: 0.45197427\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004379052\n",
      "Norm of the params: 1.0583472\n",
      "Flipped loss: 0.45197. Accuracy: 0.822\n",
      "### Flips: 50, rs: 5, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [154] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5878841\n",
      "Train loss (w/o reg) on all data: 0.58782035\n",
      "Test loss (w/o reg) on all data: 0.44531137\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.007627258\n",
      "Norm of the params: 1.1291575\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.44531. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5317644\n",
      "Train loss (w/o reg) on all data: 0.53162587\n",
      "Test loss (w/o reg) on all data: 0.3747707\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003270972\n",
      "Norm of the params: 1.6645252\n",
      "                Loss: fixed   9 labels. Loss 0.37477. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5969708\n",
      "Train loss (w/o reg) on all data: 0.5969055\n",
      "Test loss (w/o reg) on all data: 0.42928034\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010131495\n",
      "Norm of the params: 1.1423713\n",
      "              Random: fixed   3 labels. Loss 0.42928. Accuracy 0.822.\n",
      "### Flips: 50, rs: 5, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [43] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55472153\n",
      "Train loss (w/o reg) on all data: 0.554632\n",
      "Test loss (w/o reg) on all data: 0.40193838\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017005731\n",
      "Norm of the params: 1.3379428\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.40194. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.458636\n",
      "Train loss (w/o reg) on all data: 0.4583486\n",
      "Test loss (w/o reg) on all data: 0.39415985\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0057751457\n",
      "Norm of the params: 2.3973734\n",
      "                Loss: fixed  18 labels. Loss 0.39416. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5848104\n",
      "Train loss (w/o reg) on all data: 0.5847505\n",
      "Test loss (w/o reg) on all data: 0.43279827\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0031819984\n",
      "Norm of the params: 1.0944566\n",
      "              Random: fixed   7 labels. Loss 0.43280. Accuracy 0.822.\n",
      "### Flips: 50, rs: 5, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52335167\n",
      "Train loss (w/o reg) on all data: 0.52322316\n",
      "Test loss (w/o reg) on all data: 0.40266854\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0046062255\n",
      "Norm of the params: 1.6029853\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.40267. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [344] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41928023\n",
      "Train loss (w/o reg) on all data: 0.4189444\n",
      "Test loss (w/o reg) on all data: 0.3976184\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005068333\n",
      "Norm of the params: 2.591676\n",
      "                Loss: fixed  23 labels. Loss 0.39762. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5635037\n",
      "Train loss (w/o reg) on all data: 0.5634382\n",
      "Test loss (w/o reg) on all data: 0.41991174\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0026533217\n",
      "Norm of the params: 1.1448506\n",
      "              Random: fixed  13 labels. Loss 0.41991. Accuracy 0.822.\n",
      "### Flips: 50, rs: 5, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5014075\n",
      "Train loss (w/o reg) on all data: 0.50129837\n",
      "Test loss (w/o reg) on all data: 0.38105333\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014947951\n",
      "Norm of the params: 1.4774565\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.38105. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37592497\n",
      "Train loss (w/o reg) on all data: 0.3754716\n",
      "Test loss (w/o reg) on all data: 0.4304109\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00090347225\n",
      "Norm of the params: 3.0112088\n",
      "                Loss: fixed  29 labels. Loss 0.43041. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55414337\n",
      "Train loss (w/o reg) on all data: 0.55406654\n",
      "Test loss (w/o reg) on all data: 0.41871232\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001542559\n",
      "Norm of the params: 1.2397232\n",
      "              Random: fixed  14 labels. Loss 0.41871. Accuracy 0.822.\n",
      "### Flips: 50, rs: 5, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [138] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45577\n",
      "Train loss (w/o reg) on all data: 0.45563722\n",
      "Test loss (w/o reg) on all data: 0.37678245\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011212381\n",
      "Norm of the params: 1.6295196\n",
      "     Influence (LOO): fixed  29 labels. Loss 0.37678. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [454] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3548324\n",
      "Train loss (w/o reg) on all data: 0.3543804\n",
      "Test loss (w/o reg) on all data: 0.42344293\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0016021546\n",
      "Norm of the params: 3.006665\n",
      "                Loss: fixed  32 labels. Loss 0.42344. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [299] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5365544\n",
      "Train loss (w/o reg) on all data: 0.536472\n",
      "Test loss (w/o reg) on all data: 0.3996045\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0075618764\n",
      "Norm of the params: 1.2836908\n",
      "              Random: fixed  18 labels. Loss 0.39960. Accuracy 0.844.\n",
      "### Flips: 50, rs: 5, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44071788\n",
      "Train loss (w/o reg) on all data: 0.44054952\n",
      "Test loss (w/o reg) on all data: 0.36596006\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0028554192\n",
      "Norm of the params: 1.8350284\n",
      "     Influence (LOO): fixed  32 labels. Loss 0.36596. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35032162\n",
      "Train loss (w/o reg) on all data: 0.34986806\n",
      "Test loss (w/o reg) on all data: 0.4350624\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018274268\n",
      "Norm of the params: 3.0118814\n",
      "                Loss: fixed  35 labels. Loss 0.43506. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [367] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53638357\n",
      "Train loss (w/o reg) on all data: 0.5363038\n",
      "Test loss (w/o reg) on all data: 0.3994554\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0025968056\n",
      "Norm of the params: 1.2628435\n",
      "              Random: fixed  20 labels. Loss 0.39946. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6042815\n",
      "Train loss (w/o reg) on all data: 0.604223\n",
      "Test loss (w/o reg) on all data: 0.49754795\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015098372\n",
      "Norm of the params: 1.0811559\n",
      "Flipped loss: 0.49755. Accuracy: 0.778\n",
      "### Flips: 50, rs: 6, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [122] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5749534\n",
      "Train loss (w/o reg) on all data: 0.5748706\n",
      "Test loss (w/o reg) on all data: 0.47644186\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002927112\n",
      "Norm of the params: 1.2867074\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.47644. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [131] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5179001\n",
      "Train loss (w/o reg) on all data: 0.5177544\n",
      "Test loss (w/o reg) on all data: 0.44239986\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.003206873\n",
      "Norm of the params: 1.7072631\n",
      "                Loss: fixed  10 labels. Loss 0.44240. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [158] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6010931\n",
      "Train loss (w/o reg) on all data: 0.6010284\n",
      "Test loss (w/o reg) on all data: 0.48599514\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0018885784\n",
      "Norm of the params: 1.1375569\n",
      "              Random: fixed   3 labels. Loss 0.48600. Accuracy 0.778.\n",
      "### Flips: 50, rs: 6, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [109] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55102676\n",
      "Train loss (w/o reg) on all data: 0.55091584\n",
      "Test loss (w/o reg) on all data: 0.47111985\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0063931784\n",
      "Norm of the params: 1.4895456\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.47112. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45913044\n",
      "Train loss (w/o reg) on all data: 0.45891726\n",
      "Test loss (w/o reg) on all data: 0.46187147\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0020259114\n",
      "Norm of the params: 2.0649006\n",
      "                Loss: fixed  16 labels. Loss 0.46187. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59959537\n",
      "Train loss (w/o reg) on all data: 0.5995317\n",
      "Test loss (w/o reg) on all data: 0.4871299\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.006415325\n",
      "Norm of the params: 1.1282696\n",
      "              Random: fixed   4 labels. Loss 0.48713. Accuracy 0.800.\n",
      "### Flips: 50, rs: 6, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53360337\n",
      "Train loss (w/o reg) on all data: 0.5334482\n",
      "Test loss (w/o reg) on all data: 0.45343733\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0004475415\n",
      "Norm of the params: 1.761546\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.45344. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4262525\n",
      "Train loss (w/o reg) on all data: 0.4259871\n",
      "Test loss (w/o reg) on all data: 0.46940944\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0030704527\n",
      "Norm of the params: 2.3040426\n",
      "                Loss: fixed  20 labels. Loss 0.46941. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59560084\n",
      "Train loss (w/o reg) on all data: 0.5955394\n",
      "Test loss (w/o reg) on all data: 0.4882917\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.009047179\n",
      "Norm of the params: 1.1085769\n",
      "              Random: fixed   6 labels. Loss 0.48829. Accuracy 0.800.\n",
      "### Flips: 50, rs: 6, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [161] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5312995\n",
      "Train loss (w/o reg) on all data: 0.5311664\n",
      "Test loss (w/o reg) on all data: 0.44059643\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00039835155\n",
      "Norm of the params: 1.6316407\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.44060. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [282] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3948913\n",
      "Train loss (w/o reg) on all data: 0.39459652\n",
      "Test loss (w/o reg) on all data: 0.4964664\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0008073638\n",
      "Norm of the params: 2.4280496\n",
      "                Loss: fixed  25 labels. Loss 0.49647. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5884924\n",
      "Train loss (w/o reg) on all data: 0.58842385\n",
      "Test loss (w/o reg) on all data: 0.46358478\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009998961\n",
      "Norm of the params: 1.1710191\n",
      "              Random: fixed   8 labels. Loss 0.46358. Accuracy 0.800.\n",
      "### Flips: 50, rs: 6, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5193013\n",
      "Train loss (w/o reg) on all data: 0.519168\n",
      "Test loss (w/o reg) on all data: 0.42428416\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0030321283\n",
      "Norm of the params: 1.6324663\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.42428. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [348] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37368858\n",
      "Train loss (w/o reg) on all data: 0.37328076\n",
      "Test loss (w/o reg) on all data: 0.49461523\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00049134996\n",
      "Norm of the params: 2.8559465\n",
      "                Loss: fixed  28 labels. Loss 0.49462. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5797344\n",
      "Train loss (w/o reg) on all data: 0.5796683\n",
      "Test loss (w/o reg) on all data: 0.4479768\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0064229476\n",
      "Norm of the params: 1.1496861\n",
      "              Random: fixed  11 labels. Loss 0.44798. Accuracy 0.800.\n",
      "### Flips: 50, rs: 6, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [201] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49066275\n",
      "Train loss (w/o reg) on all data: 0.49050382\n",
      "Test loss (w/o reg) on all data: 0.40195403\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010056843\n",
      "Norm of the params: 1.7828896\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.40195. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36106\n",
      "Train loss (w/o reg) on all data: 0.3606162\n",
      "Test loss (w/o reg) on all data: 0.48468968\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0038162104\n",
      "Norm of the params: 2.9791963\n",
      "                Loss: fixed  30 labels. Loss 0.48469. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [324] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56843257\n",
      "Train loss (w/o reg) on all data: 0.5683557\n",
      "Test loss (w/o reg) on all data: 0.448667\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015452986\n",
      "Norm of the params: 1.2401956\n",
      "              Random: fixed  14 labels. Loss 0.44867. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [185] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57405907\n",
      "Train loss (w/o reg) on all data: 0.57397175\n",
      "Test loss (w/o reg) on all data: 0.48524362\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.000770441\n",
      "Norm of the params: 1.3214395\n",
      "Flipped loss: 0.48524. Accuracy: 0.778\n",
      "### Flips: 50, rs: 7, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [121] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5227423\n",
      "Train loss (w/o reg) on all data: 0.5225965\n",
      "Test loss (w/o reg) on all data: 0.4530191\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.005525895\n",
      "Norm of the params: 1.7076371\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.45302. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [125] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49563313\n",
      "Train loss (w/o reg) on all data: 0.49548492\n",
      "Test loss (w/o reg) on all data: 0.4424803\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002247658\n",
      "Norm of the params: 1.7216018\n",
      "                Loss: fixed   8 labels. Loss 0.44248. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [131] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56794435\n",
      "Train loss (w/o reg) on all data: 0.56785977\n",
      "Test loss (w/o reg) on all data: 0.4603102\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015052999\n",
      "Norm of the params: 1.3006876\n",
      "              Random: fixed   3 labels. Loss 0.46031. Accuracy 0.800.\n",
      "### Flips: 50, rs: 7, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5006044\n",
      "Train loss (w/o reg) on all data: 0.5004636\n",
      "Test loss (w/o reg) on all data: 0.4489189\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00076370937\n",
      "Norm of the params: 1.6779722\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.44892. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42190298\n",
      "Train loss (w/o reg) on all data: 0.4216031\n",
      "Test loss (w/o reg) on all data: 0.47755867\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0017176282\n",
      "Norm of the params: 2.4489403\n",
      "                Loss: fixed  17 labels. Loss 0.47756. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [336] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55832785\n",
      "Train loss (w/o reg) on all data: 0.5582431\n",
      "Test loss (w/o reg) on all data: 0.42936835\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0052116527\n",
      "Norm of the params: 1.3018528\n",
      "              Random: fixed   8 labels. Loss 0.42937. Accuracy 0.778.\n",
      "### Flips: 50, rs: 7, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48064765\n",
      "Train loss (w/o reg) on all data: 0.48048598\n",
      "Test loss (w/o reg) on all data: 0.43653688\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0007397956\n",
      "Norm of the params: 1.7981594\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.43654. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [289] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3991188\n",
      "Train loss (w/o reg) on all data: 0.39874923\n",
      "Test loss (w/o reg) on all data: 0.50495803\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0049087894\n",
      "Norm of the params: 2.7187812\n",
      "                Loss: fixed  20 labels. Loss 0.50496. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54315555\n",
      "Train loss (w/o reg) on all data: 0.5430586\n",
      "Test loss (w/o reg) on all data: 0.4145775\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0014309686\n",
      "Norm of the params: 1.3926964\n",
      "              Random: fixed  11 labels. Loss 0.41458. Accuracy 0.778.\n",
      "### Flips: 50, rs: 7, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4489854\n",
      "Train loss (w/o reg) on all data: 0.44881713\n",
      "Test loss (w/o reg) on all data: 0.41516995\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022712229\n",
      "Norm of the params: 1.8344209\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.41517. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [208] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37369746\n",
      "Train loss (w/o reg) on all data: 0.3733576\n",
      "Test loss (w/o reg) on all data: 0.46982986\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00041987293\n",
      "Norm of the params: 2.6071963\n",
      "                Loss: fixed  25 labels. Loss 0.46983. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [334] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5383313\n",
      "Train loss (w/o reg) on all data: 0.5382298\n",
      "Test loss (w/o reg) on all data: 0.42040843\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0030496533\n",
      "Norm of the params: 1.4244653\n",
      "              Random: fixed  12 labels. Loss 0.42041. Accuracy 0.778.\n",
      "### Flips: 50, rs: 7, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43615046\n",
      "Train loss (w/o reg) on all data: 0.43594468\n",
      "Test loss (w/o reg) on all data: 0.4121579\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0017313792\n",
      "Norm of the params: 2.0286705\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.41216. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33981845\n",
      "Train loss (w/o reg) on all data: 0.33947942\n",
      "Test loss (w/o reg) on all data: 0.47614557\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00035415663\n",
      "Norm of the params: 2.604005\n",
      "                Loss: fixed  30 labels. Loss 0.47615. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53435946\n",
      "Train loss (w/o reg) on all data: 0.5342465\n",
      "Test loss (w/o reg) on all data: 0.41007844\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002329914\n",
      "Norm of the params: 1.5028464\n",
      "              Random: fixed  14 labels. Loss 0.41008. Accuracy 0.778.\n",
      "### Flips: 50, rs: 7, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [457] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41762307\n",
      "Train loss (w/o reg) on all data: 0.41739592\n",
      "Test loss (w/o reg) on all data: 0.3865056\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0038464994\n",
      "Norm of the params: 2.1314175\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.38651. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32266164\n",
      "Train loss (w/o reg) on all data: 0.32226926\n",
      "Test loss (w/o reg) on all data: 0.48207828\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.002213545\n",
      "Norm of the params: 2.8013716\n",
      "                Loss: fixed  33 labels. Loss 0.48208. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5276362\n",
      "Train loss (w/o reg) on all data: 0.52750885\n",
      "Test loss (w/o reg) on all data: 0.40034115\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of the mean of gradients: 0.013273966\n",
      "Norm of the params: 1.5959028\n",
      "              Random: fixed  17 labels. Loss 0.40034. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5891473\n",
      "Train loss (w/o reg) on all data: 0.5890355\n",
      "Test loss (w/o reg) on all data: 0.55408853\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00233876\n",
      "Norm of the params: 1.4954288\n",
      "Flipped loss: 0.55409. Accuracy: 0.756\n",
      "### Flips: 50, rs: 8, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [124] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5501272\n",
      "Train loss (w/o reg) on all data: 0.549976\n",
      "Test loss (w/o reg) on all data: 0.49300876\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0030793967\n",
      "Norm of the params: 1.7391783\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.49301. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [463] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5165267\n",
      "Train loss (w/o reg) on all data: 0.5162631\n",
      "Test loss (w/o reg) on all data: 0.5333251\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.002162456\n",
      "Norm of the params: 2.296057\n",
      "                Loss: fixed   8 labels. Loss 0.53333. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [315] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5882939\n",
      "Train loss (w/o reg) on all data: 0.5881812\n",
      "Test loss (w/o reg) on all data: 0.54218084\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.001648443\n",
      "Norm of the params: 1.5012689\n",
      "              Random: fixed   3 labels. Loss 0.54218. Accuracy 0.756.\n",
      "### Flips: 50, rs: 8, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [398] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52704227\n",
      "Train loss (w/o reg) on all data: 0.5268796\n",
      "Test loss (w/o reg) on all data: 0.44444305\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003283241\n",
      "Norm of the params: 1.8036954\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.44444. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [414] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47615102\n",
      "Train loss (w/o reg) on all data: 0.47582656\n",
      "Test loss (w/o reg) on all data: 0.5421142\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.000629043\n",
      "Norm of the params: 2.5473304\n",
      "                Loss: fixed  13 labels. Loss 0.54211. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [350] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57933545\n",
      "Train loss (w/o reg) on all data: 0.57922333\n",
      "Test loss (w/o reg) on all data: 0.54176545\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.001162579\n",
      "Norm of the params: 1.4975394\n",
      "              Random: fixed   8 labels. Loss 0.54177. Accuracy 0.756.\n",
      "### Flips: 50, rs: 8, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [318] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5076674\n",
      "Train loss (w/o reg) on all data: 0.5074862\n",
      "Test loss (w/o reg) on all data: 0.4296052\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008105048\n",
      "Norm of the params: 1.9035144\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.42961. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [283] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41482928\n",
      "Train loss (w/o reg) on all data: 0.4143895\n",
      "Test loss (w/o reg) on all data: 0.5272068\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.003356833\n",
      "Norm of the params: 2.9658022\n",
      "                Loss: fixed  20 labels. Loss 0.52721. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [313] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57514274\n",
      "Train loss (w/o reg) on all data: 0.5750171\n",
      "Test loss (w/o reg) on all data: 0.52677774\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.011744127\n",
      "Norm of the params: 1.5852917\n",
      "              Random: fixed  10 labels. Loss 0.52678. Accuracy 0.756.\n",
      "### Flips: 50, rs: 8, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [382] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46907163\n",
      "Train loss (w/o reg) on all data: 0.4688215\n",
      "Test loss (w/o reg) on all data: 0.4224679\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0030836365\n",
      "Norm of the params: 2.2365942\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.42247. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [154] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3918489\n",
      "Train loss (w/o reg) on all data: 0.39139023\n",
      "Test loss (w/o reg) on all data: 0.5073797\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0024233558\n",
      "Norm of the params: 3.0286963\n",
      "                Loss: fixed  24 labels. Loss 0.50738. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55915445\n",
      "Train loss (w/o reg) on all data: 0.5589946\n",
      "Test loss (w/o reg) on all data: 0.502093\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.008490673\n",
      "Norm of the params: 1.787962\n",
      "              Random: fixed  13 labels. Loss 0.50209. Accuracy 0.733.\n",
      "### Flips: 50, rs: 8, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [425] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44640496\n",
      "Train loss (w/o reg) on all data: 0.4461094\n",
      "Test loss (w/o reg) on all data: 0.41574818\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0036730126\n",
      "Norm of the params: 2.4311976\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.41575. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37895513\n",
      "Train loss (w/o reg) on all data: 0.3784515\n",
      "Test loss (w/o reg) on all data: 0.51674306\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0019160648\n",
      "Norm of the params: 3.1737406\n",
      "                Loss: fixed  27 labels. Loss 0.51674. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5445724\n",
      "Train loss (w/o reg) on all data: 0.54439336\n",
      "Test loss (w/o reg) on all data: 0.50780845\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0034350776\n",
      "Norm of the params: 1.8924989\n",
      "              Random: fixed  15 labels. Loss 0.50781. Accuracy 0.733.\n",
      "### Flips: 50, rs: 8, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [418] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44049233\n",
      "Train loss (w/o reg) on all data: 0.4402079\n",
      "Test loss (w/o reg) on all data: 0.3762486\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0003442778\n",
      "Norm of the params: 2.3850543\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.37625. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [329] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36910033\n",
      "Train loss (w/o reg) on all data: 0.3685618\n",
      "Test loss (w/o reg) on all data: 0.48749927\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00058013765\n",
      "Norm of the params: 3.2818916\n",
      "                Loss: fixed  31 labels. Loss 0.48750. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53956586\n",
      "Train loss (w/o reg) on all data: 0.539424\n",
      "Test loss (w/o reg) on all data: 0.47750935\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0012689454\n",
      "Norm of the params: 1.6844208\n",
      "              Random: fixed  19 labels. Loss 0.47751. Accuracy 0.756.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [313] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5663625\n",
      "Train loss (w/o reg) on all data: 0.5662715\n",
      "Test loss (w/o reg) on all data: 0.39964584\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0088341255\n",
      "Norm of the params: 1.3493168\n",
      "Flipped loss: 0.39965. Accuracy: 0.844\n",
      "### Flips: 50, rs: 9, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [69] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5317662\n",
      "Train loss (w/o reg) on all data: 0.53165925\n",
      "Test loss (w/o reg) on all data: 0.37444162\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011544148\n",
      "Norm of the params: 1.4625798\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.37444. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48057497\n",
      "Train loss (w/o reg) on all data: 0.48039004\n",
      "Test loss (w/o reg) on all data: 0.36146516\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00095466356\n",
      "Norm of the params: 1.9231052\n",
      "                Loss: fixed   9 labels. Loss 0.36147. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54901344\n",
      "Train loss (w/o reg) on all data: 0.5489118\n",
      "Test loss (w/o reg) on all data: 0.39029884\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0061363517\n",
      "Norm of the params: 1.4255981\n",
      "              Random: fixed   2 labels. Loss 0.39030. Accuracy 0.844.\n",
      "### Flips: 50, rs: 9, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [121] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5142722\n",
      "Train loss (w/o reg) on all data: 0.5141574\n",
      "Test loss (w/o reg) on all data: 0.35103318\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.003848571\n",
      "Norm of the params: 1.5151792\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.35103. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [385] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40168953\n",
      "Train loss (w/o reg) on all data: 0.4012814\n",
      "Test loss (w/o reg) on all data: 0.32670072\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00044271105\n",
      "Norm of the params: 2.8570383\n",
      "                Loss: fixed  17 labels. Loss 0.32670. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [411] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54270977\n",
      "Train loss (w/o reg) on all data: 0.54261124\n",
      "Test loss (w/o reg) on all data: 0.39240348\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00051417004\n",
      "Norm of the params: 1.4039413\n",
      "              Random: fixed   3 labels. Loss 0.39240. Accuracy 0.822.\n",
      "### Flips: 50, rs: 9, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48124605\n",
      "Train loss (w/o reg) on all data: 0.48112106\n",
      "Test loss (w/o reg) on all data: 0.3480175\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0029388724\n",
      "Norm of the params: 1.5810466\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.34802. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34384245\n",
      "Train loss (w/o reg) on all data: 0.34339565\n",
      "Test loss (w/o reg) on all data: 0.34685364\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.002384254\n",
      "Norm of the params: 2.9892538\n",
      "                Loss: fixed  23 labels. Loss 0.34685. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [364] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5334266\n",
      "Train loss (w/o reg) on all data: 0.5333185\n",
      "Test loss (w/o reg) on all data: 0.39557675\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001709282\n",
      "Norm of the params: 1.4702988\n",
      "              Random: fixed   6 labels. Loss 0.39558. Accuracy 0.822.\n",
      "### Flips: 50, rs: 9, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4414029\n",
      "Train loss (w/o reg) on all data: 0.44122177\n",
      "Test loss (w/o reg) on all data: 0.33608705\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0033114369\n",
      "Norm of the params: 1.9033614\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.33609. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [134] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32497123\n",
      "Train loss (w/o reg) on all data: 0.32441193\n",
      "Test loss (w/o reg) on all data: 0.37095198\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.001617754\n",
      "Norm of the params: 3.344587\n",
      "                Loss: fixed  27 labels. Loss 0.37095. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [334] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5326157\n",
      "Train loss (w/o reg) on all data: 0.53253365\n",
      "Test loss (w/o reg) on all data: 0.38756743\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009478224\n",
      "Norm of the params: 1.2809998\n",
      "              Random: fixed   9 labels. Loss 0.38757. Accuracy 0.822.\n",
      "### Flips: 50, rs: 9, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [372] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4058635\n",
      "Train loss (w/o reg) on all data: 0.4056044\n",
      "Test loss (w/o reg) on all data: 0.34670165\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00074351416\n",
      "Norm of the params: 2.2763898\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.34670. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.30053642\n",
      "Train loss (w/o reg) on all data: 0.29993618\n",
      "Test loss (w/o reg) on all data: 0.36246076\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0003251554\n",
      "Norm of the params: 3.4647772\n",
      "                Loss: fixed  31 labels. Loss 0.36246. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [398] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5353568\n",
      "Train loss (w/o reg) on all data: 0.53529227\n",
      "Test loss (w/o reg) on all data: 0.40124992\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0025283543\n",
      "Norm of the params: 1.1363332\n",
      "              Random: fixed  11 labels. Loss 0.40125. Accuracy 0.822.\n",
      "### Flips: 50, rs: 9, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [387] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3840943\n",
      "Train loss (w/o reg) on all data: 0.38375023\n",
      "Test loss (w/o reg) on all data: 0.36650795\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0034476053\n",
      "Norm of the params: 2.6232708\n",
      "     Influence (LOO): fixed  30 labels. Loss 0.36651. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.29380664\n",
      "Train loss (w/o reg) on all data: 0.29319432\n",
      "Test loss (w/o reg) on all data: 0.3696075\n",
      "Train acc on all data:  0.8820754716981132\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0011224989\n",
      "Norm of the params: 3.499458\n",
      "                Loss: fixed  34 labels. Loss 0.36961. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52750653\n",
      "Train loss (w/o reg) on all data: 0.5274463\n",
      "Test loss (w/o reg) on all data: 0.40571332\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0037621593\n",
      "Norm of the params: 1.0970237\n",
      "              Random: fixed  13 labels. Loss 0.40571. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [258] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5140405\n",
      "Train loss (w/o reg) on all data: 0.5140049\n",
      "Test loss (w/o reg) on all data: 0.4344087\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00047032366\n",
      "Norm of the params: 0.8442468\n",
      "Flipped loss: 0.43441. Accuracy: 0.800\n",
      "### Flips: 50, rs: 10, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45641628\n",
      "Train loss (w/o reg) on all data: 0.4563363\n",
      "Test loss (w/o reg) on all data: 0.42076078\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0030745205\n",
      "Norm of the params: 1.2648445\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.42076. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4476755\n",
      "Train loss (w/o reg) on all data: 0.44759804\n",
      "Test loss (w/o reg) on all data: 0.4513126\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0030957107\n",
      "Norm of the params: 1.244547\n",
      "                Loss: fixed   6 labels. Loss 0.45131. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51084155\n",
      "Train loss (w/o reg) on all data: 0.5108035\n",
      "Test loss (w/o reg) on all data: 0.42582524\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00068053656\n",
      "Norm of the params: 0.8723607\n",
      "              Random: fixed   3 labels. Loss 0.42583. Accuracy 0.778.\n",
      "### Flips: 50, rs: 10, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43762335\n",
      "Train loss (w/o reg) on all data: 0.4375278\n",
      "Test loss (w/o reg) on all data: 0.40039024\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003961215\n",
      "Norm of the params: 1.3822669\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.40039. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [208] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36337858\n",
      "Train loss (w/o reg) on all data: 0.36324418\n",
      "Test loss (w/o reg) on all data: 0.45406833\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00085966813\n",
      "Norm of the params: 1.6395289\n",
      "                Loss: fixed  15 labels. Loss 0.45407. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [184] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5046933\n",
      "Train loss (w/o reg) on all data: 0.50464755\n",
      "Test loss (w/o reg) on all data: 0.40789834\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0026006347\n",
      "Norm of the params: 0.9567832\n",
      "              Random: fixed   5 labels. Loss 0.40790. Accuracy 0.800.\n",
      "### Flips: 50, rs: 10, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40640795\n",
      "Train loss (w/o reg) on all data: 0.40628648\n",
      "Test loss (w/o reg) on all data: 0.42655936\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013626654\n",
      "Norm of the params: 1.5586119\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.42656. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [118] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33342734\n",
      "Train loss (w/o reg) on all data: 0.3332802\n",
      "Test loss (w/o reg) on all data: 0.4645218\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00068139733\n",
      "Norm of the params: 1.7153409\n",
      "                Loss: fixed  20 labels. Loss 0.46452. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [102] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4954952\n",
      "Train loss (w/o reg) on all data: 0.49544472\n",
      "Test loss (w/o reg) on all data: 0.4054243\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00070248917\n",
      "Norm of the params: 1.0048189\n",
      "              Random: fixed   7 labels. Loss 0.40542. Accuracy 0.778.\n",
      "### Flips: 50, rs: 10, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39397112\n",
      "Train loss (w/o reg) on all data: 0.3938352\n",
      "Test loss (w/o reg) on all data: 0.4247998\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0008434211\n",
      "Norm of the params: 1.6488875\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.42480. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [171] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.30722055\n",
      "Train loss (w/o reg) on all data: 0.30700964\n",
      "Test loss (w/o reg) on all data: 0.50598854\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0025303878\n",
      "Norm of the params: 2.0537655\n",
      "                Loss: fixed  25 labels. Loss 0.50599. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [118] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4877091\n",
      "Train loss (w/o reg) on all data: 0.4876568\n",
      "Test loss (w/o reg) on all data: 0.42649648\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0020749979\n",
      "Norm of the params: 1.0227778\n",
      "              Random: fixed   9 labels. Loss 0.42650. Accuracy 0.778.\n",
      "### Flips: 50, rs: 10, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [306] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37468666\n",
      "Train loss (w/o reg) on all data: 0.37450016\n",
      "Test loss (w/o reg) on all data: 0.44267887\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0035722987\n",
      "Norm of the params: 1.9313318\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.44268. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [291] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.29595676\n",
      "Train loss (w/o reg) on all data: 0.29570994\n",
      "Test loss (w/o reg) on all data: 0.49734315\n",
      "Train acc on all data:  0.8867924528301887\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00062037085\n",
      "Norm of the params: 2.2217531\n",
      "                Loss: fixed  28 labels. Loss 0.49734. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46642792\n",
      "Train loss (w/o reg) on all data: 0.466375\n",
      "Test loss (w/o reg) on all data: 0.4322379\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0021680777\n",
      "Norm of the params: 1.0287582\n",
      "              Random: fixed  13 labels. Loss 0.43224. Accuracy 0.756.\n",
      "### Flips: 50, rs: 10, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [264] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.353999\n",
      "Train loss (w/o reg) on all data: 0.35378245\n",
      "Test loss (w/o reg) on all data: 0.44868702\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012210787\n",
      "Norm of the params: 2.0810232\n",
      "     Influence (LOO): fixed  32 labels. Loss 0.44869. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [99] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.27721053\n",
      "Train loss (w/o reg) on all data: 0.2768472\n",
      "Test loss (w/o reg) on all data: 0.44836834\n",
      "Train acc on all data:  0.8962264150943396\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015840796\n",
      "Norm of the params: 2.6955872\n",
      "                Loss: fixed  34 labels. Loss 0.44837. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [320] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44987902\n",
      "Train loss (w/o reg) on all data: 0.44980282\n",
      "Test loss (w/o reg) on all data: 0.41541368\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0057943193\n",
      "Norm of the params: 1.2346504\n",
      "              Random: fixed  17 labels. Loss 0.41541. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [208] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5737941\n",
      "Train loss (w/o reg) on all data: 0.5736894\n",
      "Test loss (w/o reg) on all data: 0.4722333\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0020654735\n",
      "Norm of the params: 1.4474186\n",
      "Flipped loss: 0.47223. Accuracy: 0.756\n",
      "### Flips: 50, rs: 11, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51043016\n",
      "Train loss (w/o reg) on all data: 0.51025206\n",
      "Test loss (w/o reg) on all data: 0.45556095\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015080293\n",
      "Norm of the params: 1.8871882\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.45556. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [118] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49029753\n",
      "Train loss (w/o reg) on all data: 0.49010283\n",
      "Test loss (w/o reg) on all data: 0.43164673\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002719789\n",
      "Norm of the params: 1.9732627\n",
      "                Loss: fixed   8 labels. Loss 0.43165. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [308] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57047236\n",
      "Train loss (w/o reg) on all data: 0.57036984\n",
      "Test loss (w/o reg) on all data: 0.47219646\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0033842402\n",
      "Norm of the params: 1.4320163\n",
      "              Random: fixed   2 labels. Loss 0.47220. Accuracy 0.800.\n",
      "### Flips: 50, rs: 11, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4774552\n",
      "Train loss (w/o reg) on all data: 0.47728348\n",
      "Test loss (w/o reg) on all data: 0.43481287\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012472508\n",
      "Norm of the params: 1.8532805\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.43481. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41759968\n",
      "Train loss (w/o reg) on all data: 0.4172672\n",
      "Test loss (w/o reg) on all data: 0.44439477\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013992478\n",
      "Norm of the params: 2.5786195\n",
      "                Loss: fixed  16 labels. Loss 0.44439. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54714555\n",
      "Train loss (w/o reg) on all data: 0.5470178\n",
      "Test loss (w/o reg) on all data: 0.46039528\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0026426832\n",
      "Norm of the params: 1.598351\n",
      "              Random: fixed   7 labels. Loss 0.46040. Accuracy 0.822.\n",
      "### Flips: 50, rs: 11, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [338] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45568892\n",
      "Train loss (w/o reg) on all data: 0.45546585\n",
      "Test loss (w/o reg) on all data: 0.4200788\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0064507043\n",
      "Norm of the params: 2.1122286\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.42008. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36189133\n",
      "Train loss (w/o reg) on all data: 0.36148524\n",
      "Test loss (w/o reg) on all data: 0.44978923\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0014817906\n",
      "Norm of the params: 2.8498683\n",
      "                Loss: fixed  22 labels. Loss 0.44979. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [272] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.522266\n",
      "Train loss (w/o reg) on all data: 0.5221205\n",
      "Test loss (w/o reg) on all data: 0.44688725\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0022923732\n",
      "Norm of the params: 1.7059982\n",
      "              Random: fixed  10 labels. Loss 0.44689. Accuracy 0.844.\n",
      "### Flips: 50, rs: 11, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44852948\n",
      "Train loss (w/o reg) on all data: 0.44829625\n",
      "Test loss (w/o reg) on all data: 0.43192038\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.007650812\n",
      "Norm of the params: 2.15975\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.43192. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [367] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33608097\n",
      "Train loss (w/o reg) on all data: 0.33559757\n",
      "Test loss (w/o reg) on all data: 0.44846377\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0032300856\n",
      "Norm of the params: 3.1092858\n",
      "                Loss: fixed  26 labels. Loss 0.44846. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [383] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5128542\n",
      "Train loss (w/o reg) on all data: 0.5127016\n",
      "Test loss (w/o reg) on all data: 0.4356836\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005353392\n",
      "Norm of the params: 1.74727\n",
      "              Random: fixed  12 labels. Loss 0.43568. Accuracy 0.822.\n",
      "### Flips: 50, rs: 11, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [345] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42708907\n",
      "Train loss (w/o reg) on all data: 0.42684707\n",
      "Test loss (w/o reg) on all data: 0.41999716\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00027281616\n",
      "Norm of the params: 2.1999223\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.42000. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32031754\n",
      "Train loss (w/o reg) on all data: 0.31973034\n",
      "Test loss (w/o reg) on all data: 0.4718835\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0005392731\n",
      "Norm of the params: 3.4269257\n",
      "                Loss: fixed  29 labels. Loss 0.47188. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [346] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5128547\n",
      "Train loss (w/o reg) on all data: 0.5127036\n",
      "Test loss (w/o reg) on all data: 0.43581653\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0075564445\n",
      "Norm of the params: 1.7383734\n",
      "              Random: fixed  12 labels. Loss 0.43582. Accuracy 0.822.\n",
      "### Flips: 50, rs: 11, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41383722\n",
      "Train loss (w/o reg) on all data: 0.4135579\n",
      "Test loss (w/o reg) on all data: 0.42124408\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003167333\n",
      "Norm of the params: 2.3636234\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.42124. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31314448\n",
      "Train loss (w/o reg) on all data: 0.31247678\n",
      "Test loss (w/o reg) on all data: 0.48185432\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0023243777\n",
      "Norm of the params: 3.6542857\n",
      "                Loss: fixed  31 labels. Loss 0.48185. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5101887\n",
      "Train loss (w/o reg) on all data: 0.5100279\n",
      "Test loss (w/o reg) on all data: 0.4397096\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022454897\n",
      "Norm of the params: 1.7934256\n",
      "              Random: fixed  15 labels. Loss 0.43971. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.622519\n",
      "Train loss (w/o reg) on all data: 0.62247723\n",
      "Test loss (w/o reg) on all data: 0.49513108\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003142966\n",
      "Norm of the params: 0.91434634\n",
      "Flipped loss: 0.49513. Accuracy: 0.800\n",
      "### Flips: 50, rs: 12, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [332] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58333486\n",
      "Train loss (w/o reg) on all data: 0.5832399\n",
      "Test loss (w/o reg) on all data: 0.4579001\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002186185\n",
      "Norm of the params: 1.3779603\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.45790. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5444781\n",
      "Train loss (w/o reg) on all data: 0.54436725\n",
      "Test loss (w/o reg) on all data: 0.4725915\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00071136554\n",
      "Norm of the params: 1.4891765\n",
      "                Loss: fixed   9 labels. Loss 0.47259. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6196556\n",
      "Train loss (w/o reg) on all data: 0.6196115\n",
      "Test loss (w/o reg) on all data: 0.48352173\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006312556\n",
      "Norm of the params: 0.9391823\n",
      "              Random: fixed   1 labels. Loss 0.48352. Accuracy 0.800.\n",
      "### Flips: 50, rs: 12, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5615443\n",
      "Train loss (w/o reg) on all data: 0.56144065\n",
      "Test loss (w/o reg) on all data: 0.44698697\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012960562\n",
      "Norm of the params: 1.4396989\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.44699. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47838184\n",
      "Train loss (w/o reg) on all data: 0.47814816\n",
      "Test loss (w/o reg) on all data: 0.46876645\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0007733775\n",
      "Norm of the params: 2.1618927\n",
      "                Loss: fixed  16 labels. Loss 0.46877. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [177] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60586494\n",
      "Train loss (w/o reg) on all data: 0.60581183\n",
      "Test loss (w/o reg) on all data: 0.48247817\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00080822146\n",
      "Norm of the params: 1.0303992\n",
      "              Random: fixed   5 labels. Loss 0.48248. Accuracy 0.778.\n",
      "### Flips: 50, rs: 12, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5324388\n",
      "Train loss (w/o reg) on all data: 0.53231233\n",
      "Test loss (w/o reg) on all data: 0.43932045\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0036772627\n",
      "Norm of the params: 1.5903848\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.43932. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42841485\n",
      "Train loss (w/o reg) on all data: 0.42807105\n",
      "Test loss (w/o reg) on all data: 0.4457174\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011488693\n",
      "Norm of the params: 2.622155\n",
      "                Loss: fixed  22 labels. Loss 0.44572. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5997064\n",
      "Train loss (w/o reg) on all data: 0.59964365\n",
      "Test loss (w/o reg) on all data: 0.48037857\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.008520094\n",
      "Norm of the params: 1.120478\n",
      "              Random: fixed   8 labels. Loss 0.48038. Accuracy 0.822.\n",
      "### Flips: 50, rs: 12, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [272] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5084253\n",
      "Train loss (w/o reg) on all data: 0.5082584\n",
      "Test loss (w/o reg) on all data: 0.42071503\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00078348845\n",
      "Norm of the params: 1.8268199\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.42072. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [375] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36496204\n",
      "Train loss (w/o reg) on all data: 0.36446586\n",
      "Test loss (w/o reg) on all data: 0.4633058\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0032339275\n",
      "Norm of the params: 3.150187\n",
      "                Loss: fixed  29 labels. Loss 0.46331. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [198] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.597842\n",
      "Train loss (w/o reg) on all data: 0.597782\n",
      "Test loss (w/o reg) on all data: 0.47643217\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.006151925\n",
      "Norm of the params: 1.0952914\n",
      "              Random: fixed   9 labels. Loss 0.47643. Accuracy 0.822.\n",
      "### Flips: 50, rs: 12, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [302] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47917584\n",
      "Train loss (w/o reg) on all data: 0.47897077\n",
      "Test loss (w/o reg) on all data: 0.41290283\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010470708\n",
      "Norm of the params: 2.0251253\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.41290. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [389] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33714455\n",
      "Train loss (w/o reg) on all data: 0.33667588\n",
      "Test loss (w/o reg) on all data: 0.46723637\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00056119094\n",
      "Norm of the params: 3.0616326\n",
      "                Loss: fixed  34 labels. Loss 0.46724. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [369] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5883735\n",
      "Train loss (w/o reg) on all data: 0.588302\n",
      "Test loss (w/o reg) on all data: 0.46952575\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0033271795\n",
      "Norm of the params: 1.1957494\n",
      "              Random: fixed  10 labels. Loss 0.46953. Accuracy 0.822.\n",
      "### Flips: 50, rs: 12, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [285] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44447684\n",
      "Train loss (w/o reg) on all data: 0.44424868\n",
      "Test loss (w/o reg) on all data: 0.3931666\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0019923158\n",
      "Norm of the params: 2.1361938\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.39317. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [186] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3315224\n",
      "Train loss (w/o reg) on all data: 0.33104527\n",
      "Test loss (w/o reg) on all data: 0.474544\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0029943166\n",
      "Norm of the params: 3.0891602\n",
      "                Loss: fixed  35 labels. Loss 0.47454. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [456] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5707693\n",
      "Train loss (w/o reg) on all data: 0.5706903\n",
      "Test loss (w/o reg) on all data: 0.43112648\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0016699702\n",
      "Norm of the params: 1.2573969\n",
      "              Random: fixed  15 labels. Loss 0.43113. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [107] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6060513\n",
      "Train loss (w/o reg) on all data: 0.6060096\n",
      "Test loss (w/o reg) on all data: 0.47528037\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0004925755\n",
      "Norm of the params: 0.9132731\n",
      "Flipped loss: 0.47528. Accuracy: 0.867\n",
      "### Flips: 50, rs: 13, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [171] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.542212\n",
      "Train loss (w/o reg) on all data: 0.54212046\n",
      "Test loss (w/o reg) on all data: 0.42897874\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0008146792\n",
      "Norm of the params: 1.3533654\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.42898. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [310] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5095856\n",
      "Train loss (w/o reg) on all data: 0.5094649\n",
      "Test loss (w/o reg) on all data: 0.41197658\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0033535578\n",
      "Norm of the params: 1.5535282\n",
      "                Loss: fixed  10 labels. Loss 0.41198. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.613831\n",
      "Train loss (w/o reg) on all data: 0.6137861\n",
      "Test loss (w/o reg) on all data: 0.47194722\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0010071048\n",
      "Norm of the params: 0.94726473\n",
      "              Random: fixed   3 labels. Loss 0.47195. Accuracy 0.867.\n",
      "### Flips: 50, rs: 13, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4999861\n",
      "Train loss (w/o reg) on all data: 0.49987355\n",
      "Test loss (w/o reg) on all data: 0.43666404\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013831984\n",
      "Norm of the params: 1.5004101\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.43666. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43433487\n",
      "Train loss (w/o reg) on all data: 0.43411586\n",
      "Test loss (w/o reg) on all data: 0.39993787\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.017256334\n",
      "Norm of the params: 2.0929956\n",
      "                Loss: fixed  17 labels. Loss 0.39994. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [366] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6138332\n",
      "Train loss (w/o reg) on all data: 0.61378944\n",
      "Test loss (w/o reg) on all data: 0.47292984\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0011409313\n",
      "Norm of the params: 0.93566096\n",
      "              Random: fixed   3 labels. Loss 0.47293. Accuracy 0.867.\n",
      "### Flips: 50, rs: 13, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [154] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4951758\n",
      "Train loss (w/o reg) on all data: 0.49506494\n",
      "Test loss (w/o reg) on all data: 0.42240274\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011277162\n",
      "Norm of the params: 1.4890481\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.42240. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [342] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3871825\n",
      "Train loss (w/o reg) on all data: 0.386828\n",
      "Test loss (w/o reg) on all data: 0.4158554\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0018632544\n",
      "Norm of the params: 2.6626801\n",
      "                Loss: fixed  22 labels. Loss 0.41586. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6112805\n",
      "Train loss (w/o reg) on all data: 0.6112361\n",
      "Test loss (w/o reg) on all data: 0.48130807\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0022203706\n",
      "Norm of the params: 0.94245285\n",
      "              Random: fixed   5 labels. Loss 0.48131. Accuracy 0.867.\n",
      "### Flips: 50, rs: 13, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [124] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5031788\n",
      "Train loss (w/o reg) on all data: 0.50307333\n",
      "Test loss (w/o reg) on all data: 0.3957134\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.004439475\n",
      "Norm of the params: 1.4522179\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.39571. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36408806\n",
      "Train loss (w/o reg) on all data: 0.36374924\n",
      "Test loss (w/o reg) on all data: 0.44703788\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0016194606\n",
      "Norm of the params: 2.6031842\n",
      "                Loss: fixed  26 labels. Loss 0.44704. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [327] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60915977\n",
      "Train loss (w/o reg) on all data: 0.60910976\n",
      "Test loss (w/o reg) on all data: 0.47266087\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.007015711\n",
      "Norm of the params: 0.9999284\n",
      "              Random: fixed   6 labels. Loss 0.47266. Accuracy 0.867.\n",
      "### Flips: 50, rs: 13, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [345] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45466468\n",
      "Train loss (w/o reg) on all data: 0.45449927\n",
      "Test loss (w/o reg) on all data: 0.37903684\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0022225142\n",
      "Norm of the params: 1.818734\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.37904. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [308] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.341866\n",
      "Train loss (w/o reg) on all data: 0.3414754\n",
      "Test loss (w/o reg) on all data: 0.42899883\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016535682\n",
      "Norm of the params: 2.7949216\n",
      "                Loss: fixed  30 labels. Loss 0.42900. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [369] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6089959\n",
      "Train loss (w/o reg) on all data: 0.60895306\n",
      "Test loss (w/o reg) on all data: 0.46379027\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.005028194\n",
      "Norm of the params: 0.9259623\n",
      "              Random: fixed   7 labels. Loss 0.46379. Accuracy 0.844.\n",
      "### Flips: 50, rs: 13, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [126] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42898196\n",
      "Train loss (w/o reg) on all data: 0.4287831\n",
      "Test loss (w/o reg) on all data: 0.36164543\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.003160907\n",
      "Norm of the params: 1.994352\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.36165. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3097554\n",
      "Train loss (w/o reg) on all data: 0.30930364\n",
      "Test loss (w/o reg) on all data: 0.44475067\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010451422\n",
      "Norm of the params: 3.0058954\n",
      "                Loss: fixed  35 labels. Loss 0.44475. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [458] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5956773\n",
      "Train loss (w/o reg) on all data: 0.5956258\n",
      "Test loss (w/o reg) on all data: 0.44656444\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.011252952\n",
      "Norm of the params: 1.0151367\n",
      "              Random: fixed  10 labels. Loss 0.44656. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.561146\n",
      "Train loss (w/o reg) on all data: 0.5610078\n",
      "Test loss (w/o reg) on all data: 0.45043018\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0021615373\n",
      "Norm of the params: 1.6625239\n",
      "Flipped loss: 0.45043. Accuracy: 0.800\n",
      "### Flips: 50, rs: 14, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [127] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5165703\n",
      "Train loss (w/o reg) on all data: 0.5163738\n",
      "Test loss (w/o reg) on all data: 0.42581326\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0034238952\n",
      "Norm of the params: 1.9825022\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.42581. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4711196\n",
      "Train loss (w/o reg) on all data: 0.47077388\n",
      "Test loss (w/o reg) on all data: 0.41974732\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0077438042\n",
      "Norm of the params: 2.6296072\n",
      "                Loss: fixed   8 labels. Loss 0.41975. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5530652\n",
      "Train loss (w/o reg) on all data: 0.55290335\n",
      "Test loss (w/o reg) on all data: 0.46716106\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.016842272\n",
      "Norm of the params: 1.7991401\n",
      "              Random: fixed   2 labels. Loss 0.46716. Accuracy 0.778.\n",
      "### Flips: 50, rs: 14, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46055955\n",
      "Train loss (w/o reg) on all data: 0.46025547\n",
      "Test loss (w/o reg) on all data: 0.39948604\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011455648\n",
      "Norm of the params: 2.466055\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.39949. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [125] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42227477\n",
      "Train loss (w/o reg) on all data: 0.42178106\n",
      "Test loss (w/o reg) on all data: 0.41131675\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005172592\n",
      "Norm of the params: 3.1423085\n",
      "                Loss: fixed  14 labels. Loss 0.41132. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5527019\n",
      "Train loss (w/o reg) on all data: 0.5525548\n",
      "Test loss (w/o reg) on all data: 0.4624962\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0041919025\n",
      "Norm of the params: 1.7153925\n",
      "              Random: fixed   3 labels. Loss 0.46250. Accuracy 0.822.\n",
      "### Flips: 50, rs: 14, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4319346\n",
      "Train loss (w/o reg) on all data: 0.43155628\n",
      "Test loss (w/o reg) on all data: 0.39715162\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002316493\n",
      "Norm of the params: 2.750654\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.39715. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [189] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38083214\n",
      "Train loss (w/o reg) on all data: 0.38017803\n",
      "Test loss (w/o reg) on all data: 0.40523118\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014053388\n",
      "Norm of the params: 3.6168907\n",
      "                Loss: fixed  20 labels. Loss 0.40523. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [290] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54295194\n",
      "Train loss (w/o reg) on all data: 0.54280704\n",
      "Test loss (w/o reg) on all data: 0.44786942\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018040335\n",
      "Norm of the params: 1.7023613\n",
      "              Random: fixed   5 labels. Loss 0.44787. Accuracy 0.822.\n",
      "### Flips: 50, rs: 14, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.408457\n",
      "Train loss (w/o reg) on all data: 0.40803203\n",
      "Test loss (w/o reg) on all data: 0.37956718\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010148975\n",
      "Norm of the params: 2.9154425\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.37957. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [150] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35166234\n",
      "Train loss (w/o reg) on all data: 0.35093307\n",
      "Test loss (w/o reg) on all data: 0.4083981\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0006554053\n",
      "Norm of the params: 3.819037\n",
      "                Loss: fixed  25 labels. Loss 0.40840. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.531658\n",
      "Train loss (w/o reg) on all data: 0.53152597\n",
      "Test loss (w/o reg) on all data: 0.43569934\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00125333\n",
      "Norm of the params: 1.6248523\n",
      "              Random: fixed   9 labels. Loss 0.43570. Accuracy 0.800.\n",
      "### Flips: 50, rs: 14, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [305] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39613438\n",
      "Train loss (w/o reg) on all data: 0.3957215\n",
      "Test loss (w/o reg) on all data: 0.3923108\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0025503372\n",
      "Norm of the params: 2.8736134\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.39231. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [321] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.335487\n",
      "Train loss (w/o reg) on all data: 0.3346549\n",
      "Test loss (w/o reg) on all data: 0.42464605\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0028638882\n",
      "Norm of the params: 4.0794764\n",
      "                Loss: fixed  29 labels. Loss 0.42465. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5288452\n",
      "Train loss (w/o reg) on all data: 0.52874064\n",
      "Test loss (w/o reg) on all data: 0.42960578\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.002485665\n",
      "Norm of the params: 1.4459074\n",
      "              Random: fixed  12 labels. Loss 0.42961. Accuracy 0.756.\n",
      "### Flips: 50, rs: 14, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [345] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37767902\n",
      "Train loss (w/o reg) on all data: 0.3771668\n",
      "Test loss (w/o reg) on all data: 0.4131475\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0024967154\n",
      "Norm of the params: 3.2006388\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.41315. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [317] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32059368\n",
      "Train loss (w/o reg) on all data: 0.319779\n",
      "Test loss (w/o reg) on all data: 0.44326052\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003078489\n",
      "Norm of the params: 4.0365357\n",
      "                Loss: fixed  32 labels. Loss 0.44326. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [404] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5200606\n",
      "Train loss (w/o reg) on all data: 0.5199564\n",
      "Test loss (w/o reg) on all data: 0.43252882\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0018766237\n",
      "Norm of the params: 1.4435208\n",
      "              Random: fixed  16 labels. Loss 0.43253. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6004036\n",
      "Train loss (w/o reg) on all data: 0.60035735\n",
      "Test loss (w/o reg) on all data: 0.50968915\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0044155535\n",
      "Norm of the params: 0.9617691\n",
      "Flipped loss: 0.50969. Accuracy: 0.756\n",
      "### Flips: 50, rs: 15, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5421941\n",
      "Train loss (w/o reg) on all data: 0.5420852\n",
      "Test loss (w/o reg) on all data: 0.493372\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0049603716\n",
      "Norm of the params: 1.476065\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.49337. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [155] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5027404\n",
      "Train loss (w/o reg) on all data: 0.50261074\n",
      "Test loss (w/o reg) on all data: 0.51450896\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0071162134\n",
      "Norm of the params: 1.6102718\n",
      "                Loss: fixed   9 labels. Loss 0.51451. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [269] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58976525\n",
      "Train loss (w/o reg) on all data: 0.58971775\n",
      "Test loss (w/o reg) on all data: 0.49267796\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00084570516\n",
      "Norm of the params: 0.97447586\n",
      "              Random: fixed   5 labels. Loss 0.49268. Accuracy 0.756.\n",
      "### Flips: 50, rs: 15, checks: 20\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50010526\n",
      "Train loss (w/o reg) on all data: 0.4999738\n",
      "Test loss (w/o reg) on all data: 0.4817224\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0015617693\n",
      "Norm of the params: 1.6214051\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.48172. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [165] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41821408\n",
      "Train loss (w/o reg) on all data: 0.41787842\n",
      "Test loss (w/o reg) on all data: 0.5435883\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.6888888888888889\n",
      "Norm of the mean of gradients: 0.0010284577\n",
      "Norm of the params: 2.5909553\n",
      "                Loss: fixed  17 labels. Loss 0.54359. Accuracy 0.689.\n",
      "Using normal model\n",
      "LBFGS training took [377] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5886189\n",
      "Train loss (w/o reg) on all data: 0.58856696\n",
      "Test loss (w/o reg) on all data: 0.48265958\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010845758\n",
      "Norm of the params: 1.0190079\n",
      "              Random: fixed   6 labels. Loss 0.48266. Accuracy 0.778.\n",
      "### Flips: 50, rs: 15, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46330002\n",
      "Train loss (w/o reg) on all data: 0.46313256\n",
      "Test loss (w/o reg) on all data: 0.48335037\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0019879688\n",
      "Norm of the params: 1.830099\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.48335. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [373] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36384323\n",
      "Train loss (w/o reg) on all data: 0.3634392\n",
      "Test loss (w/o reg) on all data: 0.5377323\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.005313652\n",
      "Norm of the params: 2.8426895\n",
      "                Loss: fixed  23 labels. Loss 0.53773. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [454] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5799991\n",
      "Train loss (w/o reg) on all data: 0.5799374\n",
      "Test loss (w/o reg) on all data: 0.4630593\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0047527766\n",
      "Norm of the params: 1.1105628\n",
      "              Random: fixed   7 labels. Loss 0.46306. Accuracy 0.778.\n",
      "### Flips: 50, rs: 15, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [347] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4487877\n",
      "Train loss (w/o reg) on all data: 0.44860974\n",
      "Test loss (w/o reg) on all data: 0.47782087\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0027796372\n",
      "Norm of the params: 1.8865014\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.47782. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33983845\n",
      "Train loss (w/o reg) on all data: 0.3394422\n",
      "Test loss (w/o reg) on all data: 0.5391687\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0032510383\n",
      "Norm of the params: 2.8151078\n",
      "                Loss: fixed  27 labels. Loss 0.53917. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [343] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5704672\n",
      "Train loss (w/o reg) on all data: 0.5704083\n",
      "Test loss (w/o reg) on all data: 0.43664744\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.008849687\n",
      "Norm of the params: 1.0850265\n",
      "              Random: fixed  11 labels. Loss 0.43665. Accuracy 0.800.\n",
      "### Flips: 50, rs: 15, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42924905\n",
      "Train loss (w/o reg) on all data: 0.42905304\n",
      "Test loss (w/o reg) on all data: 0.48553357\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0014033577\n",
      "Norm of the params: 1.979901\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.48553. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [189] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.317028\n",
      "Train loss (w/o reg) on all data: 0.3165788\n",
      "Test loss (w/o reg) on all data: 0.53022414\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0014734553\n",
      "Norm of the params: 2.9972541\n",
      "                Loss: fixed  31 labels. Loss 0.53022. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5620779\n",
      "Train loss (w/o reg) on all data: 0.56201243\n",
      "Test loss (w/o reg) on all data: 0.4143962\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0020289558\n",
      "Norm of the params: 1.1440704\n",
      "              Random: fixed  13 labels. Loss 0.41440. Accuracy 0.844.\n",
      "### Flips: 50, rs: 15, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [413] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42010716\n",
      "Train loss (w/o reg) on all data: 0.4199011\n",
      "Test loss (w/o reg) on all data: 0.47825202\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.000907542\n",
      "Norm of the params: 2.0300653\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.47825. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31525534\n",
      "Train loss (w/o reg) on all data: 0.31482443\n",
      "Test loss (w/o reg) on all data: 0.54986745\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0009981116\n",
      "Norm of the params: 2.9356358\n",
      "                Loss: fixed  33 labels. Loss 0.54987. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [382] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5453582\n",
      "Train loss (w/o reg) on all data: 0.54526645\n",
      "Test loss (w/o reg) on all data: 0.406137\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00045314833\n",
      "Norm of the params: 1.3545437\n",
      "              Random: fixed  16 labels. Loss 0.40614. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58468425\n",
      "Train loss (w/o reg) on all data: 0.5846239\n",
      "Test loss (w/o reg) on all data: 0.50197816\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.012170856\n",
      "Norm of the params: 1.0988513\n",
      "Flipped loss: 0.50198. Accuracy: 0.778\n",
      "### Flips: 50, rs: 16, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [155] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.538682\n",
      "Train loss (w/o reg) on all data: 0.53861076\n",
      "Test loss (w/o reg) on all data: 0.45284796\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0045933123\n",
      "Norm of the params: 1.1937666\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.45285. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5014065\n",
      "Train loss (w/o reg) on all data: 0.5012945\n",
      "Test loss (w/o reg) on all data: 0.44669807\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017603864\n",
      "Norm of the params: 1.4965743\n",
      "                Loss: fixed   8 labels. Loss 0.44670. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [160] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5846751\n",
      "Train loss (w/o reg) on all data: 0.58461475\n",
      "Test loss (w/o reg) on all data: 0.5022396\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00036510557\n",
      "Norm of the params: 1.0983695\n",
      "              Random: fixed   0 labels. Loss 0.50224. Accuracy 0.778.\n",
      "### Flips: 50, rs: 16, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51757085\n",
      "Train loss (w/o reg) on all data: 0.51749194\n",
      "Test loss (w/o reg) on all data: 0.40556252\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020185325\n",
      "Norm of the params: 1.2565523\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.40556. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [322] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.441325\n",
      "Train loss (w/o reg) on all data: 0.44113216\n",
      "Test loss (w/o reg) on all data: 0.4498073\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0067645377\n",
      "Norm of the params: 1.9639382\n",
      "                Loss: fixed  15 labels. Loss 0.44981. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5833822\n",
      "Train loss (w/o reg) on all data: 0.5833276\n",
      "Test loss (w/o reg) on all data: 0.49967277\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0029888703\n",
      "Norm of the params: 1.0447204\n",
      "              Random: fixed   2 labels. Loss 0.49967. Accuracy 0.778.\n",
      "### Flips: 50, rs: 16, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [279] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47152996\n",
      "Train loss (w/o reg) on all data: 0.4714156\n",
      "Test loss (w/o reg) on all data: 0.39341754\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008767169\n",
      "Norm of the params: 1.5123703\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.39342. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41890633\n",
      "Train loss (w/o reg) on all data: 0.41872212\n",
      "Test loss (w/o reg) on all data: 0.44810453\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.000883547\n",
      "Norm of the params: 1.9194851\n",
      "                Loss: fixed  18 labels. Loss 0.44810. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [343] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5749253\n",
      "Train loss (w/o reg) on all data: 0.5748633\n",
      "Test loss (w/o reg) on all data: 0.48628628\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0052425237\n",
      "Norm of the params: 1.1131873\n",
      "              Random: fixed   3 labels. Loss 0.48629. Accuracy 0.778.\n",
      "### Flips: 50, rs: 16, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [360] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4531332\n",
      "Train loss (w/o reg) on all data: 0.45298767\n",
      "Test loss (w/o reg) on all data: 0.42089123\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010994232\n",
      "Norm of the params: 1.7059562\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.42089. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [266] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3620797\n",
      "Train loss (w/o reg) on all data: 0.3617555\n",
      "Test loss (w/o reg) on all data: 0.4028205\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0049288655\n",
      "Norm of the params: 2.5464034\n",
      "                Loss: fixed  25 labels. Loss 0.40282. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [345] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5751621\n",
      "Train loss (w/o reg) on all data: 0.57510144\n",
      "Test loss (w/o reg) on all data: 0.47398746\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002030371\n",
      "Norm of the params: 1.1016963\n",
      "              Random: fixed   5 labels. Loss 0.47399. Accuracy 0.800.\n",
      "### Flips: 50, rs: 16, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [343] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41817483\n",
      "Train loss (w/o reg) on all data: 0.41798458\n",
      "Test loss (w/o reg) on all data: 0.44342962\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001040428\n",
      "Norm of the params: 1.9507185\n",
      "     Influence (LOO): fixed  28 labels. Loss 0.44343. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [340] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33671877\n",
      "Train loss (w/o reg) on all data: 0.33631155\n",
      "Test loss (w/o reg) on all data: 0.39348504\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0036204217\n",
      "Norm of the params: 2.853796\n",
      "                Loss: fixed  30 labels. Loss 0.39349. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [434] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5648469\n",
      "Train loss (w/o reg) on all data: 0.5647807\n",
      "Test loss (w/o reg) on all data: 0.46819574\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017671038\n",
      "Norm of the params: 1.1504923\n",
      "              Random: fixed   7 labels. Loss 0.46820. Accuracy 0.800.\n",
      "### Flips: 50, rs: 16, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [286] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40973586\n",
      "Train loss (w/o reg) on all data: 0.40956095\n",
      "Test loss (w/o reg) on all data: 0.42961702\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002008343\n",
      "Norm of the params: 1.8703125\n",
      "     Influence (LOO): fixed  30 labels. Loss 0.42962. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [458] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.338903\n",
      "Train loss (w/o reg) on all data: 0.3384939\n",
      "Test loss (w/o reg) on all data: 0.4006763\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005530507\n",
      "Norm of the params: 2.860445\n",
      "                Loss: fixed  33 labels. Loss 0.40068. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56058985\n",
      "Train loss (w/o reg) on all data: 0.56051487\n",
      "Test loss (w/o reg) on all data: 0.45426357\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019052663\n",
      "Norm of the params: 1.2248116\n",
      "              Random: fixed   8 labels. Loss 0.45426. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [144] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58994293\n",
      "Train loss (w/o reg) on all data: 0.5898843\n",
      "Test loss (w/o reg) on all data: 0.41913536\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0026914966\n",
      "Norm of the params: 1.0832258\n",
      "Flipped loss: 0.41914. Accuracy: 0.844\n",
      "### Flips: 50, rs: 17, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5506964\n",
      "Train loss (w/o reg) on all data: 0.55058885\n",
      "Test loss (w/o reg) on all data: 0.37806866\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0019366117\n",
      "Norm of the params: 1.4664478\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.37807. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [152] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5154038\n",
      "Train loss (w/o reg) on all data: 0.515266\n",
      "Test loss (w/o reg) on all data: 0.37753195\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003178878\n",
      "Norm of the params: 1.6602333\n",
      "                Loss: fixed   9 labels. Loss 0.37753. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58277875\n",
      "Train loss (w/o reg) on all data: 0.58272773\n",
      "Test loss (w/o reg) on all data: 0.42227036\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0038371123\n",
      "Norm of the params: 1.0098968\n",
      "              Random: fixed   3 labels. Loss 0.42227. Accuracy 0.822.\n",
      "### Flips: 50, rs: 17, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [345] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5301827\n",
      "Train loss (w/o reg) on all data: 0.5300705\n",
      "Test loss (w/o reg) on all data: 0.37364033\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.000567852\n",
      "Norm of the params: 1.4983103\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.37364. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4803365\n",
      "Train loss (w/o reg) on all data: 0.48016027\n",
      "Test loss (w/o reg) on all data: 0.38177708\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006174525\n",
      "Norm of the params: 1.8774105\n",
      "                Loss: fixed  14 labels. Loss 0.38178. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5652332\n",
      "Train loss (w/o reg) on all data: 0.5651648\n",
      "Test loss (w/o reg) on all data: 0.40746215\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002006427\n",
      "Norm of the params: 1.1694378\n",
      "              Random: fixed   6 labels. Loss 0.40746. Accuracy 0.822.\n",
      "### Flips: 50, rs: 17, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51459336\n",
      "Train loss (w/o reg) on all data: 0.5144528\n",
      "Test loss (w/o reg) on all data: 0.37725914\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00095091684\n",
      "Norm of the params: 1.676456\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.37726. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4414471\n",
      "Train loss (w/o reg) on all data: 0.44121766\n",
      "Test loss (w/o reg) on all data: 0.3809946\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003026502\n",
      "Norm of the params: 2.142124\n",
      "                Loss: fixed  20 labels. Loss 0.38099. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5639495\n",
      "Train loss (w/o reg) on all data: 0.56388116\n",
      "Test loss (w/o reg) on all data: 0.41103792\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0026357789\n",
      "Norm of the params: 1.1693299\n",
      "              Random: fixed   8 labels. Loss 0.41104. Accuracy 0.822.\n",
      "### Flips: 50, rs: 17, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [186] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49306336\n",
      "Train loss (w/o reg) on all data: 0.49289346\n",
      "Test loss (w/o reg) on all data: 0.37436926\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0006161283\n",
      "Norm of the params: 1.8433535\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.37437. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [240] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41185945\n",
      "Train loss (w/o reg) on all data: 0.4115897\n",
      "Test loss (w/o reg) on all data: 0.40172228\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0023215807\n",
      "Norm of the params: 2.3226173\n",
      "                Loss: fixed  25 labels. Loss 0.40172. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [198] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54877514\n",
      "Train loss (w/o reg) on all data: 0.54869777\n",
      "Test loss (w/o reg) on all data: 0.41453642\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0026157897\n",
      "Norm of the params: 1.2437352\n",
      "              Random: fixed  10 labels. Loss 0.41454. Accuracy 0.822.\n",
      "### Flips: 50, rs: 17, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48728845\n",
      "Train loss (w/o reg) on all data: 0.48711756\n",
      "Test loss (w/o reg) on all data: 0.3536786\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.003072495\n",
      "Norm of the params: 1.8487753\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.35368. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [283] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37951946\n",
      "Train loss (w/o reg) on all data: 0.37917393\n",
      "Test loss (w/o reg) on all data: 0.39165056\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0020907377\n",
      "Norm of the params: 2.6288419\n",
      "                Loss: fixed  31 labels. Loss 0.39165. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.520852\n",
      "Train loss (w/o reg) on all data: 0.52073836\n",
      "Test loss (w/o reg) on all data: 0.40791842\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0030625626\n",
      "Norm of the params: 1.5079299\n",
      "              Random: fixed  16 labels. Loss 0.40792. Accuracy 0.822.\n",
      "### Flips: 50, rs: 17, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4820459\n",
      "Train loss (w/o reg) on all data: 0.48190033\n",
      "Test loss (w/o reg) on all data: 0.3470007\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0012405273\n",
      "Norm of the params: 1.7062728\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.34700. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [328] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36642408\n",
      "Train loss (w/o reg) on all data: 0.36604285\n",
      "Test loss (w/o reg) on all data: 0.38532278\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0004658808\n",
      "Norm of the params: 2.7612557\n",
      "                Loss: fixed  35 labels. Loss 0.38532. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5083341\n",
      "Train loss (w/o reg) on all data: 0.5082054\n",
      "Test loss (w/o reg) on all data: 0.39136535\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009980616\n",
      "Norm of the params: 1.6042435\n",
      "              Random: fixed  18 labels. Loss 0.39137. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5978367\n",
      "Train loss (w/o reg) on all data: 0.5977786\n",
      "Test loss (w/o reg) on all data: 0.47314093\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003972876\n",
      "Norm of the params: 1.0774064\n",
      "Flipped loss: 0.47314. Accuracy: 0.822\n",
      "### Flips: 50, rs: 18, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55376667\n",
      "Train loss (w/o reg) on all data: 0.55367875\n",
      "Test loss (w/o reg) on all data: 0.46177214\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.001625491\n",
      "Norm of the params: 1.32621\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.46177. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [201] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53042376\n",
      "Train loss (w/o reg) on all data: 0.5302785\n",
      "Test loss (w/o reg) on all data: 0.42530218\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012715849\n",
      "Norm of the params: 1.7044173\n",
      "                Loss: fixed   7 labels. Loss 0.42530. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [252] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59000474\n",
      "Train loss (w/o reg) on all data: 0.58993965\n",
      "Test loss (w/o reg) on all data: 0.46492666\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00063997344\n",
      "Norm of the params: 1.1408981\n",
      "              Random: fixed   2 labels. Loss 0.46493. Accuracy 0.822.\n",
      "### Flips: 50, rs: 18, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5437265\n",
      "Train loss (w/o reg) on all data: 0.5436527\n",
      "Test loss (w/o reg) on all data: 0.4441753\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0030790975\n",
      "Norm of the params: 1.2146595\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.44418. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4589522\n",
      "Train loss (w/o reg) on all data: 0.45875147\n",
      "Test loss (w/o reg) on all data: 0.3945783\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0021832287\n",
      "Norm of the params: 2.0035691\n",
      "                Loss: fixed  16 labels. Loss 0.39458. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [289] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57859665\n",
      "Train loss (w/o reg) on all data: 0.57851577\n",
      "Test loss (w/o reg) on all data: 0.4473069\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012382058\n",
      "Norm of the params: 1.2717017\n",
      "              Random: fixed   6 labels. Loss 0.44731. Accuracy 0.822.\n",
      "### Flips: 50, rs: 18, checks: 30\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5224498\n",
      "Train loss (w/o reg) on all data: 0.5223648\n",
      "Test loss (w/o reg) on all data: 0.4501635\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0049283043\n",
      "Norm of the params: 1.303899\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.45016. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [193] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4213836\n",
      "Train loss (w/o reg) on all data: 0.42112547\n",
      "Test loss (w/o reg) on all data: 0.39059526\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014107315\n",
      "Norm of the params: 2.272072\n",
      "                Loss: fixed  21 labels. Loss 0.39060. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [305] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56815463\n",
      "Train loss (w/o reg) on all data: 0.5680815\n",
      "Test loss (w/o reg) on all data: 0.44800174\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0028586672\n",
      "Norm of the params: 1.2092582\n",
      "              Random: fixed   8 labels. Loss 0.44800. Accuracy 0.800.\n",
      "### Flips: 50, rs: 18, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [289] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47378346\n",
      "Train loss (w/o reg) on all data: 0.47365236\n",
      "Test loss (w/o reg) on all data: 0.43187833\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00063676265\n",
      "Norm of the params: 1.619257\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.43188. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3770874\n",
      "Train loss (w/o reg) on all data: 0.37678984\n",
      "Test loss (w/o reg) on all data: 0.41578674\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0043596663\n",
      "Norm of the params: 2.4395685\n",
      "                Loss: fixed  28 labels. Loss 0.41579. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [355] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5681441\n",
      "Train loss (w/o reg) on all data: 0.56807107\n",
      "Test loss (w/o reg) on all data: 0.4480106\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0041456358\n",
      "Norm of the params: 1.2086308\n",
      "              Random: fixed   8 labels. Loss 0.44801. Accuracy 0.800.\n",
      "### Flips: 50, rs: 18, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [362] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46757042\n",
      "Train loss (w/o reg) on all data: 0.46743745\n",
      "Test loss (w/o reg) on all data: 0.4214106\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019741496\n",
      "Norm of the params: 1.6308521\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.42141. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35963842\n",
      "Train loss (w/o reg) on all data: 0.3593304\n",
      "Test loss (w/o reg) on all data: 0.42240244\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005261953\n",
      "Norm of the params: 2.4820333\n",
      "                Loss: fixed  32 labels. Loss 0.42240. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.566506\n",
      "Train loss (w/o reg) on all data: 0.566434\n",
      "Test loss (w/o reg) on all data: 0.44455636\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010691899\n",
      "Norm of the params: 1.1997775\n",
      "              Random: fixed   9 labels. Loss 0.44456. Accuracy 0.822.\n",
      "### Flips: 50, rs: 18, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42067617\n",
      "Train loss (w/o reg) on all data: 0.42047852\n",
      "Test loss (w/o reg) on all data: 0.41550773\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0024655114\n",
      "Norm of the params: 1.9882374\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.41551. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [310] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34160545\n",
      "Train loss (w/o reg) on all data: 0.34129444\n",
      "Test loss (w/o reg) on all data: 0.45925793\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00048503384\n",
      "Norm of the params: 2.4940197\n",
      "                Loss: fixed  36 labels. Loss 0.45926. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [318] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5499947\n",
      "Train loss (w/o reg) on all data: 0.54991263\n",
      "Test loss (w/o reg) on all data: 0.4269055\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0058415835\n",
      "Norm of the params: 1.2812365\n",
      "              Random: fixed  11 labels. Loss 0.42691. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54063404\n",
      "Train loss (w/o reg) on all data: 0.5404874\n",
      "Test loss (w/o reg) on all data: 0.45514756\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.005186981\n",
      "Norm of the params: 1.7123595\n",
      "Flipped loss: 0.45515. Accuracy: 0.800\n",
      "### Flips: 50, rs: 19, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [199] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50841147\n",
      "Train loss (w/o reg) on all data: 0.50819457\n",
      "Test loss (w/o reg) on all data: 0.4414803\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0023752516\n",
      "Norm of the params: 2.0828936\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.44148. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [77] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49155784\n",
      "Train loss (w/o reg) on all data: 0.49132273\n",
      "Test loss (w/o reg) on all data: 0.44467518\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00106887\n",
      "Norm of the params: 2.1684282\n",
      "                Loss: fixed   5 labels. Loss 0.44468. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52252513\n",
      "Train loss (w/o reg) on all data: 0.52236235\n",
      "Test loss (w/o reg) on all data: 0.441731\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.005519056\n",
      "Norm of the params: 1.804316\n",
      "              Random: fixed   3 labels. Loss 0.44173. Accuracy 0.844.\n",
      "### Flips: 50, rs: 19, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49828476\n",
      "Train loss (w/o reg) on all data: 0.49803182\n",
      "Test loss (w/o reg) on all data: 0.43027413\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010584045\n",
      "Norm of the params: 2.2491927\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.43027. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [360] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42964178\n",
      "Train loss (w/o reg) on all data: 0.42922932\n",
      "Test loss (w/o reg) on all data: 0.41797727\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00048753776\n",
      "Norm of the params: 2.8721132\n",
      "                Loss: fixed  13 labels. Loss 0.41798. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5186653\n",
      "Train loss (w/o reg) on all data: 0.51848847\n",
      "Test loss (w/o reg) on all data: 0.43748474\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0038461208\n",
      "Norm of the params: 1.8806815\n",
      "              Random: fixed   4 labels. Loss 0.43748. Accuracy 0.844.\n",
      "### Flips: 50, rs: 19, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [321] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45978078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss (w/o reg) on all data: 0.459455\n",
      "Test loss (w/o reg) on all data: 0.45907784\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0031474675\n",
      "Norm of the params: 2.5524879\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.45908. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38915616\n",
      "Train loss (w/o reg) on all data: 0.3886483\n",
      "Test loss (w/o reg) on all data: 0.44352287\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.006351652\n",
      "Norm of the params: 3.1870458\n",
      "                Loss: fixed  19 labels. Loss 0.44352. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5198902\n",
      "Train loss (w/o reg) on all data: 0.5197227\n",
      "Test loss (w/o reg) on all data: 0.44589564\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002490038\n",
      "Norm of the params: 1.8303541\n",
      "              Random: fixed   7 labels. Loss 0.44590. Accuracy 0.844.\n",
      "### Flips: 50, rs: 19, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [357] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44201612\n",
      "Train loss (w/o reg) on all data: 0.4416608\n",
      "Test loss (w/o reg) on all data: 0.44140303\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004568489\n",
      "Norm of the params: 2.6658485\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.44140. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [385] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3636009\n",
      "Train loss (w/o reg) on all data: 0.36301458\n",
      "Test loss (w/o reg) on all data: 0.41107422\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0007178218\n",
      "Norm of the params: 3.4244115\n",
      "                Loss: fixed  23 labels. Loss 0.41107. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50184894\n",
      "Train loss (w/o reg) on all data: 0.5016274\n",
      "Test loss (w/o reg) on all data: 0.43172687\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0036673103\n",
      "Norm of the params: 2.1048658\n",
      "              Random: fixed  10 labels. Loss 0.43173. Accuracy 0.844.\n",
      "### Flips: 50, rs: 19, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43733943\n",
      "Train loss (w/o reg) on all data: 0.43704742\n",
      "Test loss (w/o reg) on all data: 0.43754047\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013047826\n",
      "Norm of the params: 2.416574\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.43754. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34628695\n",
      "Train loss (w/o reg) on all data: 0.34571302\n",
      "Test loss (w/o reg) on all data: 0.40857708\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0025467863\n",
      "Norm of the params: 3.3879797\n",
      "                Loss: fixed  27 labels. Loss 0.40858. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [306] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5010972\n",
      "Train loss (w/o reg) on all data: 0.50086683\n",
      "Test loss (w/o reg) on all data: 0.4077077\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001256005\n",
      "Norm of the params: 2.146603\n",
      "              Random: fixed  12 labels. Loss 0.40771. Accuracy 0.844.\n",
      "### Flips: 50, rs: 19, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4060128\n",
      "Train loss (w/o reg) on all data: 0.40565324\n",
      "Test loss (w/o reg) on all data: 0.41787046\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0030098157\n",
      "Norm of the params: 2.681626\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.41787. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [386] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32667065\n",
      "Train loss (w/o reg) on all data: 0.32599282\n",
      "Test loss (w/o reg) on all data: 0.38423407\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004476392\n",
      "Norm of the params: 3.6818728\n",
      "                Loss: fixed  31 labels. Loss 0.38423. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [407] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5026793\n",
      "Train loss (w/o reg) on all data: 0.5024709\n",
      "Test loss (w/o reg) on all data: 0.40157536\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014293862\n",
      "Norm of the params: 2.0415258\n",
      "              Random: fixed  13 labels. Loss 0.40158. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6147116\n",
      "Train loss (w/o reg) on all data: 0.614664\n",
      "Test loss (w/o reg) on all data: 0.5157228\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0035411955\n",
      "Norm of the params: 0.97550195\n",
      "Flipped loss: 0.51572. Accuracy: 0.756\n",
      "### Flips: 50, rs: 20, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [144] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59720993\n",
      "Train loss (w/o reg) on all data: 0.5971601\n",
      "Test loss (w/o reg) on all data: 0.5004906\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001832545\n",
      "Norm of the params: 0.9982116\n",
      "     Influence (LOO): fixed   2 labels. Loss 0.50049. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52627707\n",
      "Train loss (w/o reg) on all data: 0.5261796\n",
      "Test loss (w/o reg) on all data: 0.4340021\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0013086604\n",
      "Norm of the params: 1.3961405\n",
      "                Loss: fixed  10 labels. Loss 0.43400. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60016793\n",
      "Train loss (w/o reg) on all data: 0.6001218\n",
      "Test loss (w/o reg) on all data: 0.51809806\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00064816524\n",
      "Norm of the params: 0.96036965\n",
      "              Random: fixed   3 labels. Loss 0.51810. Accuracy 0.778.\n",
      "### Flips: 50, rs: 20, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56569934\n",
      "Train loss (w/o reg) on all data: 0.56563216\n",
      "Test loss (w/o reg) on all data: 0.45069847\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0035331005\n",
      "Norm of the params: 1.1591645\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.45070. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45015097\n",
      "Train loss (w/o reg) on all data: 0.45001054\n",
      "Test loss (w/o reg) on all data: 0.4546769\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0006265442\n",
      "Norm of the params: 1.675873\n",
      "                Loss: fixed  18 labels. Loss 0.45468. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [226] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6013199\n",
      "Train loss (w/o reg) on all data: 0.6012847\n",
      "Test loss (w/o reg) on all data: 0.523009\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0064974204\n",
      "Norm of the params: 0.8395532\n",
      "              Random: fixed   5 labels. Loss 0.52301. Accuracy 0.756.\n",
      "### Flips: 50, rs: 20, checks: 30\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [125] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5596109\n",
      "Train loss (w/o reg) on all data: 0.5595451\n",
      "Test loss (w/o reg) on all data: 0.44510642\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.01681161\n",
      "Norm of the params: 1.1471123\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.44511. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [283] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42328107\n",
      "Train loss (w/o reg) on all data: 0.42309728\n",
      "Test loss (w/o reg) on all data: 0.44585207\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010654484\n",
      "Norm of the params: 1.9171684\n",
      "                Loss: fixed  21 labels. Loss 0.44585. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5995817\n",
      "Train loss (w/o reg) on all data: 0.5995418\n",
      "Test loss (w/o reg) on all data: 0.53434294\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0034977628\n",
      "Norm of the params: 0.8934301\n",
      "              Random: fixed   8 labels. Loss 0.53434. Accuracy 0.756.\n",
      "### Flips: 50, rs: 20, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [184] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5334784\n",
      "Train loss (w/o reg) on all data: 0.53340536\n",
      "Test loss (w/o reg) on all data: 0.43275866\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0033585187\n",
      "Norm of the params: 1.20829\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.43276. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37325922\n",
      "Train loss (w/o reg) on all data: 0.37308335\n",
      "Test loss (w/o reg) on all data: 0.48615408\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.008754335\n",
      "Norm of the params: 1.8754102\n",
      "                Loss: fixed  27 labels. Loss 0.48615. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5908073\n",
      "Train loss (w/o reg) on all data: 0.5907652\n",
      "Test loss (w/o reg) on all data: 0.53752583\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0019792554\n",
      "Norm of the params: 0.91778314\n",
      "              Random: fixed   9 labels. Loss 0.53753. Accuracy 0.756.\n",
      "### Flips: 50, rs: 20, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48387513\n",
      "Train loss (w/o reg) on all data: 0.4837529\n",
      "Test loss (w/o reg) on all data: 0.39600295\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.000547369\n",
      "Norm of the params: 1.5634687\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.39600. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [171] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3508422\n",
      "Train loss (w/o reg) on all data: 0.35059518\n",
      "Test loss (w/o reg) on all data: 0.49901283\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.005416378\n",
      "Norm of the params: 2.222775\n",
      "                Loss: fixed  31 labels. Loss 0.49901. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5837818\n",
      "Train loss (w/o reg) on all data: 0.5837339\n",
      "Test loss (w/o reg) on all data: 0.50249696\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0010788741\n",
      "Norm of the params: 0.97812384\n",
      "              Random: fixed  12 labels. Loss 0.50250. Accuracy 0.756.\n",
      "### Flips: 50, rs: 20, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [295] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43338957\n",
      "Train loss (w/o reg) on all data: 0.4331966\n",
      "Test loss (w/o reg) on all data: 0.40860075\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00062686554\n",
      "Norm of the params: 1.9644728\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.40860. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35040045\n",
      "Train loss (w/o reg) on all data: 0.3501382\n",
      "Test loss (w/o reg) on all data: 0.49555364\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.003891863\n",
      "Norm of the params: 2.290192\n",
      "                Loss: fixed  32 labels. Loss 0.49555. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55982053\n",
      "Train loss (w/o reg) on all data: 0.5597602\n",
      "Test loss (w/o reg) on all data: 0.47697878\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0019473484\n",
      "Norm of the params: 1.0983406\n",
      "              Random: fixed  17 labels. Loss 0.47698. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.62285006\n",
      "Train loss (w/o reg) on all data: 0.6227934\n",
      "Test loss (w/o reg) on all data: 0.47044075\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015352318\n",
      "Norm of the params: 1.064832\n",
      "Flipped loss: 0.47044. Accuracy: 0.800\n",
      "### Flips: 50, rs: 21, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [144] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59654516\n",
      "Train loss (w/o reg) on all data: 0.5964961\n",
      "Test loss (w/o reg) on all data: 0.4428682\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.005086983\n",
      "Norm of the params: 0.9902415\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.44287. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [184] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55487496\n",
      "Train loss (w/o reg) on all data: 0.5547473\n",
      "Test loss (w/o reg) on all data: 0.41149762\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003723275\n",
      "Norm of the params: 1.5980881\n",
      "                Loss: fixed   8 labels. Loss 0.41150. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [112] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6223402\n",
      "Train loss (w/o reg) on all data: 0.62228674\n",
      "Test loss (w/o reg) on all data: 0.45999324\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022208807\n",
      "Norm of the params: 1.0338024\n",
      "              Random: fixed   1 labels. Loss 0.45999. Accuracy 0.800.\n",
      "### Flips: 50, rs: 21, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59063464\n",
      "Train loss (w/o reg) on all data: 0.5905849\n",
      "Test loss (w/o reg) on all data: 0.43905056\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002577363\n",
      "Norm of the params: 0.99763477\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.43905. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5001904\n",
      "Train loss (w/o reg) on all data: 0.49999145\n",
      "Test loss (w/o reg) on all data: 0.39654046\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014180834\n",
      "Norm of the params: 1.9945682\n",
      "                Loss: fixed  15 labels. Loss 0.39654. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [307] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61672235\n",
      "Train loss (w/o reg) on all data: 0.61666894\n",
      "Test loss (w/o reg) on all data: 0.43613333\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002043219\n",
      "Norm of the params: 1.0334736\n",
      "              Random: fixed   4 labels. Loss 0.43613. Accuracy 0.800.\n",
      "### Flips: 50, rs: 21, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5636859\n",
      "Train loss (w/o reg) on all data: 0.56362164\n",
      "Test loss (w/o reg) on all data: 0.43670192\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0023527811\n",
      "Norm of the params: 1.1337825\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.43670. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [362] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44375685\n",
      "Train loss (w/o reg) on all data: 0.4434685\n",
      "Test loss (w/o reg) on all data: 0.36274937\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010338596\n",
      "Norm of the params: 2.4014611\n",
      "                Loss: fixed  22 labels. Loss 0.36275. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61148363\n",
      "Train loss (w/o reg) on all data: 0.61142427\n",
      "Test loss (w/o reg) on all data: 0.43177113\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0008000364\n",
      "Norm of the params: 1.0896108\n",
      "              Random: fixed   6 labels. Loss 0.43177. Accuracy 0.778.\n",
      "### Flips: 50, rs: 21, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5484828\n",
      "Train loss (w/o reg) on all data: 0.5484055\n",
      "Test loss (w/o reg) on all data: 0.40203935\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016564128\n",
      "Norm of the params: 1.2430363\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.40204. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [359] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42207152\n",
      "Train loss (w/o reg) on all data: 0.42177892\n",
      "Test loss (w/o reg) on all data: 0.36453176\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012312125\n",
      "Norm of the params: 2.419098\n",
      "                Loss: fixed  25 labels. Loss 0.36453. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [328] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6029387\n",
      "Train loss (w/o reg) on all data: 0.6028719\n",
      "Test loss (w/o reg) on all data: 0.42918816\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0057298774\n",
      "Norm of the params: 1.156116\n",
      "              Random: fixed   9 labels. Loss 0.42919. Accuracy 0.822.\n",
      "### Flips: 50, rs: 21, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.525655\n",
      "Train loss (w/o reg) on all data: 0.5255622\n",
      "Test loss (w/o reg) on all data: 0.39701954\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.006892133\n",
      "Norm of the params: 1.3619825\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.39702. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [302] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3678723\n",
      "Train loss (w/o reg) on all data: 0.3675313\n",
      "Test loss (w/o reg) on all data: 0.3979522\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00034585854\n",
      "Norm of the params: 2.6115582\n",
      "                Loss: fixed  33 labels. Loss 0.39795. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6029444\n",
      "Train loss (w/o reg) on all data: 0.60287786\n",
      "Test loss (w/o reg) on all data: 0.42915428\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0022716047\n",
      "Norm of the params: 1.1535767\n",
      "              Random: fixed   9 labels. Loss 0.42915. Accuracy 0.822.\n",
      "### Flips: 50, rs: 21, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.511994\n",
      "Train loss (w/o reg) on all data: 0.5118855\n",
      "Test loss (w/o reg) on all data: 0.3957489\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.008414928\n",
      "Norm of the params: 1.4728373\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.39575. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36269632\n",
      "Train loss (w/o reg) on all data: 0.3623262\n",
      "Test loss (w/o reg) on all data: 0.40409252\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0006061651\n",
      "Norm of the params: 2.7206995\n",
      "                Loss: fixed  34 labels. Loss 0.40409. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5994806\n",
      "Train loss (w/o reg) on all data: 0.5994085\n",
      "Test loss (w/o reg) on all data: 0.42123467\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0024822135\n",
      "Norm of the params: 1.2008058\n",
      "              Random: fixed  11 labels. Loss 0.42123. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [102] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60805064\n",
      "Train loss (w/o reg) on all data: 0.60801893\n",
      "Test loss (w/o reg) on all data: 0.50524837\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0014630297\n",
      "Norm of the params: 0.7965493\n",
      "Flipped loss: 0.50525. Accuracy: 0.756\n",
      "### Flips: 50, rs: 22, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57287383\n",
      "Train loss (w/o reg) on all data: 0.5728181\n",
      "Test loss (w/o reg) on all data: 0.4402465\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011386258\n",
      "Norm of the params: 1.0557327\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.44025. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5180232\n",
      "Train loss (w/o reg) on all data: 0.51793134\n",
      "Test loss (w/o reg) on all data: 0.45355687\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002494611\n",
      "Norm of the params: 1.3552198\n",
      "                Loss: fixed  10 labels. Loss 0.45356. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6073865\n",
      "Train loss (w/o reg) on all data: 0.6073587\n",
      "Test loss (w/o reg) on all data: 0.5074593\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0024290825\n",
      "Norm of the params: 0.7463294\n",
      "              Random: fixed   1 labels. Loss 0.50746. Accuracy 0.756.\n",
      "### Flips: 50, rs: 22, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [128] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53446275\n",
      "Train loss (w/o reg) on all data: 0.5343841\n",
      "Test loss (w/o reg) on all data: 0.42648363\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00890219\n",
      "Norm of the params: 1.2545779\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.42648. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44398713\n",
      "Train loss (w/o reg) on all data: 0.4438187\n",
      "Test loss (w/o reg) on all data: 0.45643455\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0026959723\n",
      "Norm of the params: 1.835479\n",
      "                Loss: fixed  18 labels. Loss 0.45643. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [342] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5935215\n",
      "Train loss (w/o reg) on all data: 0.59349036\n",
      "Test loss (w/o reg) on all data: 0.49570888\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0034830044\n",
      "Norm of the params: 0.7889462\n",
      "              Random: fixed   4 labels. Loss 0.49571. Accuracy 0.778.\n",
      "### Flips: 50, rs: 22, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5055033\n",
      "Train loss (w/o reg) on all data: 0.5054092\n",
      "Test loss (w/o reg) on all data: 0.43095177\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0021950076\n",
      "Norm of the params: 1.3721204\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.43095. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38938087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss (w/o reg) on all data: 0.38913044\n",
      "Test loss (w/o reg) on all data: 0.41268924\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0035666756\n",
      "Norm of the params: 2.238027\n",
      "                Loss: fixed  23 labels. Loss 0.41269. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [290] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5828662\n",
      "Train loss (w/o reg) on all data: 0.58283174\n",
      "Test loss (w/o reg) on all data: 0.5008489\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0036024507\n",
      "Norm of the params: 0.8297497\n",
      "              Random: fixed   6 labels. Loss 0.50085. Accuracy 0.756.\n",
      "### Flips: 50, rs: 22, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [280] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46546265\n",
      "Train loss (w/o reg) on all data: 0.4652935\n",
      "Test loss (w/o reg) on all data: 0.4062312\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001086493\n",
      "Norm of the params: 1.8394084\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.40623. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [157] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34842905\n",
      "Train loss (w/o reg) on all data: 0.34807888\n",
      "Test loss (w/o reg) on all data: 0.40574506\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002077921\n",
      "Norm of the params: 2.6464748\n",
      "                Loss: fixed  27 labels. Loss 0.40575. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58476776\n",
      "Train loss (w/o reg) on all data: 0.58473265\n",
      "Test loss (w/o reg) on all data: 0.5014451\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0008636539\n",
      "Norm of the params: 0.8380839\n",
      "              Random: fixed   8 labels. Loss 0.50145. Accuracy 0.733.\n",
      "### Flips: 50, rs: 22, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4222027\n",
      "Train loss (w/o reg) on all data: 0.42196983\n",
      "Test loss (w/o reg) on all data: 0.38859078\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004768556\n",
      "Norm of the params: 2.1581519\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.38859. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33581644\n",
      "Train loss (w/o reg) on all data: 0.33546096\n",
      "Test loss (w/o reg) on all data: 0.40166602\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00038480718\n",
      "Norm of the params: 2.6663585\n",
      "                Loss: fixed  30 labels. Loss 0.40167. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58027947\n",
      "Train loss (w/o reg) on all data: 0.58024454\n",
      "Test loss (w/o reg) on all data: 0.49318466\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.006421201\n",
      "Norm of the params: 0.8355166\n",
      "              Random: fixed  11 labels. Loss 0.49318. Accuracy 0.756.\n",
      "### Flips: 50, rs: 22, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42128238\n",
      "Train loss (w/o reg) on all data: 0.42107442\n",
      "Test loss (w/o reg) on all data: 0.40386894\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0021326605\n",
      "Norm of the params: 2.0394807\n",
      "     Influence (LOO): fixed  29 labels. Loss 0.40387. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [189] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3006661\n",
      "Train loss (w/o reg) on all data: 0.3002003\n",
      "Test loss (w/o reg) on all data: 0.42988548\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009814422\n",
      "Norm of the params: 3.052115\n",
      "                Loss: fixed  35 labels. Loss 0.42989. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5655794\n",
      "Train loss (w/o reg) on all data: 0.5655364\n",
      "Test loss (w/o reg) on all data: 0.48193663\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0011041898\n",
      "Norm of the params: 0.92774665\n",
      "              Random: fixed  14 labels. Loss 0.48194. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [199] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60301906\n",
      "Train loss (w/o reg) on all data: 0.6029585\n",
      "Test loss (w/o reg) on all data: 0.46962997\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004370627\n",
      "Norm of the params: 1.1005831\n",
      "Flipped loss: 0.46963. Accuracy: 0.844\n",
      "### Flips: 50, rs: 23, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57142675\n",
      "Train loss (w/o reg) on all data: 0.5713543\n",
      "Test loss (w/o reg) on all data: 0.43307388\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00093911245\n",
      "Norm of the params: 1.2034864\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.43307. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5362962\n",
      "Train loss (w/o reg) on all data: 0.5361587\n",
      "Test loss (w/o reg) on all data: 0.43244043\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00085552956\n",
      "Norm of the params: 1.65834\n",
      "                Loss: fixed   8 labels. Loss 0.43244. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5884798\n",
      "Train loss (w/o reg) on all data: 0.58840626\n",
      "Test loss (w/o reg) on all data: 0.42786148\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0015511592\n",
      "Norm of the params: 1.2130791\n",
      "              Random: fixed   4 labels. Loss 0.42786. Accuracy 0.867.\n",
      "### Flips: 50, rs: 23, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [175] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5408833\n",
      "Train loss (w/o reg) on all data: 0.54079825\n",
      "Test loss (w/o reg) on all data: 0.39463446\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0018997787\n",
      "Norm of the params: 1.3043394\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.39463. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47175172\n",
      "Train loss (w/o reg) on all data: 0.47152594\n",
      "Test loss (w/o reg) on all data: 0.4124521\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.003292986\n",
      "Norm of the params: 2.1249683\n",
      "                Loss: fixed  16 labels. Loss 0.41245. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5724865\n",
      "Train loss (w/o reg) on all data: 0.57240945\n",
      "Test loss (w/o reg) on all data: 0.41275287\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0030052138\n",
      "Norm of the params: 1.2414886\n",
      "              Random: fixed   7 labels. Loss 0.41275. Accuracy 0.867.\n",
      "### Flips: 50, rs: 23, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5262841\n",
      "Train loss (w/o reg) on all data: 0.5261987\n",
      "Test loss (w/o reg) on all data: 0.40535012\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004711904\n",
      "Norm of the params: 1.3069353\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.40535. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42064473\n",
      "Train loss (w/o reg) on all data: 0.42037064\n",
      "Test loss (w/o reg) on all data: 0.40259278\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00064875034\n",
      "Norm of the params: 2.341357\n",
      "                Loss: fixed  22 labels. Loss 0.40259. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [184] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5627459\n",
      "Train loss (w/o reg) on all data: 0.5626699\n",
      "Test loss (w/o reg) on all data: 0.4098297\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0009429283\n",
      "Norm of the params: 1.2335207\n",
      "              Random: fixed   8 labels. Loss 0.40983. Accuracy 0.844.\n",
      "### Flips: 50, rs: 23, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [333] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49142626\n",
      "Train loss (w/o reg) on all data: 0.49129692\n",
      "Test loss (w/o reg) on all data: 0.39149958\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017885489\n",
      "Norm of the params: 1.608449\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.39150. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38659033\n",
      "Train loss (w/o reg) on all data: 0.3862303\n",
      "Test loss (w/o reg) on all data: 0.38442558\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00070543995\n",
      "Norm of the params: 2.6833913\n",
      "                Loss: fixed  28 labels. Loss 0.38443. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [302] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56897885\n",
      "Train loss (w/o reg) on all data: 0.56890345\n",
      "Test loss (w/o reg) on all data: 0.41098145\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0021655243\n",
      "Norm of the params: 1.2280645\n",
      "              Random: fixed   9 labels. Loss 0.41098. Accuracy 0.844.\n",
      "### Flips: 50, rs: 23, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [305] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47676292\n",
      "Train loss (w/o reg) on all data: 0.4766357\n",
      "Test loss (w/o reg) on all data: 0.37901697\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00081617426\n",
      "Norm of the params: 1.595107\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.37902. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37791017\n",
      "Train loss (w/o reg) on all data: 0.3775204\n",
      "Test loss (w/o reg) on all data: 0.39245695\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0069470233\n",
      "Norm of the params: 2.7919526\n",
      "                Loss: fixed  30 labels. Loss 0.39246. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.561133\n",
      "Train loss (w/o reg) on all data: 0.5610621\n",
      "Test loss (w/o reg) on all data: 0.40613934\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0010084471\n",
      "Norm of the params: 1.1908611\n",
      "              Random: fixed  14 labels. Loss 0.40614. Accuracy 0.867.\n",
      "### Flips: 50, rs: 23, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [193] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44423416\n",
      "Train loss (w/o reg) on all data: 0.4440679\n",
      "Test loss (w/o reg) on all data: 0.36634073\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0029759302\n",
      "Norm of the params: 1.8235732\n",
      "     Influence (LOO): fixed  28 labels. Loss 0.36634. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [383] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36441404\n",
      "Train loss (w/o reg) on all data: 0.36407104\n",
      "Test loss (w/o reg) on all data: 0.40745533\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011924902\n",
      "Norm of the params: 2.6191611\n",
      "                Loss: fixed  35 labels. Loss 0.40746. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [284] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55797946\n",
      "Train loss (w/o reg) on all data: 0.55791265\n",
      "Test loss (w/o reg) on all data: 0.40852097\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.014768651\n",
      "Norm of the params: 1.1557449\n",
      "              Random: fixed  17 labels. Loss 0.40852. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [124] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5924016\n",
      "Train loss (w/o reg) on all data: 0.59231704\n",
      "Test loss (w/o reg) on all data: 0.5356692\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.6888888888888889\n",
      "Norm of the mean of gradients: 0.0032230527\n",
      "Norm of the params: 1.3006504\n",
      "Flipped loss: 0.53567. Accuracy: 0.689\n",
      "### Flips: 50, rs: 24, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [133] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55199015\n",
      "Train loss (w/o reg) on all data: 0.5518805\n",
      "Test loss (w/o reg) on all data: 0.50083095\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.00075276004\n",
      "Norm of the params: 1.481222\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.50083. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51478624\n",
      "Train loss (w/o reg) on all data: 0.5145825\n",
      "Test loss (w/o reg) on all data: 0.5070585\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0023912801\n",
      "Norm of the params: 2.0184424\n",
      "                Loss: fixed   8 labels. Loss 0.50706. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [141] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5794907\n",
      "Train loss (w/o reg) on all data: 0.57941675\n",
      "Test loss (w/o reg) on all data: 0.5091641\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0021859375\n",
      "Norm of the params: 1.2162395\n",
      "              Random: fixed   3 labels. Loss 0.50916. Accuracy 0.756.\n",
      "### Flips: 50, rs: 24, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [288] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.535411\n",
      "Train loss (w/o reg) on all data: 0.5352782\n",
      "Test loss (w/o reg) on all data: 0.48778585\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0009593448\n",
      "Norm of the params: 1.6298528\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.48779. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [295] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45362815\n",
      "Train loss (w/o reg) on all data: 0.4533454\n",
      "Test loss (w/o reg) on all data: 0.50349593\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.00096038805\n",
      "Norm of the params: 2.3780692\n",
      "                Loss: fixed  15 labels. Loss 0.50350. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5734416\n",
      "Train loss (w/o reg) on all data: 0.57337207\n",
      "Test loss (w/o reg) on all data: 0.5060072\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.004891756\n",
      "Norm of the params: 1.1796299\n",
      "              Random: fixed   5 labels. Loss 0.50601. Accuracy 0.756.\n",
      "### Flips: 50, rs: 24, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [199] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5084847\n",
      "Train loss (w/o reg) on all data: 0.5083309\n",
      "Test loss (w/o reg) on all data: 0.46951175\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.001049453\n",
      "Norm of the params: 1.7539407\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.46951. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42987812\n",
      "Train loss (w/o reg) on all data: 0.42952958\n",
      "Test loss (w/o reg) on all data: 0.51898474\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.00023072607\n",
      "Norm of the params: 2.6402323\n",
      "                Loss: fixed  18 labels. Loss 0.51898. Accuracy 0.733.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [104] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.564109\n",
      "Train loss (w/o reg) on all data: 0.5640393\n",
      "Test loss (w/o reg) on all data: 0.48904115\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0014087075\n",
      "Norm of the params: 1.1810725\n",
      "              Random: fixed   8 labels. Loss 0.48904. Accuracy 0.756.\n",
      "### Flips: 50, rs: 24, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4735874\n",
      "Train loss (w/o reg) on all data: 0.47335014\n",
      "Test loss (w/o reg) on all data: 0.47654974\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0038078912\n",
      "Norm of the params: 2.1783316\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.47655. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [290] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3836734\n",
      "Train loss (w/o reg) on all data: 0.38321298\n",
      "Test loss (w/o reg) on all data: 0.50779396\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0061774887\n",
      "Norm of the params: 3.0345535\n",
      "                Loss: fixed  26 labels. Loss 0.50779. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [395] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5575962\n",
      "Train loss (w/o reg) on all data: 0.55751425\n",
      "Test loss (w/o reg) on all data: 0.4857819\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.002107716\n",
      "Norm of the params: 1.2803553\n",
      "              Random: fixed  11 labels. Loss 0.48578. Accuracy 0.756.\n",
      "### Flips: 50, rs: 24, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [387] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43868113\n",
      "Train loss (w/o reg) on all data: 0.43841878\n",
      "Test loss (w/o reg) on all data: 0.4550916\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0016417876\n",
      "Norm of the params: 2.2906256\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.45509. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35041702\n",
      "Train loss (w/o reg) on all data: 0.34999564\n",
      "Test loss (w/o reg) on all data: 0.48277256\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006138009\n",
      "Norm of the params: 2.903033\n",
      "                Loss: fixed  31 labels. Loss 0.48277. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [344] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.547751\n",
      "Train loss (w/o reg) on all data: 0.54766804\n",
      "Test loss (w/o reg) on all data: 0.4796268\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00066030415\n",
      "Norm of the params: 1.2879847\n",
      "              Random: fixed  14 labels. Loss 0.47963. Accuracy 0.756.\n",
      "### Flips: 50, rs: 24, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [291] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43303996\n",
      "Train loss (w/o reg) on all data: 0.43274775\n",
      "Test loss (w/o reg) on all data: 0.46391478\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00980177\n",
      "Norm of the params: 2.4174979\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.46391. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3459619\n",
      "Train loss (w/o reg) on all data: 0.3455166\n",
      "Test loss (w/o reg) on all data: 0.51276743\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00043151862\n",
      "Norm of the params: 2.9843204\n",
      "                Loss: fixed  34 labels. Loss 0.51277. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [317] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5337935\n",
      "Train loss (w/o reg) on all data: 0.5336949\n",
      "Test loss (w/o reg) on all data: 0.47338638\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0012170164\n",
      "Norm of the params: 1.4041129\n",
      "              Random: fixed  16 labels. Loss 0.47339. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [269] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61288947\n",
      "Train loss (w/o reg) on all data: 0.6128529\n",
      "Test loss (w/o reg) on all data: 0.50691307\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0022156893\n",
      "Norm of the params: 0.8552342\n",
      "Flipped loss: 0.50691. Accuracy: 0.756\n",
      "### Flips: 50, rs: 25, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5776656\n",
      "Train loss (w/o reg) on all data: 0.57759124\n",
      "Test loss (w/o reg) on all data: 0.4930211\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0053828135\n",
      "Norm of the params: 1.2197435\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.49302. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54534596\n",
      "Train loss (w/o reg) on all data: 0.54526585\n",
      "Test loss (w/o reg) on all data: 0.44434425\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0004548469\n",
      "Norm of the params: 1.2655817\n",
      "                Loss: fixed   8 labels. Loss 0.44434. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [148] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60671294\n",
      "Train loss (w/o reg) on all data: 0.6066694\n",
      "Test loss (w/o reg) on all data: 0.50102943\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0016691339\n",
      "Norm of the params: 0.93304074\n",
      "              Random: fixed   3 labels. Loss 0.50103. Accuracy 0.778.\n",
      "### Flips: 50, rs: 25, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [151] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5508186\n",
      "Train loss (w/o reg) on all data: 0.5507506\n",
      "Test loss (w/o reg) on all data: 0.4338562\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0045469534\n",
      "Norm of the params: 1.166198\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.43386. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [322] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47002807\n",
      "Train loss (w/o reg) on all data: 0.4698549\n",
      "Test loss (w/o reg) on all data: 0.45911178\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022871438\n",
      "Norm of the params: 1.8610489\n",
      "                Loss: fixed  17 labels. Loss 0.45911. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.599815\n",
      "Train loss (w/o reg) on all data: 0.599767\n",
      "Test loss (w/o reg) on all data: 0.50056994\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.005641636\n",
      "Norm of the params: 0.97935516\n",
      "              Random: fixed   4 labels. Loss 0.50057. Accuracy 0.778.\n",
      "### Flips: 50, rs: 25, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51999676\n",
      "Train loss (w/o reg) on all data: 0.5198963\n",
      "Test loss (w/o reg) on all data: 0.41796902\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00052121386\n",
      "Norm of the params: 1.4172907\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.41797. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [320] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42471343\n",
      "Train loss (w/o reg) on all data: 0.42448905\n",
      "Test loss (w/o reg) on all data: 0.47451052\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0025808746\n",
      "Norm of the params: 2.1183898\n",
      "                Loss: fixed  23 labels. Loss 0.47451. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5998119\n",
      "Train loss (w/o reg) on all data: 0.5997642\n",
      "Test loss (w/o reg) on all data: 0.5006284\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0082629\n",
      "Norm of the params: 0.97637624\n",
      "              Random: fixed   4 labels. Loss 0.50063. Accuracy 0.778.\n",
      "### Flips: 50, rs: 25, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49789625\n",
      "Train loss (w/o reg) on all data: 0.4977681\n",
      "Test loss (w/o reg) on all data: 0.41003457\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0019309926\n",
      "Norm of the params: 1.600956\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.41003. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [293] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38877818\n",
      "Train loss (w/o reg) on all data: 0.38850623\n",
      "Test loss (w/o reg) on all data: 0.48743457\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009248831\n",
      "Norm of the params: 2.332186\n",
      "                Loss: fixed  28 labels. Loss 0.48743. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5980008\n",
      "Train loss (w/o reg) on all data: 0.59795636\n",
      "Test loss (w/o reg) on all data: 0.49778977\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0026451005\n",
      "Norm of the params: 0.94307715\n",
      "              Random: fixed   5 labels. Loss 0.49779. Accuracy 0.756.\n",
      "### Flips: 50, rs: 25, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [291] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48522085\n",
      "Train loss (w/o reg) on all data: 0.48508108\n",
      "Test loss (w/o reg) on all data: 0.3985669\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0008244727\n",
      "Norm of the params: 1.671976\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.39857. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35356373\n",
      "Train loss (w/o reg) on all data: 0.35317484\n",
      "Test loss (w/o reg) on all data: 0.50628155\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0007374913\n",
      "Norm of the params: 2.7888782\n",
      "                Loss: fixed  32 labels. Loss 0.50628. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [378] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6016872\n",
      "Train loss (w/o reg) on all data: 0.60164547\n",
      "Test loss (w/o reg) on all data: 0.48946598\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007316392\n",
      "Norm of the params: 0.9137699\n",
      "              Random: fixed   6 labels. Loss 0.48947. Accuracy 0.822.\n",
      "### Flips: 50, rs: 25, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4765421\n",
      "Train loss (w/o reg) on all data: 0.4764092\n",
      "Test loss (w/o reg) on all data: 0.3985622\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0014615142\n",
      "Norm of the params: 1.6302567\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.39856. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33526894\n",
      "Train loss (w/o reg) on all data: 0.33486956\n",
      "Test loss (w/o reg) on all data: 0.4756262\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009011729\n",
      "Norm of the params: 2.826227\n",
      "                Loss: fixed  36 labels. Loss 0.47563. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [282] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5941068\n",
      "Train loss (w/o reg) on all data: 0.5940622\n",
      "Test loss (w/o reg) on all data: 0.4879852\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015954934\n",
      "Norm of the params: 0.9443754\n",
      "              Random: fixed   7 labels. Loss 0.48799. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [393] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59831953\n",
      "Train loss (w/o reg) on all data: 0.59827644\n",
      "Test loss (w/o reg) on all data: 0.5085707\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008250961\n",
      "Norm of the params: 0.9283564\n",
      "Flipped loss: 0.50857. Accuracy: 0.800\n",
      "### Flips: 50, rs: 26, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [102] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5748262\n",
      "Train loss (w/o reg) on all data: 0.57476866\n",
      "Test loss (w/o reg) on all data: 0.49352074\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011937639\n",
      "Norm of the params: 1.072566\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.49352. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53093183\n",
      "Train loss (w/o reg) on all data: 0.53084916\n",
      "Test loss (w/o reg) on all data: 0.5055553\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0026760497\n",
      "Norm of the params: 1.2859231\n",
      "                Loss: fixed   8 labels. Loss 0.50556. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [138] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5851169\n",
      "Train loss (w/o reg) on all data: 0.5850746\n",
      "Test loss (w/o reg) on all data: 0.49745336\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005400838\n",
      "Norm of the params: 0.91980726\n",
      "              Random: fixed   3 labels. Loss 0.49745. Accuracy 0.800.\n",
      "### Flips: 50, rs: 26, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [240] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5352409\n",
      "Train loss (w/o reg) on all data: 0.5351659\n",
      "Test loss (w/o reg) on all data: 0.44265363\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0033647195\n",
      "Norm of the params: 1.224833\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.44265. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [355] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47160164\n",
      "Train loss (w/o reg) on all data: 0.4714709\n",
      "Test loss (w/o reg) on all data: 0.4752879\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011188167\n",
      "Norm of the params: 1.617008\n",
      "                Loss: fixed  15 labels. Loss 0.47529. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.571495\n",
      "Train loss (w/o reg) on all data: 0.57144994\n",
      "Test loss (w/o reg) on all data: 0.50715935\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0055408077\n",
      "Norm of the params: 0.94948906\n",
      "              Random: fixed   5 labels. Loss 0.50716. Accuracy 0.800.\n",
      "### Flips: 50, rs: 26, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.508884\n",
      "Train loss (w/o reg) on all data: 0.50879866\n",
      "Test loss (w/o reg) on all data: 0.4217276\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0076539046\n",
      "Norm of the params: 1.3066012\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.42173. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40939456\n",
      "Train loss (w/o reg) on all data: 0.40920913\n",
      "Test loss (w/o reg) on all data: 0.493332\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011946433\n",
      "Norm of the params: 1.9257962\n",
      "                Loss: fixed  23 labels. Loss 0.49333. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [364] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56328976\n",
      "Train loss (w/o reg) on all data: 0.5632365\n",
      "Test loss (w/o reg) on all data: 0.49348086\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0013424836\n",
      "Norm of the params: 1.0323225\n",
      "              Random: fixed   9 labels. Loss 0.49348. Accuracy 0.778.\n",
      "### Flips: 50, rs: 26, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47196415\n",
      "Train loss (w/o reg) on all data: 0.47185168\n",
      "Test loss (w/o reg) on all data: 0.40012202\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0031163888\n",
      "Norm of the params: 1.499746\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.40012. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [362] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36654657\n",
      "Train loss (w/o reg) on all data: 0.36629042\n",
      "Test loss (w/o reg) on all data: 0.4656875\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010647448\n",
      "Norm of the params: 2.2634187\n",
      "                Loss: fixed  28 labels. Loss 0.46569. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [326] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5558546\n",
      "Train loss (w/o reg) on all data: 0.55579746\n",
      "Test loss (w/o reg) on all data: 0.50355947\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018486218\n",
      "Norm of the params: 1.069108\n",
      "              Random: fixed  10 labels. Loss 0.50356. Accuracy 0.800.\n",
      "### Flips: 50, rs: 26, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [327] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.464714\n",
      "Train loss (w/o reg) on all data: 0.46459767\n",
      "Test loss (w/o reg) on all data: 0.39294204\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019183737\n",
      "Norm of the params: 1.5251565\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.39294. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32225198\n",
      "Train loss (w/o reg) on all data: 0.32187596\n",
      "Test loss (w/o reg) on all data: 0.47935405\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015823344\n",
      "Norm of the params: 2.7423522\n",
      "                Loss: fixed  33 labels. Loss 0.47935. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [387] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5555514\n",
      "Train loss (w/o reg) on all data: 0.555491\n",
      "Test loss (w/o reg) on all data: 0.4993971\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0048837834\n",
      "Norm of the params: 1.0994307\n",
      "              Random: fixed  11 labels. Loss 0.49940. Accuracy 0.800.\n",
      "### Flips: 50, rs: 26, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4431853\n",
      "Train loss (w/o reg) on all data: 0.44302708\n",
      "Test loss (w/o reg) on all data: 0.39135006\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001031507\n",
      "Norm of the params: 1.7788293\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.39135. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [334] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31423214\n",
      "Train loss (w/o reg) on all data: 0.31383047\n",
      "Test loss (w/o reg) on all data: 0.48830748\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0028402675\n",
      "Norm of the params: 2.8343024\n",
      "                Loss: fixed  34 labels. Loss 0.48831. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [285] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53595203\n",
      "Train loss (w/o reg) on all data: 0.5358804\n",
      "Test loss (w/o reg) on all data: 0.483648\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.006682939\n",
      "Norm of the params: 1.1971668\n",
      "              Random: fixed  15 labels. Loss 0.48365. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [179] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.62199533\n",
      "Train loss (w/o reg) on all data: 0.6219435\n",
      "Test loss (w/o reg) on all data: 0.56322044\n",
      "Train acc on all data:  0.6367924528301887\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.003522941\n",
      "Norm of the params: 1.0182695\n",
      "Flipped loss: 0.56322. Accuracy: 0.756\n",
      "### Flips: 50, rs: 27, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5733385\n",
      "Train loss (w/o reg) on all data: 0.57321936\n",
      "Test loss (w/o reg) on all data: 0.52880853\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015852579\n",
      "Norm of the params: 1.5438689\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.52881. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5653049\n",
      "Train loss (w/o reg) on all data: 0.5651779\n",
      "Test loss (w/o reg) on all data: 0.53618807\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0011800585\n",
      "Norm of the params: 1.593424\n",
      "                Loss: fixed   7 labels. Loss 0.53619. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61482406\n",
      "Train loss (w/o reg) on all data: 0.61477447\n",
      "Test loss (w/o reg) on all data: 0.5542251\n",
      "Train acc on all data:  0.6415094339622641\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0071082385\n",
      "Norm of the params: 0.9960832\n",
      "              Random: fixed   2 labels. Loss 0.55423. Accuracy 0.756.\n",
      "### Flips: 50, rs: 27, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55500406\n",
      "Train loss (w/o reg) on all data: 0.55485976\n",
      "Test loss (w/o reg) on all data: 0.5208513\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001959724\n",
      "Norm of the params: 1.6986835\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.52085. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5138707\n",
      "Train loss (w/o reg) on all data: 0.5137117\n",
      "Test loss (w/o reg) on all data: 0.47411537\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0023290752\n",
      "Norm of the params: 1.7832549\n",
      "                Loss: fixed  14 labels. Loss 0.47412. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [124] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6054838\n",
      "Train loss (w/o reg) on all data: 0.60541576\n",
      "Test loss (w/o reg) on all data: 0.5466433\n",
      "Train acc on all data:  0.6509433962264151\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0040475368\n",
      "Norm of the params: 1.1663128\n",
      "              Random: fixed   3 labels. Loss 0.54664. Accuracy 0.756.\n",
      "### Flips: 50, rs: 27, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53148055\n",
      "Train loss (w/o reg) on all data: 0.53127855\n",
      "Test loss (w/o reg) on all data: 0.50559956\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0017005391\n",
      "Norm of the params: 2.010092\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.50560. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47387663\n",
      "Train loss (w/o reg) on all data: 0.47360733\n",
      "Test loss (w/o reg) on all data: 0.48911384\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00797397\n",
      "Norm of the params: 2.3207407\n",
      "                Loss: fixed  20 labels. Loss 0.48911. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [342] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5907678\n",
      "Train loss (w/o reg) on all data: 0.59070814\n",
      "Test loss (w/o reg) on all data: 0.529177\n",
      "Train acc on all data:  0.6556603773584906\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001209098\n",
      "Norm of the params: 1.0926092\n",
      "              Random: fixed   6 labels. Loss 0.52918. Accuracy 0.778.\n",
      "### Flips: 50, rs: 27, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5214918\n",
      "Train loss (w/o reg) on all data: 0.5213059\n",
      "Test loss (w/o reg) on all data: 0.48489764\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0016401324\n",
      "Norm of the params: 1.9283915\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.48490. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [295] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43987593\n",
      "Train loss (w/o reg) on all data: 0.43952885\n",
      "Test loss (w/o reg) on all data: 0.49101037\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.009310977\n",
      "Norm of the params: 2.6347315\n",
      "                Loss: fixed  25 labels. Loss 0.49101. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [305] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57712156\n",
      "Train loss (w/o reg) on all data: 0.57704747\n",
      "Test loss (w/o reg) on all data: 0.5386601\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0008656229\n",
      "Norm of the params: 1.2170917\n",
      "              Random: fixed  10 labels. Loss 0.53866. Accuracy 0.733.\n",
      "### Flips: 50, rs: 27, checks: 50\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [295] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51459247\n",
      "Train loss (w/o reg) on all data: 0.5144124\n",
      "Test loss (w/o reg) on all data: 0.4743\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0057585\n",
      "Norm of the params: 1.8976097\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.47430. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [356] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4023809\n",
      "Train loss (w/o reg) on all data: 0.40201706\n",
      "Test loss (w/o reg) on all data: 0.50508744\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0008949725\n",
      "Norm of the params: 2.6976357\n",
      "                Loss: fixed  30 labels. Loss 0.50509. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [226] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5709953\n",
      "Train loss (w/o reg) on all data: 0.5709138\n",
      "Test loss (w/o reg) on all data: 0.53091246\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.007030314\n",
      "Norm of the params: 1.2766706\n",
      "              Random: fixed  13 labels. Loss 0.53091. Accuracy 0.733.\n",
      "### Flips: 50, rs: 27, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48916578\n",
      "Train loss (w/o reg) on all data: 0.48895547\n",
      "Test loss (w/o reg) on all data: 0.4341206\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018309508\n",
      "Norm of the params: 2.0508883\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.43412. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38635424\n",
      "Train loss (w/o reg) on all data: 0.38600317\n",
      "Test loss (w/o reg) on all data: 0.5491899\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.001327736\n",
      "Norm of the params: 2.6497645\n",
      "                Loss: fixed  33 labels. Loss 0.54919. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56648064\n",
      "Train loss (w/o reg) on all data: 0.56639975\n",
      "Test loss (w/o reg) on all data: 0.5385687\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0019140108\n",
      "Norm of the params: 1.2717018\n",
      "              Random: fixed  14 labels. Loss 0.53857. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61190695\n",
      "Train loss (w/o reg) on all data: 0.6118741\n",
      "Test loss (w/o reg) on all data: 0.44983414\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0078483075\n",
      "Norm of the params: 0.8106833\n",
      "Flipped loss: 0.44983. Accuracy: 0.822\n",
      "### Flips: 50, rs: 28, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5706254\n",
      "Train loss (w/o reg) on all data: 0.5705633\n",
      "Test loss (w/o reg) on all data: 0.39397863\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010675454\n",
      "Norm of the params: 1.1147739\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.39398. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52993774\n",
      "Train loss (w/o reg) on all data: 0.5298597\n",
      "Test loss (w/o reg) on all data: 0.3763812\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007783901\n",
      "Norm of the params: 1.2490543\n",
      "                Loss: fixed   9 labels. Loss 0.37638. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [140] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6083596\n",
      "Train loss (w/o reg) on all data: 0.6083363\n",
      "Test loss (w/o reg) on all data: 0.44645554\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010750919\n",
      "Norm of the params: 0.68195677\n",
      "              Random: fixed   3 labels. Loss 0.44646. Accuracy 0.800.\n",
      "### Flips: 50, rs: 28, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [115] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55747783\n",
      "Train loss (w/o reg) on all data: 0.5574196\n",
      "Test loss (w/o reg) on all data: 0.37762985\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0019807238\n",
      "Norm of the params: 1.079013\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.37763. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [337] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45111778\n",
      "Train loss (w/o reg) on all data: 0.4509345\n",
      "Test loss (w/o reg) on all data: 0.37985966\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00083161646\n",
      "Norm of the params: 1.91459\n",
      "                Loss: fixed  18 labels. Loss 0.37986. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60865104\n",
      "Train loss (w/o reg) on all data: 0.60862964\n",
      "Test loss (w/o reg) on all data: 0.44894448\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.008783715\n",
      "Norm of the params: 0.65411246\n",
      "              Random: fixed   4 labels. Loss 0.44894. Accuracy 0.822.\n",
      "### Flips: 50, rs: 28, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54974\n",
      "Train loss (w/o reg) on all data: 0.54967123\n",
      "Test loss (w/o reg) on all data: 0.37019756\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010731613\n",
      "Norm of the params: 1.1727948\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.37020. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [386] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41476393\n",
      "Train loss (w/o reg) on all data: 0.41453868\n",
      "Test loss (w/o reg) on all data: 0.37511215\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0005230797\n",
      "Norm of the params: 2.122536\n",
      "                Loss: fixed  22 labels. Loss 0.37511. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [128] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.600635\n",
      "Train loss (w/o reg) on all data: 0.6006109\n",
      "Test loss (w/o reg) on all data: 0.4587529\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003871134\n",
      "Norm of the params: 0.69411695\n",
      "              Random: fixed   6 labels. Loss 0.45875. Accuracy 0.800.\n",
      "### Flips: 50, rs: 28, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [122] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.537847\n",
      "Train loss (w/o reg) on all data: 0.53777534\n",
      "Test loss (w/o reg) on all data: 0.35758573\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0046786848\n",
      "Norm of the params: 1.1968501\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.35759. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [363] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3683999\n",
      "Train loss (w/o reg) on all data: 0.36813188\n",
      "Test loss (w/o reg) on all data: 0.3933551\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0054635596\n",
      "Norm of the params: 2.3152647\n",
      "                Loss: fixed  28 labels. Loss 0.39336. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58871365\n",
      "Train loss (w/o reg) on all data: 0.58868515\n",
      "Test loss (w/o reg) on all data: 0.4567994\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.009370728\n",
      "Norm of the params: 0.75448585\n",
      "              Random: fixed   9 labels. Loss 0.45680. Accuracy 0.822.\n",
      "### Flips: 50, rs: 28, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50688034\n",
      "Train loss (w/o reg) on all data: 0.506782\n",
      "Test loss (w/o reg) on all data: 0.34528062\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010146854\n",
      "Norm of the params: 1.4024576\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.34528. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3457745\n",
      "Train loss (w/o reg) on all data: 0.34542853\n",
      "Test loss (w/o reg) on all data: 0.396567\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0004963702\n",
      "Norm of the params: 2.6305182\n",
      "                Loss: fixed  31 labels. Loss 0.39657. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [284] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59127057\n",
      "Train loss (w/o reg) on all data: 0.5912405\n",
      "Test loss (w/o reg) on all data: 0.45611623\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019093229\n",
      "Norm of the params: 0.77484864\n",
      "              Random: fixed  10 labels. Loss 0.45612. Accuracy 0.822.\n",
      "### Flips: 50, rs: 28, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [134] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5051274\n",
      "Train loss (w/o reg) on all data: 0.5050282\n",
      "Test loss (w/o reg) on all data: 0.34644338\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0015098777\n",
      "Norm of the params: 1.4085314\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.34644. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33793253\n",
      "Train loss (w/o reg) on all data: 0.33756104\n",
      "Test loss (w/o reg) on all data: 0.3887756\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.008064917\n",
      "Norm of the params: 2.7257044\n",
      "                Loss: fixed  34 labels. Loss 0.38878. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5690719\n",
      "Train loss (w/o reg) on all data: 0.5690326\n",
      "Test loss (w/o reg) on all data: 0.44536108\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018231256\n",
      "Norm of the params: 0.88649464\n",
      "              Random: fixed  13 labels. Loss 0.44536. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6200846\n",
      "Train loss (w/o reg) on all data: 0.62003994\n",
      "Test loss (w/o reg) on all data: 0.5388019\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0056757885\n",
      "Norm of the params: 0.9447302\n",
      "Flipped loss: 0.53880. Accuracy: 0.800\n",
      "### Flips: 50, rs: 29, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [179] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5988063\n",
      "Train loss (w/o reg) on all data: 0.5987611\n",
      "Test loss (w/o reg) on all data: 0.49827814\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004779124\n",
      "Norm of the params: 0.9514874\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.49828. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56928533\n",
      "Train loss (w/o reg) on all data: 0.5691958\n",
      "Test loss (w/o reg) on all data: 0.518482\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0020197355\n",
      "Norm of the params: 1.3380173\n",
      "                Loss: fixed   7 labels. Loss 0.51848. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6124101\n",
      "Train loss (w/o reg) on all data: 0.6123575\n",
      "Test loss (w/o reg) on all data: 0.5338775\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0048950263\n",
      "Norm of the params: 1.0260772\n",
      "              Random: fixed   1 labels. Loss 0.53388. Accuracy 0.822.\n",
      "### Flips: 50, rs: 29, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [167] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58535194\n",
      "Train loss (w/o reg) on all data: 0.58527404\n",
      "Test loss (w/o reg) on all data: 0.4829713\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013454232\n",
      "Norm of the params: 1.2483644\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.48297. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [300] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51940984\n",
      "Train loss (w/o reg) on all data: 0.51928747\n",
      "Test loss (w/o reg) on all data: 0.4622583\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00053132785\n",
      "Norm of the params: 1.5645069\n",
      "                Loss: fixed  14 labels. Loss 0.46226. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [413] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5897982\n",
      "Train loss (w/o reg) on all data: 0.5897279\n",
      "Test loss (w/o reg) on all data: 0.53654534\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0040537436\n",
      "Norm of the params: 1.1860906\n",
      "              Random: fixed   8 labels. Loss 0.53655. Accuracy 0.778.\n",
      "### Flips: 50, rs: 29, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [374] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55760896\n",
      "Train loss (w/o reg) on all data: 0.55752903\n",
      "Test loss (w/o reg) on all data: 0.43292114\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013374517\n",
      "Norm of the params: 1.2644837\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.43292. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47177207\n",
      "Train loss (w/o reg) on all data: 0.47157353\n",
      "Test loss (w/o reg) on all data: 0.4588652\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00047256908\n",
      "Norm of the params: 1.9927684\n",
      "                Loss: fixed  20 labels. Loss 0.45887. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5785486\n",
      "Train loss (w/o reg) on all data: 0.5784781\n",
      "Test loss (w/o reg) on all data: 0.5394181\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0023753238\n",
      "Norm of the params: 1.1877478\n",
      "              Random: fixed  10 labels. Loss 0.53942. Accuracy 0.800.\n",
      "### Flips: 50, rs: 29, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54297364\n",
      "Train loss (w/o reg) on all data: 0.54287314\n",
      "Test loss (w/o reg) on all data: 0.44025102\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0047945473\n",
      "Norm of the params: 1.4176408\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.44025. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [282] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4470227\n",
      "Train loss (w/o reg) on all data: 0.44680765\n",
      "Test loss (w/o reg) on all data: 0.4560557\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010893185\n",
      "Norm of the params: 2.0738897\n",
      "                Loss: fixed  24 labels. Loss 0.45606. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56242895\n",
      "Train loss (w/o reg) on all data: 0.5623509\n",
      "Test loss (w/o reg) on all data: 0.50988144\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012522063\n",
      "Norm of the params: 1.2491164\n",
      "              Random: fixed  14 labels. Loss 0.50988. Accuracy 0.800.\n",
      "### Flips: 50, rs: 29, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5041588\n",
      "Train loss (w/o reg) on all data: 0.5040215\n",
      "Test loss (w/o reg) on all data: 0.4284493\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.006781297\n",
      "Norm of the params: 1.6568274\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.42845. Accuracy 0.844.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42089596\n",
      "Train loss (w/o reg) on all data: 0.42063665\n",
      "Test loss (w/o reg) on all data: 0.47133476\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00047474907\n",
      "Norm of the params: 2.277348\n",
      "                Loss: fixed  28 labels. Loss 0.47133. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [380] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5514\n",
      "Train loss (w/o reg) on all data: 0.55131626\n",
      "Test loss (w/o reg) on all data: 0.51533353\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0062336978\n",
      "Norm of the params: 1.2941021\n",
      "              Random: fixed  16 labels. Loss 0.51533. Accuracy 0.822.\n",
      "### Flips: 50, rs: 29, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [291] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48835966\n",
      "Train loss (w/o reg) on all data: 0.48822474\n",
      "Test loss (w/o reg) on all data: 0.39603135\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0038733194\n",
      "Norm of the params: 1.64274\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.39603. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38896224\n",
      "Train loss (w/o reg) on all data: 0.38870367\n",
      "Test loss (w/o reg) on all data: 0.4420169\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0013077041\n",
      "Norm of the params: 2.2741036\n",
      "                Loss: fixed  34 labels. Loss 0.44202. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [473] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5455283\n",
      "Train loss (w/o reg) on all data: 0.5454402\n",
      "Test loss (w/o reg) on all data: 0.5238623\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0037180723\n",
      "Norm of the params: 1.3271978\n",
      "              Random: fixed  17 labels. Loss 0.52386. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [332] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59207094\n",
      "Train loss (w/o reg) on all data: 0.5920041\n",
      "Test loss (w/o reg) on all data: 0.49286026\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.002016407\n",
      "Norm of the params: 1.1560181\n",
      "Flipped loss: 0.49286. Accuracy: 0.756\n",
      "### Flips: 50, rs: 30, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5417973\n",
      "Train loss (w/o reg) on all data: 0.54166955\n",
      "Test loss (w/o reg) on all data: 0.47337067\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.002233424\n",
      "Norm of the params: 1.5983244\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.47337. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [157] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5145779\n",
      "Train loss (w/o reg) on all data: 0.5143992\n",
      "Test loss (w/o reg) on all data: 0.4611947\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0009851512\n",
      "Norm of the params: 1.8906674\n",
      "                Loss: fixed   8 labels. Loss 0.46119. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58853966\n",
      "Train loss (w/o reg) on all data: 0.58847165\n",
      "Test loss (w/o reg) on all data: 0.47875217\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.004023135\n",
      "Norm of the params: 1.1663407\n",
      "              Random: fixed   2 labels. Loss 0.47875. Accuracy 0.733.\n",
      "### Flips: 50, rs: 30, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [185] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50346506\n",
      "Train loss (w/o reg) on all data: 0.5032847\n",
      "Test loss (w/o reg) on all data: 0.4452877\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0027111792\n",
      "Norm of the params: 1.8991789\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.44529. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46082908\n",
      "Train loss (w/o reg) on all data: 0.46059346\n",
      "Test loss (w/o reg) on all data: 0.4666136\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0032914204\n",
      "Norm of the params: 2.1708415\n",
      "                Loss: fixed  14 labels. Loss 0.46661. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.580066\n",
      "Train loss (w/o reg) on all data: 0.57998\n",
      "Test loss (w/o reg) on all data: 0.46750388\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.00090832985\n",
      "Norm of the params: 1.3113859\n",
      "              Random: fixed   5 labels. Loss 0.46750. Accuracy 0.733.\n",
      "### Flips: 50, rs: 30, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48165452\n",
      "Train loss (w/o reg) on all data: 0.4814582\n",
      "Test loss (w/o reg) on all data: 0.40496576\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010592614\n",
      "Norm of the params: 1.9816418\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.40497. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [127] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43157402\n",
      "Train loss (w/o reg) on all data: 0.4313327\n",
      "Test loss (w/o reg) on all data: 0.43990427\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0009847519\n",
      "Norm of the params: 2.19687\n",
      "                Loss: fixed  19 labels. Loss 0.43990. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [272] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56911105\n",
      "Train loss (w/o reg) on all data: 0.5689983\n",
      "Test loss (w/o reg) on all data: 0.47151408\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0015977629\n",
      "Norm of the params: 1.5017341\n",
      "              Random: fixed   7 labels. Loss 0.47151. Accuracy 0.756.\n",
      "### Flips: 50, rs: 30, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47372034\n",
      "Train loss (w/o reg) on all data: 0.47353777\n",
      "Test loss (w/o reg) on all data: 0.4219844\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012680142\n",
      "Norm of the params: 1.9109299\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.42198. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41120148\n",
      "Train loss (w/o reg) on all data: 0.41094774\n",
      "Test loss (w/o reg) on all data: 0.41253433\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0019945235\n",
      "Norm of the params: 2.2527394\n",
      "                Loss: fixed  23 labels. Loss 0.41253. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [438] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5681142\n",
      "Train loss (w/o reg) on all data: 0.56800294\n",
      "Test loss (w/o reg) on all data: 0.4794943\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.002543587\n",
      "Norm of the params: 1.491886\n",
      "              Random: fixed   8 labels. Loss 0.47949. Accuracy 0.756.\n",
      "### Flips: 50, rs: 30, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [297] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46337587\n",
      "Train loss (w/o reg) on all data: 0.4631866\n",
      "Test loss (w/o reg) on all data: 0.4101884\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001544538\n",
      "Norm of the params: 1.9456091\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.41019. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37446997\n",
      "Train loss (w/o reg) on all data: 0.37416846\n",
      "Test loss (w/o reg) on all data: 0.40755793\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003353212\n",
      "Norm of the params: 2.4556272\n",
      "                Loss: fixed  29 labels. Loss 0.40756. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55888593\n",
      "Train loss (w/o reg) on all data: 0.5587843\n",
      "Test loss (w/o reg) on all data: 0.4749735\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0017819413\n",
      "Norm of the params: 1.4254725\n",
      "              Random: fixed  10 labels. Loss 0.47497. Accuracy 0.733.\n",
      "### Flips: 50, rs: 30, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [269] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43353844\n",
      "Train loss (w/o reg) on all data: 0.43334413\n",
      "Test loss (w/o reg) on all data: 0.40728098\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019796146\n",
      "Norm of the params: 1.9713959\n",
      "     Influence (LOO): fixed  30 labels. Loss 0.40728. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [380] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36750016\n",
      "Train loss (w/o reg) on all data: 0.3671808\n",
      "Test loss (w/o reg) on all data: 0.4246282\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002036445\n",
      "Norm of the params: 2.5273423\n",
      "                Loss: fixed  34 labels. Loss 0.42463. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55789113\n",
      "Train loss (w/o reg) on all data: 0.5577889\n",
      "Test loss (w/o reg) on all data: 0.46910417\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0034654252\n",
      "Norm of the params: 1.4297915\n",
      "              Random: fixed  11 labels. Loss 0.46910. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.604892\n",
      "Train loss (w/o reg) on all data: 0.604843\n",
      "Test loss (w/o reg) on all data: 0.41877866\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0034801746\n",
      "Norm of the params: 0.9898858\n",
      "Flipped loss: 0.41878. Accuracy: 0.867\n",
      "### Flips: 50, rs: 31, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [133] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5869002\n",
      "Train loss (w/o reg) on all data: 0.5868508\n",
      "Test loss (w/o reg) on all data: 0.4011163\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.004120724\n",
      "Norm of the params: 0.9934837\n",
      "     Influence (LOO): fixed   3 labels. Loss 0.40112. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [114] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5253206\n",
      "Train loss (w/o reg) on all data: 0.5252016\n",
      "Test loss (w/o reg) on all data: 0.3684833\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0028943212\n",
      "Norm of the params: 1.5425861\n",
      "                Loss: fixed   9 labels. Loss 0.36848. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [175] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5995518\n",
      "Train loss (w/o reg) on all data: 0.59949493\n",
      "Test loss (w/o reg) on all data: 0.4054726\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.001318733\n",
      "Norm of the params: 1.0663542\n",
      "              Random: fixed   1 labels. Loss 0.40547. Accuracy 0.867.\n",
      "### Flips: 50, rs: 31, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [175] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55216473\n",
      "Train loss (w/o reg) on all data: 0.5520652\n",
      "Test loss (w/o reg) on all data: 0.3604903\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004080321\n",
      "Norm of the params: 1.4108171\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.36049. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4583982\n",
      "Train loss (w/o reg) on all data: 0.45816225\n",
      "Test loss (w/o reg) on all data: 0.3377063\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004400658\n",
      "Norm of the params: 2.1723294\n",
      "                Loss: fixed  16 labels. Loss 0.33771. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5891838\n",
      "Train loss (w/o reg) on all data: 0.5891154\n",
      "Test loss (w/o reg) on all data: 0.40155217\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0022859012\n",
      "Norm of the params: 1.1698654\n",
      "              Random: fixed   4 labels. Loss 0.40155. Accuracy 0.844.\n",
      "### Flips: 50, rs: 31, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [180] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.531209\n",
      "Train loss (w/o reg) on all data: 0.5311019\n",
      "Test loss (w/o reg) on all data: 0.34406635\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00021456409\n",
      "Norm of the params: 1.463592\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.34407. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [301] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4017319\n",
      "Train loss (w/o reg) on all data: 0.4013681\n",
      "Test loss (w/o reg) on all data: 0.37391812\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00034733283\n",
      "Norm of the params: 2.6973796\n",
      "                Loss: fixed  23 labels. Loss 0.37392. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [330] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5870497\n",
      "Train loss (w/o reg) on all data: 0.58698165\n",
      "Test loss (w/o reg) on all data: 0.3976553\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0010893713\n",
      "Norm of the params: 1.1667806\n",
      "              Random: fixed   6 labels. Loss 0.39766. Accuracy 0.867.\n",
      "### Flips: 50, rs: 31, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [122] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51524186\n",
      "Train loss (w/o reg) on all data: 0.5151082\n",
      "Test loss (w/o reg) on all data: 0.34572798\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011722504\n",
      "Norm of the params: 1.634924\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.34573. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [324] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37421715\n",
      "Train loss (w/o reg) on all data: 0.37380102\n",
      "Test loss (w/o reg) on all data: 0.40618867\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006753279\n",
      "Norm of the params: 2.8848803\n",
      "                Loss: fixed  28 labels. Loss 0.40619. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [284] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58123565\n",
      "Train loss (w/o reg) on all data: 0.58115834\n",
      "Test loss (w/o reg) on all data: 0.39079556\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002469401\n",
      "Norm of the params: 1.2432202\n",
      "              Random: fixed   7 labels. Loss 0.39080. Accuracy 0.844.\n",
      "### Flips: 50, rs: 31, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [385] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47576103\n",
      "Train loss (w/o reg) on all data: 0.47559154\n",
      "Test loss (w/o reg) on all data: 0.32781553\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0008452913\n",
      "Norm of the params: 1.8411059\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.32782. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35197273\n",
      "Train loss (w/o reg) on all data: 0.35155365\n",
      "Test loss (w/o reg) on all data: 0.3973047\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.017604627\n",
      "Norm of the params: 2.895148\n",
      "                Loss: fixed  34 labels. Loss 0.39730. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5675015\n",
      "Train loss (w/o reg) on all data: 0.56740737\n",
      "Test loss (w/o reg) on all data: 0.37754318\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00082395587\n",
      "Norm of the params: 1.371935\n",
      "              Random: fixed  10 labels. Loss 0.37754. Accuracy 0.844.\n",
      "### Flips: 50, rs: 31, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4519417\n",
      "Train loss (w/o reg) on all data: 0.45173207\n",
      "Test loss (w/o reg) on all data: 0.33059663\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0038954502\n",
      "Norm of the params: 2.0475852\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.33060. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [331] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32777503\n",
      "Train loss (w/o reg) on all data: 0.32728434\n",
      "Test loss (w/o reg) on all data: 0.41941032\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00053486816\n",
      "Norm of the params: 3.1326866\n",
      "                Loss: fixed  37 labels. Loss 0.41941. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55385125\n",
      "Train loss (w/o reg) on all data: 0.55375147\n",
      "Test loss (w/o reg) on all data: 0.36943227\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.007435284\n",
      "Norm of the params: 1.412743\n",
      "              Random: fixed  13 labels. Loss 0.36943. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [419] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57383835\n",
      "Train loss (w/o reg) on all data: 0.5737556\n",
      "Test loss (w/o reg) on all data: 0.4593296\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00090551033\n",
      "Norm of the params: 1.2861605\n",
      "Flipped loss: 0.45933. Accuracy: 0.822\n",
      "### Flips: 50, rs: 32, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [146] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5362317\n",
      "Train loss (w/o reg) on all data: 0.5361206\n",
      "Test loss (w/o reg) on all data: 0.4379402\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014206131\n",
      "Norm of the params: 1.4908408\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.43794. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48148367\n",
      "Train loss (w/o reg) on all data: 0.4812958\n",
      "Test loss (w/o reg) on all data: 0.42565048\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011406261\n",
      "Norm of the params: 1.9384195\n",
      "                Loss: fixed   9 labels. Loss 0.42565. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56438196\n",
      "Train loss (w/o reg) on all data: 0.564288\n",
      "Test loss (w/o reg) on all data: 0.4527086\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0041392315\n",
      "Norm of the params: 1.3705701\n",
      "              Random: fixed   3 labels. Loss 0.45271. Accuracy 0.822.\n",
      "### Flips: 50, rs: 32, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5139755\n",
      "Train loss (w/o reg) on all data: 0.5138591\n",
      "Test loss (w/o reg) on all data: 0.42576146\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00087057723\n",
      "Norm of the params: 1.5258353\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.42576. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43018043\n",
      "Train loss (w/o reg) on all data: 0.42989478\n",
      "Test loss (w/o reg) on all data: 0.4470687\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0020714346\n",
      "Norm of the params: 2.3902652\n",
      "                Loss: fixed  14 labels. Loss 0.44707. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57144105\n",
      "Train loss (w/o reg) on all data: 0.5713614\n",
      "Test loss (w/o reg) on all data: 0.45712522\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.008630279\n",
      "Norm of the params: 1.2619421\n",
      "              Random: fixed   6 labels. Loss 0.45713. Accuracy 0.822.\n",
      "### Flips: 50, rs: 32, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.491039\n",
      "Train loss (w/o reg) on all data: 0.49089742\n",
      "Test loss (w/o reg) on all data: 0.41354853\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0036030663\n",
      "Norm of the params: 1.6828398\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.41355. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38860136\n",
      "Train loss (w/o reg) on all data: 0.38820022\n",
      "Test loss (w/o reg) on all data: 0.43665096\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014440722\n",
      "Norm of the params: 2.8324225\n",
      "                Loss: fixed  20 labels. Loss 0.43665. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [335] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56941116\n",
      "Train loss (w/o reg) on all data: 0.5693311\n",
      "Test loss (w/o reg) on all data: 0.44485736\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0024706542\n",
      "Norm of the params: 1.2654946\n",
      "              Random: fixed   8 labels. Loss 0.44486. Accuracy 0.822.\n",
      "### Flips: 50, rs: 32, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [136] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4518515\n",
      "Train loss (w/o reg) on all data: 0.45168355\n",
      "Test loss (w/o reg) on all data: 0.39575955\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019224533\n",
      "Norm of the params: 1.8326908\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.39576. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36752388\n",
      "Train loss (w/o reg) on all data: 0.36710393\n",
      "Test loss (w/o reg) on all data: 0.43336132\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0016054328\n",
      "Norm of the params: 2.898085\n",
      "                Loss: fixed  24 labels. Loss 0.43336. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [421] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5603168\n",
      "Train loss (w/o reg) on all data: 0.5602383\n",
      "Test loss (w/o reg) on all data: 0.4290576\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.006777275\n",
      "Norm of the params: 1.2531081\n",
      "              Random: fixed  10 labels. Loss 0.42906. Accuracy 0.822.\n",
      "### Flips: 50, rs: 32, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [305] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4368525\n",
      "Train loss (w/o reg) on all data: 0.4366776\n",
      "Test loss (w/o reg) on all data: 0.39196235\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.005308761\n",
      "Norm of the params: 1.8703253\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.39196. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [341] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34461012\n",
      "Train loss (w/o reg) on all data: 0.3441751\n",
      "Test loss (w/o reg) on all data: 0.42885396\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0005544174\n",
      "Norm of the params: 2.949609\n",
      "                Loss: fixed  28 labels. Loss 0.42885. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [379] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54644924\n",
      "Train loss (w/o reg) on all data: 0.54635996\n",
      "Test loss (w/o reg) on all data: 0.4255441\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.007128004\n",
      "Norm of the params: 1.3361133\n",
      "              Random: fixed  12 labels. Loss 0.42554. Accuracy 0.800.\n",
      "### Flips: 50, rs: 32, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [307] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.404219\n",
      "Train loss (w/o reg) on all data: 0.40401512\n",
      "Test loss (w/o reg) on all data: 0.377066\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005003117\n",
      "Norm of the params: 2.0193257\n",
      "     Influence (LOO): fixed  28 labels. Loss 0.37707. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [347] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33791402\n",
      "Train loss (w/o reg) on all data: 0.33746687\n",
      "Test loss (w/o reg) on all data: 0.44006\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00053144566\n",
      "Norm of the params: 2.9904768\n",
      "                Loss: fixed  33 labels. Loss 0.44006. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54594976\n",
      "Train loss (w/o reg) on all data: 0.54586345\n",
      "Test loss (w/o reg) on all data: 0.4232264\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.009326488\n",
      "Norm of the params: 1.3136054\n",
      "              Random: fixed  15 labels. Loss 0.42323. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5978846\n",
      "Train loss (w/o reg) on all data: 0.5978436\n",
      "Test loss (w/o reg) on all data: 0.47922435\n",
      "Train acc on all data:  0.660377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00090586994\n",
      "Norm of the params: 0.90566313\n",
      "Flipped loss: 0.47922. Accuracy: 0.800\n",
      "### Flips: 50, rs: 33, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [301] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.553322\n",
      "Train loss (w/o reg) on all data: 0.5532482\n",
      "Test loss (w/o reg) on all data: 0.46082988\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00030817435\n",
      "Norm of the params: 1.2146503\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.46083. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [187] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5166626\n",
      "Train loss (w/o reg) on all data: 0.5165827\n",
      "Test loss (w/o reg) on all data: 0.43283203\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00053476443\n",
      "Norm of the params: 1.2641011\n",
      "                Loss: fixed   9 labels. Loss 0.43283. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [143] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6000443\n",
      "Train loss (w/o reg) on all data: 0.60000074\n",
      "Test loss (w/o reg) on all data: 0.4780354\n",
      "Train acc on all data:  0.6509433962264151\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004782014\n",
      "Norm of the params: 0.9332851\n",
      "              Random: fixed   1 labels. Loss 0.47804. Accuracy 0.800.\n",
      "### Flips: 50, rs: 33, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [201] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5380811\n",
      "Train loss (w/o reg) on all data: 0.5379933\n",
      "Test loss (w/o reg) on all data: 0.44856495\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00039980054\n",
      "Norm of the params: 1.3251913\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.44856. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [289] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4693917\n",
      "Train loss (w/o reg) on all data: 0.46924362\n",
      "Test loss (w/o reg) on all data: 0.45835787\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005511122\n",
      "Norm of the params: 1.7209567\n",
      "                Loss: fixed  15 labels. Loss 0.45836. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5951983\n",
      "Train loss (w/o reg) on all data: 0.5951538\n",
      "Test loss (w/o reg) on all data: 0.47156775\n",
      "Train acc on all data:  0.6650943396226415\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0027031244\n",
      "Norm of the params: 0.9430239\n",
      "              Random: fixed   3 labels. Loss 0.47157. Accuracy 0.800.\n",
      "### Flips: 50, rs: 33, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [131] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5160253\n",
      "Train loss (w/o reg) on all data: 0.5159164\n",
      "Test loss (w/o reg) on all data: 0.43519711\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004979148\n",
      "Norm of the params: 1.4756554\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.43520. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [153] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43298677\n",
      "Train loss (w/o reg) on all data: 0.43280572\n",
      "Test loss (w/o reg) on all data: 0.45703244\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017683876\n",
      "Norm of the params: 1.902928\n",
      "                Loss: fixed  20 labels. Loss 0.45703. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [283] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5799389\n",
      "Train loss (w/o reg) on all data: 0.5798829\n",
      "Test loss (w/o reg) on all data: 0.4680745\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0024795325\n",
      "Norm of the params: 1.057819\n",
      "              Random: fixed   5 labels. Loss 0.46807. Accuracy 0.778.\n",
      "### Flips: 50, rs: 33, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4883216\n",
      "Train loss (w/o reg) on all data: 0.48820585\n",
      "Test loss (w/o reg) on all data: 0.40449768\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00083796604\n",
      "Norm of the params: 1.5215052\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.40450. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [289] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41090134\n",
      "Train loss (w/o reg) on all data: 0.4107109\n",
      "Test loss (w/o reg) on all data: 0.44251293\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015536433\n",
      "Norm of the params: 1.9516037\n",
      "                Loss: fixed  24 labels. Loss 0.44251. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57155544\n",
      "Train loss (w/o reg) on all data: 0.5715011\n",
      "Test loss (w/o reg) on all data: 0.46424276\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018300476\n",
      "Norm of the params: 1.042824\n",
      "              Random: fixed   6 labels. Loss 0.46424. Accuracy 0.800.\n",
      "### Flips: 50, rs: 33, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [116] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46695048\n",
      "Train loss (w/o reg) on all data: 0.46682277\n",
      "Test loss (w/o reg) on all data: 0.41402394\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013048955\n",
      "Norm of the params: 1.5981125\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.41402. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [289] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3747738\n",
      "Train loss (w/o reg) on all data: 0.37455738\n",
      "Test loss (w/o reg) on all data: 0.44190148\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0065027573\n",
      "Norm of the params: 2.0804462\n",
      "                Loss: fixed  31 labels. Loss 0.44190. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [348] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56897956\n",
      "Train loss (w/o reg) on all data: 0.56892943\n",
      "Test loss (w/o reg) on all data: 0.4645959\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0004804998\n",
      "Norm of the params: 1.0009842\n",
      "              Random: fixed   7 labels. Loss 0.46460. Accuracy 0.800.\n",
      "### Flips: 50, rs: 33, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44716117\n",
      "Train loss (w/o reg) on all data: 0.44701093\n",
      "Test loss (w/o reg) on all data: 0.4043235\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00082298747\n",
      "Norm of the params: 1.7333641\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.40432. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [269] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35972458\n",
      "Train loss (w/o reg) on all data: 0.35945284\n",
      "Test loss (w/o reg) on all data: 0.43964165\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009854722\n",
      "Norm of the params: 2.3312879\n",
      "                Loss: fixed  34 labels. Loss 0.43964. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55491453\n",
      "Train loss (w/o reg) on all data: 0.55485123\n",
      "Test loss (w/o reg) on all data: 0.45391375\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00084539974\n",
      "Norm of the params: 1.1251675\n",
      "              Random: fixed  11 labels. Loss 0.45391. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [288] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57100713\n",
      "Train loss (w/o reg) on all data: 0.5709337\n",
      "Test loss (w/o reg) on all data: 0.4009062\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.009481736\n",
      "Norm of the params: 1.2118369\n",
      "Flipped loss: 0.40091. Accuracy: 0.844\n",
      "### Flips: 50, rs: 34, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.507803\n",
      "Train loss (w/o reg) on all data: 0.5076373\n",
      "Test loss (w/o reg) on all data: 0.390152\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0010409362\n",
      "Norm of the params: 1.8202957\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.39015. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44472498\n",
      "Train loss (w/o reg) on all data: 0.44443756\n",
      "Test loss (w/o reg) on all data: 0.39524242\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0003315397\n",
      "Norm of the params: 2.3975656\n",
      "                Loss: fixed   9 labels. Loss 0.39524. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [168] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57273656\n",
      "Train loss (w/o reg) on all data: 0.5726782\n",
      "Test loss (w/o reg) on all data: 0.40282238\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0018845858\n",
      "Norm of the params: 1.0800698\n",
      "              Random: fixed   5 labels. Loss 0.40282. Accuracy 0.844.\n",
      "### Flips: 50, rs: 34, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [339] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4740839\n",
      "Train loss (w/o reg) on all data: 0.473892\n",
      "Test loss (w/o reg) on all data: 0.3685196\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0010914828\n",
      "Norm of the params: 1.9590607\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.36852. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41382533\n",
      "Train loss (w/o reg) on all data: 0.41346502\n",
      "Test loss (w/o reg) on all data: 0.39773571\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0014867616\n",
      "Norm of the params: 2.6844382\n",
      "                Loss: fixed  13 labels. Loss 0.39774. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5704954\n",
      "Train loss (w/o reg) on all data: 0.57043326\n",
      "Test loss (w/o reg) on all data: 0.406848\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0024755846\n",
      "Norm of the params: 1.1151568\n",
      "              Random: fixed   6 labels. Loss 0.40685. Accuracy 0.822.\n",
      "### Flips: 50, rs: 34, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43358657\n",
      "Train loss (w/o reg) on all data: 0.43332514\n",
      "Test loss (w/o reg) on all data: 0.3687523\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0030298284\n",
      "Norm of the params: 2.2865446\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.36875. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [293] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3785638\n",
      "Train loss (w/o reg) on all data: 0.37813145\n",
      "Test loss (w/o reg) on all data: 0.38358417\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0009779063\n",
      "Norm of the params: 2.940537\n",
      "                Loss: fixed  19 labels. Loss 0.38358. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [403] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56097716\n",
      "Train loss (w/o reg) on all data: 0.5609072\n",
      "Test loss (w/o reg) on all data: 0.40376446\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004510719\n",
      "Norm of the params: 1.1827773\n",
      "              Random: fixed   7 labels. Loss 0.40376. Accuracy 0.822.\n",
      "### Flips: 50, rs: 34, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42077568\n",
      "Train loss (w/o reg) on all data: 0.42056483\n",
      "Test loss (w/o reg) on all data: 0.35418984\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0017043044\n",
      "Norm of the params: 2.053525\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.35419. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33504617\n",
      "Train loss (w/o reg) on all data: 0.3344451\n",
      "Test loss (w/o reg) on all data: 0.41813818\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0028606108\n",
      "Norm of the params: 3.4671948\n",
      "                Loss: fixed  26 labels. Loss 0.41814. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [378] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.557806\n",
      "Train loss (w/o reg) on all data: 0.5577272\n",
      "Test loss (w/o reg) on all data: 0.40397617\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011967418\n",
      "Norm of the params: 1.2552018\n",
      "              Random: fixed  11 labels. Loss 0.40398. Accuracy 0.822.\n",
      "### Flips: 50, rs: 34, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [365] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39441168\n",
      "Train loss (w/o reg) on all data: 0.39411673\n",
      "Test loss (w/o reg) on all data: 0.38944155\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004454392\n",
      "Norm of the params: 2.428766\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.38944. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [289] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32060015\n",
      "Train loss (w/o reg) on all data: 0.3200234\n",
      "Test loss (w/o reg) on all data: 0.45140004\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018472358\n",
      "Norm of the params: 3.396375\n",
      "                Loss: fixed  30 labels. Loss 0.45140. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [329] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.538576\n",
      "Train loss (w/o reg) on all data: 0.5384785\n",
      "Test loss (w/o reg) on all data: 0.38189748\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00026573517\n",
      "Norm of the params: 1.3964536\n",
      "              Random: fixed  13 labels. Loss 0.38190. Accuracy 0.844.\n",
      "### Flips: 50, rs: 34, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3847474\n",
      "Train loss (w/o reg) on all data: 0.38449237\n",
      "Test loss (w/o reg) on all data: 0.3781696\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007412805\n",
      "Norm of the params: 2.258425\n",
      "     Influence (LOO): fixed  30 labels. Loss 0.37817. Accuracy 0.822.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3128005\n",
      "Train loss (w/o reg) on all data: 0.31230178\n",
      "Test loss (w/o reg) on all data: 0.44060513\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00040474354\n",
      "Norm of the params: 3.1582491\n",
      "                Loss: fixed  33 labels. Loss 0.44061. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [289] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53858215\n",
      "Train loss (w/o reg) on all data: 0.5384845\n",
      "Test loss (w/o reg) on all data: 0.38236165\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010505486\n",
      "Norm of the params: 1.3975568\n",
      "              Random: fixed  13 labels. Loss 0.38236. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58182716\n",
      "Train loss (w/o reg) on all data: 0.58173984\n",
      "Test loss (w/o reg) on all data: 0.4174728\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0021546176\n",
      "Norm of the params: 1.3217456\n",
      "Flipped loss: 0.41747. Accuracy: 0.778\n",
      "### Flips: 50, rs: 35, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [193] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.526818\n",
      "Train loss (w/o reg) on all data: 0.5267062\n",
      "Test loss (w/o reg) on all data: 0.37549227\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.003739411\n",
      "Norm of the params: 1.4951533\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.37549. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [297] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49881217\n",
      "Train loss (w/o reg) on all data: 0.49861938\n",
      "Test loss (w/o reg) on all data: 0.35499173\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.005736661\n",
      "Norm of the params: 1.963653\n",
      "                Loss: fixed   8 labels. Loss 0.35499. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [177] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58298737\n",
      "Train loss (w/o reg) on all data: 0.582902\n",
      "Test loss (w/o reg) on all data: 0.40817806\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002244583\n",
      "Norm of the params: 1.3066099\n",
      "              Random: fixed   2 labels. Loss 0.40818. Accuracy 0.800.\n",
      "### Flips: 50, rs: 35, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [319] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50317866\n",
      "Train loss (w/o reg) on all data: 0.5030185\n",
      "Test loss (w/o reg) on all data: 0.38001364\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012102412\n",
      "Norm of the params: 1.7898687\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.38001. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4280962\n",
      "Train loss (w/o reg) on all data: 0.42779207\n",
      "Test loss (w/o reg) on all data: 0.3545351\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0020568783\n",
      "Norm of the params: 2.4662516\n",
      "                Loss: fixed  16 labels. Loss 0.35454. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58299565\n",
      "Train loss (w/o reg) on all data: 0.58291197\n",
      "Test loss (w/o reg) on all data: 0.40864795\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.005073415\n",
      "Norm of the params: 1.2937088\n",
      "              Random: fixed   2 labels. Loss 0.40865. Accuracy 0.800.\n",
      "### Flips: 50, rs: 35, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47660214\n",
      "Train loss (w/o reg) on all data: 0.47644237\n",
      "Test loss (w/o reg) on all data: 0.36492154\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00988781\n",
      "Norm of the params: 1.7876383\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.36492. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38911605\n",
      "Train loss (w/o reg) on all data: 0.3887796\n",
      "Test loss (w/o reg) on all data: 0.36481708\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0017318794\n",
      "Norm of the params: 2.593992\n",
      "                Loss: fixed  22 labels. Loss 0.36482. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [272] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5783006\n",
      "Train loss (w/o reg) on all data: 0.578212\n",
      "Test loss (w/o reg) on all data: 0.41485766\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0028073438\n",
      "Norm of the params: 1.3311255\n",
      "              Random: fixed   4 labels. Loss 0.41486. Accuracy 0.778.\n",
      "### Flips: 50, rs: 35, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42754936\n",
      "Train loss (w/o reg) on all data: 0.42730546\n",
      "Test loss (w/o reg) on all data: 0.353178\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011738314\n",
      "Norm of the params: 2.2086372\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.35318. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36019567\n",
      "Train loss (w/o reg) on all data: 0.35975298\n",
      "Test loss (w/o reg) on all data: 0.39250225\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0025720114\n",
      "Norm of the params: 2.975465\n",
      "                Loss: fixed  27 labels. Loss 0.39250. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [319] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57984596\n",
      "Train loss (w/o reg) on all data: 0.5797538\n",
      "Test loss (w/o reg) on all data: 0.41391274\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0051636104\n",
      "Norm of the params: 1.3575451\n",
      "              Random: fixed   6 labels. Loss 0.41391. Accuracy 0.778.\n",
      "### Flips: 50, rs: 35, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [202] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41903755\n",
      "Train loss (w/o reg) on all data: 0.41877308\n",
      "Test loss (w/o reg) on all data: 0.34521914\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00075051823\n",
      "Norm of the params: 2.2998838\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.34522. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [179] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33057532\n",
      "Train loss (w/o reg) on all data: 0.3300826\n",
      "Test loss (w/o reg) on all data: 0.43975943\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012687407\n",
      "Norm of the params: 3.139143\n",
      "                Loss: fixed  32 labels. Loss 0.43976. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [323] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5746387\n",
      "Train loss (w/o reg) on all data: 0.5745553\n",
      "Test loss (w/o reg) on all data: 0.41729435\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015808071\n",
      "Norm of the params: 1.292044\n",
      "              Random: fixed   9 labels. Loss 0.41729. Accuracy 0.778.\n",
      "### Flips: 50, rs: 35, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39367804\n",
      "Train loss (w/o reg) on all data: 0.39335158\n",
      "Test loss (w/o reg) on all data: 0.3453907\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00060603174\n",
      "Norm of the params: 2.5552201\n",
      "     Influence (LOO): fixed  29 labels. Loss 0.34539. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3119734\n",
      "Train loss (w/o reg) on all data: 0.3114154\n",
      "Test loss (w/o reg) on all data: 0.45422113\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018797066\n",
      "Norm of the params: 3.340626\n",
      "                Loss: fixed  37 labels. Loss 0.45422. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [348] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5485455\n",
      "Train loss (w/o reg) on all data: 0.5484484\n",
      "Test loss (w/o reg) on all data: 0.40178558\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009832102\n",
      "Norm of the params: 1.3937\n",
      "              Random: fixed  13 labels. Loss 0.40179. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [102] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60423285\n",
      "Train loss (w/o reg) on all data: 0.6041909\n",
      "Test loss (w/o reg) on all data: 0.5155646\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.005385265\n",
      "Norm of the params: 0.91577524\n",
      "Flipped loss: 0.51556. Accuracy: 0.756\n",
      "### Flips: 50, rs: 36, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56548816\n",
      "Train loss (w/o reg) on all data: 0.5654029\n",
      "Test loss (w/o reg) on all data: 0.4656868\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.009158049\n",
      "Norm of the params: 1.3057144\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.46569. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5290542\n",
      "Train loss (w/o reg) on all data: 0.52894866\n",
      "Test loss (w/o reg) on all data: 0.52509475\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0005256226\n",
      "Norm of the params: 1.4528685\n",
      "                Loss: fixed   8 labels. Loss 0.52509. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5994958\n",
      "Train loss (w/o reg) on all data: 0.5994527\n",
      "Test loss (w/o reg) on all data: 0.4980065\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016775309\n",
      "Norm of the params: 0.9287107\n",
      "              Random: fixed   1 labels. Loss 0.49801. Accuracy 0.800.\n",
      "### Flips: 50, rs: 36, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5304851\n",
      "Train loss (w/o reg) on all data: 0.53038025\n",
      "Test loss (w/o reg) on all data: 0.4538923\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0009208171\n",
      "Norm of the params: 1.447932\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.45389. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [266] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44261903\n",
      "Train loss (w/o reg) on all data: 0.442395\n",
      "Test loss (w/o reg) on all data: 0.47406867\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0008367367\n",
      "Norm of the params: 2.1166747\n",
      "                Loss: fixed  18 labels. Loss 0.47407. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6043049\n",
      "Train loss (w/o reg) on all data: 0.6042613\n",
      "Test loss (w/o reg) on all data: 0.50379723\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00091209\n",
      "Norm of the params: 0.9344485\n",
      "              Random: fixed   2 labels. Loss 0.50380. Accuracy 0.800.\n",
      "### Flips: 50, rs: 36, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5008169\n",
      "Train loss (w/o reg) on all data: 0.50069785\n",
      "Test loss (w/o reg) on all data: 0.4420358\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0021079667\n",
      "Norm of the params: 1.5429629\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.44204. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4179608\n",
      "Train loss (w/o reg) on all data: 0.41775396\n",
      "Test loss (w/o reg) on all data: 0.4635618\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0038651333\n",
      "Norm of the params: 2.0338638\n",
      "                Loss: fixed  22 labels. Loss 0.46356. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [302] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6043046\n",
      "Train loss (w/o reg) on all data: 0.60426074\n",
      "Test loss (w/o reg) on all data: 0.50356\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0006036296\n",
      "Norm of the params: 0.93684745\n",
      "              Random: fixed   2 labels. Loss 0.50356. Accuracy 0.778.\n",
      "### Flips: 50, rs: 36, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48703852\n",
      "Train loss (w/o reg) on all data: 0.48691162\n",
      "Test loss (w/o reg) on all data: 0.4141516\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006155157\n",
      "Norm of the params: 1.5930431\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.41415. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [465] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37234935\n",
      "Train loss (w/o reg) on all data: 0.372024\n",
      "Test loss (w/o reg) on all data: 0.41512376\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0035360754\n",
      "Norm of the params: 2.5509012\n",
      "                Loss: fixed  27 labels. Loss 0.41512. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [310] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5941005\n",
      "Train loss (w/o reg) on all data: 0.5940553\n",
      "Test loss (w/o reg) on all data: 0.47197092\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0029383767\n",
      "Norm of the params: 0.95061773\n",
      "              Random: fixed   5 labels. Loss 0.47197. Accuracy 0.800.\n",
      "### Flips: 50, rs: 36, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [258] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44479597\n",
      "Train loss (w/o reg) on all data: 0.4445919\n",
      "Test loss (w/o reg) on all data: 0.43503693\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011019479\n",
      "Norm of the params: 2.0201948\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.43504. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [343] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33882022\n",
      "Train loss (w/o reg) on all data: 0.338463\n",
      "Test loss (w/o reg) on all data: 0.40865394\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0014182733\n",
      "Norm of the params: 2.6728828\n",
      "                Loss: fixed  32 labels. Loss 0.40865. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [198] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58972985\n",
      "Train loss (w/o reg) on all data: 0.58968216\n",
      "Test loss (w/o reg) on all data: 0.4888213\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0019820088\n",
      "Norm of the params: 0.97654647\n",
      "              Random: fixed   9 labels. Loss 0.48882. Accuracy 0.778.\n",
      "### Flips: 50, rs: 36, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [172] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42210275\n",
      "Train loss (w/o reg) on all data: 0.4218559\n",
      "Test loss (w/o reg) on all data: 0.43752718\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007323476\n",
      "Norm of the params: 2.2219992\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.43753. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32019892\n",
      "Train loss (w/o reg) on all data: 0.31976297\n",
      "Test loss (w/o reg) on all data: 0.4242028\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00035377027\n",
      "Norm of the params: 2.952775\n",
      "                Loss: fixed  34 labels. Loss 0.42420. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5787151\n",
      "Train loss (w/o reg) on all data: 0.5786626\n",
      "Test loss (w/o reg) on all data: 0.49255726\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00120875\n",
      "Norm of the params: 1.0247602\n",
      "              Random: fixed  12 labels. Loss 0.49256. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [118] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6009342\n",
      "Train loss (w/o reg) on all data: 0.6008734\n",
      "Test loss (w/o reg) on all data: 0.5368659\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.016164372\n",
      "Norm of the params: 1.1029637\n",
      "Flipped loss: 0.53687. Accuracy: 0.733\n",
      "### Flips: 50, rs: 37, checks: 10\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55503035\n",
      "Train loss (w/o reg) on all data: 0.5549291\n",
      "Test loss (w/o reg) on all data: 0.49228364\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0023964683\n",
      "Norm of the params: 1.4231023\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.49228. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [118] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5323309\n",
      "Train loss (w/o reg) on all data: 0.5322113\n",
      "Test loss (w/o reg) on all data: 0.48722157\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0007163836\n",
      "Norm of the params: 1.5465661\n",
      "                Loss: fixed   8 labels. Loss 0.48722. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.596107\n",
      "Train loss (w/o reg) on all data: 0.59604007\n",
      "Test loss (w/o reg) on all data: 0.52475953\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.005436314\n",
      "Norm of the params: 1.156951\n",
      "              Random: fixed   2 labels. Loss 0.52476. Accuracy 0.733.\n",
      "### Flips: 50, rs: 37, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [167] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5346165\n",
      "Train loss (w/o reg) on all data: 0.53448945\n",
      "Test loss (w/o reg) on all data: 0.47646153\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018984226\n",
      "Norm of the params: 1.5939827\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.47646. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46965817\n",
      "Train loss (w/o reg) on all data: 0.46945405\n",
      "Test loss (w/o reg) on all data: 0.47136748\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0075029363\n",
      "Norm of the params: 2.020513\n",
      "                Loss: fixed  16 labels. Loss 0.47137. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5961067\n",
      "Train loss (w/o reg) on all data: 0.5960397\n",
      "Test loss (w/o reg) on all data: 0.5238361\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.007047557\n",
      "Norm of the params: 1.1574028\n",
      "              Random: fixed   2 labels. Loss 0.52384. Accuracy 0.733.\n",
      "### Flips: 50, rs: 37, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [190] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50193673\n",
      "Train loss (w/o reg) on all data: 0.50176316\n",
      "Test loss (w/o reg) on all data: 0.47131258\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0032163395\n",
      "Norm of the params: 1.8631634\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.47131. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4163408\n",
      "Train loss (w/o reg) on all data: 0.41606897\n",
      "Test loss (w/o reg) on all data: 0.4474679\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0027008166\n",
      "Norm of the params: 2.3316007\n",
      "                Loss: fixed  22 labels. Loss 0.44747. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [288] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58909476\n",
      "Train loss (w/o reg) on all data: 0.5890251\n",
      "Test loss (w/o reg) on all data: 0.5329909\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0019123141\n",
      "Norm of the params: 1.1804196\n",
      "              Random: fixed   3 labels. Loss 0.53299. Accuracy 0.711.\n",
      "### Flips: 50, rs: 37, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [103] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46719536\n",
      "Train loss (w/o reg) on all data: 0.46694916\n",
      "Test loss (w/o reg) on all data: 0.45952147\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0009907342\n",
      "Norm of the params: 2.2189574\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.45952. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [140] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38080218\n",
      "Train loss (w/o reg) on all data: 0.38047796\n",
      "Test loss (w/o reg) on all data: 0.4867143\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0010472385\n",
      "Norm of the params: 2.5464735\n",
      "                Loss: fixed  27 labels. Loss 0.48671. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5846427\n",
      "Train loss (w/o reg) on all data: 0.584581\n",
      "Test loss (w/o reg) on all data: 0.5172246\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.012247193\n",
      "Norm of the params: 1.1107538\n",
      "              Random: fixed   5 labels. Loss 0.51722. Accuracy 0.733.\n",
      "### Flips: 50, rs: 37, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4487903\n",
      "Train loss (w/o reg) on all data: 0.4485215\n",
      "Test loss (w/o reg) on all data: 0.44283745\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0020246801\n",
      "Norm of the params: 2.3186963\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.44284. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [159] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34226862\n",
      "Train loss (w/o reg) on all data: 0.34188923\n",
      "Test loss (w/o reg) on all data: 0.48730963\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.00094828225\n",
      "Norm of the params: 2.7545977\n",
      "                Loss: fixed  32 labels. Loss 0.48731. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.580321\n",
      "Train loss (w/o reg) on all data: 0.58025914\n",
      "Test loss (w/o reg) on all data: 0.5161643\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0013227258\n",
      "Norm of the params: 1.1121527\n",
      "              Random: fixed   6 labels. Loss 0.51616. Accuracy 0.756.\n",
      "### Flips: 50, rs: 37, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4386291\n",
      "Train loss (w/o reg) on all data: 0.43837968\n",
      "Test loss (w/o reg) on all data: 0.46257874\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0075735734\n",
      "Norm of the params: 2.233442\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.46258. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [316] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32196712\n",
      "Train loss (w/o reg) on all data: 0.32153818\n",
      "Test loss (w/o reg) on all data: 0.4842788\n",
      "Train acc on all data:  0.8867924528301887\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0008779092\n",
      "Norm of the params: 2.9290054\n",
      "                Loss: fixed  36 labels. Loss 0.48428. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57838553\n",
      "Train loss (w/o reg) on all data: 0.57832843\n",
      "Test loss (w/o reg) on all data: 0.5169238\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0022757808\n",
      "Norm of the params: 1.0687652\n",
      "              Random: fixed   7 labels. Loss 0.51692. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.62389547\n",
      "Train loss (w/o reg) on all data: 0.6238576\n",
      "Test loss (w/o reg) on all data: 0.49807537\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0032550683\n",
      "Norm of the params: 0.86974245\n",
      "Flipped loss: 0.49808. Accuracy: 0.778\n",
      "### Flips: 50, rs: 38, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59026116\n",
      "Train loss (w/o reg) on all data: 0.59017175\n",
      "Test loss (w/o reg) on all data: 0.45911518\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002367035\n",
      "Norm of the params: 1.3371197\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.45912. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54160607\n",
      "Train loss (w/o reg) on all data: 0.5415022\n",
      "Test loss (w/o reg) on all data: 0.40180716\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004078146\n",
      "Norm of the params: 1.4416257\n",
      "                Loss: fixed   9 labels. Loss 0.40181. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60240984\n",
      "Train loss (w/o reg) on all data: 0.60235894\n",
      "Test loss (w/o reg) on all data: 0.47951847\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.004588444\n",
      "Norm of the params: 1.0089678\n",
      "              Random: fixed   5 labels. Loss 0.47952. Accuracy 0.778.\n",
      "### Flips: 50, rs: 38, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [155] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58525807\n",
      "Train loss (w/o reg) on all data: 0.58517563\n",
      "Test loss (w/o reg) on all data: 0.45230627\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0017373676\n",
      "Norm of the params: 1.2839686\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.45231. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [307] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4588893\n",
      "Train loss (w/o reg) on all data: 0.4586702\n",
      "Test loss (w/o reg) on all data: 0.43350282\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003272394\n",
      "Norm of the params: 2.093379\n",
      "                Loss: fixed  18 labels. Loss 0.43350. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59552056\n",
      "Train loss (w/o reg) on all data: 0.5954673\n",
      "Test loss (w/o reg) on all data: 0.46308246\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00088357565\n",
      "Norm of the params: 1.0315053\n",
      "              Random: fixed   6 labels. Loss 0.46308. Accuracy 0.778.\n",
      "### Flips: 50, rs: 38, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55211836\n",
      "Train loss (w/o reg) on all data: 0.5520289\n",
      "Test loss (w/o reg) on all data: 0.42673072\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0022431684\n",
      "Norm of the params: 1.3375336\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.42673. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4011614\n",
      "Train loss (w/o reg) on all data: 0.4008139\n",
      "Test loss (w/o reg) on all data: 0.42273217\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0050669448\n",
      "Norm of the params: 2.6362581\n",
      "                Loss: fixed  23 labels. Loss 0.42273. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [400] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5926162\n",
      "Train loss (w/o reg) on all data: 0.5925581\n",
      "Test loss (w/o reg) on all data: 0.4714549\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0035834615\n",
      "Norm of the params: 1.078369\n",
      "              Random: fixed   7 labels. Loss 0.47145. Accuracy 0.778.\n",
      "### Flips: 50, rs: 38, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [126] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54455096\n",
      "Train loss (w/o reg) on all data: 0.5444603\n",
      "Test loss (w/o reg) on all data: 0.4404416\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015506811\n",
      "Norm of the params: 1.3464849\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.44044. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [453] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38809225\n",
      "Train loss (w/o reg) on all data: 0.38772655\n",
      "Test loss (w/o reg) on all data: 0.41906074\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00068695704\n",
      "Norm of the params: 2.7045002\n",
      "                Loss: fixed  25 labels. Loss 0.41906. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5882977\n",
      "Train loss (w/o reg) on all data: 0.58823895\n",
      "Test loss (w/o reg) on all data: 0.46601066\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011695925\n",
      "Norm of the params: 1.0839833\n",
      "              Random: fixed   9 labels. Loss 0.46601. Accuracy 0.778.\n",
      "### Flips: 50, rs: 38, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49753788\n",
      "Train loss (w/o reg) on all data: 0.49742183\n",
      "Test loss (w/o reg) on all data: 0.3989922\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008387789\n",
      "Norm of the params: 1.5234957\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.39899. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36133507\n",
      "Train loss (w/o reg) on all data: 0.36094818\n",
      "Test loss (w/o reg) on all data: 0.4299131\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011779215\n",
      "Norm of the params: 2.7817166\n",
      "                Loss: fixed  30 labels. Loss 0.42991. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [307] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5708194\n",
      "Train loss (w/o reg) on all data: 0.57075065\n",
      "Test loss (w/o reg) on all data: 0.45064858\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.010275561\n",
      "Norm of the params: 1.1723335\n",
      "              Random: fixed  12 labels. Loss 0.45065. Accuracy 0.778.\n",
      "### Flips: 50, rs: 38, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [83] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4771163\n",
      "Train loss (w/o reg) on all data: 0.47697845\n",
      "Test loss (w/o reg) on all data: 0.39074057\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010364993\n",
      "Norm of the params: 1.6603665\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.39074. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33191323\n",
      "Train loss (w/o reg) on all data: 0.33145985\n",
      "Test loss (w/o reg) on all data: 0.4185049\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0060679116\n",
      "Norm of the params: 3.011233\n",
      "                Loss: fixed  35 labels. Loss 0.41850. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [302] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5533737\n",
      "Train loss (w/o reg) on all data: 0.5532896\n",
      "Test loss (w/o reg) on all data: 0.42844248\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00038150596\n",
      "Norm of the params: 1.2970753\n",
      "              Random: fixed  15 labels. Loss 0.42844. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [323] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5606457\n",
      "Train loss (w/o reg) on all data: 0.56057286\n",
      "Test loss (w/o reg) on all data: 0.413133\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0021074247\n",
      "Norm of the params: 1.2067157\n",
      "Flipped loss: 0.41313. Accuracy: 0.822\n",
      "### Flips: 50, rs: 39, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5197566\n",
      "Train loss (w/o reg) on all data: 0.51964825\n",
      "Test loss (w/o reg) on all data: 0.3886177\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004049637\n",
      "Norm of the params: 1.4720451\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.38862. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48595214\n",
      "Train loss (w/o reg) on all data: 0.4857786\n",
      "Test loss (w/o reg) on all data: 0.357823\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0012567248\n",
      "Norm of the params: 1.8629901\n",
      "                Loss: fixed   7 labels. Loss 0.35782. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5603163\n",
      "Train loss (w/o reg) on all data: 0.5602378\n",
      "Test loss (w/o reg) on all data: 0.40705875\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0028301298\n",
      "Norm of the params: 1.2528404\n",
      "              Random: fixed   1 labels. Loss 0.40706. Accuracy 0.822.\n",
      "### Flips: 50, rs: 39, checks: 20\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4801966\n",
      "Train loss (w/o reg) on all data: 0.4800156\n",
      "Test loss (w/o reg) on all data: 0.37083808\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0071107056\n",
      "Norm of the params: 1.9025986\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.37084. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [393] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40520212\n",
      "Train loss (w/o reg) on all data: 0.40486988\n",
      "Test loss (w/o reg) on all data: 0.32176673\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.000994582\n",
      "Norm of the params: 2.5777867\n",
      "                Loss: fixed  15 labels. Loss 0.32177. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [354] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55050164\n",
      "Train loss (w/o reg) on all data: 0.5504213\n",
      "Test loss (w/o reg) on all data: 0.4096185\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0009747042\n",
      "Norm of the params: 1.2674531\n",
      "              Random: fixed   6 labels. Loss 0.40962. Accuracy 0.822.\n",
      "### Flips: 50, rs: 39, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [138] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45683298\n",
      "Train loss (w/o reg) on all data: 0.45665\n",
      "Test loss (w/o reg) on all data: 0.36362448\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.004076707\n",
      "Norm of the params: 1.9130034\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.36362. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [286] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35680735\n",
      "Train loss (w/o reg) on all data: 0.35647273\n",
      "Test loss (w/o reg) on all data: 0.33058608\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.003397537\n",
      "Norm of the params: 2.5869656\n",
      "                Loss: fixed  22 labels. Loss 0.33059. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54786986\n",
      "Train loss (w/o reg) on all data: 0.54777694\n",
      "Test loss (w/o reg) on all data: 0.41302845\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015184041\n",
      "Norm of the params: 1.363107\n",
      "              Random: fixed   7 labels. Loss 0.41303. Accuracy 0.822.\n",
      "### Flips: 50, rs: 39, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44438738\n",
      "Train loss (w/o reg) on all data: 0.44422027\n",
      "Test loss (w/o reg) on all data: 0.37622228\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007334096\n",
      "Norm of the params: 1.8281416\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.37622. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [284] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31278595\n",
      "Train loss (w/o reg) on all data: 0.31226262\n",
      "Test loss (w/o reg) on all data: 0.33551058\n",
      "Train acc on all data:  0.8915094339622641\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0005903802\n",
      "Norm of the params: 3.2351832\n",
      "                Loss: fixed  27 labels. Loss 0.33551. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [359] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5381475\n",
      "Train loss (w/o reg) on all data: 0.538041\n",
      "Test loss (w/o reg) on all data: 0.4110181\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.008419351\n",
      "Norm of the params: 1.4595703\n",
      "              Random: fixed  10 labels. Loss 0.41102. Accuracy 0.822.\n",
      "### Flips: 50, rs: 39, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41833943\n",
      "Train loss (w/o reg) on all data: 0.41813922\n",
      "Test loss (w/o reg) on all data: 0.3621324\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0028464093\n",
      "Norm of the params: 2.0010917\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.36213. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [278] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.30299246\n",
      "Train loss (w/o reg) on all data: 0.30242032\n",
      "Test loss (w/o reg) on all data: 0.34168476\n",
      "Train acc on all data:  0.8820754716981132\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0014390622\n",
      "Norm of the params: 3.382731\n",
      "                Loss: fixed  29 labels. Loss 0.34168. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5232748\n",
      "Train loss (w/o reg) on all data: 0.5231584\n",
      "Test loss (w/o reg) on all data: 0.39412868\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0025542232\n",
      "Norm of the params: 1.5259368\n",
      "              Random: fixed  12 labels. Loss 0.39413. Accuracy 0.844.\n",
      "### Flips: 50, rs: 39, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39313418\n",
      "Train loss (w/o reg) on all data: 0.39291447\n",
      "Test loss (w/o reg) on all data: 0.35573447\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.009488939\n",
      "Norm of the params: 2.096185\n",
      "     Influence (LOO): fixed  30 labels. Loss 0.35573. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [490] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.30147237\n",
      "Train loss (w/o reg) on all data: 0.30082837\n",
      "Test loss (w/o reg) on all data: 0.3493228\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.001567422\n",
      "Norm of the params: 3.5888777\n",
      "                Loss: fixed  31 labels. Loss 0.34932. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51802266\n",
      "Train loss (w/o reg) on all data: 0.5178929\n",
      "Test loss (w/o reg) on all data: 0.3757445\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011083347\n",
      "Norm of the params: 1.6108928\n",
      "              Random: fixed  15 labels. Loss 0.37574. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [434] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.642312\n",
      "Train loss (w/o reg) on all data: 0.6422761\n",
      "Test loss (w/o reg) on all data: 0.47660154\n",
      "Train acc on all data:  0.6320754716981132\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0005013542\n",
      "Norm of the params: 0.8474157\n",
      "Flipped loss: 0.47660. Accuracy: 0.844\n",
      "### Flips: 60, rs: 0, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [157] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59985393\n",
      "Train loss (w/o reg) on all data: 0.59978217\n",
      "Test loss (w/o reg) on all data: 0.40028122\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.001565419\n",
      "Norm of the params: 1.1980772\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.40028. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [114] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5683805\n",
      "Train loss (w/o reg) on all data: 0.5682542\n",
      "Test loss (w/o reg) on all data: 0.38423097\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0037515117\n",
      "Norm of the params: 1.5894274\n",
      "                Loss: fixed   9 labels. Loss 0.38423. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [139] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.642326\n",
      "Train loss (w/o reg) on all data: 0.6422892\n",
      "Test loss (w/o reg) on all data: 0.47721002\n",
      "Train acc on all data:  0.6273584905660378\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0022499252\n",
      "Norm of the params: 0.8576641\n",
      "              Random: fixed   0 labels. Loss 0.47721. Accuracy 0.844.\n",
      "### Flips: 60, rs: 0, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58678526\n",
      "Train loss (w/o reg) on all data: 0.5866998\n",
      "Test loss (w/o reg) on all data: 0.38928276\n",
      "Train acc on all data:  0.6650943396226415\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0006221548\n",
      "Norm of the params: 1.3075963\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.38928. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [134] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.520053\n",
      "Train loss (w/o reg) on all data: 0.5198539\n",
      "Test loss (w/o reg) on all data: 0.34011823\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0047635343\n",
      "Norm of the params: 1.9957993\n",
      "                Loss: fixed  16 labels. Loss 0.34012. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [284] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6425453\n",
      "Train loss (w/o reg) on all data: 0.64251524\n",
      "Test loss (w/o reg) on all data: 0.47844824\n",
      "Train acc on all data:  0.6367924528301887\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0029861755\n",
      "Norm of the params: 0.7752148\n",
      "              Random: fixed   1 labels. Loss 0.47845. Accuracy 0.844.\n",
      "### Flips: 60, rs: 0, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5683602\n",
      "Train loss (w/o reg) on all data: 0.56824666\n",
      "Test loss (w/o reg) on all data: 0.36925986\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.007498124\n",
      "Norm of the params: 1.5071061\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.36926. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [324] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4681698\n",
      "Train loss (w/o reg) on all data: 0.46788582\n",
      "Test loss (w/o reg) on all data: 0.31880066\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00081665476\n",
      "Norm of the params: 2.3832042\n",
      "                Loss: fixed  23 labels. Loss 0.31880. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [329] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6310242\n",
      "Train loss (w/o reg) on all data: 0.6309819\n",
      "Test loss (w/o reg) on all data: 0.47262213\n",
      "Train acc on all data:  0.6462264150943396\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0046098186\n",
      "Norm of the params: 0.91962737\n",
      "              Random: fixed   5 labels. Loss 0.47262. Accuracy 0.844.\n",
      "### Flips: 60, rs: 0, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [202] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54653573\n",
      "Train loss (w/o reg) on all data: 0.5463711\n",
      "Test loss (w/o reg) on all data: 0.37474734\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0011815967\n",
      "Norm of the params: 1.8145655\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.37475. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [175] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4459634\n",
      "Train loss (w/o reg) on all data: 0.4456352\n",
      "Test loss (w/o reg) on all data: 0.33103937\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0010147515\n",
      "Norm of the params: 2.5620382\n",
      "                Loss: fixed  27 labels. Loss 0.33104. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [450] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.62882197\n",
      "Train loss (w/o reg) on all data: 0.62878287\n",
      "Test loss (w/o reg) on all data: 0.47760794\n",
      "Train acc on all data:  0.6320754716981132\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.010460961\n",
      "Norm of the params: 0.8845975\n",
      "              Random: fixed   7 labels. Loss 0.47761. Accuracy 0.844.\n",
      "### Flips: 60, rs: 0, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [297] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52146447\n",
      "Train loss (w/o reg) on all data: 0.521268\n",
      "Test loss (w/o reg) on all data: 0.37135124\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016103854\n",
      "Norm of the params: 1.9823439\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.37135. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [288] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42307892\n",
      "Train loss (w/o reg) on all data: 0.42270586\n",
      "Test loss (w/o reg) on all data: 0.33221278\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0011749129\n",
      "Norm of the params: 2.7315004\n",
      "                Loss: fixed  32 labels. Loss 0.33221. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61513454\n",
      "Train loss (w/o reg) on all data: 0.615091\n",
      "Test loss (w/o reg) on all data: 0.47371283\n",
      "Train acc on all data:  0.6509433962264151\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0021707078\n",
      "Norm of the params: 0.93258715\n",
      "              Random: fixed  10 labels. Loss 0.47371. Accuracy 0.867.\n",
      "### Flips: 60, rs: 0, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.529267\n",
      "Train loss (w/o reg) on all data: 0.5291171\n",
      "Test loss (w/o reg) on all data: 0.3596631\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00090920675\n",
      "Norm of the params: 1.7316343\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.35966. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [415] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39931747\n",
      "Train loss (w/o reg) on all data: 0.39886793\n",
      "Test loss (w/o reg) on all data: 0.35150737\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0009770552\n",
      "Norm of the params: 2.998497\n",
      "                Loss: fixed  36 labels. Loss 0.35151. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6071558\n",
      "Train loss (w/o reg) on all data: 0.6071143\n",
      "Test loss (w/o reg) on all data: 0.44882914\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0017015892\n",
      "Norm of the params: 0.9106104\n",
      "              Random: fixed  13 labels. Loss 0.44883. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.613116\n",
      "Train loss (w/o reg) on all data: 0.61304903\n",
      "Test loss (w/o reg) on all data: 0.47726408\n",
      "Train acc on all data:  0.6367924528301887\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012762403\n",
      "Norm of the params: 1.1574765\n",
      "Flipped loss: 0.47726. Accuracy: 0.778\n",
      "### Flips: 60, rs: 1, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [134] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5819887\n",
      "Train loss (w/o reg) on all data: 0.5819034\n",
      "Test loss (w/o reg) on all data: 0.42174542\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00060656195\n",
      "Norm of the params: 1.3061705\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.42175. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [118] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54995435\n",
      "Train loss (w/o reg) on all data: 0.54983807\n",
      "Test loss (w/o reg) on all data: 0.39793676\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003151706\n",
      "Norm of the params: 1.5252149\n",
      "                Loss: fixed   8 labels. Loss 0.39794. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [119] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60861427\n",
      "Train loss (w/o reg) on all data: 0.60853565\n",
      "Test loss (w/o reg) on all data: 0.45624182\n",
      "Train acc on all data:  0.6556603773584906\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013694336\n",
      "Norm of the params: 1.2539693\n",
      "              Random: fixed   4 labels. Loss 0.45624. Accuracy 0.822.\n",
      "### Flips: 60, rs: 1, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [134] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5530444\n",
      "Train loss (w/o reg) on all data: 0.55292994\n",
      "Test loss (w/o reg) on all data: 0.38932186\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0007843096\n",
      "Norm of the params: 1.5128753\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.38932. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49420118\n",
      "Train loss (w/o reg) on all data: 0.49400023\n",
      "Test loss (w/o reg) on all data: 0.37416738\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012281252\n",
      "Norm of the params: 2.0047848\n",
      "                Loss: fixed  15 labels. Loss 0.37417. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.596137\n",
      "Train loss (w/o reg) on all data: 0.59606165\n",
      "Test loss (w/o reg) on all data: 0.4371604\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0015867868\n",
      "Norm of the params: 1.2276375\n",
      "              Random: fixed   6 labels. Loss 0.43716. Accuracy 0.844.\n",
      "### Flips: 60, rs: 1, checks: 30\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [91] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52714175\n",
      "Train loss (w/o reg) on all data: 0.52701324\n",
      "Test loss (w/o reg) on all data: 0.35428512\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0019579842\n",
      "Norm of the params: 1.6033385\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.35429. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46367115\n",
      "Train loss (w/o reg) on all data: 0.46345305\n",
      "Test loss (w/o reg) on all data: 0.37952954\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002333013\n",
      "Norm of the params: 2.0884566\n",
      "                Loss: fixed  20 labels. Loss 0.37953. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59277976\n",
      "Train loss (w/o reg) on all data: 0.59270155\n",
      "Test loss (w/o reg) on all data: 0.43133453\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0039234525\n",
      "Norm of the params: 1.2506189\n",
      "              Random: fixed   7 labels. Loss 0.43133. Accuracy 0.822.\n",
      "### Flips: 60, rs: 1, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51684165\n",
      "Train loss (w/o reg) on all data: 0.5167233\n",
      "Test loss (w/o reg) on all data: 0.359159\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0005405518\n",
      "Norm of the params: 1.5385617\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.35916. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [288] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43035015\n",
      "Train loss (w/o reg) on all data: 0.43008962\n",
      "Test loss (w/o reg) on all data: 0.3850207\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.005220632\n",
      "Norm of the params: 2.2826262\n",
      "                Loss: fixed  25 labels. Loss 0.38502. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [307] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59277505\n",
      "Train loss (w/o reg) on all data: 0.5926973\n",
      "Test loss (w/o reg) on all data: 0.43154413\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0020622718\n",
      "Norm of the params: 1.2466645\n",
      "              Random: fixed   7 labels. Loss 0.43154. Accuracy 0.822.\n",
      "### Flips: 60, rs: 1, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [94] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4880811\n",
      "Train loss (w/o reg) on all data: 0.48793253\n",
      "Test loss (w/o reg) on all data: 0.351968\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0051188753\n",
      "Norm of the params: 1.7237757\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.35197. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39432594\n",
      "Train loss (w/o reg) on all data: 0.39408514\n",
      "Test loss (w/o reg) on all data: 0.39808968\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001868951\n",
      "Norm of the params: 2.194616\n",
      "                Loss: fixed  31 labels. Loss 0.39809. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58288026\n",
      "Train loss (w/o reg) on all data: 0.5827956\n",
      "Test loss (w/o reg) on all data: 0.4132401\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019819674\n",
      "Norm of the params: 1.3012902\n",
      "              Random: fixed  11 labels. Loss 0.41324. Accuracy 0.822.\n",
      "### Flips: 60, rs: 1, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [185] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47070497\n",
      "Train loss (w/o reg) on all data: 0.4705431\n",
      "Test loss (w/o reg) on all data: 0.37090966\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0021250583\n",
      "Norm of the params: 1.7993146\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.37091. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38823768\n",
      "Train loss (w/o reg) on all data: 0.38797465\n",
      "Test loss (w/o reg) on all data: 0.39411154\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0030829415\n",
      "Norm of the params: 2.2935984\n",
      "                Loss: fixed  33 labels. Loss 0.39411. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [201] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.557778\n",
      "Train loss (w/o reg) on all data: 0.55767816\n",
      "Test loss (w/o reg) on all data: 0.40345645\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0019404462\n",
      "Norm of the params: 1.413043\n",
      "              Random: fixed  15 labels. Loss 0.40346. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [317] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5753029\n",
      "Train loss (w/o reg) on all data: 0.5752053\n",
      "Test loss (w/o reg) on all data: 0.39047864\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002540544\n",
      "Norm of the params: 1.3971267\n",
      "Flipped loss: 0.39048. Accuracy: 0.822\n",
      "### Flips: 60, rs: 2, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [145] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5396572\n",
      "Train loss (w/o reg) on all data: 0.53951377\n",
      "Test loss (w/o reg) on all data: 0.3540292\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0012482115\n",
      "Norm of the params: 1.693634\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.35403. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [141] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4964543\n",
      "Train loss (w/o reg) on all data: 0.49623448\n",
      "Test loss (w/o reg) on all data: 0.3467692\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0014227093\n",
      "Norm of the params: 2.0967345\n",
      "                Loss: fixed   8 labels. Loss 0.34677. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [187] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5776935\n",
      "Train loss (w/o reg) on all data: 0.5776105\n",
      "Test loss (w/o reg) on all data: 0.40295413\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0028635887\n",
      "Norm of the params: 1.2886392\n",
      "              Random: fixed   3 labels. Loss 0.40295. Accuracy 0.822.\n",
      "### Flips: 60, rs: 2, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [313] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5227132\n",
      "Train loss (w/o reg) on all data: 0.5225459\n",
      "Test loss (w/o reg) on all data: 0.3552453\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0020318262\n",
      "Norm of the params: 1.8293031\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.35525. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4363314\n",
      "Train loss (w/o reg) on all data: 0.43594402\n",
      "Test loss (w/o reg) on all data: 0.34316868\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00092639757\n",
      "Norm of the params: 2.7834487\n",
      "                Loss: fixed  14 labels. Loss 0.34317. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56257725\n",
      "Train loss (w/o reg) on all data: 0.5624808\n",
      "Test loss (w/o reg) on all data: 0.38728014\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015161737\n",
      "Norm of the params: 1.3888042\n",
      "              Random: fixed   5 labels. Loss 0.38728. Accuracy 0.822.\n",
      "### Flips: 60, rs: 2, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4832752\n",
      "Train loss (w/o reg) on all data: 0.48304814\n",
      "Test loss (w/o reg) on all data: 0.33233914\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0012818368\n",
      "Norm of the params: 2.130999\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.33234. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [240] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3883391\n",
      "Train loss (w/o reg) on all data: 0.38781184\n",
      "Test loss (w/o reg) on all data: 0.34125933\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0005621077\n",
      "Norm of the params: 3.247385\n",
      "                Loss: fixed  20 labels. Loss 0.34126. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [323] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5625767\n",
      "Train loss (w/o reg) on all data: 0.5624807\n",
      "Test loss (w/o reg) on all data: 0.3877694\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018422647\n",
      "Norm of the params: 1.3859856\n",
      "              Random: fixed   5 labels. Loss 0.38777. Accuracy 0.822.\n",
      "### Flips: 60, rs: 2, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44715762\n",
      "Train loss (w/o reg) on all data: 0.44681066\n",
      "Test loss (w/o reg) on all data: 0.308667\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00089719414\n",
      "Norm of the params: 2.6342328\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.30867. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34758273\n",
      "Train loss (w/o reg) on all data: 0.34689596\n",
      "Test loss (w/o reg) on all data: 0.3313748\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0008695698\n",
      "Norm of the params: 3.7060819\n",
      "                Loss: fixed  27 labels. Loss 0.33137. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [322] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.548886\n",
      "Train loss (w/o reg) on all data: 0.5487831\n",
      "Test loss (w/o reg) on all data: 0.39538676\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.008154232\n",
      "Norm of the params: 1.434567\n",
      "              Random: fixed   8 labels. Loss 0.39539. Accuracy 0.822.\n",
      "### Flips: 60, rs: 2, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [313] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40931964\n",
      "Train loss (w/o reg) on all data: 0.40890682\n",
      "Test loss (w/o reg) on all data: 0.32670587\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0035965\n",
      "Norm of the params: 2.873395\n",
      "     Influence (LOO): fixed  30 labels. Loss 0.32671. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [165] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3237965\n",
      "Train loss (w/o reg) on all data: 0.32314816\n",
      "Test loss (w/o reg) on all data: 0.3857763\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0005236799\n",
      "Norm of the params: 3.6009328\n",
      "                Loss: fixed  32 labels. Loss 0.38578. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [323] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5390027\n",
      "Train loss (w/o reg) on all data: 0.5389043\n",
      "Test loss (w/o reg) on all data: 0.40773058\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0048896903\n",
      "Norm of the params: 1.4028355\n",
      "              Random: fixed  14 labels. Loss 0.40773. Accuracy 0.822.\n",
      "### Flips: 60, rs: 2, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [334] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40091485\n",
      "Train loss (w/o reg) on all data: 0.4004558\n",
      "Test loss (w/o reg) on all data: 0.32527158\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0026345302\n",
      "Norm of the params: 3.029995\n",
      "     Influence (LOO): fixed  32 labels. Loss 0.32527. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.30872557\n",
      "Train loss (w/o reg) on all data: 0.30802447\n",
      "Test loss (w/o reg) on all data: 0.41869086\n",
      "Train acc on all data:  0.8820754716981132\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0020631202\n",
      "Norm of the params: 3.7445865\n",
      "                Loss: fixed  38 labels. Loss 0.41869. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [325] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52228343\n",
      "Train loss (w/o reg) on all data: 0.52216583\n",
      "Test loss (w/o reg) on all data: 0.3844686\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0016880907\n",
      "Norm of the params: 1.5335284\n",
      "              Random: fixed  18 labels. Loss 0.38447. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6300452\n",
      "Train loss (w/o reg) on all data: 0.6300012\n",
      "Test loss (w/o reg) on all data: 0.50464684\n",
      "Train acc on all data:  0.6367924528301887\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0068900255\n",
      "Norm of the params: 0.9376751\n",
      "Flipped loss: 0.50465. Accuracy: 0.844\n",
      "### Flips: 60, rs: 3, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [105] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59803665\n",
      "Train loss (w/o reg) on all data: 0.59793603\n",
      "Test loss (w/o reg) on all data: 0.48413458\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0024474133\n",
      "Norm of the params: 1.4184167\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.48413. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56647044\n",
      "Train loss (w/o reg) on all data: 0.56637096\n",
      "Test loss (w/o reg) on all data: 0.4384179\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.027771814\n",
      "Norm of the params: 1.410538\n",
      "                Loss: fixed   8 labels. Loss 0.43842. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.626529\n",
      "Train loss (w/o reg) on all data: 0.62647605\n",
      "Test loss (w/o reg) on all data: 0.5044054\n",
      "Train acc on all data:  0.6509433962264151\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001374169\n",
      "Norm of the params: 1.0291551\n",
      "              Random: fixed   1 labels. Loss 0.50441. Accuracy 0.822.\n",
      "### Flips: 60, rs: 3, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5850077\n",
      "Train loss (w/o reg) on all data: 0.58489203\n",
      "Test loss (w/o reg) on all data: 0.48321253\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.005548747\n",
      "Norm of the params: 1.5210638\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.48321. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5008138\n",
      "Train loss (w/o reg) on all data: 0.5006124\n",
      "Test loss (w/o reg) on all data: 0.45529467\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0012663249\n",
      "Norm of the params: 2.0071025\n",
      "                Loss: fixed  16 labels. Loss 0.45529. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60997826\n",
      "Train loss (w/o reg) on all data: 0.60992604\n",
      "Test loss (w/o reg) on all data: 0.47479236\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00071337435\n",
      "Norm of the params: 1.0219035\n",
      "              Random: fixed   4 labels. Loss 0.47479. Accuracy 0.844.\n",
      "### Flips: 60, rs: 3, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [327] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5890183\n",
      "Train loss (w/o reg) on all data: 0.58893925\n",
      "Test loss (w/o reg) on all data: 0.46736878\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0046734805\n",
      "Norm of the params: 1.2571352\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.46737. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41971356\n",
      "Train loss (w/o reg) on all data: 0.41935968\n",
      "Test loss (w/o reg) on all data: 0.47540686\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015739292\n",
      "Norm of the params: 2.6603584\n",
      "                Loss: fixed  24 labels. Loss 0.47541. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [401] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5996219\n",
      "Train loss (w/o reg) on all data: 0.5995587\n",
      "Test loss (w/o reg) on all data: 0.46346295\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0013143307\n",
      "Norm of the params: 1.1239426\n",
      "              Random: fixed   8 labels. Loss 0.46346. Accuracy 0.844.\n",
      "### Flips: 60, rs: 3, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [141] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56929547\n",
      "Train loss (w/o reg) on all data: 0.56923306\n",
      "Test loss (w/o reg) on all data: 0.42348006\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0024585617\n",
      "Norm of the params: 1.117303\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.42348. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3797707\n",
      "Train loss (w/o reg) on all data: 0.3792752\n",
      "Test loss (w/o reg) on all data: 0.4866807\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0008397648\n",
      "Norm of the params: 3.1480381\n",
      "                Loss: fixed  29 labels. Loss 0.48668. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [332] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5940805\n",
      "Train loss (w/o reg) on all data: 0.59401035\n",
      "Test loss (w/o reg) on all data: 0.46593598\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0020805146\n",
      "Norm of the params: 1.1843144\n",
      "              Random: fixed  11 labels. Loss 0.46594. Accuracy 0.844.\n",
      "### Flips: 60, rs: 3, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5436882\n",
      "Train loss (w/o reg) on all data: 0.5435985\n",
      "Test loss (w/o reg) on all data: 0.41695154\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004275972\n",
      "Norm of the params: 1.3393184\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.41695. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [431] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3607815\n",
      "Train loss (w/o reg) on all data: 0.36019582\n",
      "Test loss (w/o reg) on all data: 0.48728654\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015266198\n",
      "Norm of the params: 3.4224606\n",
      "                Loss: fixed  34 labels. Loss 0.48729. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55610836\n",
      "Train loss (w/o reg) on all data: 0.5560127\n",
      "Test loss (w/o reg) on all data: 0.43514293\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.006105948\n",
      "Norm of the params: 1.3830185\n",
      "              Random: fixed  17 labels. Loss 0.43514. Accuracy 0.822.\n",
      "### Flips: 60, rs: 3, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52762383\n",
      "Train loss (w/o reg) on all data: 0.52752924\n",
      "Test loss (w/o reg) on all data: 0.4042169\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0021825188\n",
      "Norm of the params: 1.3755112\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.40422. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35126662\n",
      "Train loss (w/o reg) on all data: 0.3507125\n",
      "Test loss (w/o reg) on all data: 0.48190457\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020400956\n",
      "Norm of the params: 3.328966\n",
      "                Loss: fixed  36 labels. Loss 0.48190. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5511303\n",
      "Train loss (w/o reg) on all data: 0.55102336\n",
      "Test loss (w/o reg) on all data: 0.4415236\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010316196\n",
      "Norm of the params: 1.4622663\n",
      "              Random: fixed  19 labels. Loss 0.44152. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [352] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.62108564\n",
      "Train loss (w/o reg) on all data: 0.62102234\n",
      "Test loss (w/o reg) on all data: 0.5867652\n",
      "Train acc on all data:  0.660377358490566\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.00047306472\n",
      "Norm of the params: 1.1251508\n",
      "Flipped loss: 0.58677. Accuracy: 0.711\n",
      "### Flips: 60, rs: 4, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5764265\n",
      "Train loss (w/o reg) on all data: 0.5763314\n",
      "Test loss (w/o reg) on all data: 0.54044455\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0019402626\n",
      "Norm of the params: 1.3795557\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.54044. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [313] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53916526\n",
      "Train loss (w/o reg) on all data: 0.53897244\n",
      "Test loss (w/o reg) on all data: 0.57242286\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.6888888888888889\n",
      "Norm of the mean of gradients: 0.000854585\n",
      "Norm of the params: 1.9638805\n",
      "                Loss: fixed   9 labels. Loss 0.57242. Accuracy 0.689.\n",
      "Using normal model\n",
      "LBFGS training took [293] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61785984\n",
      "Train loss (w/o reg) on all data: 0.61780554\n",
      "Test loss (w/o reg) on all data: 0.5910688\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.002011074\n",
      "Norm of the params: 1.041871\n",
      "              Random: fixed   4 labels. Loss 0.59107. Accuracy 0.733.\n",
      "### Flips: 60, rs: 4, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55756605\n",
      "Train loss (w/o reg) on all data: 0.5574511\n",
      "Test loss (w/o reg) on all data: 0.52503026\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0011506268\n",
      "Norm of the params: 1.5161457\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.52503. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49478558\n",
      "Train loss (w/o reg) on all data: 0.4945256\n",
      "Test loss (w/o reg) on all data: 0.58166623\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.6888888888888889\n",
      "Norm of the mean of gradients: 0.00041923576\n",
      "Norm of the params: 2.280205\n",
      "                Loss: fixed  15 labels. Loss 0.58167. Accuracy 0.689.\n",
      "Using normal model\n",
      "LBFGS training took [185] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60785913\n",
      "Train loss (w/o reg) on all data: 0.6077891\n",
      "Test loss (w/o reg) on all data: 0.5664575\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00068122684\n",
      "Norm of the params: 1.1835681\n",
      "              Random: fixed   6 labels. Loss 0.56646. Accuracy 0.756.\n",
      "### Flips: 60, rs: 4, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [151] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55239284\n",
      "Train loss (w/o reg) on all data: 0.5522765\n",
      "Test loss (w/o reg) on all data: 0.481314\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.010351362\n",
      "Norm of the params: 1.5255057\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.48131. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [198] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45577124\n",
      "Train loss (w/o reg) on all data: 0.45545232\n",
      "Test loss (w/o reg) on all data: 0.6133388\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.6888888888888889\n",
      "Norm of the mean of gradients: 0.0006196685\n",
      "Norm of the params: 2.5255725\n",
      "                Loss: fixed  21 labels. Loss 0.61334. Accuracy 0.689.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.600484\n",
      "Train loss (w/o reg) on all data: 0.6004059\n",
      "Test loss (w/o reg) on all data: 0.54592925\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0009397728\n",
      "Norm of the params: 1.2499101\n",
      "              Random: fixed  10 labels. Loss 0.54593. Accuracy 0.756.\n",
      "### Flips: 60, rs: 4, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [279] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52865803\n",
      "Train loss (w/o reg) on all data: 0.5285051\n",
      "Test loss (w/o reg) on all data: 0.4710792\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.005044731\n",
      "Norm of the params: 1.7491068\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.47108. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43893084\n",
      "Train loss (w/o reg) on all data: 0.43857992\n",
      "Test loss (w/o reg) on all data: 0.59056526\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.009281798\n",
      "Norm of the params: 2.6491826\n",
      "                Loss: fixed  24 labels. Loss 0.59057. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5852989\n",
      "Train loss (w/o reg) on all data: 0.5852034\n",
      "Test loss (w/o reg) on all data: 0.5620257\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0006425621\n",
      "Norm of the params: 1.381959\n",
      "              Random: fixed  13 labels. Loss 0.56203. Accuracy 0.778.\n",
      "### Flips: 60, rs: 4, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [336] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48019713\n",
      "Train loss (w/o reg) on all data: 0.4799352\n",
      "Test loss (w/o reg) on all data: 0.47164294\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0016572302\n",
      "Norm of the params: 2.288817\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.47164. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40474325\n",
      "Train loss (w/o reg) on all data: 0.40440348\n",
      "Test loss (w/o reg) on all data: 0.576145\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0008726556\n",
      "Norm of the params: 2.6068656\n",
      "                Loss: fixed  30 labels. Loss 0.57614. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58174163\n",
      "Train loss (w/o reg) on all data: 0.581632\n",
      "Test loss (w/o reg) on all data: 0.56462955\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0032100754\n",
      "Norm of the params: 1.4807553\n",
      "              Random: fixed  14 labels. Loss 0.56463. Accuracy 0.778.\n",
      "### Flips: 60, rs: 4, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.463198\n",
      "Train loss (w/o reg) on all data: 0.46292531\n",
      "Test loss (w/o reg) on all data: 0.43638408\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0018987494\n",
      "Norm of the params: 2.335323\n",
      "     Influence (LOO): fixed  30 labels. Loss 0.43638. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37733895\n",
      "Train loss (w/o reg) on all data: 0.37691772\n",
      "Test loss (w/o reg) on all data: 0.5524717\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00090615003\n",
      "Norm of the params: 2.9024796\n",
      "                Loss: fixed  35 labels. Loss 0.55247. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.579419\n",
      "Train loss (w/o reg) on all data: 0.57930434\n",
      "Test loss (w/o reg) on all data: 0.55221695\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0024600509\n",
      "Norm of the params: 1.5144708\n",
      "              Random: fixed  15 labels. Loss 0.55222. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [285] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59373724\n",
      "Train loss (w/o reg) on all data: 0.59364307\n",
      "Test loss (w/o reg) on all data: 0.43005154\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.006478045\n",
      "Norm of the params: 1.3723086\n",
      "Flipped loss: 0.43005. Accuracy: 0.844\n",
      "### Flips: 60, rs: 5, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55056804\n",
      "Train loss (w/o reg) on all data: 0.5504529\n",
      "Test loss (w/o reg) on all data: 0.39793333\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.011826915\n",
      "Norm of the params: 1.5177685\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.39793. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [64] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50713825\n",
      "Train loss (w/o reg) on all data: 0.5069695\n",
      "Test loss (w/o reg) on all data: 0.37592956\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0017702036\n",
      "Norm of the params: 1.8369931\n",
      "                Loss: fixed   8 labels. Loss 0.37593. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58277977\n",
      "Train loss (w/o reg) on all data: 0.58267313\n",
      "Test loss (w/o reg) on all data: 0.42933628\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0035072442\n",
      "Norm of the params: 1.4605429\n",
      "              Random: fixed   3 labels. Loss 0.42934. Accuracy 0.844.\n",
      "### Flips: 60, rs: 5, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5250888\n",
      "Train loss (w/o reg) on all data: 0.52495605\n",
      "Test loss (w/o reg) on all data: 0.40636215\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0068594287\n",
      "Norm of the params: 1.6294402\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.40636. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [282] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44041148\n",
      "Train loss (w/o reg) on all data: 0.44016108\n",
      "Test loss (w/o reg) on all data: 0.36769566\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001206347\n",
      "Norm of the params: 2.2378085\n",
      "                Loss: fixed  16 labels. Loss 0.36770. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5529149\n",
      "Train loss (w/o reg) on all data: 0.5527795\n",
      "Test loss (w/o reg) on all data: 0.41225362\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0005475767\n",
      "Norm of the params: 1.64572\n",
      "              Random: fixed   7 labels. Loss 0.41225. Accuracy 0.867.\n",
      "### Flips: 60, rs: 5, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [184] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49274695\n",
      "Train loss (w/o reg) on all data: 0.4925938\n",
      "Test loss (w/o reg) on all data: 0.3915853\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0044946964\n",
      "Norm of the params: 1.7501172\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.39159. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36442834\n",
      "Train loss (w/o reg) on all data: 0.36395112\n",
      "Test loss (w/o reg) on all data: 0.42476073\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011376352\n",
      "Norm of the params: 3.0894406\n",
      "                Loss: fixed  24 labels. Loss 0.42476. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5488054\n",
      "Train loss (w/o reg) on all data: 0.5486671\n",
      "Test loss (w/o reg) on all data: 0.40403917\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.004828977\n",
      "Norm of the params: 1.6634232\n",
      "              Random: fixed   9 labels. Loss 0.40404. Accuracy 0.867.\n",
      "### Flips: 60, rs: 5, checks: 40\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [208] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44023702\n",
      "Train loss (w/o reg) on all data: 0.44000396\n",
      "Test loss (w/o reg) on all data: 0.38972092\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00034246146\n",
      "Norm of the params: 2.1589384\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.38972. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33572933\n",
      "Train loss (w/o reg) on all data: 0.33508644\n",
      "Test loss (w/o reg) on all data: 0.40555465\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0019007251\n",
      "Norm of the params: 3.5858123\n",
      "                Loss: fixed  28 labels. Loss 0.40555. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5358503\n",
      "Train loss (w/o reg) on all data: 0.53569853\n",
      "Test loss (w/o reg) on all data: 0.4050602\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016949283\n",
      "Norm of the params: 1.7422329\n",
      "              Random: fixed  11 labels. Loss 0.40506. Accuracy 0.822.\n",
      "### Flips: 60, rs: 5, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41412258\n",
      "Train loss (w/o reg) on all data: 0.41384196\n",
      "Test loss (w/o reg) on all data: 0.3553454\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00029250194\n",
      "Norm of the params: 2.3690994\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.35535. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.29799733\n",
      "Train loss (w/o reg) on all data: 0.2971685\n",
      "Test loss (w/o reg) on all data: 0.42757815\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013762428\n",
      "Norm of the params: 4.0714498\n",
      "                Loss: fixed  35 labels. Loss 0.42758. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [309] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5387012\n",
      "Train loss (w/o reg) on all data: 0.5385504\n",
      "Test loss (w/o reg) on all data: 0.40870774\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002951266\n",
      "Norm of the params: 1.7367909\n",
      "              Random: fixed  13 labels. Loss 0.40871. Accuracy 0.844.\n",
      "### Flips: 60, rs: 5, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [384] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41563675\n",
      "Train loss (w/o reg) on all data: 0.41533569\n",
      "Test loss (w/o reg) on all data: 0.37260357\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0006040354\n",
      "Norm of the params: 2.4538164\n",
      "     Influence (LOO): fixed  32 labels. Loss 0.37260. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [314] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.26954186\n",
      "Train loss (w/o reg) on all data: 0.2683862\n",
      "Test loss (w/o reg) on all data: 0.4726424\n",
      "Train acc on all data:  0.8962264150943396\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00067449675\n",
      "Norm of the params: 4.8076468\n",
      "                Loss: fixed  40 labels. Loss 0.47264. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [421] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5463492\n",
      "Train loss (w/o reg) on all data: 0.5462299\n",
      "Test loss (w/o reg) on all data: 0.41589746\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.012973074\n",
      "Norm of the params: 1.5447729\n",
      "              Random: fixed  17 labels. Loss 0.41590. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6297493\n",
      "Train loss (w/o reg) on all data: 0.6296691\n",
      "Test loss (w/o reg) on all data: 0.52030945\n",
      "Train acc on all data:  0.6556603773584906\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0032376235\n",
      "Norm of the params: 1.2665068\n",
      "Flipped loss: 0.52031. Accuracy: 0.778\n",
      "### Flips: 60, rs: 6, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [146] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5998953\n",
      "Train loss (w/o reg) on all data: 0.59979683\n",
      "Test loss (w/o reg) on all data: 0.48032218\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0006846377\n",
      "Norm of the params: 1.4033648\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.48032. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [187] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5543367\n",
      "Train loss (w/o reg) on all data: 0.55415744\n",
      "Test loss (w/o reg) on all data: 0.46734917\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0049869893\n",
      "Norm of the params: 1.8934805\n",
      "                Loss: fixed   8 labels. Loss 0.46735. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [140] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6263027\n",
      "Train loss (w/o reg) on all data: 0.6262367\n",
      "Test loss (w/o reg) on all data: 0.50867295\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.004984694\n",
      "Norm of the params: 1.149314\n",
      "              Random: fixed   3 labels. Loss 0.50867. Accuracy 0.778.\n",
      "### Flips: 60, rs: 6, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [169] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5625215\n",
      "Train loss (w/o reg) on all data: 0.5623914\n",
      "Test loss (w/o reg) on all data: 0.47404683\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008030648\n",
      "Norm of the params: 1.61324\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.47405. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51871926\n",
      "Train loss (w/o reg) on all data: 0.51848453\n",
      "Test loss (w/o reg) on all data: 0.48462436\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003400172\n",
      "Norm of the params: 2.1665614\n",
      "                Loss: fixed  13 labels. Loss 0.48462. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6229214\n",
      "Train loss (w/o reg) on all data: 0.62285775\n",
      "Test loss (w/o reg) on all data: 0.5018797\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0032013508\n",
      "Norm of the params: 1.1281391\n",
      "              Random: fixed   4 labels. Loss 0.50188. Accuracy 0.778.\n",
      "### Flips: 60, rs: 6, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [123] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51815885\n",
      "Train loss (w/o reg) on all data: 0.51798683\n",
      "Test loss (w/o reg) on all data: 0.43284488\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0004640555\n",
      "Norm of the params: 1.8547773\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.43284. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4872552\n",
      "Train loss (w/o reg) on all data: 0.48697\n",
      "Test loss (w/o reg) on all data: 0.4473318\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.008246594\n",
      "Norm of the params: 2.3881593\n",
      "                Loss: fixed  18 labels. Loss 0.44733. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [462] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61919177\n",
      "Train loss (w/o reg) on all data: 0.6191262\n",
      "Test loss (w/o reg) on all data: 0.5269795\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.003562997\n",
      "Norm of the params: 1.1453735\n",
      "              Random: fixed   6 labels. Loss 0.52698. Accuracy 0.733.\n",
      "### Flips: 60, rs: 6, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4950888\n",
      "Train loss (w/o reg) on all data: 0.49490237\n",
      "Test loss (w/o reg) on all data: 0.41590703\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0018923902\n",
      "Norm of the params: 1.930926\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.41591. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [384] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4547751\n",
      "Train loss (w/o reg) on all data: 0.4544381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss (w/o reg) on all data: 0.4233035\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00065310404\n",
      "Norm of the params: 2.5961916\n",
      "                Loss: fixed  23 labels. Loss 0.42330. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [339] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6191934\n",
      "Train loss (w/o reg) on all data: 0.61912686\n",
      "Test loss (w/o reg) on all data: 0.5268771\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0019958732\n",
      "Norm of the params: 1.1532294\n",
      "              Random: fixed   6 labels. Loss 0.52688. Accuracy 0.733.\n",
      "### Flips: 60, rs: 6, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [197] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.486859\n",
      "Train loss (w/o reg) on all data: 0.48665798\n",
      "Test loss (w/o reg) on all data: 0.42730454\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0013105173\n",
      "Norm of the params: 2.0050223\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.42730. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42679498\n",
      "Train loss (w/o reg) on all data: 0.42642558\n",
      "Test loss (w/o reg) on all data: 0.42842743\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0013552939\n",
      "Norm of the params: 2.7181168\n",
      "                Loss: fixed  27 labels. Loss 0.42843. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.62333053\n",
      "Train loss (w/o reg) on all data: 0.62328464\n",
      "Test loss (w/o reg) on all data: 0.50696903\n",
      "Train acc on all data:  0.6650943396226415\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0020972982\n",
      "Norm of the params: 0.95784193\n",
      "              Random: fixed   9 labels. Loss 0.50697. Accuracy 0.756.\n",
      "### Flips: 60, rs: 6, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48765072\n",
      "Train loss (w/o reg) on all data: 0.4874348\n",
      "Test loss (w/o reg) on all data: 0.42347154\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0008423405\n",
      "Norm of the params: 2.0780537\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.42347. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [110] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39863887\n",
      "Train loss (w/o reg) on all data: 0.39822555\n",
      "Test loss (w/o reg) on all data: 0.43984506\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0014474428\n",
      "Norm of the params: 2.87513\n",
      "                Loss: fixed  31 labels. Loss 0.43985. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6107725\n",
      "Train loss (w/o reg) on all data: 0.6107226\n",
      "Test loss (w/o reg) on all data: 0.48239827\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0026506847\n",
      "Norm of the params: 0.9990133\n",
      "              Random: fixed  11 labels. Loss 0.48240. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [322] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5962568\n",
      "Train loss (w/o reg) on all data: 0.5962024\n",
      "Test loss (w/o reg) on all data: 0.45197117\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0010085907\n",
      "Norm of the params: 1.0432198\n",
      "Flipped loss: 0.45197. Accuracy: 0.756\n",
      "### Flips: 60, rs: 7, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [172] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.568199\n",
      "Train loss (w/o reg) on all data: 0.5681013\n",
      "Test loss (w/o reg) on all data: 0.43345192\n",
      "Train acc on all data:  0.660377358490566\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0072225663\n",
      "Norm of the params: 1.3979445\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.43345. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [107] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5481892\n",
      "Train loss (w/o reg) on all data: 0.54808587\n",
      "Test loss (w/o reg) on all data: 0.42954403\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0005517033\n",
      "Norm of the params: 1.4378865\n",
      "                Loss: fixed   6 labels. Loss 0.42954. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57624114\n",
      "Train loss (w/o reg) on all data: 0.5761655\n",
      "Test loss (w/o reg) on all data: 0.43098637\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014929083\n",
      "Norm of the params: 1.2300245\n",
      "              Random: fixed   4 labels. Loss 0.43099. Accuracy 0.822.\n",
      "### Flips: 60, rs: 7, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [106] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54750973\n",
      "Train loss (w/o reg) on all data: 0.54740566\n",
      "Test loss (w/o reg) on all data: 0.4067642\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003072467\n",
      "Norm of the params: 1.4426714\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.40676. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [97] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50644565\n",
      "Train loss (w/o reg) on all data: 0.50631005\n",
      "Test loss (w/o reg) on all data: 0.40374595\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0051424513\n",
      "Norm of the params: 1.6466705\n",
      "                Loss: fixed  12 labels. Loss 0.40375. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [131] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5742479\n",
      "Train loss (w/o reg) on all data: 0.57416695\n",
      "Test loss (w/o reg) on all data: 0.43713006\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0013318011\n",
      "Norm of the params: 1.2724138\n",
      "              Random: fixed   6 labels. Loss 0.43713. Accuracy 0.778.\n",
      "### Flips: 60, rs: 7, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [173] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5443053\n",
      "Train loss (w/o reg) on all data: 0.5442134\n",
      "Test loss (w/o reg) on all data: 0.39627868\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0025555242\n",
      "Norm of the params: 1.3556508\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.39628. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45192578\n",
      "Train loss (w/o reg) on all data: 0.45171428\n",
      "Test loss (w/o reg) on all data: 0.40845817\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001872264\n",
      "Norm of the params: 2.0567954\n",
      "                Loss: fixed  20 labels. Loss 0.40846. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [140] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5725503\n",
      "Train loss (w/o reg) on all data: 0.57245123\n",
      "Test loss (w/o reg) on all data: 0.4479399\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.002042714\n",
      "Norm of the params: 1.4074689\n",
      "              Random: fixed   9 labels. Loss 0.44794. Accuracy 0.733.\n",
      "### Flips: 60, rs: 7, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [165] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5207587\n",
      "Train loss (w/o reg) on all data: 0.5206596\n",
      "Test loss (w/o reg) on all data: 0.38662958\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0024698318\n",
      "Norm of the params: 1.4076619\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.38663. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4332736\n",
      "Train loss (w/o reg) on all data: 0.43304545\n",
      "Test loss (w/o reg) on all data: 0.40945336\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0005983865\n",
      "Norm of the params: 2.136156\n",
      "                Loss: fixed  23 labels. Loss 0.40945. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5615294\n",
      "Train loss (w/o reg) on all data: 0.5614297\n",
      "Test loss (w/o reg) on all data: 0.4475198\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0016661705\n",
      "Norm of the params: 1.4123577\n",
      "              Random: fixed  12 labels. Loss 0.44752. Accuracy 0.756.\n",
      "### Flips: 60, rs: 7, checks: 50\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [315] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50070167\n",
      "Train loss (w/o reg) on all data: 0.5006151\n",
      "Test loss (w/o reg) on all data: 0.35753363\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0017846269\n",
      "Norm of the params: 1.3157638\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.35753. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4061402\n",
      "Train loss (w/o reg) on all data: 0.40587276\n",
      "Test loss (w/o reg) on all data: 0.428633\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0053800424\n",
      "Norm of the params: 2.3127625\n",
      "                Loss: fixed  28 labels. Loss 0.42863. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [314] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5507454\n",
      "Train loss (w/o reg) on all data: 0.5506288\n",
      "Test loss (w/o reg) on all data: 0.46061116\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.006996049\n",
      "Norm of the params: 1.5274093\n",
      "              Random: fixed  14 labels. Loss 0.46061. Accuracy 0.711.\n",
      "### Flips: 60, rs: 7, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [323] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48213023\n",
      "Train loss (w/o reg) on all data: 0.48201376\n",
      "Test loss (w/o reg) on all data: 0.33839405\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0008661603\n",
      "Norm of the params: 1.5262122\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.33839. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3922225\n",
      "Train loss (w/o reg) on all data: 0.39191824\n",
      "Test loss (w/o reg) on all data: 0.40994394\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0020944648\n",
      "Norm of the params: 2.466787\n",
      "                Loss: fixed  31 labels. Loss 0.40994. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [336] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53826237\n",
      "Train loss (w/o reg) on all data: 0.53812784\n",
      "Test loss (w/o reg) on all data: 0.44815332\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.00086309266\n",
      "Norm of the params: 1.6402913\n",
      "              Random: fixed  19 labels. Loss 0.44815. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6044288\n",
      "Train loss (w/o reg) on all data: 0.6043656\n",
      "Test loss (w/o reg) on all data: 0.59512115\n",
      "Train acc on all data:  0.6650943396226415\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.01044786\n",
      "Norm of the params: 1.1246836\n",
      "Flipped loss: 0.59512. Accuracy: 0.733\n",
      "### Flips: 60, rs: 8, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [201] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55621547\n",
      "Train loss (w/o reg) on all data: 0.5560815\n",
      "Test loss (w/o reg) on all data: 0.600968\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.6888888888888889\n",
      "Norm of the mean of gradients: 0.0007858679\n",
      "Norm of the params: 1.637012\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.60097. Accuracy 0.689.\n",
      "Using normal model\n",
      "LBFGS training took [93] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55251247\n",
      "Train loss (w/o reg) on all data: 0.5523833\n",
      "Test loss (w/o reg) on all data: 0.5896365\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0029531706\n",
      "Norm of the params: 1.6072123\n",
      "                Loss: fixed   6 labels. Loss 0.58964. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5985088\n",
      "Train loss (w/o reg) on all data: 0.5984343\n",
      "Test loss (w/o reg) on all data: 0.5840858\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0021748873\n",
      "Norm of the params: 1.2203997\n",
      "              Random: fixed   3 labels. Loss 0.58409. Accuracy 0.733.\n",
      "### Flips: 60, rs: 8, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [222] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5484663\n",
      "Train loss (w/o reg) on all data: 0.54833615\n",
      "Test loss (w/o reg) on all data: 0.5675384\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.004855085\n",
      "Norm of the params: 1.6133868\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.56754. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [61] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49304637\n",
      "Train loss (w/o reg) on all data: 0.49288592\n",
      "Test loss (w/o reg) on all data: 0.5494036\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0021889864\n",
      "Norm of the params: 1.7914035\n",
      "                Loss: fixed  14 labels. Loss 0.54940. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [113] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59864205\n",
      "Train loss (w/o reg) on all data: 0.59856737\n",
      "Test loss (w/o reg) on all data: 0.5812975\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0005906922\n",
      "Norm of the params: 1.2221599\n",
      "              Random: fixed   5 labels. Loss 0.58130. Accuracy 0.733.\n",
      "### Flips: 60, rs: 8, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [163] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.535845\n",
      "Train loss (w/o reg) on all data: 0.5356988\n",
      "Test loss (w/o reg) on all data: 0.55375654\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.004063967\n",
      "Norm of the params: 1.7100701\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.55376. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [288] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43918929\n",
      "Train loss (w/o reg) on all data: 0.4389297\n",
      "Test loss (w/o reg) on all data: 0.60599023\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00058583333\n",
      "Norm of the params: 2.2785084\n",
      "                Loss: fixed  21 labels. Loss 0.60599. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59516716\n",
      "Train loss (w/o reg) on all data: 0.5950986\n",
      "Test loss (w/o reg) on all data: 0.54681516\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0025692396\n",
      "Norm of the params: 1.170741\n",
      "              Random: fixed   7 labels. Loss 0.54682. Accuracy 0.733.\n",
      "### Flips: 60, rs: 8, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5091245\n",
      "Train loss (w/o reg) on all data: 0.5089509\n",
      "Test loss (w/o reg) on all data: 0.53625953\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00194984\n",
      "Norm of the params: 1.8633369\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.53626. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4096625\n",
      "Train loss (w/o reg) on all data: 0.40931824\n",
      "Test loss (w/o reg) on all data: 0.597769\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0026359537\n",
      "Norm of the params: 2.6239548\n",
      "                Loss: fixed  26 labels. Loss 0.59777. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [338] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58318084\n",
      "Train loss (w/o reg) on all data: 0.58311003\n",
      "Test loss (w/o reg) on all data: 0.52098113\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0015427029\n",
      "Norm of the params: 1.1901712\n",
      "              Random: fixed  12 labels. Loss 0.52098. Accuracy 0.756.\n",
      "### Flips: 60, rs: 8, checks: 50\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [147] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49239212\n",
      "Train loss (w/o reg) on all data: 0.4921845\n",
      "Test loss (w/o reg) on all data: 0.53615874\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.001667509\n",
      "Norm of the params: 2.037746\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.53616. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37979898\n",
      "Train loss (w/o reg) on all data: 0.3794151\n",
      "Test loss (w/o reg) on all data: 0.6078518\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00057605904\n",
      "Norm of the params: 2.7708209\n",
      "                Loss: fixed  31 labels. Loss 0.60785. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56578344\n",
      "Train loss (w/o reg) on all data: 0.5656935\n",
      "Test loss (w/o reg) on all data: 0.4952295\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.008622662\n",
      "Norm of the params: 1.341115\n",
      "              Random: fixed  16 labels. Loss 0.49523. Accuracy 0.756.\n",
      "### Flips: 60, rs: 8, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [432] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4764115\n",
      "Train loss (w/o reg) on all data: 0.47620988\n",
      "Test loss (w/o reg) on all data: 0.51925886\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0024234573\n",
      "Norm of the params: 2.0080059\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.51926. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35217673\n",
      "Train loss (w/o reg) on all data: 0.3516791\n",
      "Test loss (w/o reg) on all data: 0.62463427\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010770157\n",
      "Norm of the params: 3.154776\n",
      "                Loss: fixed  35 labels. Loss 0.62463. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5588937\n",
      "Train loss (w/o reg) on all data: 0.5587945\n",
      "Test loss (w/o reg) on all data: 0.48449615\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0009106743\n",
      "Norm of the params: 1.4083945\n",
      "              Random: fixed  19 labels. Loss 0.48450. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [228] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.607642\n",
      "Train loss (w/o reg) on all data: 0.60755837\n",
      "Test loss (w/o reg) on all data: 0.5009947\n",
      "Train acc on all data:  0.6650943396226415\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0049521024\n",
      "Norm of the params: 1.2932149\n",
      "Flipped loss: 0.50099. Accuracy: 0.778\n",
      "### Flips: 60, rs: 9, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [132] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5686877\n",
      "Train loss (w/o reg) on all data: 0.5685601\n",
      "Test loss (w/o reg) on all data: 0.51481295\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0011448048\n",
      "Norm of the params: 1.597101\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.51481. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53904635\n",
      "Train loss (w/o reg) on all data: 0.53889304\n",
      "Test loss (w/o reg) on all data: 0.46540624\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0003993836\n",
      "Norm of the params: 1.7511128\n",
      "                Loss: fixed   9 labels. Loss 0.46541. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [141] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60756195\n",
      "Train loss (w/o reg) on all data: 0.6074759\n",
      "Test loss (w/o reg) on all data: 0.49165934\n",
      "Train acc on all data:  0.6650943396226415\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0012256046\n",
      "Norm of the params: 1.3121153\n",
      "              Random: fixed   2 labels. Loss 0.49166. Accuracy 0.756.\n",
      "### Flips: 60, rs: 9, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [133] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5369783\n",
      "Train loss (w/o reg) on all data: 0.5368169\n",
      "Test loss (w/o reg) on all data: 0.47048932\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0042475616\n",
      "Norm of the params: 1.7968186\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.47049. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [199] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48800835\n",
      "Train loss (w/o reg) on all data: 0.48779303\n",
      "Test loss (w/o reg) on all data: 0.5029482\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.004678678\n",
      "Norm of the params: 2.0752194\n",
      "                Loss: fixed  16 labels. Loss 0.50295. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [175] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60693234\n",
      "Train loss (w/o reg) on all data: 0.6068522\n",
      "Test loss (w/o reg) on all data: 0.4801641\n",
      "Train acc on all data:  0.660377358490566\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0013648152\n",
      "Norm of the params: 1.2661823\n",
      "              Random: fixed   3 labels. Loss 0.48016. Accuracy 0.756.\n",
      "### Flips: 60, rs: 9, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [186] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50786203\n",
      "Train loss (w/o reg) on all data: 0.5076455\n",
      "Test loss (w/o reg) on all data: 0.44935176\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.001782472\n",
      "Norm of the params: 2.0811276\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.44935. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [299] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44524875\n",
      "Train loss (w/o reg) on all data: 0.44496974\n",
      "Test loss (w/o reg) on all data: 0.48364627\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0009197035\n",
      "Norm of the params: 2.3622327\n",
      "                Loss: fixed  22 labels. Loss 0.48365. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5837715\n",
      "Train loss (w/o reg) on all data: 0.5836767\n",
      "Test loss (w/o reg) on all data: 0.44817737\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0021249377\n",
      "Norm of the params: 1.3770298\n",
      "              Random: fixed   7 labels. Loss 0.44818. Accuracy 0.756.\n",
      "### Flips: 60, rs: 9, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [104] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48367932\n",
      "Train loss (w/o reg) on all data: 0.48346144\n",
      "Test loss (w/o reg) on all data: 0.43830302\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0049479315\n",
      "Norm of the params: 2.0874608\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.43830. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [364] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37774944\n",
      "Train loss (w/o reg) on all data: 0.3773377\n",
      "Test loss (w/o reg) on all data: 0.47815305\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00077312806\n",
      "Norm of the params: 2.869717\n",
      "                Loss: fixed  30 labels. Loss 0.47815. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5812513\n",
      "Train loss (w/o reg) on all data: 0.5811523\n",
      "Test loss (w/o reg) on all data: 0.4403116\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0017719581\n",
      "Norm of the params: 1.4070064\n",
      "              Random: fixed   9 labels. Loss 0.44031. Accuracy 0.778.\n",
      "### Flips: 60, rs: 9, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47170123\n",
      "Train loss (w/o reg) on all data: 0.4715036\n",
      "Test loss (w/o reg) on all data: 0.43296772\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001027952\n",
      "Norm of the params: 1.9881474\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.43297. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [392] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34624073\n",
      "Train loss (w/o reg) on all data: 0.3457232\n",
      "Test loss (w/o reg) on all data: 0.50701296\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7555555555555555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of the mean of gradients: 0.000990992\n",
      "Norm of the params: 3.2171586\n",
      "                Loss: fixed  33 labels. Loss 0.50701. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [420] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57724774\n",
      "Train loss (w/o reg) on all data: 0.5771657\n",
      "Test loss (w/o reg) on all data: 0.4390044\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00074584445\n",
      "Norm of the params: 1.2808329\n",
      "              Random: fixed  12 labels. Loss 0.43900. Accuracy 0.756.\n",
      "### Flips: 60, rs: 9, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [185] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46704936\n",
      "Train loss (w/o reg) on all data: 0.46683884\n",
      "Test loss (w/o reg) on all data: 0.4318475\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004677784\n",
      "Norm of the params: 2.0520082\n",
      "     Influence (LOO): fixed  28 labels. Loss 0.43185. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [199] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33255476\n",
      "Train loss (w/o reg) on all data: 0.33202785\n",
      "Test loss (w/o reg) on all data: 0.48826098\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017908199\n",
      "Norm of the params: 3.2462585\n",
      "                Loss: fixed  35 labels. Loss 0.48826. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [314] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55629534\n",
      "Train loss (w/o reg) on all data: 0.5562058\n",
      "Test loss (w/o reg) on all data: 0.41801935\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0038346134\n",
      "Norm of the params: 1.3379239\n",
      "              Random: fixed  18 labels. Loss 0.41802. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [330] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61751616\n",
      "Train loss (w/o reg) on all data: 0.61746156\n",
      "Test loss (w/o reg) on all data: 0.56086314\n",
      "Train acc on all data:  0.660377358490566\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.010348253\n",
      "Norm of the params: 1.0451045\n",
      "Flipped loss: 0.56086. Accuracy: 0.733\n",
      "### Flips: 60, rs: 10, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.582446\n",
      "Train loss (w/o reg) on all data: 0.58237034\n",
      "Test loss (w/o reg) on all data: 0.49924633\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0026229944\n",
      "Norm of the params: 1.229957\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.49925. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5472045\n",
      "Train loss (w/o reg) on all data: 0.5470999\n",
      "Test loss (w/o reg) on all data: 0.5331269\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0009668987\n",
      "Norm of the params: 1.4462134\n",
      "                Loss: fixed   8 labels. Loss 0.53313. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [155] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60126144\n",
      "Train loss (w/o reg) on all data: 0.6011993\n",
      "Test loss (w/o reg) on all data: 0.5336641\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0016523327\n",
      "Norm of the params: 1.1144524\n",
      "              Random: fixed   4 labels. Loss 0.53366. Accuracy 0.756.\n",
      "### Flips: 60, rs: 10, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [117] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5595967\n",
      "Train loss (w/o reg) on all data: 0.55950505\n",
      "Test loss (w/o reg) on all data: 0.4760094\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004730735\n",
      "Norm of the params: 1.3539978\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.47601. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50083333\n",
      "Train loss (w/o reg) on all data: 0.50068516\n",
      "Test loss (w/o reg) on all data: 0.5127259\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0010346733\n",
      "Norm of the params: 1.7215203\n",
      "                Loss: fixed  14 labels. Loss 0.51273. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5889745\n",
      "Train loss (w/o reg) on all data: 0.588908\n",
      "Test loss (w/o reg) on all data: 0.52185625\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0020301908\n",
      "Norm of the params: 1.1528245\n",
      "              Random: fixed   6 labels. Loss 0.52186. Accuracy 0.756.\n",
      "### Flips: 60, rs: 10, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [369] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5358916\n",
      "Train loss (w/o reg) on all data: 0.53580624\n",
      "Test loss (w/o reg) on all data: 0.42007053\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00068341405\n",
      "Norm of the params: 1.306402\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.42007. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [343] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44216564\n",
      "Train loss (w/o reg) on all data: 0.4419547\n",
      "Test loss (w/o reg) on all data: 0.49404657\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011745444\n",
      "Norm of the params: 2.0539575\n",
      "                Loss: fixed  21 labels. Loss 0.49405. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [93] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57530785\n",
      "Train loss (w/o reg) on all data: 0.57522696\n",
      "Test loss (w/o reg) on all data: 0.50580114\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011496155\n",
      "Norm of the params: 1.2720823\n",
      "              Random: fixed   9 labels. Loss 0.50580. Accuracy 0.778.\n",
      "### Flips: 60, rs: 10, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5197488\n",
      "Train loss (w/o reg) on all data: 0.5196545\n",
      "Test loss (w/o reg) on all data: 0.41158268\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019711428\n",
      "Norm of the params: 1.373274\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.41158. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [358] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3959821\n",
      "Train loss (w/o reg) on all data: 0.3956932\n",
      "Test loss (w/o reg) on all data: 0.50119585\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013299623\n",
      "Norm of the params: 2.4036481\n",
      "                Loss: fixed  26 labels. Loss 0.50120. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5709026\n",
      "Train loss (w/o reg) on all data: 0.5708242\n",
      "Test loss (w/o reg) on all data: 0.49280053\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0013268188\n",
      "Norm of the params: 1.2519441\n",
      "              Random: fixed  11 labels. Loss 0.49280. Accuracy 0.756.\n",
      "### Flips: 60, rs: 10, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [308] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5101376\n",
      "Train loss (w/o reg) on all data: 0.51003873\n",
      "Test loss (w/o reg) on all data: 0.39148793\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002052643\n",
      "Norm of the params: 1.4064678\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.39149. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34230095\n",
      "Train loss (w/o reg) on all data: 0.34193563\n",
      "Test loss (w/o reg) on all data: 0.4905782\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016822531\n",
      "Norm of the params: 2.7030325\n",
      "                Loss: fixed  33 labels. Loss 0.49058. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [357] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56593335\n",
      "Train loss (w/o reg) on all data: 0.56584984\n",
      "Test loss (w/o reg) on all data: 0.48854646\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0011909788\n",
      "Norm of the params: 1.2922595\n",
      "              Random: fixed  15 labels. Loss 0.48855. Accuracy 0.756.\n",
      "### Flips: 60, rs: 10, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [364] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49164635\n",
      "Train loss (w/o reg) on all data: 0.49154854\n",
      "Test loss (w/o reg) on all data: 0.39058498\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0025136957\n",
      "Norm of the params: 1.3986459\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.39058. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [416] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33831495\n",
      "Train loss (w/o reg) on all data: 0.33791727\n",
      "Test loss (w/o reg) on all data: 0.47677374\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0005869203\n",
      "Norm of the params: 2.820204\n",
      "                Loss: fixed  34 labels. Loss 0.47677. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.561917\n",
      "Train loss (w/o reg) on all data: 0.5618337\n",
      "Test loss (w/o reg) on all data: 0.47291076\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001951951\n",
      "Norm of the params: 1.2910738\n",
      "              Random: fixed  17 labels. Loss 0.47291. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [320] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.63453317\n",
      "Train loss (w/o reg) on all data: 0.6344767\n",
      "Test loss (w/o reg) on all data: 0.50454366\n",
      "Train acc on all data:  0.6462264150943396\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0007219104\n",
      "Norm of the params: 1.0626409\n",
      "Flipped loss: 0.50454. Accuracy: 0.867\n",
      "### Flips: 60, rs: 11, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [87] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61977893\n",
      "Train loss (w/o reg) on all data: 0.6197133\n",
      "Test loss (w/o reg) on all data: 0.47497106\n",
      "Train acc on all data:  0.6556603773584906\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0018970856\n",
      "Norm of the params: 1.145575\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.47497. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [152] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5670733\n",
      "Train loss (w/o reg) on all data: 0.5669637\n",
      "Test loss (w/o reg) on all data: 0.4206889\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002869961\n",
      "Norm of the params: 1.4807211\n",
      "                Loss: fixed   9 labels. Loss 0.42069. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [222] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6371421\n",
      "Train loss (w/o reg) on all data: 0.63710254\n",
      "Test loss (w/o reg) on all data: 0.49913895\n",
      "Train acc on all data:  0.6462264150943396\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010671377\n",
      "Norm of the params: 0.8896061\n",
      "              Random: fixed   2 labels. Loss 0.49914. Accuracy 0.844.\n",
      "### Flips: 60, rs: 11, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.592699\n",
      "Train loss (w/o reg) on all data: 0.59260434\n",
      "Test loss (w/o reg) on all data: 0.4363129\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0044218376\n",
      "Norm of the params: 1.3757782\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.43631. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [341] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49861124\n",
      "Train loss (w/o reg) on all data: 0.49838933\n",
      "Test loss (w/o reg) on all data: 0.41463512\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0021929233\n",
      "Norm of the params: 2.1066995\n",
      "                Loss: fixed  18 labels. Loss 0.41464. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [282] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6320361\n",
      "Train loss (w/o reg) on all data: 0.631994\n",
      "Test loss (w/o reg) on all data: 0.4861802\n",
      "Train acc on all data:  0.660377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.005456624\n",
      "Norm of the params: 0.9171644\n",
      "              Random: fixed   4 labels. Loss 0.48618. Accuracy 0.822.\n",
      "### Flips: 60, rs: 11, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57853925\n",
      "Train loss (w/o reg) on all data: 0.57842934\n",
      "Test loss (w/o reg) on all data: 0.42707643\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0020499846\n",
      "Norm of the params: 1.4825411\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.42708. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [279] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4156901\n",
      "Train loss (w/o reg) on all data: 0.4152147\n",
      "Test loss (w/o reg) on all data: 0.41876915\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014609775\n",
      "Norm of the params: 3.083479\n",
      "                Loss: fixed  27 labels. Loss 0.41877. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [308] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6306876\n",
      "Train loss (w/o reg) on all data: 0.6306439\n",
      "Test loss (w/o reg) on all data: 0.48768723\n",
      "Train acc on all data:  0.6509433962264151\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015375487\n",
      "Norm of the params: 0.9348656\n",
      "              Random: fixed   6 labels. Loss 0.48769. Accuracy 0.800.\n",
      "### Flips: 60, rs: 11, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [100] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55851877\n",
      "Train loss (w/o reg) on all data: 0.5583909\n",
      "Test loss (w/o reg) on all data: 0.4187376\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016091359\n",
      "Norm of the params: 1.5989732\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.41874. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38756222\n",
      "Train loss (w/o reg) on all data: 0.3870142\n",
      "Test loss (w/o reg) on all data: 0.42915723\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002379177\n",
      "Norm of the params: 3.3105779\n",
      "                Loss: fixed  30 labels. Loss 0.42916. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [414] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6224337\n",
      "Train loss (w/o reg) on all data: 0.6223702\n",
      "Test loss (w/o reg) on all data: 0.48680896\n",
      "Train acc on all data:  0.6556603773584906\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0014851729\n",
      "Norm of the params: 1.1271077\n",
      "              Random: fixed   9 labels. Loss 0.48681. Accuracy 0.844.\n",
      "### Flips: 60, rs: 11, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [322] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5317565\n",
      "Train loss (w/o reg) on all data: 0.53160244\n",
      "Test loss (w/o reg) on all data: 0.39424202\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.007468067\n",
      "Norm of the params: 1.7554882\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.39424. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [305] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35239366\n",
      "Train loss (w/o reg) on all data: 0.3517686\n",
      "Test loss (w/o reg) on all data: 0.44545466\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00061304047\n",
      "Norm of the params: 3.5356936\n",
      "                Loss: fixed  35 labels. Loss 0.44545. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [503] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6155937\n",
      "Train loss (w/o reg) on all data: 0.615524\n",
      "Test loss (w/o reg) on all data: 0.47735694\n",
      "Train acc on all data:  0.6556603773584906\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0016184807\n",
      "Norm of the params: 1.180325\n",
      "              Random: fixed  10 labels. Loss 0.47736. Accuracy 0.844.\n",
      "### Flips: 60, rs: 11, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5317528\n",
      "Train loss (w/o reg) on all data: 0.53159726\n",
      "Test loss (w/o reg) on all data: 0.3940042\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00045125265\n",
      "Norm of the params: 1.7640611\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.39400. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32991892\n",
      "Train loss (w/o reg) on all data: 0.3291447\n",
      "Test loss (w/o reg) on all data: 0.45713562\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0034422094\n",
      "Norm of the params: 3.9350483\n",
      "                Loss: fixed  38 labels. Loss 0.45714. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [347] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5975526\n",
      "Train loss (w/o reg) on all data: 0.5974784\n",
      "Test loss (w/o reg) on all data: 0.43969965\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0034245735\n",
      "Norm of the params: 1.2182765\n",
      "              Random: fixed  13 labels. Loss 0.43970. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6263124\n",
      "Train loss (w/o reg) on all data: 0.6262617\n",
      "Test loss (w/o reg) on all data: 0.49050754\n",
      "Train acc on all data:  0.6415094339622641\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0031454123\n",
      "Norm of the params: 1.0064247\n",
      "Flipped loss: 0.49051. Accuracy: 0.844\n",
      "### Flips: 60, rs: 12, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60871613\n",
      "Train loss (w/o reg) on all data: 0.6086695\n",
      "Test loss (w/o reg) on all data: 0.4438102\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.012736484\n",
      "Norm of the params: 0.96525705\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.44381. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57569754\n",
      "Train loss (w/o reg) on all data: 0.57558036\n",
      "Test loss (w/o reg) on all data: 0.4419328\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0023721298\n",
      "Norm of the params: 1.5309016\n",
      "                Loss: fixed   7 labels. Loss 0.44193. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61582303\n",
      "Train loss (w/o reg) on all data: 0.6157763\n",
      "Test loss (w/o reg) on all data: 0.47527042\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0016779411\n",
      "Norm of the params: 0.966952\n",
      "              Random: fixed   4 labels. Loss 0.47527. Accuracy 0.867.\n",
      "### Flips: 60, rs: 12, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [240] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5899281\n",
      "Train loss (w/o reg) on all data: 0.589871\n",
      "Test loss (w/o reg) on all data: 0.4270176\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0012768445\n",
      "Norm of the params: 1.0685972\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.42702. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5228981\n",
      "Train loss (w/o reg) on all data: 0.5227069\n",
      "Test loss (w/o reg) on all data: 0.4002988\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0034835134\n",
      "Norm of the params: 1.9553263\n",
      "                Loss: fixed  15 labels. Loss 0.40030. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [272] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6123675\n",
      "Train loss (w/o reg) on all data: 0.612308\n",
      "Test loss (w/o reg) on all data: 0.46709815\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0045051007\n",
      "Norm of the params: 1.0909907\n",
      "              Random: fixed   9 labels. Loss 0.46710. Accuracy 0.844.\n",
      "### Flips: 60, rs: 12, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56015325\n",
      "Train loss (w/o reg) on all data: 0.5600661\n",
      "Test loss (w/o reg) on all data: 0.40192276\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.001427614\n",
      "Norm of the params: 1.3201069\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.40192. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [305] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46405017\n",
      "Train loss (w/o reg) on all data: 0.4637773\n",
      "Test loss (w/o reg) on all data: 0.3973691\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.007461663\n",
      "Norm of the params: 2.3361695\n",
      "                Loss: fixed  23 labels. Loss 0.39737. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6093575\n",
      "Train loss (w/o reg) on all data: 0.60930175\n",
      "Test loss (w/o reg) on all data: 0.47517443\n",
      "Train acc on all data:  0.6556603773584906\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019136439\n",
      "Norm of the params: 1.0559134\n",
      "              Random: fixed  11 labels. Loss 0.47517. Accuracy 0.822.\n",
      "### Flips: 60, rs: 12, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [198] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53977627\n",
      "Train loss (w/o reg) on all data: 0.53967243\n",
      "Test loss (w/o reg) on all data: 0.38709158\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.002757706\n",
      "Norm of the params: 1.4411438\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.38709. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.423223\n",
      "Train loss (w/o reg) on all data: 0.42288697\n",
      "Test loss (w/o reg) on all data: 0.3466212\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0007499661\n",
      "Norm of the params: 2.5924325\n",
      "                Loss: fixed  29 labels. Loss 0.34662. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [332] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60009366\n",
      "Train loss (w/o reg) on all data: 0.6000364\n",
      "Test loss (w/o reg) on all data: 0.465121\n",
      "Train acc on all data:  0.660377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0036451546\n",
      "Norm of the params: 1.0701123\n",
      "              Random: fixed  14 labels. Loss 0.46512. Accuracy 0.822.\n",
      "### Flips: 60, rs: 12, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [166] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52066344\n",
      "Train loss (w/o reg) on all data: 0.5205274\n",
      "Test loss (w/o reg) on all data: 0.3864392\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012284518\n",
      "Norm of the params: 1.6493214\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.38644. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [301] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40474424\n",
      "Train loss (w/o reg) on all data: 0.40440607\n",
      "Test loss (w/o reg) on all data: 0.3703542\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0027258047\n",
      "Norm of the params: 2.6006587\n",
      "                Loss: fixed  33 labels. Loss 0.37035. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [265] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58757323\n",
      "Train loss (w/o reg) on all data: 0.5875032\n",
      "Test loss (w/o reg) on all data: 0.4754442\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.009047372\n",
      "Norm of the params: 1.1834918\n",
      "              Random: fixed  17 labels. Loss 0.47544. Accuracy 0.800.\n",
      "### Flips: 60, rs: 12, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4762271\n",
      "Train loss (w/o reg) on all data: 0.47603393\n",
      "Test loss (w/o reg) on all data: 0.3787144\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00092081656\n",
      "Norm of the params: 1.9656701\n",
      "     Influence (LOO): fixed  31 labels. Loss 0.37871. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [302] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37739348\n",
      "Train loss (w/o reg) on all data: 0.3770397\n",
      "Test loss (w/o reg) on all data: 0.40286776\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.000858146\n",
      "Norm of the params: 2.660012\n",
      "                Loss: fixed  39 labels. Loss 0.40287. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [435] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55992603\n",
      "Train loss (w/o reg) on all data: 0.5598382\n",
      "Test loss (w/o reg) on all data: 0.46003532\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0054865046\n",
      "Norm of the params: 1.3253615\n",
      "              Random: fixed  23 labels. Loss 0.46004. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6188179\n",
      "Train loss (w/o reg) on all data: 0.6187852\n",
      "Test loss (w/o reg) on all data: 0.4910692\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008233325\n",
      "Norm of the params: 0.80919594\n",
      "Flipped loss: 0.49107. Accuracy: 0.822\n",
      "### Flips: 60, rs: 13, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57024455\n",
      "Train loss (w/o reg) on all data: 0.5701503\n",
      "Test loss (w/o reg) on all data: 0.4373613\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001102518\n",
      "Norm of the params: 1.3727001\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.43736. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53558713\n",
      "Train loss (w/o reg) on all data: 0.5354308\n",
      "Test loss (w/o reg) on all data: 0.41207826\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001099819\n",
      "Norm of the params: 1.768321\n",
      "                Loss: fixed   9 labels. Loss 0.41208. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6065527\n",
      "Train loss (w/o reg) on all data: 0.60652\n",
      "Test loss (w/o reg) on all data: 0.46504143\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0021766159\n",
      "Norm of the params: 0.80934095\n",
      "              Random: fixed   6 labels. Loss 0.46504. Accuracy 0.822.\n",
      "### Flips: 60, rs: 13, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54144686\n",
      "Train loss (w/o reg) on all data: 0.5413192\n",
      "Test loss (w/o reg) on all data: 0.41505203\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011373012\n",
      "Norm of the params: 1.5980546\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.41505. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47145808\n",
      "Train loss (w/o reg) on all data: 0.4712646\n",
      "Test loss (w/o reg) on all data: 0.40362576\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00073864334\n",
      "Norm of the params: 1.967103\n",
      "                Loss: fixed  18 labels. Loss 0.40363. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5820708\n",
      "Train loss (w/o reg) on all data: 0.5820214\n",
      "Test loss (w/o reg) on all data: 0.42893267\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0012973691\n",
      "Norm of the params: 0.994066\n",
      "              Random: fixed   9 labels. Loss 0.42893. Accuracy 0.844.\n",
      "### Flips: 60, rs: 13, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [160] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51652634\n",
      "Train loss (w/o reg) on all data: 0.516398\n",
      "Test loss (w/o reg) on all data: 0.3952161\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0018805789\n",
      "Norm of the params: 1.6020362\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.39522. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [329] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39760536\n",
      "Train loss (w/o reg) on all data: 0.39729485\n",
      "Test loss (w/o reg) on all data: 0.39784715\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00071870553\n",
      "Norm of the params: 2.4919822\n",
      "                Loss: fixed  26 labels. Loss 0.39785. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5665864\n",
      "Train loss (w/o reg) on all data: 0.56652915\n",
      "Test loss (w/o reg) on all data: 0.42930865\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00095087715\n",
      "Norm of the params: 1.0695084\n",
      "              Random: fixed  13 labels. Loss 0.42931. Accuracy 0.844.\n",
      "### Flips: 60, rs: 13, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4844733\n",
      "Train loss (w/o reg) on all data: 0.48431256\n",
      "Test loss (w/o reg) on all data: 0.38214773\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0018720907\n",
      "Norm of the params: 1.7928679\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.38215. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [322] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34197235\n",
      "Train loss (w/o reg) on all data: 0.34153885\n",
      "Test loss (w/o reg) on all data: 0.40509316\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0018008737\n",
      "Norm of the params: 2.9445095\n",
      "                Loss: fixed  32 labels. Loss 0.40509. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [352] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55372316\n",
      "Train loss (w/o reg) on all data: 0.55365396\n",
      "Test loss (w/o reg) on all data: 0.4351661\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0029597683\n",
      "Norm of the params: 1.1765766\n",
      "              Random: fixed  16 labels. Loss 0.43517. Accuracy 0.800.\n",
      "### Flips: 60, rs: 13, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [154] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45775193\n",
      "Train loss (w/o reg) on all data: 0.45757055\n",
      "Test loss (w/o reg) on all data: 0.38109368\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0014574558\n",
      "Norm of the params: 1.9046375\n",
      "     Influence (LOO): fixed  28 labels. Loss 0.38109. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [352] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32479388\n",
      "Train loss (w/o reg) on all data: 0.32429153\n",
      "Test loss (w/o reg) on all data: 0.43449733\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0017558391\n",
      "Norm of the params: 3.169741\n",
      "                Loss: fixed  36 labels. Loss 0.43450. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [313] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55949914\n",
      "Train loss (w/o reg) on all data: 0.55943346\n",
      "Test loss (w/o reg) on all data: 0.42641622\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.005346452\n",
      "Norm of the params: 1.145961\n",
      "              Random: fixed  17 labels. Loss 0.42642. Accuracy 0.800.\n",
      "### Flips: 60, rs: 13, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [332] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4138499\n",
      "Train loss (w/o reg) on all data: 0.4136087\n",
      "Test loss (w/o reg) on all data: 0.3882944\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008982355\n",
      "Norm of the params: 2.1963143\n",
      "     Influence (LOO): fixed  33 labels. Loss 0.38829. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31941754\n",
      "Train loss (w/o reg) on all data: 0.31893712\n",
      "Test loss (w/o reg) on all data: 0.4239011\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0018729231\n",
      "Norm of the params: 3.099716\n",
      "                Loss: fixed  37 labels. Loss 0.42390. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54232484\n",
      "Train loss (w/o reg) on all data: 0.5422423\n",
      "Test loss (w/o reg) on all data: 0.40947646\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010073457\n",
      "Norm of the params: 1.2851429\n",
      "              Random: fixed  20 labels. Loss 0.40948. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [342] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61098015\n",
      "Train loss (w/o reg) on all data: 0.61088884\n",
      "Test loss (w/o reg) on all data: 0.47074065\n",
      "Train acc on all data:  0.660377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0022028983\n",
      "Norm of the params: 1.351223\n",
      "Flipped loss: 0.47074. Accuracy: 0.822\n",
      "### Flips: 60, rs: 14, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [51] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58827686\n",
      "Train loss (w/o reg) on all data: 0.5881687\n",
      "Test loss (w/o reg) on all data: 0.4347415\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010192541\n",
      "Norm of the params: 1.4711083\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.43474. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [127] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5395485\n",
      "Train loss (w/o reg) on all data: 0.539368\n",
      "Test loss (w/o reg) on all data: 0.39089522\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0034376183\n",
      "Norm of the params: 1.9001001\n",
      "                Loss: fixed   8 labels. Loss 0.39090. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [175] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6060359\n",
      "Train loss (w/o reg) on all data: 0.6059345\n",
      "Test loss (w/o reg) on all data: 0.47404337\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004155908\n",
      "Norm of the params: 1.4240062\n",
      "              Random: fixed   2 labels. Loss 0.47404. Accuracy 0.800.\n",
      "### Flips: 60, rs: 14, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [196] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5588568\n",
      "Train loss (w/o reg) on all data: 0.5587119\n",
      "Test loss (w/o reg) on all data: 0.42311054\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0049992017\n",
      "Norm of the params: 1.702246\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.42311. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4818117\n",
      "Train loss (w/o reg) on all data: 0.48147178\n",
      "Test loss (w/o reg) on all data: 0.35984966\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010727796\n",
      "Norm of the params: 2.6073582\n",
      "                Loss: fixed  15 labels. Loss 0.35985. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5930953\n",
      "Train loss (w/o reg) on all data: 0.59298897\n",
      "Test loss (w/o reg) on all data: 0.44152\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016728779\n",
      "Norm of the params: 1.4583856\n",
      "              Random: fixed   5 labels. Loss 0.44152. Accuracy 0.822.\n",
      "### Flips: 60, rs: 14, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5464952\n",
      "Train loss (w/o reg) on all data: 0.54632\n",
      "Test loss (w/o reg) on all data: 0.41482922\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0058410484\n",
      "Norm of the params: 1.8718308\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.41483. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44204968\n",
      "Train loss (w/o reg) on all data: 0.4415853\n",
      "Test loss (w/o reg) on all data: 0.32154417\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.002763736\n",
      "Norm of the params: 3.0475113\n",
      "                Loss: fixed  21 labels. Loss 0.32154. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5795663\n",
      "Train loss (w/o reg) on all data: 0.5794404\n",
      "Test loss (w/o reg) on all data: 0.42657357\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0073654293\n",
      "Norm of the params: 1.5869007\n",
      "              Random: fixed   9 labels. Loss 0.42657. Accuracy 0.822.\n",
      "### Flips: 60, rs: 14, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [120] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52462226\n",
      "Train loss (w/o reg) on all data: 0.5244261\n",
      "Test loss (w/o reg) on all data: 0.40765008\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003269905\n",
      "Norm of the params: 1.9806702\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.40765. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [365] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41014692\n",
      "Train loss (w/o reg) on all data: 0.40960643\n",
      "Test loss (w/o reg) on all data: 0.3065062\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0016230971\n",
      "Norm of the params: 3.2878158\n",
      "                Loss: fixed  26 labels. Loss 0.30651. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5830696\n",
      "Train loss (w/o reg) on all data: 0.58295155\n",
      "Test loss (w/o reg) on all data: 0.4325399\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018418588\n",
      "Norm of the params: 1.5368313\n",
      "              Random: fixed  12 labels. Loss 0.43254. Accuracy 0.800.\n",
      "### Flips: 60, rs: 14, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [141] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51893824\n",
      "Train loss (w/o reg) on all data: 0.51875186\n",
      "Test loss (w/o reg) on all data: 0.42108122\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0072558476\n",
      "Norm of the params: 1.9308039\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.42108. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [269] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38119978\n",
      "Train loss (w/o reg) on all data: 0.3804479\n",
      "Test loss (w/o reg) on all data: 0.30463746\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.011718861\n",
      "Norm of the params: 3.877809\n",
      "                Loss: fixed  32 labels. Loss 0.30464. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [309] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5833433\n",
      "Train loss (w/o reg) on all data: 0.5832374\n",
      "Test loss (w/o reg) on all data: 0.433675\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0025934428\n",
      "Norm of the params: 1.4554623\n",
      "              Random: fixed  17 labels. Loss 0.43367. Accuracy 0.800.\n",
      "### Flips: 60, rs: 14, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49762323\n",
      "Train loss (w/o reg) on all data: 0.4973999\n",
      "Test loss (w/o reg) on all data: 0.39263174\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0015688828\n",
      "Norm of the params: 2.1134522\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.39263. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [487] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36899066\n",
      "Train loss (w/o reg) on all data: 0.3683332\n",
      "Test loss (w/o reg) on all data: 0.30509847\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0037724301\n",
      "Norm of the params: 3.6261795\n",
      "                Loss: fixed  36 labels. Loss 0.30510. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [295] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55942696\n",
      "Train loss (w/o reg) on all data: 0.5592949\n",
      "Test loss (w/o reg) on all data: 0.4018944\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.004575047\n",
      "Norm of the params: 1.6252276\n",
      "              Random: fixed  22 labels. Loss 0.40189. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5829801\n",
      "Train loss (w/o reg) on all data: 0.5828827\n",
      "Test loss (w/o reg) on all data: 0.42724127\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0026731938\n",
      "Norm of the params: 1.3956983\n",
      "Flipped loss: 0.42724. Accuracy: 0.844\n",
      "### Flips: 60, rs: 15, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [93] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5364571\n",
      "Train loss (w/o reg) on all data: 0.53631616\n",
      "Test loss (w/o reg) on all data: 0.3851156\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.00075255276\n",
      "Norm of the params: 1.6791644\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.38512. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [307] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49092996\n",
      "Train loss (w/o reg) on all data: 0.49072543\n",
      "Test loss (w/o reg) on all data: 0.34722912\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0034150246\n",
      "Norm of the params: 2.0225337\n",
      "                Loss: fixed   9 labels. Loss 0.34723. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [301] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5931466\n",
      "Train loss (w/o reg) on all data: 0.5930491\n",
      "Test loss (w/o reg) on all data: 0.43560097\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006645205\n",
      "Norm of the params: 1.3966329\n",
      "              Random: fixed   2 labels. Loss 0.43560. Accuracy 0.800.\n",
      "### Flips: 60, rs: 15, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [86] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5365328\n",
      "Train loss (w/o reg) on all data: 0.5363935\n",
      "Test loss (w/o reg) on all data: 0.38822508\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0031391361\n",
      "Norm of the params: 1.6690884\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.38823. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [316] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45053175\n",
      "Train loss (w/o reg) on all data: 0.45024994\n",
      "Test loss (w/o reg) on all data: 0.3402448\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0018276958\n",
      "Norm of the params: 2.374053\n",
      "                Loss: fixed  14 labels. Loss 0.34024. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5808994\n",
      "Train loss (w/o reg) on all data: 0.5808075\n",
      "Test loss (w/o reg) on all data: 0.42223316\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0029194956\n",
      "Norm of the params: 1.355756\n",
      "              Random: fixed   6 labels. Loss 0.42223. Accuracy 0.822.\n",
      "### Flips: 60, rs: 15, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [152] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50192636\n",
      "Train loss (w/o reg) on all data: 0.5017587\n",
      "Test loss (w/o reg) on all data: 0.36406696\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0008081477\n",
      "Norm of the params: 1.831277\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.36407. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [326] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39957377\n",
      "Train loss (w/o reg) on all data: 0.39922318\n",
      "Test loss (w/o reg) on all data: 0.35230222\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0014787277\n",
      "Norm of the params: 2.6479573\n",
      "                Loss: fixed  20 labels. Loss 0.35230. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [372] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57831424\n",
      "Train loss (w/o reg) on all data: 0.5782315\n",
      "Test loss (w/o reg) on all data: 0.40814942\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.007797941\n",
      "Norm of the params: 1.2864747\n",
      "              Random: fixed  10 labels. Loss 0.40815. Accuracy 0.844.\n",
      "### Flips: 60, rs: 15, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4617479\n",
      "Train loss (w/o reg) on all data: 0.46157944\n",
      "Test loss (w/o reg) on all data: 0.33112162\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0014305738\n",
      "Norm of the params: 1.8355623\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.33112. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [315] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36825746\n",
      "Train loss (w/o reg) on all data: 0.36782506\n",
      "Test loss (w/o reg) on all data: 0.38687482\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0009978624\n",
      "Norm of the params: 2.9407792\n",
      "                Loss: fixed  24 labels. Loss 0.38687. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5746408\n",
      "Train loss (w/o reg) on all data: 0.57456124\n",
      "Test loss (w/o reg) on all data: 0.39478892\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0039274083\n",
      "Norm of the params: 1.2616518\n",
      "              Random: fixed  14 labels. Loss 0.39479. Accuracy 0.844.\n",
      "### Flips: 60, rs: 15, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45731702\n",
      "Train loss (w/o reg) on all data: 0.45716786\n",
      "Test loss (w/o reg) on all data: 0.3352272\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00073544745\n",
      "Norm of the params: 1.727186\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.33523. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [463] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34132951\n",
      "Train loss (w/o reg) on all data: 0.34077352\n",
      "Test loss (w/o reg) on all data: 0.42165837\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00010625238\n",
      "Norm of the params: 3.3346093\n",
      "                Loss: fixed  28 labels. Loss 0.42166. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [280] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.567519\n",
      "Train loss (w/o reg) on all data: 0.5674336\n",
      "Test loss (w/o reg) on all data: 0.40072334\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0028072216\n",
      "Norm of the params: 1.3071828\n",
      "              Random: fixed  16 labels. Loss 0.40072. Accuracy 0.844.\n",
      "### Flips: 60, rs: 15, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44178376\n",
      "Train loss (w/o reg) on all data: 0.4416363\n",
      "Test loss (w/o reg) on all data: 0.3267216\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00029366146\n",
      "Norm of the params: 1.7172573\n",
      "     Influence (LOO): fixed  28 labels. Loss 0.32672. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [301] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31469655\n",
      "Train loss (w/o reg) on all data: 0.31414402\n",
      "Test loss (w/o reg) on all data: 0.43256527\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0004765053\n",
      "Norm of the params: 3.3242776\n",
      "                Loss: fixed  32 labels. Loss 0.43257. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.573038\n",
      "Train loss (w/o reg) on all data: 0.5729668\n",
      "Test loss (w/o reg) on all data: 0.41043073\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00077057775\n",
      "Norm of the params: 1.1932818\n",
      "              Random: fixed  21 labels. Loss 0.41043. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60303843\n",
      "Train loss (w/o reg) on all data: 0.6029837\n",
      "Test loss (w/o reg) on all data: 0.47780398\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.005270222\n",
      "Norm of the params: 1.0459905\n",
      "Flipped loss: 0.47780. Accuracy: 0.756\n",
      "### Flips: 60, rs: 16, checks: 10\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5658514\n",
      "Train loss (w/o reg) on all data: 0.56578666\n",
      "Test loss (w/o reg) on all data: 0.4339764\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.006504424\n",
      "Norm of the params: 1.1379142\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.43398. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [194] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5170638\n",
      "Train loss (w/o reg) on all data: 0.51692575\n",
      "Test loss (w/o reg) on all data: 0.43377367\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0066042547\n",
      "Norm of the params: 1.6616254\n",
      "                Loss: fixed   9 labels. Loss 0.43377. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6053987\n",
      "Train loss (w/o reg) on all data: 0.60534763\n",
      "Test loss (w/o reg) on all data: 0.4738081\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00085501384\n",
      "Norm of the params: 1.0106648\n",
      "              Random: fixed   2 labels. Loss 0.47381. Accuracy 0.800.\n",
      "### Flips: 60, rs: 16, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5447701\n",
      "Train loss (w/o reg) on all data: 0.5446854\n",
      "Test loss (w/o reg) on all data: 0.41944936\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0060231066\n",
      "Norm of the params: 1.3016729\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.41945. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46936682\n",
      "Train loss (w/o reg) on all data: 0.46914536\n",
      "Test loss (w/o reg) on all data: 0.44021425\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00583172\n",
      "Norm of the params: 2.1045964\n",
      "                Loss: fixed  15 labels. Loss 0.44021. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [152] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60109836\n",
      "Train loss (w/o reg) on all data: 0.60105073\n",
      "Test loss (w/o reg) on all data: 0.4527346\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00046980332\n",
      "Norm of the params: 0.97590226\n",
      "              Random: fixed   4 labels. Loss 0.45273. Accuracy 0.844.\n",
      "### Flips: 60, rs: 16, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [134] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5347698\n",
      "Train loss (w/o reg) on all data: 0.5346724\n",
      "Test loss (w/o reg) on all data: 0.41837564\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0020360108\n",
      "Norm of the params: 1.3957455\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.41838. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43002325\n",
      "Train loss (w/o reg) on all data: 0.42977753\n",
      "Test loss (w/o reg) on all data: 0.43377897\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00078658847\n",
      "Norm of the params: 2.216814\n",
      "                Loss: fixed  20 labels. Loss 0.43378. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [126] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5960199\n",
      "Train loss (w/o reg) on all data: 0.5959637\n",
      "Test loss (w/o reg) on all data: 0.4388775\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.001620947\n",
      "Norm of the params: 1.0600318\n",
      "              Random: fixed   8 labels. Loss 0.43888. Accuracy 0.844.\n",
      "### Flips: 60, rs: 16, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [237] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5000826\n",
      "Train loss (w/o reg) on all data: 0.4999481\n",
      "Test loss (w/o reg) on all data: 0.40123668\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0014979353\n",
      "Norm of the params: 1.6399467\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.40124. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [303] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39575863\n",
      "Train loss (w/o reg) on all data: 0.39542228\n",
      "Test loss (w/o reg) on all data: 0.42338982\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017635666\n",
      "Norm of the params: 2.5936954\n",
      "                Loss: fixed  24 labels. Loss 0.42339. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [185] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5754976\n",
      "Train loss (w/o reg) on all data: 0.5754209\n",
      "Test loss (w/o reg) on all data: 0.42181277\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.007720699\n",
      "Norm of the params: 1.238465\n",
      "              Random: fixed  13 labels. Loss 0.42181. Accuracy 0.822.\n",
      "### Flips: 60, rs: 16, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48401228\n",
      "Train loss (w/o reg) on all data: 0.48387012\n",
      "Test loss (w/o reg) on all data: 0.3816128\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0024611438\n",
      "Norm of the params: 1.6862267\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.38161. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [362] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36177564\n",
      "Train loss (w/o reg) on all data: 0.3613264\n",
      "Test loss (w/o reg) on all data: 0.41226435\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00067313045\n",
      "Norm of the params: 2.9974363\n",
      "                Loss: fixed  30 labels. Loss 0.41226. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54979604\n",
      "Train loss (w/o reg) on all data: 0.5496933\n",
      "Test loss (w/o reg) on all data: 0.40931675\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012409108\n",
      "Norm of the params: 1.433444\n",
      "              Random: fixed  17 labels. Loss 0.40932. Accuracy 0.800.\n",
      "### Flips: 60, rs: 16, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [305] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45374537\n",
      "Train loss (w/o reg) on all data: 0.45354527\n",
      "Test loss (w/o reg) on all data: 0.36860645\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00792511\n",
      "Norm of the params: 2.0004563\n",
      "     Influence (LOO): fixed  28 labels. Loss 0.36861. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [316] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33579862\n",
      "Train loss (w/o reg) on all data: 0.33532387\n",
      "Test loss (w/o reg) on all data: 0.4421704\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0021478084\n",
      "Norm of the params: 3.0813901\n",
      "                Loss: fixed  36 labels. Loss 0.44217. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [320] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54795605\n",
      "Train loss (w/o reg) on all data: 0.5478606\n",
      "Test loss (w/o reg) on all data: 0.41674533\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0027012667\n",
      "Norm of the params: 1.3816876\n",
      "              Random: fixed  19 labels. Loss 0.41675. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6324804\n",
      "Train loss (w/o reg) on all data: 0.63245565\n",
      "Test loss (w/o reg) on all data: 0.5671052\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.6666666666666666\n",
      "Norm of the mean of gradients: 0.002451703\n",
      "Norm of the params: 0.7035096\n",
      "Flipped loss: 0.56711. Accuracy: 0.667\n",
      "### Flips: 60, rs: 17, checks: 10\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [246] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58273643\n",
      "Train loss (w/o reg) on all data: 0.58268505\n",
      "Test loss (w/o reg) on all data: 0.48841485\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008680745\n",
      "Norm of the params: 1.013837\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.48841. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57705194\n",
      "Train loss (w/o reg) on all data: 0.57700896\n",
      "Test loss (w/o reg) on all data: 0.5064652\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0010269721\n",
      "Norm of the params: 0.92687106\n",
      "                Loss: fixed   7 labels. Loss 0.50647. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.62314564\n",
      "Train loss (w/o reg) on all data: 0.6231027\n",
      "Test loss (w/o reg) on all data: 0.5495705\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.006989645\n",
      "Norm of the params: 0.9266008\n",
      "              Random: fixed   5 labels. Loss 0.54957. Accuracy 0.711.\n",
      "### Flips: 60, rs: 17, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [149] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57931477\n",
      "Train loss (w/o reg) on all data: 0.57926464\n",
      "Test loss (w/o reg) on all data: 0.49430585\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.008778749\n",
      "Norm of the params: 1.0011777\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.49431. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50801545\n",
      "Train loss (w/o reg) on all data: 0.5079049\n",
      "Test loss (w/o reg) on all data: 0.4630403\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.008658433\n",
      "Norm of the params: 1.4871578\n",
      "                Loss: fixed  16 labels. Loss 0.46304. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6218934\n",
      "Train loss (w/o reg) on all data: 0.621864\n",
      "Test loss (w/o reg) on all data: 0.5175645\n",
      "Train acc on all data:  0.6650943396226415\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0015848253\n",
      "Norm of the params: 0.7662898\n",
      "              Random: fixed   8 labels. Loss 0.51756. Accuracy 0.778.\n",
      "### Flips: 60, rs: 17, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [312] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5769351\n",
      "Train loss (w/o reg) on all data: 0.5769011\n",
      "Test loss (w/o reg) on all data: 0.45983833\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003707719\n",
      "Norm of the params: 0.82508427\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.45984. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47169572\n",
      "Train loss (w/o reg) on all data: 0.47156748\n",
      "Test loss (w/o reg) on all data: 0.43845528\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0035360465\n",
      "Norm of the params: 1.6014472\n",
      "                Loss: fixed  20 labels. Loss 0.43846. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60717183\n",
      "Train loss (w/o reg) on all data: 0.60713273\n",
      "Test loss (w/o reg) on all data: 0.49934515\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019073711\n",
      "Norm of the params: 0.8845133\n",
      "              Random: fixed  12 labels. Loss 0.49935. Accuracy 0.800.\n",
      "### Flips: 60, rs: 17, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [225] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55895853\n",
      "Train loss (w/o reg) on all data: 0.55891675\n",
      "Test loss (w/o reg) on all data: 0.44848225\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010757903\n",
      "Norm of the params: 0.91400605\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.44848. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [266] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42065027\n",
      "Train loss (w/o reg) on all data: 0.42046738\n",
      "Test loss (w/o reg) on all data: 0.43661767\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0012116218\n",
      "Norm of the params: 1.9125859\n",
      "                Loss: fixed  27 labels. Loss 0.43662. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5987973\n",
      "Train loss (w/o reg) on all data: 0.5987556\n",
      "Test loss (w/o reg) on all data: 0.4814448\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002645747\n",
      "Norm of the params: 0.9133566\n",
      "              Random: fixed  14 labels. Loss 0.48144. Accuracy 0.800.\n",
      "### Flips: 60, rs: 17, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5379084\n",
      "Train loss (w/o reg) on all data: 0.53786063\n",
      "Test loss (w/o reg) on all data: 0.44350412\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0020218913\n",
      "Norm of the params: 0.97705173\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.44350. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [325] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40518865\n",
      "Train loss (w/o reg) on all data: 0.4049444\n",
      "Test loss (w/o reg) on all data: 0.4259221\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008548348\n",
      "Norm of the params: 2.2103026\n",
      "                Loss: fixed  29 labels. Loss 0.42592. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5882192\n",
      "Train loss (w/o reg) on all data: 0.5881442\n",
      "Test loss (w/o reg) on all data: 0.47047883\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0057159774\n",
      "Norm of the params: 1.2249981\n",
      "              Random: fixed  18 labels. Loss 0.47048. Accuracy 0.800.\n",
      "### Flips: 60, rs: 17, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [181] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50952417\n",
      "Train loss (w/o reg) on all data: 0.5094522\n",
      "Test loss (w/o reg) on all data: 0.41612488\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0040614177\n",
      "Norm of the params: 1.1996237\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.41612. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [329] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37849286\n",
      "Train loss (w/o reg) on all data: 0.37818348\n",
      "Test loss (w/o reg) on all data: 0.4395332\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017586716\n",
      "Norm of the params: 2.4874256\n",
      "                Loss: fixed  33 labels. Loss 0.43953. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5882127\n",
      "Train loss (w/o reg) on all data: 0.5881383\n",
      "Test loss (w/o reg) on all data: 0.47193336\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0030647598\n",
      "Norm of the params: 1.2204536\n",
      "              Random: fixed  18 labels. Loss 0.47193. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.63310903\n",
      "Train loss (w/o reg) on all data: 0.63305146\n",
      "Test loss (w/o reg) on all data: 0.5315031\n",
      "Train acc on all data:  0.6226415094339622\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0014708939\n",
      "Norm of the params: 1.0728605\n",
      "Flipped loss: 0.53150. Accuracy: 0.756\n",
      "### Flips: 60, rs: 18, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5932333\n",
      "Train loss (w/o reg) on all data: 0.5931243\n",
      "Test loss (w/o reg) on all data: 0.5147138\n",
      "Train acc on all data:  0.6462264150943396\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0023744348\n",
      "Norm of the params: 1.4767139\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.51471. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [89] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55836666\n",
      "Train loss (w/o reg) on all data: 0.55819786\n",
      "Test loss (w/o reg) on all data: 0.49043292\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001235748\n",
      "Norm of the params: 1.837274\n",
      "                Loss: fixed   9 labels. Loss 0.49043. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6319983\n",
      "Train loss (w/o reg) on all data: 0.63194275\n",
      "Test loss (w/o reg) on all data: 0.5288084\n",
      "Train acc on all data:  0.6273584905660378\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.004132746\n",
      "Norm of the params: 1.054218\n",
      "              Random: fixed   1 labels. Loss 0.52881. Accuracy 0.778.\n",
      "### Flips: 60, rs: 18, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5725223\n",
      "Train loss (w/o reg) on all data: 0.57240987\n",
      "Test loss (w/o reg) on all data: 0.4743313\n",
      "Train acc on all data:  0.6650943396226415\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.02400355\n",
      "Norm of the params: 1.4995505\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.47433. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5213179\n",
      "Train loss (w/o reg) on all data: 0.5210932\n",
      "Test loss (w/o reg) on all data: 0.46436438\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00089790806\n",
      "Norm of the params: 2.1200156\n",
      "                Loss: fixed  15 labels. Loss 0.46436. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6304883\n",
      "Train loss (w/o reg) on all data: 0.6304368\n",
      "Test loss (w/o reg) on all data: 0.5334985\n",
      "Train acc on all data:  0.6415094339622641\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00028621985\n",
      "Norm of the params: 1.0148687\n",
      "              Random: fixed   2 labels. Loss 0.53350. Accuracy 0.778.\n",
      "### Flips: 60, rs: 18, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5500589\n",
      "Train loss (w/o reg) on all data: 0.5499376\n",
      "Test loss (w/o reg) on all data: 0.4522994\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0029561343\n",
      "Norm of the params: 1.5575702\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.45230. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [198] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4913418\n",
      "Train loss (w/o reg) on all data: 0.49103597\n",
      "Test loss (w/o reg) on all data: 0.45535868\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0022737912\n",
      "Norm of the params: 2.4731302\n",
      "                Loss: fixed  19 labels. Loss 0.45536. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [356] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6268274\n",
      "Train loss (w/o reg) on all data: 0.62677306\n",
      "Test loss (w/o reg) on all data: 0.5280168\n",
      "Train acc on all data:  0.6509433962264151\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.009422083\n",
      "Norm of the params: 1.0424267\n",
      "              Random: fixed   5 labels. Loss 0.52802. Accuracy 0.844.\n",
      "### Flips: 60, rs: 18, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [83] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5296502\n",
      "Train loss (w/o reg) on all data: 0.5295142\n",
      "Test loss (w/o reg) on all data: 0.4516814\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0026499175\n",
      "Norm of the params: 1.6494702\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.45168. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [183] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44409475\n",
      "Train loss (w/o reg) on all data: 0.44367492\n",
      "Test loss (w/o reg) on all data: 0.46294865\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.001579662\n",
      "Norm of the params: 2.8977168\n",
      "                Loss: fixed  27 labels. Loss 0.46295. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6192522\n",
      "Train loss (w/o reg) on all data: 0.6191752\n",
      "Test loss (w/o reg) on all data: 0.5339683\n",
      "Train acc on all data:  0.6556603773584906\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009829125\n",
      "Norm of the params: 1.2409129\n",
      "              Random: fixed   7 labels. Loss 0.53397. Accuracy 0.800.\n",
      "### Flips: 60, rs: 18, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [262] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5188909\n",
      "Train loss (w/o reg) on all data: 0.5187204\n",
      "Test loss (w/o reg) on all data: 0.45659608\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0024260809\n",
      "Norm of the params: 1.8467902\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.45660. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [329] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42184362\n",
      "Train loss (w/o reg) on all data: 0.42137945\n",
      "Test loss (w/o reg) on all data: 0.4542472\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00057044654\n",
      "Norm of the params: 3.0469022\n",
      "                Loss: fixed  31 labels. Loss 0.45425. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61486524\n",
      "Train loss (w/o reg) on all data: 0.6148129\n",
      "Test loss (w/o reg) on all data: 0.4666671\n",
      "Train acc on all data:  0.6509433962264151\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015454321\n",
      "Norm of the params: 1.0229951\n",
      "              Random: fixed  12 labels. Loss 0.46667. Accuracy 0.822.\n",
      "### Flips: 60, rs: 18, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49858722\n",
      "Train loss (w/o reg) on all data: 0.49838245\n",
      "Test loss (w/o reg) on all data: 0.42762974\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007053844\n",
      "Norm of the params: 2.0236533\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.42763. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [342] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40048867\n",
      "Train loss (w/o reg) on all data: 0.4000292\n",
      "Test loss (w/o reg) on all data: 0.45918143\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00063229405\n",
      "Norm of the params: 3.0314198\n",
      "                Loss: fixed  36 labels. Loss 0.45918. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [317] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61137605\n",
      "Train loss (w/o reg) on all data: 0.61132264\n",
      "Test loss (w/o reg) on all data: 0.47067073\n",
      "Train acc on all data:  0.6462264150943396\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0016924504\n",
      "Norm of the params: 1.0332544\n",
      "              Random: fixed  13 labels. Loss 0.47067. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [264] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5975119\n",
      "Train loss (w/o reg) on all data: 0.59737885\n",
      "Test loss (w/o reg) on all data: 0.58712155\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.6888888888888889\n",
      "Norm of the mean of gradients: 0.00066841504\n",
      "Norm of the params: 1.6312664\n",
      "Flipped loss: 0.58712. Accuracy: 0.689\n",
      "### Flips: 60, rs: 19, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55230314\n",
      "Train loss (w/o reg) on all data: 0.5521077\n",
      "Test loss (w/o reg) on all data: 0.5646652\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.6888888888888889\n",
      "Norm of the mean of gradients: 0.00067313295\n",
      "Norm of the params: 1.9771297\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.56467. Accuracy 0.689.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5123439\n",
      "Train loss (w/o reg) on all data: 0.5120497\n",
      "Test loss (w/o reg) on all data: 0.62998265\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.004134036\n",
      "Norm of the params: 2.4256768\n",
      "                Loss: fixed   9 labels. Loss 0.62998. Accuracy 0.711.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58835936\n",
      "Train loss (w/o reg) on all data: 0.58821374\n",
      "Test loss (w/o reg) on all data: 0.5988284\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.6888888888888889\n",
      "Norm of the mean of gradients: 0.0011769241\n",
      "Norm of the params: 1.7064817\n",
      "              Random: fixed   2 labels. Loss 0.59883. Accuracy 0.689.\n",
      "### Flips: 60, rs: 19, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.516259\n",
      "Train loss (w/o reg) on all data: 0.5160501\n",
      "Test loss (w/o reg) on all data: 0.5060991\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0041517294\n",
      "Norm of the params: 2.044183\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.50610. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47272956\n",
      "Train loss (w/o reg) on all data: 0.47239342\n",
      "Test loss (w/o reg) on all data: 0.58273613\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0010744007\n",
      "Norm of the params: 2.5928366\n",
      "                Loss: fixed  14 labels. Loss 0.58274. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5822236\n",
      "Train loss (w/o reg) on all data: 0.58207744\n",
      "Test loss (w/o reg) on all data: 0.58038706\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0024165998\n",
      "Norm of the params: 1.7095437\n",
      "              Random: fixed   5 labels. Loss 0.58039. Accuracy 0.711.\n",
      "### Flips: 60, rs: 19, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [99] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48654202\n",
      "Train loss (w/o reg) on all data: 0.4862882\n",
      "Test loss (w/o reg) on all data: 0.49335882\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0023440362\n",
      "Norm of the params: 2.2531345\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.49336. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4403487\n",
      "Train loss (w/o reg) on all data: 0.4399467\n",
      "Test loss (w/o reg) on all data: 0.56166655\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.6888888888888889\n",
      "Norm of the mean of gradients: 0.0034247392\n",
      "Norm of the params: 2.8354537\n",
      "                Loss: fixed  19 labels. Loss 0.56167. Accuracy 0.689.\n",
      "Using normal model\n",
      "LBFGS training took [308] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5878499\n",
      "Train loss (w/o reg) on all data: 0.58771557\n",
      "Test loss (w/o reg) on all data: 0.5530412\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0014397976\n",
      "Norm of the params: 1.639235\n",
      "              Random: fixed   7 labels. Loss 0.55304. Accuracy 0.733.\n",
      "### Flips: 60, rs: 19, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46752223\n",
      "Train loss (w/o reg) on all data: 0.4672532\n",
      "Test loss (w/o reg) on all data: 0.47556174\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.003820094\n",
      "Norm of the params: 2.3195443\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.47556. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [406] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38213545\n",
      "Train loss (w/o reg) on all data: 0.38162208\n",
      "Test loss (w/o reg) on all data: 0.53021646\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0014237445\n",
      "Norm of the params: 3.2043207\n",
      "                Loss: fixed  26 labels. Loss 0.53022. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [349] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.576709\n",
      "Train loss (w/o reg) on all data: 0.57656854\n",
      "Test loss (w/o reg) on all data: 0.5500047\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0016443387\n",
      "Norm of the params: 1.675993\n",
      "              Random: fixed   9 labels. Loss 0.55000. Accuracy 0.711.\n",
      "### Flips: 60, rs: 19, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [201] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44635633\n",
      "Train loss (w/o reg) on all data: 0.44608876\n",
      "Test loss (w/o reg) on all data: 0.45575052\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.011407617\n",
      "Norm of the params: 2.313339\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.45575. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35541496\n",
      "Train loss (w/o reg) on all data: 0.35482982\n",
      "Test loss (w/o reg) on all data: 0.5464216\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0013962191\n",
      "Norm of the params: 3.4209623\n",
      "                Loss: fixed  31 labels. Loss 0.54642. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [400] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5767109\n",
      "Train loss (w/o reg) on all data: 0.5765702\n",
      "Test loss (w/o reg) on all data: 0.5518502\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0020512799\n",
      "Norm of the params: 1.6772419\n",
      "              Random: fixed   9 labels. Loss 0.55185. Accuracy 0.711.\n",
      "### Flips: 60, rs: 19, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [131] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4219898\n",
      "Train loss (w/o reg) on all data: 0.42166853\n",
      "Test loss (w/o reg) on all data: 0.45596597\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0013786037\n",
      "Norm of the params: 2.534783\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.45597. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [378] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.30535325\n",
      "Train loss (w/o reg) on all data: 0.30453628\n",
      "Test loss (w/o reg) on all data: 0.6421692\n",
      "Train acc on all data:  0.8820754716981132\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.002672858\n",
      "Norm of the params: 4.0422025\n",
      "                Loss: fixed  38 labels. Loss 0.64217. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [361] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5786926\n",
      "Train loss (w/o reg) on all data: 0.57856256\n",
      "Test loss (w/o reg) on all data: 0.5550982\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.00463071\n",
      "Norm of the params: 1.6129743\n",
      "              Random: fixed  10 labels. Loss 0.55510. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6087782\n",
      "Train loss (w/o reg) on all data: 0.60874534\n",
      "Test loss (w/o reg) on all data: 0.5163891\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0020027116\n",
      "Norm of the params: 0.81026614\n",
      "Flipped loss: 0.51639. Accuracy: 0.778\n",
      "### Flips: 60, rs: 20, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [186] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5835969\n",
      "Train loss (w/o reg) on all data: 0.5835353\n",
      "Test loss (w/o reg) on all data: 0.506932\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0010546892\n",
      "Norm of the params: 1.1095204\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.50693. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53124243\n",
      "Train loss (w/o reg) on all data: 0.53114694\n",
      "Test loss (w/o reg) on all data: 0.48608568\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.006158549\n",
      "Norm of the params: 1.3819712\n",
      "                Loss: fixed   9 labels. Loss 0.48609. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [157] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61015683\n",
      "Train loss (w/o reg) on all data: 0.6101273\n",
      "Test loss (w/o reg) on all data: 0.5016455\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002501082\n",
      "Norm of the params: 0.76895094\n",
      "              Random: fixed   4 labels. Loss 0.50165. Accuracy 0.778.\n",
      "### Flips: 60, rs: 20, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [272] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55680573\n",
      "Train loss (w/o reg) on all data: 0.55672234\n",
      "Test loss (w/o reg) on all data: 0.49092475\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0029842802\n",
      "Norm of the params: 1.2914627\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.49092. Accuracy 0.733.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47151417\n",
      "Train loss (w/o reg) on all data: 0.47136053\n",
      "Test loss (w/o reg) on all data: 0.45643628\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020762368\n",
      "Norm of the params: 1.7529013\n",
      "                Loss: fixed  17 labels. Loss 0.45644. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [132] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6053588\n",
      "Train loss (w/o reg) on all data: 0.60532355\n",
      "Test loss (w/o reg) on all data: 0.49225444\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0048646242\n",
      "Norm of the params: 0.8396218\n",
      "              Random: fixed   5 labels. Loss 0.49225. Accuracy 0.778.\n",
      "### Flips: 60, rs: 20, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51697195\n",
      "Train loss (w/o reg) on all data: 0.516874\n",
      "Test loss (w/o reg) on all data: 0.48568144\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.000734972\n",
      "Norm of the params: 1.3993165\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.48568. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [336] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4095682\n",
      "Train loss (w/o reg) on all data: 0.40933356\n",
      "Test loss (w/o reg) on all data: 0.4558969\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.003011227\n",
      "Norm of the params: 2.1662745\n",
      "                Loss: fixed  24 labels. Loss 0.45590. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [177] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57458234\n",
      "Train loss (w/o reg) on all data: 0.57453144\n",
      "Test loss (w/o reg) on all data: 0.47891596\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0030711305\n",
      "Norm of the params: 1.0091577\n",
      "              Random: fixed  10 labels. Loss 0.47892. Accuracy 0.800.\n",
      "### Flips: 60, rs: 20, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [258] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48262775\n",
      "Train loss (w/o reg) on all data: 0.48251557\n",
      "Test loss (w/o reg) on all data: 0.4668563\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00228812\n",
      "Norm of the params: 1.4979217\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.46686. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3643943\n",
      "Train loss (w/o reg) on all data: 0.36406246\n",
      "Test loss (w/o reg) on all data: 0.46545562\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0007641671\n",
      "Norm of the params: 2.5761867\n",
      "                Loss: fixed  30 labels. Loss 0.46546. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5689651\n",
      "Train loss (w/o reg) on all data: 0.5689042\n",
      "Test loss (w/o reg) on all data: 0.47565562\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0021173884\n",
      "Norm of the params: 1.1030854\n",
      "              Random: fixed  13 labels. Loss 0.47566. Accuracy 0.778.\n",
      "### Flips: 60, rs: 20, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [331] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44312567\n",
      "Train loss (w/o reg) on all data: 0.4429592\n",
      "Test loss (w/o reg) on all data: 0.4530788\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0009724136\n",
      "Norm of the params: 1.8247585\n",
      "     Influence (LOO): fixed  29 labels. Loss 0.45308. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3077091\n",
      "Train loss (w/o reg) on all data: 0.30728465\n",
      "Test loss (w/o reg) on all data: 0.5805019\n",
      "Train acc on all data:  0.8443396226415094\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0006418178\n",
      "Norm of the params: 2.9135325\n",
      "                Loss: fixed  38 labels. Loss 0.58050. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [294] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56285596\n",
      "Train loss (w/o reg) on all data: 0.5627956\n",
      "Test loss (w/o reg) on all data: 0.46818972\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013253205\n",
      "Norm of the params: 1.0987375\n",
      "              Random: fixed  17 labels. Loss 0.46819. Accuracy 0.822.\n",
      "### Flips: 60, rs: 20, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40365303\n",
      "Train loss (w/o reg) on all data: 0.40340298\n",
      "Test loss (w/o reg) on all data: 0.4347457\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0005138033\n",
      "Norm of the params: 2.2362835\n",
      "     Influence (LOO): fixed  33 labels. Loss 0.43475. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [305] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.30477485\n",
      "Train loss (w/o reg) on all data: 0.3043541\n",
      "Test loss (w/o reg) on all data: 0.5835846\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010385608\n",
      "Norm of the params: 2.9008548\n",
      "                Loss: fixed  40 labels. Loss 0.58358. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.562873\n",
      "Train loss (w/o reg) on all data: 0.5628155\n",
      "Test loss (w/o reg) on all data: 0.45787773\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0030503257\n",
      "Norm of the params: 1.0727807\n",
      "              Random: fixed  19 labels. Loss 0.45788. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61719817\n",
      "Train loss (w/o reg) on all data: 0.617131\n",
      "Test loss (w/o reg) on all data: 0.49297172\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0014962544\n",
      "Norm of the params: 1.1591656\n",
      "Flipped loss: 0.49297. Accuracy: 0.844\n",
      "### Flips: 60, rs: 21, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54126\n",
      "Train loss (w/o reg) on all data: 0.54108304\n",
      "Test loss (w/o reg) on all data: 0.4330972\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0050067143\n",
      "Norm of the params: 1.8811955\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.43310. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [169] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5383069\n",
      "Train loss (w/o reg) on all data: 0.5381446\n",
      "Test loss (w/o reg) on all data: 0.43167877\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0024998323\n",
      "Norm of the params: 1.8017235\n",
      "                Loss: fixed   9 labels. Loss 0.43168. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6090384\n",
      "Train loss (w/o reg) on all data: 0.6089578\n",
      "Test loss (w/o reg) on all data: 0.5097151\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0066491705\n",
      "Norm of the params: 1.2693408\n",
      "              Random: fixed   3 labels. Loss 0.50972. Accuracy 0.800.\n",
      "### Flips: 60, rs: 21, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [268] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5160587\n",
      "Train loss (w/o reg) on all data: 0.51581794\n",
      "Test loss (w/o reg) on all data: 0.4562826\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014394156\n",
      "Norm of the params: 2.1942844\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.45628. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [161] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4416243\n",
      "Train loss (w/o reg) on all data: 0.44126412\n",
      "Test loss (w/o reg) on all data: 0.42838407\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0025831624\n",
      "Norm of the params: 2.6840003\n",
      "                Loss: fixed  19 labels. Loss 0.42838. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5948315\n",
      "Train loss (w/o reg) on all data: 0.5947464\n",
      "Test loss (w/o reg) on all data: 0.5117963\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0061893556\n",
      "Norm of the params: 1.304877\n",
      "              Random: fixed   6 labels. Loss 0.51180. Accuracy 0.800.\n",
      "### Flips: 60, rs: 21, checks: 30\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5074076\n",
      "Train loss (w/o reg) on all data: 0.50714034\n",
      "Test loss (w/o reg) on all data: 0.46545792\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00096208556\n",
      "Norm of the params: 2.3119442\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.46546. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [171] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38521516\n",
      "Train loss (w/o reg) on all data: 0.38466442\n",
      "Test loss (w/o reg) on all data: 0.46632364\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004334995\n",
      "Norm of the params: 3.318858\n",
      "                Loss: fixed  24 labels. Loss 0.46632. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [210] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5990677\n",
      "Train loss (w/o reg) on all data: 0.59898853\n",
      "Test loss (w/o reg) on all data: 0.5016793\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0023636525\n",
      "Norm of the params: 1.2581242\n",
      "              Random: fixed  10 labels. Loss 0.50168. Accuracy 0.800.\n",
      "### Flips: 60, rs: 21, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [115] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5032008\n",
      "Train loss (w/o reg) on all data: 0.5030159\n",
      "Test loss (w/o reg) on all data: 0.4120905\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0037126606\n",
      "Norm of the params: 1.9233838\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.41209. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32952565\n",
      "Train loss (w/o reg) on all data: 0.32863238\n",
      "Test loss (w/o reg) on all data: 0.5000152\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0041675926\n",
      "Norm of the params: 4.226741\n",
      "                Loss: fixed  30 labels. Loss 0.50002. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [255] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5944128\n",
      "Train loss (w/o reg) on all data: 0.59432185\n",
      "Test loss (w/o reg) on all data: 0.49503517\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0093786875\n",
      "Norm of the params: 1.3486307\n",
      "              Random: fixed  13 labels. Loss 0.49504. Accuracy 0.822.\n",
      "### Flips: 60, rs: 21, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4561504\n",
      "Train loss (w/o reg) on all data: 0.45591852\n",
      "Test loss (w/o reg) on all data: 0.39782253\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011194207\n",
      "Norm of the params: 2.153518\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.39782. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.29482695\n",
      "Train loss (w/o reg) on all data: 0.29382938\n",
      "Test loss (w/o reg) on all data: 0.5380349\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0038407205\n",
      "Norm of the params: 4.466738\n",
      "                Loss: fixed  35 labels. Loss 0.53803. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [480] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5842975\n",
      "Train loss (w/o reg) on all data: 0.5842095\n",
      "Test loss (w/o reg) on all data: 0.4648329\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0019143707\n",
      "Norm of the params: 1.3264674\n",
      "              Random: fixed  17 labels. Loss 0.46483. Accuracy 0.822.\n",
      "### Flips: 60, rs: 21, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44429868\n",
      "Train loss (w/o reg) on all data: 0.44402984\n",
      "Test loss (w/o reg) on all data: 0.403525\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.003960366\n",
      "Norm of the params: 2.31881\n",
      "     Influence (LOO): fixed  31 labels. Loss 0.40352. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.2909468\n",
      "Train loss (w/o reg) on all data: 0.2899577\n",
      "Test loss (w/o reg) on all data: 0.55020463\n",
      "Train acc on all data:  0.8773584905660378\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.009709715\n",
      "Norm of the params: 4.4477496\n",
      "                Loss: fixed  39 labels. Loss 0.55020. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [340] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5821581\n",
      "Train loss (w/o reg) on all data: 0.5820712\n",
      "Test loss (w/o reg) on all data: 0.47494578\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0020856995\n",
      "Norm of the params: 1.3181623\n",
      "              Random: fixed  19 labels. Loss 0.47495. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6643352\n",
      "Train loss (w/o reg) on all data: 0.66432464\n",
      "Test loss (w/o reg) on all data: 0.6088074\n",
      "Train acc on all data:  0.6084905660377359\n",
      "Test acc on all data:   0.6888888888888889\n",
      "Norm of the mean of gradients: 0.002791562\n",
      "Norm of the params: 0.45964056\n",
      "Flipped loss: 0.60881. Accuracy: 0.689\n",
      "### Flips: 60, rs: 22, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [172] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.632397\n",
      "Train loss (w/o reg) on all data: 0.6323677\n",
      "Test loss (w/o reg) on all data: 0.5622687\n",
      "Train acc on all data:  0.6320754716981132\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.002246197\n",
      "Norm of the params: 0.76570386\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.56227. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [332] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60596156\n",
      "Train loss (w/o reg) on all data: 0.60591286\n",
      "Test loss (w/o reg) on all data: 0.58823025\n",
      "Train acc on all data:  0.6650943396226415\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0013419183\n",
      "Norm of the params: 0.9866369\n",
      "                Loss: fixed   9 labels. Loss 0.58823. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [201] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6587467\n",
      "Train loss (w/o reg) on all data: 0.65872884\n",
      "Test loss (w/o reg) on all data: 0.59913474\n",
      "Train acc on all data:  0.5801886792452831\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.005764349\n",
      "Norm of the params: 0.59806424\n",
      "              Random: fixed   3 labels. Loss 0.59913. Accuracy 0.756.\n",
      "### Flips: 60, rs: 22, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5902574\n",
      "Train loss (w/o reg) on all data: 0.59016216\n",
      "Test loss (w/o reg) on all data: 0.54168\n",
      "Train acc on all data:  0.6509433962264151\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0006137438\n",
      "Norm of the params: 1.3804183\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.54168. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55274445\n",
      "Train loss (w/o reg) on all data: 0.5526459\n",
      "Test loss (w/o reg) on all data: 0.57298946\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0012638037\n",
      "Norm of the params: 1.4035659\n",
      "                Loss: fixed  17 labels. Loss 0.57299. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [203] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.65228426\n",
      "Train loss (w/o reg) on all data: 0.65226316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss (w/o reg) on all data: 0.579503\n",
      "Train acc on all data:  0.6132075471698113\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0023761536\n",
      "Norm of the params: 0.6492331\n",
      "              Random: fixed   7 labels. Loss 0.57950. Accuracy 0.778.\n",
      "### Flips: 60, rs: 22, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [267] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58183765\n",
      "Train loss (w/o reg) on all data: 0.58174855\n",
      "Test loss (w/o reg) on all data: 0.5201423\n",
      "Train acc on all data:  0.660377358490566\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0016009985\n",
      "Norm of the params: 1.3350147\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.52014. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51215786\n",
      "Train loss (w/o reg) on all data: 0.51202124\n",
      "Test loss (w/o reg) on all data: 0.5745193\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0016041957\n",
      "Norm of the params: 1.6529121\n",
      "                Loss: fixed  24 labels. Loss 0.57452. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6431068\n",
      "Train loss (w/o reg) on all data: 0.64308035\n",
      "Test loss (w/o reg) on all data: 0.56802136\n",
      "Train acc on all data:  0.6226415094339622\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0077285296\n",
      "Norm of the params: 0.7277835\n",
      "              Random: fixed  10 labels. Loss 0.56802. Accuracy 0.756.\n",
      "### Flips: 60, rs: 22, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56793267\n",
      "Train loss (w/o reg) on all data: 0.5678543\n",
      "Test loss (w/o reg) on all data: 0.4965917\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0031163269\n",
      "Norm of the params: 1.252152\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.49659. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [310] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46890047\n",
      "Train loss (w/o reg) on all data: 0.4687203\n",
      "Test loss (w/o reg) on all data: 0.56068325\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.00076090766\n",
      "Norm of the params: 1.8982687\n",
      "                Loss: fixed  30 labels. Loss 0.56068. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [358] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.64309335\n",
      "Train loss (w/o reg) on all data: 0.64306724\n",
      "Test loss (w/o reg) on all data: 0.5691652\n",
      "Train acc on all data:  0.6273584905660378\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00412094\n",
      "Norm of the params: 0.7228225\n",
      "              Random: fixed  10 labels. Loss 0.56917. Accuracy 0.756.\n",
      "### Flips: 60, rs: 22, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [159] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5556805\n",
      "Train loss (w/o reg) on all data: 0.5555969\n",
      "Test loss (w/o reg) on all data: 0.49462315\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00483316\n",
      "Norm of the params: 1.2933724\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.49462. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [201] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4463178\n",
      "Train loss (w/o reg) on all data: 0.44611138\n",
      "Test loss (w/o reg) on all data: 0.5281255\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001215453\n",
      "Norm of the params: 2.031821\n",
      "                Loss: fixed  33 labels. Loss 0.52813. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [154] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6347115\n",
      "Train loss (w/o reg) on all data: 0.6346817\n",
      "Test loss (w/o reg) on all data: 0.5702888\n",
      "Train acc on all data:  0.6415094339622641\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.009756288\n",
      "Norm of the params: 0.77184826\n",
      "              Random: fixed  12 labels. Loss 0.57029. Accuracy 0.733.\n",
      "### Flips: 60, rs: 22, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [179] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54725116\n",
      "Train loss (w/o reg) on all data: 0.54717094\n",
      "Test loss (w/o reg) on all data: 0.4573732\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013305227\n",
      "Norm of the params: 1.2665166\n",
      "     Influence (LOO): fixed  29 labels. Loss 0.45737. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4202844\n",
      "Train loss (w/o reg) on all data: 0.420097\n",
      "Test loss (w/o reg) on all data: 0.4929972\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010019912\n",
      "Norm of the params: 1.9359759\n",
      "                Loss: fixed  37 labels. Loss 0.49300. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [125] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6260043\n",
      "Train loss (w/o reg) on all data: 0.6259579\n",
      "Test loss (w/o reg) on all data: 0.5479137\n",
      "Train acc on all data:  0.6509433962264151\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.004926885\n",
      "Norm of the params: 0.96318454\n",
      "              Random: fixed  16 labels. Loss 0.54791. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6642921\n",
      "Train loss (w/o reg) on all data: 0.6642809\n",
      "Test loss (w/o reg) on all data: 0.59510845\n",
      "Train acc on all data:  0.6226415094339622\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0040981206\n",
      "Norm of the params: 0.47390383\n",
      "Flipped loss: 0.59511. Accuracy: 0.778\n",
      "### Flips: 60, rs: 23, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6265243\n",
      "Train loss (w/o reg) on all data: 0.6264719\n",
      "Test loss (w/o reg) on all data: 0.58287966\n",
      "Train acc on all data:  0.6415094339622641\n",
      "Test acc on all data:   0.6666666666666666\n",
      "Norm of the mean of gradients: 0.005380643\n",
      "Norm of the params: 1.0242324\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.58288. Accuracy 0.667.\n",
      "Using normal model\n",
      "LBFGS training took [206] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6153974\n",
      "Train loss (w/o reg) on all data: 0.61536926\n",
      "Test loss (w/o reg) on all data: 0.529316\n",
      "Train acc on all data:  0.6509433962264151\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0048252563\n",
      "Norm of the params: 0.7501183\n",
      "                Loss: fixed   8 labels. Loss 0.52932. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6602522\n",
      "Train loss (w/o reg) on all data: 0.66023904\n",
      "Test loss (w/o reg) on all data: 0.58539426\n",
      "Train acc on all data:  0.6226415094339622\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0038905137\n",
      "Norm of the params: 0.51310337\n",
      "              Random: fixed   1 labels. Loss 0.58539. Accuracy 0.778.\n",
      "### Flips: 60, rs: 23, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [138] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.62433136\n",
      "Train loss (w/o reg) on all data: 0.62427175\n",
      "Test loss (w/o reg) on all data: 0.55704474\n",
      "Train acc on all data:  0.6367924528301887\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0015074636\n",
      "Norm of the params: 1.0918553\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.55704. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [240] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5708672\n",
      "Train loss (w/o reg) on all data: 0.5708188\n",
      "Test loss (w/o reg) on all data: 0.47779548\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001024263\n",
      "Norm of the params: 0.9837432\n",
      "                Loss: fixed  15 labels. Loss 0.47780. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [155] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.65199697\n",
      "Train loss (w/o reg) on all data: 0.6519809\n",
      "Test loss (w/o reg) on all data: 0.5529603\n",
      "Train acc on all data:  0.6226415094339622\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.004894729\n",
      "Norm of the params: 0.56778926\n",
      "              Random: fixed   4 labels. Loss 0.55296. Accuracy 0.756.\n",
      "### Flips: 60, rs: 23, checks: 30\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [337] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6178305\n",
      "Train loss (w/o reg) on all data: 0.6177804\n",
      "Test loss (w/o reg) on all data: 0.55524355\n",
      "Train acc on all data:  0.6509433962264151\n",
      "Test acc on all data:   0.6666666666666666\n",
      "Norm of the mean of gradients: 0.00058341405\n",
      "Norm of the params: 1.0014102\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.55524. Accuracy 0.667.\n",
      "Using normal model\n",
      "LBFGS training took [317] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5283292\n",
      "Train loss (w/o reg) on all data: 0.52826333\n",
      "Test loss (w/o reg) on all data: 0.45188224\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0059586456\n",
      "Norm of the params: 1.147962\n",
      "                Loss: fixed  21 labels. Loss 0.45188. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [159] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6487052\n",
      "Train loss (w/o reg) on all data: 0.64868575\n",
      "Test loss (w/o reg) on all data: 0.5465473\n",
      "Train acc on all data:  0.6320754716981132\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0010468862\n",
      "Norm of the params: 0.62324095\n",
      "              Random: fixed   6 labels. Loss 0.54655. Accuracy 0.756.\n",
      "### Flips: 60, rs: 23, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5989066\n",
      "Train loss (w/o reg) on all data: 0.5988562\n",
      "Test loss (w/o reg) on all data: 0.52673316\n",
      "Train acc on all data:  0.6650943396226415\n",
      "Test acc on all data:   0.6666666666666666\n",
      "Norm of the mean of gradients: 0.0054590087\n",
      "Norm of the params: 1.0038368\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.52673. Accuracy 0.667.\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49543026\n",
      "Train loss (w/o reg) on all data: 0.4953451\n",
      "Test loss (w/o reg) on all data: 0.43380436\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0021381897\n",
      "Norm of the params: 1.3052541\n",
      "                Loss: fixed  27 labels. Loss 0.43380. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.64435863\n",
      "Train loss (w/o reg) on all data: 0.64433587\n",
      "Test loss (w/o reg) on all data: 0.53418726\n",
      "Train acc on all data:  0.6367924528301887\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.008289814\n",
      "Norm of the params: 0.67482305\n",
      "              Random: fixed   8 labels. Loss 0.53419. Accuracy 0.733.\n",
      "### Flips: 60, rs: 23, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [321] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5740077\n",
      "Train loss (w/o reg) on all data: 0.573952\n",
      "Test loss (w/o reg) on all data: 0.49509743\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0011200034\n",
      "Norm of the params: 1.0549825\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.49510. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [325] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46401381\n",
      "Train loss (w/o reg) on all data: 0.46387842\n",
      "Test loss (w/o reg) on all data: 0.42254335\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00075679366\n",
      "Norm of the params: 1.6455944\n",
      "                Loss: fixed  32 labels. Loss 0.42254. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.63612276\n",
      "Train loss (w/o reg) on all data: 0.6360831\n",
      "Test loss (w/o reg) on all data: 0.5222669\n",
      "Train acc on all data:  0.6462264150943396\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0005171582\n",
      "Norm of the params: 0.8901354\n",
      "              Random: fixed  11 labels. Loss 0.52227. Accuracy 0.733.\n",
      "### Flips: 60, rs: 23, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [218] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56968087\n",
      "Train loss (w/o reg) on all data: 0.56962025\n",
      "Test loss (w/o reg) on all data: 0.49053425\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.006281375\n",
      "Norm of the params: 1.101195\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.49053. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42168522\n",
      "Train loss (w/o reg) on all data: 0.42153382\n",
      "Test loss (w/o reg) on all data: 0.4233618\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0022234805\n",
      "Norm of the params: 1.7400699\n",
      "                Loss: fixed  39 labels. Loss 0.42336. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [288] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.63087445\n",
      "Train loss (w/o reg) on all data: 0.63082814\n",
      "Test loss (w/o reg) on all data: 0.51504844\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0072317296\n",
      "Norm of the params: 0.9627114\n",
      "              Random: fixed  14 labels. Loss 0.51505. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60792315\n",
      "Train loss (w/o reg) on all data: 0.6078714\n",
      "Test loss (w/o reg) on all data: 0.5229822\n",
      "Train acc on all data:  0.6556603773584906\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00094915234\n",
      "Norm of the params: 1.0170791\n",
      "Flipped loss: 0.52298. Accuracy: 0.778\n",
      "### Flips: 60, rs: 24, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [114] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57000875\n",
      "Train loss (w/o reg) on all data: 0.56994635\n",
      "Test loss (w/o reg) on all data: 0.50368214\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0021476953\n",
      "Norm of the params: 1.1172419\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.50368. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [235] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55668193\n",
      "Train loss (w/o reg) on all data: 0.5565913\n",
      "Test loss (w/o reg) on all data: 0.50050557\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00075645704\n",
      "Norm of the params: 1.3466368\n",
      "                Loss: fixed   7 labels. Loss 0.50051. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [198] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6053739\n",
      "Train loss (w/o reg) on all data: 0.6053081\n",
      "Test loss (w/o reg) on all data: 0.5242138\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0015105087\n",
      "Norm of the params: 1.1472458\n",
      "              Random: fixed   3 labels. Loss 0.52421. Accuracy 0.756.\n",
      "### Flips: 60, rs: 24, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [163] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5436589\n",
      "Train loss (w/o reg) on all data: 0.543591\n",
      "Test loss (w/o reg) on all data: 0.48668814\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008062243\n",
      "Norm of the params: 1.165092\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.48669. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [259] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51154375\n",
      "Train loss (w/o reg) on all data: 0.51142293\n",
      "Test loss (w/o reg) on all data: 0.473787\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0019940697\n",
      "Norm of the params: 1.5544419\n",
      "                Loss: fixed  13 labels. Loss 0.47379. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [154] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60145885\n",
      "Train loss (w/o reg) on all data: 0.6013923\n",
      "Test loss (w/o reg) on all data: 0.525561\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.010170247\n",
      "Norm of the params: 1.1536633\n",
      "              Random: fixed   5 labels. Loss 0.52556. Accuracy 0.778.\n",
      "### Flips: 60, rs: 24, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52152604\n",
      "Train loss (w/o reg) on all data: 0.5214591\n",
      "Test loss (w/o reg) on all data: 0.46454874\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0035801386\n",
      "Norm of the params: 1.1568748\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.46455. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44802135\n",
      "Train loss (w/o reg) on all data: 0.44787738\n",
      "Test loss (w/o reg) on all data: 0.43709728\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0029074743\n",
      "Norm of the params: 1.6969062\n",
      "                Loss: fixed  22 labels. Loss 0.43710. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5960182\n",
      "Train loss (w/o reg) on all data: 0.59595627\n",
      "Test loss (w/o reg) on all data: 0.5303819\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004057357\n",
      "Norm of the params: 1.1126649\n",
      "              Random: fixed   7 labels. Loss 0.53038. Accuracy 0.800.\n",
      "### Flips: 60, rs: 24, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [131] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48666915\n",
      "Train loss (w/o reg) on all data: 0.48656443\n",
      "Test loss (w/o reg) on all data: 0.42671326\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0013805026\n",
      "Norm of the params: 1.4473087\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.42671. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [244] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40845278\n",
      "Train loss (w/o reg) on all data: 0.40827277\n",
      "Test loss (w/o reg) on all data: 0.42706266\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0010444024\n",
      "Norm of the params: 1.8974432\n",
      "                Loss: fixed  28 labels. Loss 0.42706. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5960141\n",
      "Train loss (w/o reg) on all data: 0.5959532\n",
      "Test loss (w/o reg) on all data: 0.5299667\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00077487796\n",
      "Norm of the params: 1.1032752\n",
      "              Random: fixed   7 labels. Loss 0.52997. Accuracy 0.800.\n",
      "### Flips: 60, rs: 24, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [190] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4841341\n",
      "Train loss (w/o reg) on all data: 0.48402807\n",
      "Test loss (w/o reg) on all data: 0.43263364\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0050610537\n",
      "Norm of the params: 1.4562255\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.43263. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [306] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39164725\n",
      "Train loss (w/o reg) on all data: 0.39143264\n",
      "Test loss (w/o reg) on all data: 0.4416292\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004019412\n",
      "Norm of the params: 2.0717204\n",
      "                Loss: fixed  31 labels. Loss 0.44163. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [315] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58700913\n",
      "Train loss (w/o reg) on all data: 0.5869468\n",
      "Test loss (w/o reg) on all data: 0.48710012\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00062931003\n",
      "Norm of the params: 1.1168107\n",
      "              Random: fixed  12 labels. Loss 0.48710. Accuracy 0.800.\n",
      "### Flips: 60, rs: 24, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [195] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45544624\n",
      "Train loss (w/o reg) on all data: 0.45532948\n",
      "Test loss (w/o reg) on all data: 0.42411292\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0008005262\n",
      "Norm of the params: 1.5282592\n",
      "     Influence (LOO): fixed  28 labels. Loss 0.42411. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3562895\n",
      "Train loss (w/o reg) on all data: 0.35602966\n",
      "Test loss (w/o reg) on all data: 0.48834\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010193089\n",
      "Norm of the params: 2.2796524\n",
      "                Loss: fixed  37 labels. Loss 0.48834. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [420] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57626796\n",
      "Train loss (w/o reg) on all data: 0.5762044\n",
      "Test loss (w/o reg) on all data: 0.45476604\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0019714637\n",
      "Norm of the params: 1.1274879\n",
      "              Random: fixed  14 labels. Loss 0.45477. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [138] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6338302\n",
      "Train loss (w/o reg) on all data: 0.63379204\n",
      "Test loss (w/o reg) on all data: 0.5464264\n",
      "Train acc on all data:  0.6509433962264151\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0031147578\n",
      "Norm of the params: 0.873549\n",
      "Flipped loss: 0.54643. Accuracy: 0.756\n",
      "### Flips: 60, rs: 25, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [202] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58074725\n",
      "Train loss (w/o reg) on all data: 0.58062077\n",
      "Test loss (w/o reg) on all data: 0.55474913\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0025332905\n",
      "Norm of the params: 1.5903406\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.55475. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [106] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5556296\n",
      "Train loss (w/o reg) on all data: 0.55546844\n",
      "Test loss (w/o reg) on all data: 0.5263328\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.005604326\n",
      "Norm of the params: 1.795306\n",
      "                Loss: fixed  10 labels. Loss 0.52633. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [144] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61129475\n",
      "Train loss (w/o reg) on all data: 0.61121964\n",
      "Test loss (w/o reg) on all data: 0.53550005\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.004480336\n",
      "Norm of the params: 1.2256248\n",
      "              Random: fixed   5 labels. Loss 0.53550. Accuracy 0.778.\n",
      "### Flips: 60, rs: 25, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [102] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5647028\n",
      "Train loss (w/o reg) on all data: 0.5645816\n",
      "Test loss (w/o reg) on all data: 0.50337535\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0017553662\n",
      "Norm of the params: 1.5570446\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.50338. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [359] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49564853\n",
      "Train loss (w/o reg) on all data: 0.49535498\n",
      "Test loss (w/o reg) on all data: 0.5008687\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0016258908\n",
      "Norm of the params: 2.422967\n",
      "                Loss: fixed  17 labels. Loss 0.50087. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [332] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60284007\n",
      "Train loss (w/o reg) on all data: 0.6027623\n",
      "Test loss (w/o reg) on all data: 0.5354123\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0069921487\n",
      "Norm of the params: 1.2471344\n",
      "              Random: fixed   8 labels. Loss 0.53541. Accuracy 0.778.\n",
      "### Flips: 60, rs: 25, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5624049\n",
      "Train loss (w/o reg) on all data: 0.5622589\n",
      "Test loss (w/o reg) on all data: 0.5174374\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.004679051\n",
      "Norm of the params: 1.7087071\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.51744. Accuracy 0.778.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [279] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44642666\n",
      "Train loss (w/o reg) on all data: 0.44604826\n",
      "Test loss (w/o reg) on all data: 0.50670177\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.005575528\n",
      "Norm of the params: 2.7509615\n",
      "                Loss: fixed  23 labels. Loss 0.50670. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [303] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6058335\n",
      "Train loss (w/o reg) on all data: 0.60575926\n",
      "Test loss (w/o reg) on all data: 0.5495295\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0018118116\n",
      "Norm of the params: 1.2180654\n",
      "              Random: fixed   9 labels. Loss 0.54953. Accuracy 0.756.\n",
      "### Flips: 60, rs: 25, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [306] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52505976\n",
      "Train loss (w/o reg) on all data: 0.52484745\n",
      "Test loss (w/o reg) on all data: 0.48419037\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0018153006\n",
      "Norm of the params: 2.0606885\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.48419. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [372] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38196668\n",
      "Train loss (w/o reg) on all data: 0.38140574\n",
      "Test loss (w/o reg) on all data: 0.5329259\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00069463253\n",
      "Norm of the params: 3.349418\n",
      "                Loss: fixed  32 labels. Loss 0.53293. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [332] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5956236\n",
      "Train loss (w/o reg) on all data: 0.59554857\n",
      "Test loss (w/o reg) on all data: 0.5332387\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0032892628\n",
      "Norm of the params: 1.2249103\n",
      "              Random: fixed  12 labels. Loss 0.53324. Accuracy 0.756.\n",
      "### Flips: 60, rs: 25, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [263] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5200968\n",
      "Train loss (w/o reg) on all data: 0.51992697\n",
      "Test loss (w/o reg) on all data: 0.46953446\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0058017415\n",
      "Norm of the params: 1.8430263\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.46953. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [355] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3472647\n",
      "Train loss (w/o reg) on all data: 0.34664503\n",
      "Test loss (w/o reg) on all data: 0.5647589\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00039181917\n",
      "Norm of the params: 3.520491\n",
      "                Loss: fixed  38 labels. Loss 0.56476. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [335] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5868263\n",
      "Train loss (w/o reg) on all data: 0.58674186\n",
      "Test loss (w/o reg) on all data: 0.4938452\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0023704756\n",
      "Norm of the params: 1.2997994\n",
      "              Random: fixed  16 labels. Loss 0.49385. Accuracy 0.778.\n",
      "### Flips: 60, rs: 25, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [277] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.490163\n",
      "Train loss (w/o reg) on all data: 0.48996973\n",
      "Test loss (w/o reg) on all data: 0.4291119\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00075028464\n",
      "Norm of the params: 1.9659791\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.42911. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33014542\n",
      "Train loss (w/o reg) on all data: 0.329472\n",
      "Test loss (w/o reg) on all data: 0.5682463\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011967728\n",
      "Norm of the params: 3.6698914\n",
      "                Loss: fixed  40 labels. Loss 0.56825. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [330] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5620939\n",
      "Train loss (w/o reg) on all data: 0.56199044\n",
      "Test loss (w/o reg) on all data: 0.52142227\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0074687423\n",
      "Norm of the params: 1.4385697\n",
      "              Random: fixed  20 labels. Loss 0.52142. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [344] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6240062\n",
      "Train loss (w/o reg) on all data: 0.6239316\n",
      "Test loss (w/o reg) on all data: 0.5109788\n",
      "Train acc on all data:  0.6509433962264151\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0074461526\n",
      "Norm of the params: 1.2215015\n",
      "Flipped loss: 0.51098. Accuracy: 0.778\n",
      "### Flips: 60, rs: 26, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [193] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5988118\n",
      "Train loss (w/o reg) on all data: 0.5987056\n",
      "Test loss (w/o reg) on all data: 0.47223786\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006833507\n",
      "Norm of the params: 1.4573834\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.47224. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [88] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5607631\n",
      "Train loss (w/o reg) on all data: 0.5605871\n",
      "Test loss (w/o reg) on all data: 0.46540862\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0073376\n",
      "Norm of the params: 1.8761065\n",
      "                Loss: fixed   8 labels. Loss 0.46541. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [214] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6181233\n",
      "Train loss (w/o reg) on all data: 0.6180474\n",
      "Test loss (w/o reg) on all data: 0.49676302\n",
      "Train acc on all data:  0.660377358490566\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001753613\n",
      "Norm of the params: 1.231711\n",
      "              Random: fixed   2 labels. Loss 0.49676. Accuracy 0.778.\n",
      "### Flips: 60, rs: 26, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [284] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5828457\n",
      "Train loss (w/o reg) on all data: 0.58271646\n",
      "Test loss (w/o reg) on all data: 0.44941652\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0017130616\n",
      "Norm of the params: 1.6076798\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.44942. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [309] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52314234\n",
      "Train loss (w/o reg) on all data: 0.52289265\n",
      "Test loss (w/o reg) on all data: 0.4663433\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011624832\n",
      "Norm of the params: 2.2347283\n",
      "                Loss: fixed  13 labels. Loss 0.46634. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61000925\n",
      "Train loss (w/o reg) on all data: 0.60995454\n",
      "Test loss (w/o reg) on all data: 0.46566468\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0010354664\n",
      "Norm of the params: 1.0460547\n",
      "              Random: fixed   7 labels. Loss 0.46566. Accuracy 0.822.\n",
      "### Flips: 60, rs: 26, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5795317\n",
      "Train loss (w/o reg) on all data: 0.57940644\n",
      "Test loss (w/o reg) on all data: 0.45476845\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012450558\n",
      "Norm of the params: 1.5828367\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.45477. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [251] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50246114\n",
      "Train loss (w/o reg) on all data: 0.5021865\n",
      "Test loss (w/o reg) on all data: 0.45625386\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0012707117\n",
      "Norm of the params: 2.3438315\n",
      "                Loss: fixed  16 labels. Loss 0.45625. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [150] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6100052\n",
      "Train loss (w/o reg) on all data: 0.60995036\n",
      "Test loss (w/o reg) on all data: 0.465346\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014008905\n",
      "Norm of the params: 1.0471535\n",
      "              Random: fixed   7 labels. Loss 0.46535. Accuracy 0.822.\n",
      "### Flips: 60, rs: 26, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [286] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54667467\n",
      "Train loss (w/o reg) on all data: 0.54650885\n",
      "Test loss (w/o reg) on all data: 0.42687547\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00831473\n",
      "Norm of the params: 1.8212624\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.42688. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [370] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45527866\n",
      "Train loss (w/o reg) on all data: 0.45493987\n",
      "Test loss (w/o reg) on all data: 0.44661328\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002339725\n",
      "Norm of the params: 2.6030343\n",
      "                Loss: fixed  23 labels. Loss 0.44661. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [370] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5963186\n",
      "Train loss (w/o reg) on all data: 0.5962511\n",
      "Test loss (w/o reg) on all data: 0.45721638\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002610411\n",
      "Norm of the params: 1.1622595\n",
      "              Random: fixed  11 labels. Loss 0.45722. Accuracy 0.800.\n",
      "### Flips: 60, rs: 26, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [166] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53177476\n",
      "Train loss (w/o reg) on all data: 0.53162324\n",
      "Test loss (w/o reg) on all data: 0.39933172\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00080909295\n",
      "Norm of the params: 1.7409301\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.39933. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41629934\n",
      "Train loss (w/o reg) on all data: 0.41589162\n",
      "Test loss (w/o reg) on all data: 0.42838165\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0044110054\n",
      "Norm of the params: 2.8555777\n",
      "                Loss: fixed  29 labels. Loss 0.42838. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [192] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5873279\n",
      "Train loss (w/o reg) on all data: 0.58725554\n",
      "Test loss (w/o reg) on all data: 0.46079713\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012712231\n",
      "Norm of the params: 1.2031074\n",
      "              Random: fixed  14 labels. Loss 0.46080. Accuracy 0.800.\n",
      "### Flips: 60, rs: 26, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5113658\n",
      "Train loss (w/o reg) on all data: 0.511185\n",
      "Test loss (w/o reg) on all data: 0.36952022\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0004968167\n",
      "Norm of the params: 1.9013364\n",
      "     Influence (LOO): fixed  28 labels. Loss 0.36952. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [167] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41013718\n",
      "Train loss (w/o reg) on all data: 0.40972131\n",
      "Test loss (w/o reg) on all data: 0.4179561\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00223792\n",
      "Norm of the params: 2.883999\n",
      "                Loss: fixed  34 labels. Loss 0.41796. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [288] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5887609\n",
      "Train loss (w/o reg) on all data: 0.58869267\n",
      "Test loss (w/o reg) on all data: 0.46618745\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011509946\n",
      "Norm of the params: 1.1684897\n",
      "              Random: fixed  15 labels. Loss 0.46619. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [175] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61374545\n",
      "Train loss (w/o reg) on all data: 0.6136711\n",
      "Test loss (w/o reg) on all data: 0.56361645\n",
      "Train acc on all data:  0.6462264150943396\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0039188694\n",
      "Norm of the params: 1.2193425\n",
      "Flipped loss: 0.56362. Accuracy: 0.711\n",
      "### Flips: 60, rs: 27, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [140] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5670218\n",
      "Train loss (w/o reg) on all data: 0.56691694\n",
      "Test loss (w/o reg) on all data: 0.50929326\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0038455685\n",
      "Norm of the params: 1.4481814\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.50929. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5402854\n",
      "Train loss (w/o reg) on all data: 0.5400919\n",
      "Test loss (w/o reg) on all data: 0.5615078\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.004194994\n",
      "Norm of the params: 1.9675319\n",
      "                Loss: fixed   9 labels. Loss 0.56151. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [178] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6075496\n",
      "Train loss (w/o reg) on all data: 0.6074576\n",
      "Test loss (w/o reg) on all data: 0.53775704\n",
      "Train acc on all data:  0.6462264150943396\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.004482875\n",
      "Norm of the params: 1.3566817\n",
      "              Random: fixed   2 labels. Loss 0.53776. Accuracy 0.733.\n",
      "### Flips: 60, rs: 27, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5402542\n",
      "Train loss (w/o reg) on all data: 0.54011834\n",
      "Test loss (w/o reg) on all data: 0.495687\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.6888888888888889\n",
      "Norm of the mean of gradients: 0.000553115\n",
      "Norm of the params: 1.6482393\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.49569. Accuracy 0.689.\n",
      "Using normal model\n",
      "LBFGS training took [232] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47709575\n",
      "Train loss (w/o reg) on all data: 0.47680116\n",
      "Test loss (w/o reg) on all data: 0.55904734\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0011866862\n",
      "Norm of the params: 2.427304\n",
      "                Loss: fixed  17 labels. Loss 0.55905. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [176] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58591855\n",
      "Train loss (w/o reg) on all data: 0.58579665\n",
      "Test loss (w/o reg) on all data: 0.5265319\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.6888888888888889\n",
      "Norm of the mean of gradients: 0.0022599774\n",
      "Norm of the params: 1.561234\n",
      "              Random: fixed   6 labels. Loss 0.52653. Accuracy 0.689.\n",
      "### Flips: 60, rs: 27, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [410] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5214025\n",
      "Train loss (w/o reg) on all data: 0.5212461\n",
      "Test loss (w/o reg) on all data: 0.48334813\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0100039765\n",
      "Norm of the params: 1.7685082\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.48335. Accuracy 0.711.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [185] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43998778\n",
      "Train loss (w/o reg) on all data: 0.439639\n",
      "Test loss (w/o reg) on all data: 0.5745389\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.002437511\n",
      "Norm of the params: 2.6411748\n",
      "                Loss: fixed  22 labels. Loss 0.57454. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [302] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57421774\n",
      "Train loss (w/o reg) on all data: 0.57407737\n",
      "Test loss (w/o reg) on all data: 0.5186893\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.6888888888888889\n",
      "Norm of the mean of gradients: 0.0026092299\n",
      "Norm of the params: 1.675664\n",
      "              Random: fixed   9 labels. Loss 0.51869. Accuracy 0.689.\n",
      "### Flips: 60, rs: 27, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [198] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5003259\n",
      "Train loss (w/o reg) on all data: 0.5001442\n",
      "Test loss (w/o reg) on all data: 0.4862799\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0018098122\n",
      "Norm of the params: 1.9065013\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.48628. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [305] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37669852\n",
      "Train loss (w/o reg) on all data: 0.3762049\n",
      "Test loss (w/o reg) on all data: 0.59664345\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.00064800674\n",
      "Norm of the params: 3.1419907\n",
      "                Loss: fixed  30 labels. Loss 0.59664. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [250] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5688205\n",
      "Train loss (w/o reg) on all data: 0.56866807\n",
      "Test loss (w/o reg) on all data: 0.5162562\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0075192754\n",
      "Norm of the params: 1.745978\n",
      "              Random: fixed  14 labels. Loss 0.51626. Accuracy 0.711.\n",
      "### Flips: 60, rs: 27, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [319] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4684221\n",
      "Train loss (w/o reg) on all data: 0.46817023\n",
      "Test loss (w/o reg) on all data: 0.47865218\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.003030982\n",
      "Norm of the params: 2.2444646\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.47865. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [175] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36691305\n",
      "Train loss (w/o reg) on all data: 0.3664175\n",
      "Test loss (w/o reg) on all data: 0.6111711\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0005211873\n",
      "Norm of the params: 3.1482177\n",
      "                Loss: fixed  31 labels. Loss 0.61117. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [284] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5555818\n",
      "Train loss (w/o reg) on all data: 0.5554202\n",
      "Test loss (w/o reg) on all data: 0.4864026\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0025203447\n",
      "Norm of the params: 1.7977469\n",
      "              Random: fixed  17 labels. Loss 0.48640. Accuracy 0.756.\n",
      "### Flips: 60, rs: 27, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [367] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46424186\n",
      "Train loss (w/o reg) on all data: 0.4640213\n",
      "Test loss (w/o reg) on all data: 0.47932938\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.00071463356\n",
      "Norm of the params: 2.1003635\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.47933. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [356] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3327194\n",
      "Train loss (w/o reg) on all data: 0.33205426\n",
      "Test loss (w/o reg) on all data: 0.58567804\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0003700339\n",
      "Norm of the params: 3.6472921\n",
      "                Loss: fixed  36 labels. Loss 0.58568. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [343] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55285686\n",
      "Train loss (w/o reg) on all data: 0.5527033\n",
      "Test loss (w/o reg) on all data: 0.46079516\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.003957798\n",
      "Norm of the params: 1.7523104\n",
      "              Random: fixed  19 labels. Loss 0.46080. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.63064677\n",
      "Train loss (w/o reg) on all data: 0.6306048\n",
      "Test loss (w/o reg) on all data: 0.48065427\n",
      "Train acc on all data:  0.6650943396226415\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00158291\n",
      "Norm of the params: 0.9160459\n",
      "Flipped loss: 0.48065. Accuracy: 0.844\n",
      "### Flips: 60, rs: 28, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [202] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.599641\n",
      "Train loss (w/o reg) on all data: 0.59959346\n",
      "Test loss (w/o reg) on all data: 0.4291143\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0010439133\n",
      "Norm of the params: 0.9754538\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.42911. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.558633\n",
      "Train loss (w/o reg) on all data: 0.558536\n",
      "Test loss (w/o reg) on all data: 0.40694115\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00041832295\n",
      "Norm of the params: 1.3929443\n",
      "                Loss: fixed   9 labels. Loss 0.40694. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [121] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6290418\n",
      "Train loss (w/o reg) on all data: 0.629002\n",
      "Test loss (w/o reg) on all data: 0.47109872\n",
      "Train acc on all data:  0.660377358490566\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.009864578\n",
      "Norm of the params: 0.89218384\n",
      "              Random: fixed   5 labels. Loss 0.47110. Accuracy 0.867.\n",
      "### Flips: 60, rs: 28, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57062095\n",
      "Train loss (w/o reg) on all data: 0.5705616\n",
      "Test loss (w/o reg) on all data: 0.40058425\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.008290405\n",
      "Norm of the params: 1.0894818\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.40058. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [186] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5154946\n",
      "Train loss (w/o reg) on all data: 0.5153624\n",
      "Test loss (w/o reg) on all data: 0.40091234\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0025013671\n",
      "Norm of the params: 1.6261967\n",
      "                Loss: fixed  15 labels. Loss 0.40091. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6241082\n",
      "Train loss (w/o reg) on all data: 0.6240662\n",
      "Test loss (w/o reg) on all data: 0.4767661\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.010315245\n",
      "Norm of the params: 0.91664624\n",
      "              Random: fixed   7 labels. Loss 0.47677. Accuracy 0.867.\n",
      "### Flips: 60, rs: 28, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [132] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5706399\n",
      "Train loss (w/o reg) on all data: 0.57057333\n",
      "Test loss (w/o reg) on all data: 0.3977577\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0042510387\n",
      "Norm of the params: 1.1540715\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.39776. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [177] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46726564\n",
      "Train loss (w/o reg) on all data: 0.46707723\n",
      "Test loss (w/o reg) on all data: 0.3785304\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.003251824\n",
      "Norm of the params: 1.9411376\n",
      "                Loss: fixed  21 labels. Loss 0.37853. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [308] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61366946\n",
      "Train loss (w/o reg) on all data: 0.6136357\n",
      "Test loss (w/o reg) on all data: 0.45840153\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.005815716\n",
      "Norm of the params: 0.8216748\n",
      "              Random: fixed  10 labels. Loss 0.45840. Accuracy 0.844.\n",
      "### Flips: 60, rs: 28, checks: 40\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [142] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54210126\n",
      "Train loss (w/o reg) on all data: 0.54201585\n",
      "Test loss (w/o reg) on all data: 0.38162473\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0017398164\n",
      "Norm of the params: 1.3072009\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.38162. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [297] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43983868\n",
      "Train loss (w/o reg) on all data: 0.4396301\n",
      "Test loss (w/o reg) on all data: 0.3644338\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0024315594\n",
      "Norm of the params: 2.0424485\n",
      "                Loss: fixed  25 labels. Loss 0.36443. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [332] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6070061\n",
      "Train loss (w/o reg) on all data: 0.6069743\n",
      "Test loss (w/o reg) on all data: 0.45457026\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0017132672\n",
      "Norm of the params: 0.796962\n",
      "              Random: fixed  14 labels. Loss 0.45457. Accuracy 0.867.\n",
      "### Flips: 60, rs: 28, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [84] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52392745\n",
      "Train loss (w/o reg) on all data: 0.52382535\n",
      "Test loss (w/o reg) on all data: 0.3768595\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011906939\n",
      "Norm of the params: 1.4289463\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.37686. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39869532\n",
      "Train loss (w/o reg) on all data: 0.39844927\n",
      "Test loss (w/o reg) on all data: 0.37523487\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0040234695\n",
      "Norm of the params: 2.21837\n",
      "                Loss: fixed  31 labels. Loss 0.37523. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [253] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6035292\n",
      "Train loss (w/o reg) on all data: 0.60349816\n",
      "Test loss (w/o reg) on all data: 0.448628\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0028965413\n",
      "Norm of the params: 0.78802454\n",
      "              Random: fixed  16 labels. Loss 0.44863. Accuracy 0.867.\n",
      "### Flips: 60, rs: 28, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [318] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51192325\n",
      "Train loss (w/o reg) on all data: 0.51180995\n",
      "Test loss (w/o reg) on all data: 0.367284\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0030137517\n",
      "Norm of the params: 1.5054855\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.36728. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [510] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.364629\n",
      "Train loss (w/o reg) on all data: 0.3643445\n",
      "Test loss (w/o reg) on all data: 0.3826761\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0005397942\n",
      "Norm of the params: 2.3852882\n",
      "                Loss: fixed  37 labels. Loss 0.38268. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [193] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6035273\n",
      "Train loss (w/o reg) on all data: 0.60349643\n",
      "Test loss (w/o reg) on all data: 0.4492566\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0019590764\n",
      "Norm of the params: 0.7859806\n",
      "              Random: fixed  16 labels. Loss 0.44926. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6181599\n",
      "Train loss (w/o reg) on all data: 0.6181054\n",
      "Test loss (w/o reg) on all data: 0.58881193\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.003084642\n",
      "Norm of the params: 1.0439162\n",
      "Flipped loss: 0.58881. Accuracy: 0.733\n",
      "### Flips: 60, rs: 29, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [229] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5578805\n",
      "Train loss (w/o reg) on all data: 0.5577285\n",
      "Test loss (w/o reg) on all data: 0.60371983\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0021163507\n",
      "Norm of the params: 1.7435089\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.60372. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [168] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5266683\n",
      "Train loss (w/o reg) on all data: 0.5264653\n",
      "Test loss (w/o reg) on all data: 0.6188034\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0016174378\n",
      "Norm of the params: 2.0150044\n",
      "                Loss: fixed   9 labels. Loss 0.61880. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [303] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61844987\n",
      "Train loss (w/o reg) on all data: 0.6183925\n",
      "Test loss (w/o reg) on all data: 0.5968191\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0032411702\n",
      "Norm of the params: 1.0706258\n",
      "              Random: fixed   1 labels. Loss 0.59682. Accuracy 0.756.\n",
      "### Flips: 60, rs: 29, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [353] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53460413\n",
      "Train loss (w/o reg) on all data: 0.5344114\n",
      "Test loss (w/o reg) on all data: 0.62115294\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.6888888888888889\n",
      "Norm of the mean of gradients: 0.00208132\n",
      "Norm of the params: 1.9634132\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.62115. Accuracy 0.689.\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47200525\n",
      "Train loss (w/o reg) on all data: 0.4717323\n",
      "Test loss (w/o reg) on all data: 0.61110187\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0014006818\n",
      "Norm of the params: 2.3364775\n",
      "                Loss: fixed  16 labels. Loss 0.61110. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [295] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6120208\n",
      "Train loss (w/o reg) on all data: 0.6119672\n",
      "Test loss (w/o reg) on all data: 0.5721899\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0036099378\n",
      "Norm of the params: 1.0352181\n",
      "              Random: fixed   4 labels. Loss 0.57219. Accuracy 0.733.\n",
      "### Flips: 60, rs: 29, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [175] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50162476\n",
      "Train loss (w/o reg) on all data: 0.50137913\n",
      "Test loss (w/o reg) on all data: 0.56855685\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0067303753\n",
      "Norm of the params: 2.2163754\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.56856. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [226] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42550445\n",
      "Train loss (w/o reg) on all data: 0.4251758\n",
      "Test loss (w/o reg) on all data: 0.60416436\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0013063459\n",
      "Norm of the params: 2.563806\n",
      "                Loss: fixed  21 labels. Loss 0.60416. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [287] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5982099\n",
      "Train loss (w/o reg) on all data: 0.59814924\n",
      "Test loss (w/o reg) on all data: 0.5103096\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0016743062\n",
      "Norm of the params: 1.1015825\n",
      "              Random: fixed   9 labels. Loss 0.51031. Accuracy 0.778.\n",
      "### Flips: 60, rs: 29, checks: 40\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [174] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49567515\n",
      "Train loss (w/o reg) on all data: 0.49545705\n",
      "Test loss (w/o reg) on all data: 0.5293877\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0009422371\n",
      "Norm of the params: 2.08856\n",
      "     Influence (LOO): fixed  22 labels. Loss 0.52939. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39506212\n",
      "Train loss (w/o reg) on all data: 0.39465985\n",
      "Test loss (w/o reg) on all data: 0.65333194\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.00048675906\n",
      "Norm of the params: 2.8364718\n",
      "                Loss: fixed  26 labels. Loss 0.65333. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [285] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60172594\n",
      "Train loss (w/o reg) on all data: 0.6016683\n",
      "Test loss (w/o reg) on all data: 0.5120709\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0057736123\n",
      "Norm of the params: 1.0735856\n",
      "              Random: fixed  10 labels. Loss 0.51207. Accuracy 0.800.\n",
      "### Flips: 60, rs: 29, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [362] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47170526\n",
      "Train loss (w/o reg) on all data: 0.4714809\n",
      "Test loss (w/o reg) on all data: 0.46481413\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.002001549\n",
      "Norm of the params: 2.1182458\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.46481. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36427087\n",
      "Train loss (w/o reg) on all data: 0.3638507\n",
      "Test loss (w/o reg) on all data: 0.6344896\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.7111111111111111\n",
      "Norm of the mean of gradients: 0.0004245229\n",
      "Norm of the params: 2.8988478\n",
      "                Loss: fixed  32 labels. Loss 0.63449. Accuracy 0.711.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5935783\n",
      "Train loss (w/o reg) on all data: 0.59352446\n",
      "Test loss (w/o reg) on all data: 0.50119674\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0044409754\n",
      "Norm of the params: 1.0376523\n",
      "              Random: fixed  12 labels. Loss 0.50120. Accuracy 0.756.\n",
      "### Flips: 60, rs: 29, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [211] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45254943\n",
      "Train loss (w/o reg) on all data: 0.4523046\n",
      "Test loss (w/o reg) on all data: 0.47513703\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0016837575\n",
      "Norm of the params: 2.2128325\n",
      "     Influence (LOO): fixed  30 labels. Loss 0.47514. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [243] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3458886\n",
      "Train loss (w/o reg) on all data: 0.34534422\n",
      "Test loss (w/o reg) on all data: 0.61451375\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0014800716\n",
      "Norm of the params: 3.2997272\n",
      "                Loss: fixed  35 labels. Loss 0.61451. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [330] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5893184\n",
      "Train loss (w/o reg) on all data: 0.58926505\n",
      "Test loss (w/o reg) on all data: 0.50547665\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0015156175\n",
      "Norm of the params: 1.032998\n",
      "              Random: fixed  13 labels. Loss 0.50548. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6255691\n",
      "Train loss (w/o reg) on all data: 0.62549007\n",
      "Test loss (w/o reg) on all data: 0.48335806\n",
      "Train acc on all data:  0.6462264150943396\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011486937\n",
      "Norm of the params: 1.2573998\n",
      "Flipped loss: 0.48336. Accuracy: 0.844\n",
      "### Flips: 60, rs: 30, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6008433\n",
      "Train loss (w/o reg) on all data: 0.6007788\n",
      "Test loss (w/o reg) on all data: 0.45110652\n",
      "Train acc on all data:  0.6650943396226415\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0009903305\n",
      "Norm of the params: 1.1356069\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.45111. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5636148\n",
      "Train loss (w/o reg) on all data: 0.5634179\n",
      "Test loss (w/o reg) on all data: 0.4076981\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013954592\n",
      "Norm of the params: 1.9842379\n",
      "                Loss: fixed   8 labels. Loss 0.40770. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [234] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.62244815\n",
      "Train loss (w/o reg) on all data: 0.62238556\n",
      "Test loss (w/o reg) on all data: 0.46845594\n",
      "Train acc on all data:  0.6650943396226415\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.008532467\n",
      "Norm of the params: 1.1186247\n",
      "              Random: fixed   2 labels. Loss 0.46846. Accuracy 0.822.\n",
      "### Flips: 60, rs: 30, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [158] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56831\n",
      "Train loss (w/o reg) on all data: 0.568199\n",
      "Test loss (w/o reg) on all data: 0.40119797\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0026273932\n",
      "Norm of the params: 1.4900954\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.40120. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [379] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5069879\n",
      "Train loss (w/o reg) on all data: 0.50669056\n",
      "Test loss (w/o reg) on all data: 0.3930397\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0033098876\n",
      "Norm of the params: 2.43884\n",
      "                Loss: fixed  16 labels. Loss 0.39304. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [221] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61985046\n",
      "Train loss (w/o reg) on all data: 0.6197874\n",
      "Test loss (w/o reg) on all data: 0.48523283\n",
      "Train acc on all data:  0.6462264150943396\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015766299\n",
      "Norm of the params: 1.1232821\n",
      "              Random: fixed   6 labels. Loss 0.48523. Accuracy 0.822.\n",
      "### Flips: 60, rs: 30, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [165] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55782837\n",
      "Train loss (w/o reg) on all data: 0.5577362\n",
      "Test loss (w/o reg) on all data: 0.38466787\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.004763771\n",
      "Norm of the params: 1.3577541\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.38467. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4529003\n",
      "Train loss (w/o reg) on all data: 0.45251727\n",
      "Test loss (w/o reg) on all data: 0.40283775\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00073092344\n",
      "Norm of the params: 2.7677135\n",
      "                Loss: fixed  23 labels. Loss 0.40284. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [470] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.62071437\n",
      "Train loss (w/o reg) on all data: 0.6206475\n",
      "Test loss (w/o reg) on all data: 0.48470068\n",
      "Train acc on all data:  0.6462264150943396\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0006470421\n",
      "Norm of the params: 1.1566448\n",
      "              Random: fixed   7 labels. Loss 0.48470. Accuracy 0.822.\n",
      "### Flips: 60, rs: 30, checks: 40\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [217] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5269755\n",
      "Train loss (w/o reg) on all data: 0.5268608\n",
      "Test loss (w/o reg) on all data: 0.34829995\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0009561615\n",
      "Norm of the params: 1.5150427\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.34830. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [398] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39668128\n",
      "Train loss (w/o reg) on all data: 0.39618716\n",
      "Test loss (w/o reg) on all data: 0.3859309\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0017163717\n",
      "Norm of the params: 3.1436758\n",
      "                Loss: fixed  29 labels. Loss 0.38593. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [404] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61878043\n",
      "Train loss (w/o reg) on all data: 0.61871934\n",
      "Test loss (w/o reg) on all data: 0.47062975\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00061261584\n",
      "Norm of the params: 1.105564\n",
      "              Random: fixed  10 labels. Loss 0.47063. Accuracy 0.822.\n",
      "### Flips: 60, rs: 30, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [311] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50136685\n",
      "Train loss (w/o reg) on all data: 0.5012223\n",
      "Test loss (w/o reg) on all data: 0.3598334\n",
      "Train acc on all data:  0.75\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0031448777\n",
      "Norm of the params: 1.7002687\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.35983. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [381] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37850028\n",
      "Train loss (w/o reg) on all data: 0.37803334\n",
      "Test loss (w/o reg) on all data: 0.40632552\n",
      "Train acc on all data:  0.7971698113207547\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005761626\n",
      "Norm of the params: 3.0559607\n",
      "                Loss: fixed  32 labels. Loss 0.40633. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [425] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60590446\n",
      "Train loss (w/o reg) on all data: 0.60583323\n",
      "Test loss (w/o reg) on all data: 0.4669128\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001657685\n",
      "Norm of the params: 1.1935972\n",
      "              Random: fixed  13 labels. Loss 0.46691. Accuracy 0.800.\n",
      "### Flips: 60, rs: 30, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [199] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49380702\n",
      "Train loss (w/o reg) on all data: 0.4936466\n",
      "Test loss (w/o reg) on all data: 0.35994512\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0008733537\n",
      "Norm of the params: 1.7911909\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.35995. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [396] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34851086\n",
      "Train loss (w/o reg) on all data: 0.34792694\n",
      "Test loss (w/o reg) on all data: 0.38716596\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0027938513\n",
      "Norm of the params: 3.41732\n",
      "                Loss: fixed  37 labels. Loss 0.38717. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [347] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5895542\n",
      "Train loss (w/o reg) on all data: 0.58948255\n",
      "Test loss (w/o reg) on all data: 0.4407153\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00087135035\n",
      "Norm of the params: 1.1969031\n",
      "              Random: fixed  17 labels. Loss 0.44072. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [157] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.62178653\n",
      "Train loss (w/o reg) on all data: 0.6216946\n",
      "Test loss (w/o reg) on all data: 0.51972604\n",
      "Train acc on all data:  0.6415094339622641\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0026441435\n",
      "Norm of the params: 1.3558104\n",
      "Flipped loss: 0.51973. Accuracy: 0.800\n",
      "### Flips: 60, rs: 31, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [117] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5925404\n",
      "Train loss (w/o reg) on all data: 0.5924201\n",
      "Test loss (w/o reg) on all data: 0.5348543\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.005440161\n",
      "Norm of the params: 1.5511061\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.53485. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [339] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5388432\n",
      "Train loss (w/o reg) on all data: 0.53861195\n",
      "Test loss (w/o reg) on all data: 0.4794253\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0044934754\n",
      "Norm of the params: 2.1507065\n",
      "                Loss: fixed  10 labels. Loss 0.47943. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [223] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61680734\n",
      "Train loss (w/o reg) on all data: 0.6167213\n",
      "Test loss (w/o reg) on all data: 0.5127683\n",
      "Train acc on all data:  0.6556603773584906\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0018629038\n",
      "Norm of the params: 1.312084\n",
      "              Random: fixed   2 labels. Loss 0.51277. Accuracy 0.778.\n",
      "### Flips: 60, rs: 31, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [157] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.575781\n",
      "Train loss (w/o reg) on all data: 0.57565176\n",
      "Test loss (w/o reg) on all data: 0.49559665\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.013535246\n",
      "Norm of the params: 1.6076914\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.49560. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [256] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4761186\n",
      "Train loss (w/o reg) on all data: 0.47580835\n",
      "Test loss (w/o reg) on all data: 0.43131205\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004278287\n",
      "Norm of the params: 2.4909909\n",
      "                Loss: fixed  17 labels. Loss 0.43131. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [247] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6120727\n",
      "Train loss (w/o reg) on all data: 0.611978\n",
      "Test loss (w/o reg) on all data: 0.5121073\n",
      "Train acc on all data:  0.6509433962264151\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0012301161\n",
      "Norm of the params: 1.3761417\n",
      "              Random: fixed   4 labels. Loss 0.51211. Accuracy 0.756.\n",
      "### Flips: 60, rs: 31, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.550405\n",
      "Train loss (w/o reg) on all data: 0.55023366\n",
      "Test loss (w/o reg) on all data: 0.4712489\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0083295535\n",
      "Norm of the params: 1.8512474\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.47125. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [242] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43037045\n",
      "Train loss (w/o reg) on all data: 0.42997116\n",
      "Test loss (w/o reg) on all data: 0.39577797\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014550398\n",
      "Norm of the params: 2.8259497\n",
      "                Loss: fixed  23 labels. Loss 0.39578. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [383] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60235065\n",
      "Train loss (w/o reg) on all data: 0.6022512\n",
      "Test loss (w/o reg) on all data: 0.48381916\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.01298065\n",
      "Norm of the params: 1.4105343\n",
      "              Random: fixed   6 labels. Loss 0.48382. Accuracy 0.778.\n",
      "### Flips: 60, rs: 31, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [363] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5178114\n",
      "Train loss (w/o reg) on all data: 0.51762897\n",
      "Test loss (w/o reg) on all data: 0.41175482\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0006946905\n",
      "Norm of the params: 1.9102526\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.41175. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [275] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38887948\n",
      "Train loss (w/o reg) on all data: 0.38834462\n",
      "Test loss (w/o reg) on all data: 0.40176043\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0020023047\n",
      "Norm of the params: 3.270665\n",
      "                Loss: fixed  29 labels. Loss 0.40176. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [334] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5826792\n",
      "Train loss (w/o reg) on all data: 0.5825532\n",
      "Test loss (w/o reg) on all data: 0.46013886\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001786724\n",
      "Norm of the params: 1.5875721\n",
      "              Random: fixed  10 labels. Loss 0.46014. Accuracy 0.822.\n",
      "### Flips: 60, rs: 31, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [167] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49679184\n",
      "Train loss (w/o reg) on all data: 0.49658632\n",
      "Test loss (w/o reg) on all data: 0.40045235\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.001855587\n",
      "Norm of the params: 2.0274653\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.40045. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [204] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36427134\n",
      "Train loss (w/o reg) on all data: 0.36367452\n",
      "Test loss (w/o reg) on all data: 0.4140407\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0007037549\n",
      "Norm of the params: 3.4549143\n",
      "                Loss: fixed  33 labels. Loss 0.41404. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [326] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58061516\n",
      "Train loss (w/o reg) on all data: 0.5804914\n",
      "Test loss (w/o reg) on all data: 0.46816793\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0015159422\n",
      "Norm of the params: 1.5732441\n",
      "              Random: fixed  12 labels. Loss 0.46817. Accuracy 0.822.\n",
      "### Flips: 60, rs: 31, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [227] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46027145\n",
      "Train loss (w/o reg) on all data: 0.46000278\n",
      "Test loss (w/o reg) on all data: 0.39238396\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.001684079\n",
      "Norm of the params: 2.3181047\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.39238. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.343581\n",
      "Train loss (w/o reg) on all data: 0.34291083\n",
      "Test loss (w/o reg) on all data: 0.41600236\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0013652247\n",
      "Norm of the params: 3.661086\n",
      "                Loss: fixed  37 labels. Loss 0.41600. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [302] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.57551026\n",
      "Train loss (w/o reg) on all data: 0.5753801\n",
      "Test loss (w/o reg) on all data: 0.46229023\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011483022\n",
      "Norm of the params: 1.6136602\n",
      "              Random: fixed  17 labels. Loss 0.46229. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [231] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5907591\n",
      "Train loss (w/o reg) on all data: 0.5906829\n",
      "Test loss (w/o reg) on all data: 0.47080773\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.002246664\n",
      "Norm of the params: 1.2345163\n",
      "Flipped loss: 0.47081. Accuracy: 0.800\n",
      "### Flips: 60, rs: 32, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [136] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5459743\n",
      "Train loss (w/o reg) on all data: 0.5457862\n",
      "Test loss (w/o reg) on all data: 0.45734775\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0037373125\n",
      "Norm of the params: 1.9395291\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.45735. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [164] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5213603\n",
      "Train loss (w/o reg) on all data: 0.5211359\n",
      "Test loss (w/o reg) on all data: 0.44859543\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011697682\n",
      "Norm of the params: 2.1183295\n",
      "                Loss: fixed   7 labels. Loss 0.44860. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5909693\n",
      "Train loss (w/o reg) on all data: 0.5909068\n",
      "Test loss (w/o reg) on all data: 0.46494353\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00081194396\n",
      "Norm of the params: 1.1183919\n",
      "              Random: fixed   1 labels. Loss 0.46494. Accuracy 0.822.\n",
      "### Flips: 60, rs: 32, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5197844\n",
      "Train loss (w/o reg) on all data: 0.5195664\n",
      "Test loss (w/o reg) on all data: 0.45494178\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014019258\n",
      "Norm of the params: 2.088022\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.45494. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48146197\n",
      "Train loss (w/o reg) on all data: 0.48115325\n",
      "Test loss (w/o reg) on all data: 0.42746454\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011830835\n",
      "Norm of the params: 2.484874\n",
      "                Loss: fixed  12 labels. Loss 0.42746. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5897592\n",
      "Train loss (w/o reg) on all data: 0.58969367\n",
      "Test loss (w/o reg) on all data: 0.4674237\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.008825122\n",
      "Norm of the params: 1.1444752\n",
      "              Random: fixed   5 labels. Loss 0.46742. Accuracy 0.844.\n",
      "### Flips: 60, rs: 32, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [163] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49347442\n",
      "Train loss (w/o reg) on all data: 0.49322525\n",
      "Test loss (w/o reg) on all data: 0.45186844\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0048750467\n",
      "Norm of the params: 2.232357\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.45187. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [329] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4260118\n",
      "Train loss (w/o reg) on all data: 0.4254882\n",
      "Test loss (w/o reg) on all data: 0.4424199\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0014493935\n",
      "Norm of the params: 3.2359962\n",
      "                Loss: fixed  19 labels. Loss 0.44242. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [329] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5553397\n",
      "Train loss (w/o reg) on all data: 0.5552573\n",
      "Test loss (w/o reg) on all data: 0.41959792\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0013567054\n",
      "Norm of the params: 1.2835189\n",
      "              Random: fixed  11 labels. Loss 0.41960. Accuracy 0.867.\n",
      "### Flips: 60, rs: 32, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [211] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48059613\n",
      "Train loss (w/o reg) on all data: 0.48039216\n",
      "Test loss (w/o reg) on all data: 0.43188655\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00299636\n",
      "Norm of the params: 2.0197222\n",
      "     Influence (LOO): fixed  18 labels. Loss 0.43189. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [434] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.37847072\n",
      "Train loss (w/o reg) on all data: 0.3779649\n",
      "Test loss (w/o reg) on all data: 0.43356583\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014436921\n",
      "Norm of the params: 3.1805658\n",
      "                Loss: fixed  27 labels. Loss 0.43357. Accuracy 0.800.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [407] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.54663455\n",
      "Train loss (w/o reg) on all data: 0.5465524\n",
      "Test loss (w/o reg) on all data: 0.4046975\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0054782764\n",
      "Norm of the params: 1.2818981\n",
      "              Random: fixed  14 labels. Loss 0.40470. Accuracy 0.867.\n",
      "### Flips: 60, rs: 32, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [383] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44641843\n",
      "Train loss (w/o reg) on all data: 0.44619888\n",
      "Test loss (w/o reg) on all data: 0.38847834\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0030479827\n",
      "Norm of the params: 2.0954957\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.38848. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [363] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3500045\n",
      "Train loss (w/o reg) on all data: 0.34942764\n",
      "Test loss (w/o reg) on all data: 0.45868552\n",
      "Train acc on all data:  0.8584905660377359\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.00171993\n",
      "Norm of the params: 3.3966208\n",
      "                Loss: fixed  32 labels. Loss 0.45869. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [409] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5435893\n",
      "Train loss (w/o reg) on all data: 0.5435007\n",
      "Test loss (w/o reg) on all data: 0.4089288\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0025935918\n",
      "Norm of the params: 1.3308364\n",
      "              Random: fixed  15 labels. Loss 0.40893. Accuracy 0.867.\n",
      "### Flips: 60, rs: 32, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [332] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42317277\n",
      "Train loss (w/o reg) on all data: 0.422928\n",
      "Test loss (w/o reg) on all data: 0.37701157\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0009937759\n",
      "Norm of the params: 2.2125309\n",
      "     Influence (LOO): fixed  31 labels. Loss 0.37701. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [421] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.32938036\n",
      "Train loss (w/o reg) on all data: 0.32876053\n",
      "Test loss (w/o reg) on all data: 0.4369183\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00077143346\n",
      "Norm of the params: 3.5209076\n",
      "                Loss: fixed  37 labels. Loss 0.43692. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [297] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5387701\n",
      "Train loss (w/o reg) on all data: 0.53868544\n",
      "Test loss (w/o reg) on all data: 0.4132289\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0014605256\n",
      "Norm of the params: 1.3010229\n",
      "              Random: fixed  18 labels. Loss 0.41323. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [339] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5834567\n",
      "Train loss (w/o reg) on all data: 0.5833585\n",
      "Test loss (w/o reg) on all data: 0.40461904\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.9333333333333333\n",
      "Norm of the mean of gradients: 0.0019228306\n",
      "Norm of the params: 1.4010316\n",
      "Flipped loss: 0.40462. Accuracy: 0.933\n",
      "### Flips: 60, rs: 33, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [170] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5449474\n",
      "Train loss (w/o reg) on all data: 0.544788\n",
      "Test loss (w/o reg) on all data: 0.3653769\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0027215017\n",
      "Norm of the params: 1.7854291\n",
      "     Influence (LOO): fixed   7 labels. Loss 0.36538. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [224] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4851209\n",
      "Train loss (w/o reg) on all data: 0.4848875\n",
      "Test loss (w/o reg) on all data: 0.32089657\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.9111111111111111\n",
      "Norm of the mean of gradients: 0.00485881\n",
      "Norm of the params: 2.160495\n",
      "                Loss: fixed  10 labels. Loss 0.32090. Accuracy 0.911.\n",
      "Using normal model\n",
      "LBFGS training took [241] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58348274\n",
      "Train loss (w/o reg) on all data: 0.58339494\n",
      "Test loss (w/o reg) on all data: 0.4111713\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0013447086\n",
      "Norm of the params: 1.3250855\n",
      "              Random: fixed   4 labels. Loss 0.41117. Accuracy 0.867.\n",
      "### Flips: 60, rs: 33, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [173] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5185345\n",
      "Train loss (w/o reg) on all data: 0.5183474\n",
      "Test loss (w/o reg) on all data: 0.35363272\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.005899079\n",
      "Norm of the params: 1.9344599\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.35363. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [370] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43694928\n",
      "Train loss (w/o reg) on all data: 0.4365582\n",
      "Test loss (w/o reg) on all data: 0.326731\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0046579633\n",
      "Norm of the params: 2.7968218\n",
      "                Loss: fixed  15 labels. Loss 0.32673. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [296] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.584016\n",
      "Train loss (w/o reg) on all data: 0.5839254\n",
      "Test loss (w/o reg) on all data: 0.4117955\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0015729343\n",
      "Norm of the params: 1.3461936\n",
      "              Random: fixed   5 labels. Loss 0.41180. Accuracy 0.867.\n",
      "### Flips: 60, rs: 33, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [286] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49273652\n",
      "Train loss (w/o reg) on all data: 0.49253654\n",
      "Test loss (w/o reg) on all data: 0.33660108\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0008294898\n",
      "Norm of the params: 1.9998288\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.33660. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [366] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39833385\n",
      "Train loss (w/o reg) on all data: 0.39774102\n",
      "Test loss (w/o reg) on all data: 0.31936282\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0009736359\n",
      "Norm of the params: 3.443337\n",
      "                Loss: fixed  20 labels. Loss 0.31936. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [459] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5679417\n",
      "Train loss (w/o reg) on all data: 0.56783456\n",
      "Test loss (w/o reg) on all data: 0.39440534\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.003140016\n",
      "Norm of the params: 1.4638517\n",
      "              Random: fixed   9 labels. Loss 0.39441. Accuracy 0.844.\n",
      "### Flips: 60, rs: 33, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [349] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45938042\n",
      "Train loss (w/o reg) on all data: 0.45915258\n",
      "Test loss (w/o reg) on all data: 0.30982476\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.00063624565\n",
      "Norm of the params: 2.1346064\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.30982. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [417] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34651017\n",
      "Train loss (w/o reg) on all data: 0.34574178\n",
      "Test loss (w/o reg) on all data: 0.3102382\n",
      "Train acc on all data:  0.8490566037735849\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0018474514\n",
      "Norm of the params: 3.9201934\n",
      "                Loss: fixed  26 labels. Loss 0.31024. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [429] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5608801\n",
      "Train loss (w/o reg) on all data: 0.5607482\n",
      "Test loss (w/o reg) on all data: 0.40104914\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.002506821\n",
      "Norm of the params: 1.6243088\n",
      "              Random: fixed  11 labels. Loss 0.40105. Accuracy 0.867.\n",
      "### Flips: 60, rs: 33, checks: 50\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [335] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.40611556\n",
      "Train loss (w/o reg) on all data: 0.4057854\n",
      "Test loss (w/o reg) on all data: 0.3093939\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.000645888\n",
      "Norm of the params: 2.5696733\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.30939. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [300] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3134939\n",
      "Train loss (w/o reg) on all data: 0.31264892\n",
      "Test loss (w/o reg) on all data: 0.31852996\n",
      "Train acc on all data:  0.8726415094339622\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.004306514\n",
      "Norm of the params: 4.110921\n",
      "                Loss: fixed  31 labels. Loss 0.31853. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [312] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5638395\n",
      "Train loss (w/o reg) on all data: 0.5637242\n",
      "Test loss (w/o reg) on all data: 0.3973648\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0014923124\n",
      "Norm of the params: 1.5185713\n",
      "              Random: fixed  13 labels. Loss 0.39736. Accuracy 0.867.\n",
      "### Flips: 60, rs: 33, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4047162\n",
      "Train loss (w/o reg) on all data: 0.40439168\n",
      "Test loss (w/o reg) on all data: 0.31593922\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0016359113\n",
      "Norm of the params: 2.547663\n",
      "     Influence (LOO): fixed  31 labels. Loss 0.31594. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [333] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3008114\n",
      "Train loss (w/o reg) on all data: 0.30007952\n",
      "Test loss (w/o reg) on all data: 0.31641963\n",
      "Train acc on all data:  0.8820754716981132\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0022235021\n",
      "Norm of the params: 3.8259304\n",
      "                Loss: fixed  34 labels. Loss 0.31642. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [387] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5544295\n",
      "Train loss (w/o reg) on all data: 0.55429846\n",
      "Test loss (w/o reg) on all data: 0.39777064\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.015822556\n",
      "Norm of the params: 1.6188089\n",
      "              Random: fixed  15 labels. Loss 0.39777. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [365] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5891027\n",
      "Train loss (w/o reg) on all data: 0.589052\n",
      "Test loss (w/o reg) on all data: 0.51660454\n",
      "Train acc on all data:  0.660377358490566\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0077459994\n",
      "Norm of the params: 1.0069022\n",
      "Flipped loss: 0.51660. Accuracy: 0.778\n",
      "### Flips: 60, rs: 34, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [236] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5494329\n",
      "Train loss (w/o reg) on all data: 0.54930836\n",
      "Test loss (w/o reg) on all data: 0.5045211\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00059191056\n",
      "Norm of the params: 1.5780557\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.50452. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5241415\n",
      "Train loss (w/o reg) on all data: 0.52404433\n",
      "Test loss (w/o reg) on all data: 0.493301\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0013612548\n",
      "Norm of the params: 1.3938943\n",
      "                Loss: fixed   8 labels. Loss 0.49330. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [127] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5837054\n",
      "Train loss (w/o reg) on all data: 0.5836325\n",
      "Test loss (w/o reg) on all data: 0.5338862\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0014532271\n",
      "Norm of the params: 1.2074906\n",
      "              Random: fixed   2 labels. Loss 0.53389. Accuracy 0.756.\n",
      "### Flips: 60, rs: 34, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [135] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5255973\n",
      "Train loss (w/o reg) on all data: 0.52549887\n",
      "Test loss (w/o reg) on all data: 0.45975927\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004825532\n",
      "Norm of the params: 1.4027752\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.45976. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [297] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4472374\n",
      "Train loss (w/o reg) on all data: 0.44700256\n",
      "Test loss (w/o reg) on all data: 0.5176832\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011525739\n",
      "Norm of the params: 2.167173\n",
      "                Loss: fixed  17 labels. Loss 0.51768. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [257] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58178407\n",
      "Train loss (w/o reg) on all data: 0.58171016\n",
      "Test loss (w/o reg) on all data: 0.53682464\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.00087483955\n",
      "Norm of the params: 1.2158904\n",
      "              Random: fixed   6 labels. Loss 0.53682. Accuracy 0.756.\n",
      "### Flips: 60, rs: 34, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [134] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5078435\n",
      "Train loss (w/o reg) on all data: 0.507734\n",
      "Test loss (w/o reg) on all data: 0.46373248\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010672905\n",
      "Norm of the params: 1.4797411\n",
      "     Influence (LOO): fixed  14 labels. Loss 0.46373. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [326] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38519964\n",
      "Train loss (w/o reg) on all data: 0.38487107\n",
      "Test loss (w/o reg) on all data: 0.5115583\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.011698103\n",
      "Norm of the params: 2.5634985\n",
      "                Loss: fixed  25 labels. Loss 0.51156. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [209] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5780917\n",
      "Train loss (w/o reg) on all data: 0.57802784\n",
      "Test loss (w/o reg) on all data: 0.5304805\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0024575249\n",
      "Norm of the params: 1.129984\n",
      "              Random: fixed   9 labels. Loss 0.53048. Accuracy 0.756.\n",
      "### Flips: 60, rs: 34, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [141] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48772725\n",
      "Train loss (w/o reg) on all data: 0.48763463\n",
      "Test loss (w/o reg) on all data: 0.47446412\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0034223213\n",
      "Norm of the params: 1.3611006\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.47446. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [326] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35573795\n",
      "Train loss (w/o reg) on all data: 0.3553917\n",
      "Test loss (w/o reg) on all data: 0.5306196\n",
      "Train acc on all data:  0.8160377358490566\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0027515502\n",
      "Norm of the params: 2.6315567\n",
      "                Loss: fixed  29 labels. Loss 0.53062. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [271] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5686495\n",
      "Train loss (w/o reg) on all data: 0.56858426\n",
      "Test loss (w/o reg) on all data: 0.5092106\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.005108216\n",
      "Norm of the params: 1.1420877\n",
      "              Random: fixed  14 labels. Loss 0.50921. Accuracy 0.778.\n",
      "### Flips: 60, rs: 34, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [101] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46871206\n",
      "Train loss (w/o reg) on all data: 0.46860024\n",
      "Test loss (w/o reg) on all data: 0.45780918\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0024282702\n",
      "Norm of the params: 1.4955194\n",
      "     Influence (LOO): fixed  24 labels. Loss 0.45781. Accuracy 0.844.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [373] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3388663\n",
      "Train loss (w/o reg) on all data: 0.33851323\n",
      "Test loss (w/o reg) on all data: 0.5430488\n",
      "Train acc on all data:  0.8349056603773585\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00028871055\n",
      "Norm of the params: 2.6572688\n",
      "                Loss: fixed  32 labels. Loss 0.54305. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [343] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5583374\n",
      "Train loss (w/o reg) on all data: 0.55827093\n",
      "Test loss (w/o reg) on all data: 0.4955609\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.004579562\n",
      "Norm of the params: 1.1530951\n",
      "              Random: fixed  17 labels. Loss 0.49556. Accuracy 0.778.\n",
      "### Flips: 60, rs: 34, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [219] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4472122\n",
      "Train loss (w/o reg) on all data: 0.44707164\n",
      "Test loss (w/o reg) on all data: 0.45840934\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0008530477\n",
      "Norm of the params: 1.6766639\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.45841. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [215] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.31282198\n",
      "Train loss (w/o reg) on all data: 0.31234172\n",
      "Test loss (w/o reg) on all data: 0.5381917\n",
      "Train acc on all data:  0.8679245283018868\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0020304504\n",
      "Norm of the params: 3.0992808\n",
      "                Loss: fixed  36 labels. Loss 0.53819. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [297] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5510885\n",
      "Train loss (w/o reg) on all data: 0.5510173\n",
      "Test loss (w/o reg) on all data: 0.46706802\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0011856029\n",
      "Norm of the params: 1.1933819\n",
      "              Random: fixed  20 labels. Loss 0.46707. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [200] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6240812\n",
      "Train loss (w/o reg) on all data: 0.6240573\n",
      "Test loss (w/o reg) on all data: 0.48859352\n",
      "Train acc on all data:  0.6650943396226415\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.004925957\n",
      "Norm of the params: 0.691329\n",
      "Flipped loss: 0.48859. Accuracy: 0.844\n",
      "### Flips: 60, rs: 35, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [149] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5722696\n",
      "Train loss (w/o reg) on all data: 0.57217795\n",
      "Test loss (w/o reg) on all data: 0.48989594\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.7333333333333333\n",
      "Norm of the mean of gradients: 0.0026451354\n",
      "Norm of the params: 1.3542092\n",
      "     Influence (LOO): fixed   8 labels. Loss 0.48990. Accuracy 0.733.\n",
      "Using normal model\n",
      "LBFGS training took [157] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53385204\n",
      "Train loss (w/o reg) on all data: 0.5337589\n",
      "Test loss (w/o reg) on all data: 0.44631547\n",
      "Train acc on all data:  0.7075471698113207\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0011122769\n",
      "Norm of the params: 1.3648105\n",
      "                Loss: fixed   9 labels. Loss 0.44632. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [165] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61974525\n",
      "Train loss (w/o reg) on all data: 0.61971897\n",
      "Test loss (w/o reg) on all data: 0.4697411\n",
      "Train acc on all data:  0.6462264150943396\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0023330417\n",
      "Norm of the params: 0.7253967\n",
      "              Random: fixed   3 labels. Loss 0.46974. Accuracy 0.844.\n",
      "### Flips: 60, rs: 35, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [363] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5493025\n",
      "Train loss (w/o reg) on all data: 0.549186\n",
      "Test loss (w/o reg) on all data: 0.46038866\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0032245454\n",
      "Norm of the params: 1.5264419\n",
      "     Influence (LOO): fixed  12 labels. Loss 0.46039. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49377102\n",
      "Train loss (w/o reg) on all data: 0.493668\n",
      "Test loss (w/o reg) on all data: 0.4040976\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0033049793\n",
      "Norm of the params: 1.4354157\n",
      "                Loss: fixed  14 labels. Loss 0.40410. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [264] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6222552\n",
      "Train loss (w/o reg) on all data: 0.62223285\n",
      "Test loss (w/o reg) on all data: 0.46932822\n",
      "Train acc on all data:  0.6367924528301887\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.008530379\n",
      "Norm of the params: 0.6688867\n",
      "              Random: fixed   4 labels. Loss 0.46933. Accuracy 0.800.\n",
      "### Flips: 60, rs: 35, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [230] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5315821\n",
      "Train loss (w/o reg) on all data: 0.5315162\n",
      "Test loss (w/o reg) on all data: 0.43998897\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.014434622\n",
      "Norm of the params: 1.1482567\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.43999. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [318] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.46075767\n",
      "Train loss (w/o reg) on all data: 0.46064255\n",
      "Test loss (w/o reg) on all data: 0.39258578\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00066226744\n",
      "Norm of the params: 1.5174782\n",
      "                Loss: fixed  19 labels. Loss 0.39259. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [162] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6116086\n",
      "Train loss (w/o reg) on all data: 0.6115834\n",
      "Test loss (w/o reg) on all data: 0.46594337\n",
      "Train acc on all data:  0.6462264150943396\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0074869622\n",
      "Norm of the params: 0.7100825\n",
      "              Random: fixed   7 labels. Loss 0.46594. Accuracy 0.800.\n",
      "### Flips: 60, rs: 35, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [281] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5230434\n",
      "Train loss (w/o reg) on all data: 0.522965\n",
      "Test loss (w/o reg) on all data: 0.42250243\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0036958468\n",
      "Norm of the params: 1.2520643\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.42250. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [313] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4262581\n",
      "Train loss (w/o reg) on all data: 0.42609486\n",
      "Test loss (w/o reg) on all data: 0.38740838\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00023519954\n",
      "Norm of the params: 1.8068011\n",
      "                Loss: fixed  25 labels. Loss 0.38741. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [125] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6148415\n",
      "Train loss (w/o reg) on all data: 0.6148211\n",
      "Test loss (w/o reg) on all data: 0.46208158\n",
      "Train acc on all data:  0.6509433962264151\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0051779733\n",
      "Norm of the params: 0.6390649\n",
      "              Random: fixed   9 labels. Loss 0.46208. Accuracy 0.822.\n",
      "### Flips: 60, rs: 35, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [359] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5127141\n",
      "Train loss (w/o reg) on all data: 0.51264286\n",
      "Test loss (w/o reg) on all data: 0.391592\n",
      "Train acc on all data:  0.7358490566037735\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00045557882\n",
      "Norm of the params: 1.1934155\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.39159. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [166] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.39139682\n",
      "Train loss (w/o reg) on all data: 0.39127335\n",
      "Test loss (w/o reg) on all data: 0.37417933\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0007916714\n",
      "Norm of the params: 1.5713487\n",
      "                Loss: fixed  31 labels. Loss 0.37418. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [205] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6084166\n",
      "Train loss (w/o reg) on all data: 0.60839295\n",
      "Test loss (w/o reg) on all data: 0.4468482\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002861655\n",
      "Norm of the params: 0.687768\n",
      "              Random: fixed  11 labels. Loss 0.44685. Accuracy 0.822.\n",
      "### Flips: 60, rs: 35, checks: 60\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [120] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49424303\n",
      "Train loss (w/o reg) on all data: 0.49416918\n",
      "Test loss (w/o reg) on all data: 0.36896637\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.006367346\n",
      "Norm of the params: 1.2152979\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.36897. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [245] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36234972\n",
      "Train loss (w/o reg) on all data: 0.3622013\n",
      "Test loss (w/o reg) on all data: 0.39500153\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0010596076\n",
      "Norm of the params: 1.722847\n",
      "                Loss: fixed  36 labels. Loss 0.39500. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [191] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59633034\n",
      "Train loss (w/o reg) on all data: 0.5962984\n",
      "Test loss (w/o reg) on all data: 0.43930948\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0066659106\n",
      "Norm of the params: 0.79938126\n",
      "              Random: fixed  14 labels. Loss 0.43931. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [381] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6184352\n",
      "Train loss (w/o reg) on all data: 0.61837876\n",
      "Test loss (w/o reg) on all data: 0.48328623\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0023877898\n",
      "Norm of the params: 1.0625539\n",
      "Flipped loss: 0.48329. Accuracy: 0.778\n",
      "### Flips: 60, rs: 36, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [168] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58668405\n",
      "Train loss (w/o reg) on all data: 0.5866224\n",
      "Test loss (w/o reg) on all data: 0.45553407\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.7555555555555555\n",
      "Norm of the mean of gradients: 0.0011042919\n",
      "Norm of the params: 1.1100831\n",
      "     Influence (LOO): fixed   5 labels. Loss 0.45553. Accuracy 0.756.\n",
      "Using normal model\n",
      "LBFGS training took [310] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5595638\n",
      "Train loss (w/o reg) on all data: 0.5594257\n",
      "Test loss (w/o reg) on all data: 0.44083703\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00578507\n",
      "Norm of the params: 1.6617975\n",
      "                Loss: fixed   7 labels. Loss 0.44084. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [274] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61676925\n",
      "Train loss (w/o reg) on all data: 0.6167178\n",
      "Test loss (w/o reg) on all data: 0.48808518\n",
      "Train acc on all data:  0.6462264150943396\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.010764032\n",
      "Norm of the params: 1.0140197\n",
      "              Random: fixed   3 labels. Loss 0.48809. Accuracy 0.778.\n",
      "### Flips: 60, rs: 36, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [166] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56462616\n",
      "Train loss (w/o reg) on all data: 0.56455266\n",
      "Test loss (w/o reg) on all data: 0.41110197\n",
      "Train acc on all data:  0.7122641509433962\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.009166109\n",
      "Norm of the params: 1.2121601\n",
      "     Influence (LOO): fixed  10 labels. Loss 0.41110. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [344] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49182633\n",
      "Train loss (w/o reg) on all data: 0.49166328\n",
      "Test loss (w/o reg) on all data: 0.39919803\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0020162645\n",
      "Norm of the params: 1.8058192\n",
      "                Loss: fixed  16 labels. Loss 0.39920. Accuracy 0.778.\n",
      "Using normal model\n",
      "LBFGS training took [314] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6120421\n",
      "Train loss (w/o reg) on all data: 0.6120111\n",
      "Test loss (w/o reg) on all data: 0.45499653\n",
      "Train acc on all data:  0.6509433962264151\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0026196034\n",
      "Norm of the params: 0.7878521\n",
      "              Random: fixed   7 labels. Loss 0.45500. Accuracy 0.800.\n",
      "### Flips: 60, rs: 36, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [142] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.52933496\n",
      "Train loss (w/o reg) on all data: 0.5292637\n",
      "Test loss (w/o reg) on all data: 0.38215375\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0003413892\n",
      "Norm of the params: 1.194064\n",
      "     Influence (LOO): fixed  15 labels. Loss 0.38215. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [248] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.44125465\n",
      "Train loss (w/o reg) on all data: 0.44104213\n",
      "Test loss (w/o reg) on all data: 0.34047446\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0016350845\n",
      "Norm of the params: 2.0615911\n",
      "                Loss: fixed  22 labels. Loss 0.34047. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [297] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59536326\n",
      "Train loss (w/o reg) on all data: 0.5953336\n",
      "Test loss (w/o reg) on all data: 0.4440484\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0048894715\n",
      "Norm of the params: 0.77083755\n",
      "              Random: fixed  11 labels. Loss 0.44405. Accuracy 0.778.\n",
      "### Flips: 60, rs: 36, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [166] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5057138\n",
      "Train loss (w/o reg) on all data: 0.5056217\n",
      "Test loss (w/o reg) on all data: 0.36569718\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0024344467\n",
      "Norm of the params: 1.3577794\n",
      "     Influence (LOO): fixed  20 labels. Loss 0.36570. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [318] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3886995\n",
      "Train loss (w/o reg) on all data: 0.38837555\n",
      "Test loss (w/o reg) on all data: 0.33207905\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0008367749\n",
      "Norm of the params: 2.5454082\n",
      "                Loss: fixed  28 labels. Loss 0.33208. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [272] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5933535\n",
      "Train loss (w/o reg) on all data: 0.59332424\n",
      "Test loss (w/o reg) on all data: 0.44428506\n",
      "Train acc on all data:  0.6886792452830188\n",
      "Test acc on all data:   0.7777777777777778\n",
      "Norm of the mean of gradients: 0.0027949233\n",
      "Norm of the params: 0.76502377\n",
      "              Random: fixed  12 labels. Loss 0.44429. Accuracy 0.778.\n",
      "### Flips: 60, rs: 36, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [151] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49133086\n",
      "Train loss (w/o reg) on all data: 0.49124005\n",
      "Test loss (w/o reg) on all data: 0.36602098\n",
      "Train acc on all data:  0.7735849056603774\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0055125398\n",
      "Norm of the params: 1.3477418\n",
      "     Influence (LOO): fixed  23 labels. Loss 0.36602. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [318] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.35509217\n",
      "Train loss (w/o reg) on all data: 0.35474503\n",
      "Test loss (w/o reg) on all data: 0.35345882\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0005812244\n",
      "Norm of the params: 2.634891\n",
      "                Loss: fixed  34 labels. Loss 0.35346. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [360] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5792238\n",
      "Train loss (w/o reg) on all data: 0.5791796\n",
      "Test loss (w/o reg) on all data: 0.4208182\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0032745327\n",
      "Norm of the params: 0.940567\n",
      "              Random: fixed  17 labels. Loss 0.42082. Accuracy 0.822.\n",
      "### Flips: 60, rs: 36, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss (w reg) on all data: 0.46089283\n",
      "Train loss (w/o reg) on all data: 0.46078584\n",
      "Test loss (w/o reg) on all data: 0.34269813\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.003137619\n",
      "Norm of the params: 1.4627448\n",
      "     Influence (LOO): fixed  27 labels. Loss 0.34270. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [388] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.34031975\n",
      "Train loss (w/o reg) on all data: 0.33992314\n",
      "Test loss (w/o reg) on all data: 0.38809314\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0012035549\n",
      "Norm of the params: 2.8164215\n",
      "                Loss: fixed  38 labels. Loss 0.38809. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [342] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.585047\n",
      "Train loss (w/o reg) on all data: 0.58501565\n",
      "Test loss (w/o reg) on all data: 0.42562184\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0013021535\n",
      "Norm of the params: 0.79172724\n",
      "              Random: fixed  21 labels. Loss 0.42562. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [331] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6246762\n",
      "Train loss (w/o reg) on all data: 0.6246529\n",
      "Test loss (w/o reg) on all data: 0.47793117\n",
      "Train acc on all data:  0.6556603773584906\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0011553599\n",
      "Norm of the params: 0.68254375\n",
      "Flipped loss: 0.47793. Accuracy: 0.822\n",
      "### Flips: 60, rs: 37, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [304] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5660113\n",
      "Train loss (w/o reg) on all data: 0.5659149\n",
      "Test loss (w/o reg) on all data: 0.43266076\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0032629506\n",
      "Norm of the params: 1.3885282\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.43266. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [175] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5530876\n",
      "Train loss (w/o reg) on all data: 0.5530029\n",
      "Test loss (w/o reg) on all data: 0.40835604\n",
      "Train acc on all data:  0.7264150943396226\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001575148\n",
      "Norm of the params: 1.3013867\n",
      "                Loss: fixed   8 labels. Loss 0.40836. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [249] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6201211\n",
      "Train loss (w/o reg) on all data: 0.6200881\n",
      "Test loss (w/o reg) on all data: 0.4830995\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0011133315\n",
      "Norm of the params: 0.81243336\n",
      "              Random: fixed   2 labels. Loss 0.48310. Accuracy 0.844.\n",
      "### Flips: 60, rs: 37, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [142] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.51754236\n",
      "Train loss (w/o reg) on all data: 0.51744074\n",
      "Test loss (w/o reg) on all data: 0.36917356\n",
      "Train acc on all data:  0.7830188679245284\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0013236373\n",
      "Norm of the params: 1.4257661\n",
      "     Influence (LOO): fixed  16 labels. Loss 0.36917. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [344] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47136232\n",
      "Train loss (w/o reg) on all data: 0.47121486\n",
      "Test loss (w/o reg) on all data: 0.35203937\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0006194532\n",
      "Norm of the params: 1.717368\n",
      "                Loss: fixed  17 labels. Loss 0.35204. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [195] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6060897\n",
      "Train loss (w/o reg) on all data: 0.60604715\n",
      "Test loss (w/o reg) on all data: 0.4497282\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.012924185\n",
      "Norm of the params: 0.92226744\n",
      "              Random: fixed   8 labels. Loss 0.44973. Accuracy 0.867.\n",
      "### Flips: 60, rs: 37, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [216] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49662098\n",
      "Train loss (w/o reg) on all data: 0.49650285\n",
      "Test loss (w/o reg) on all data: 0.35348403\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0026853583\n",
      "Norm of the params: 1.5370429\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.35348. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.43479225\n",
      "Train loss (w/o reg) on all data: 0.43463683\n",
      "Test loss (w/o reg) on all data: 0.32373002\n",
      "Train acc on all data:  0.7783018867924528\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.00081870845\n",
      "Norm of the params: 1.76306\n",
      "                Loss: fixed  23 labels. Loss 0.32373. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [323] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6012412\n",
      "Train loss (w/o reg) on all data: 0.60120153\n",
      "Test loss (w/o reg) on all data: 0.43805012\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0026100215\n",
      "Norm of the params: 0.8903\n",
      "              Random: fixed   9 labels. Loss 0.43805. Accuracy 0.867.\n",
      "### Flips: 60, rs: 37, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [233] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48481405\n",
      "Train loss (w/o reg) on all data: 0.48470163\n",
      "Test loss (w/o reg) on all data: 0.33995953\n",
      "Train acc on all data:  0.7877358490566038\n",
      "Test acc on all data:   0.9111111111111111\n",
      "Norm of the mean of gradients: 0.0020103888\n",
      "Norm of the params: 1.4994766\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.33996. Accuracy 0.911.\n",
      "Using normal model\n",
      "LBFGS training took [297] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3865797\n",
      "Train loss (w/o reg) on all data: 0.3863617\n",
      "Test loss (w/o reg) on all data: 0.30404198\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8888888888888888\n",
      "Norm of the mean of gradients: 0.0012353109\n",
      "Norm of the params: 2.0880167\n",
      "                Loss: fixed  30 labels. Loss 0.30404. Accuracy 0.889.\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5998916\n",
      "Train loss (w/o reg) on all data: 0.5998586\n",
      "Test loss (w/o reg) on all data: 0.42336595\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.003993859\n",
      "Norm of the params: 0.812459\n",
      "              Random: fixed  13 labels. Loss 0.42337. Accuracy 0.867.\n",
      "### Flips: 60, rs: 37, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [187] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4743429\n",
      "Train loss (w/o reg) on all data: 0.47421148\n",
      "Test loss (w/o reg) on all data: 0.33350447\n",
      "Train acc on all data:  0.8066037735849056\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0011042325\n",
      "Norm of the params: 1.6213205\n",
      "     Influence (LOO): fixed  25 labels. Loss 0.33350. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [207] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36279914\n",
      "Train loss (w/o reg) on all data: 0.3625997\n",
      "Test loss (w/o reg) on all data: 0.30958274\n",
      "Train acc on all data:  0.839622641509434\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.00038377108\n",
      "Norm of the params: 1.9972367\n",
      "                Loss: fixed  34 labels. Loss 0.30958. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [213] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.59154475\n",
      "Train loss (w/o reg) on all data: 0.5915079\n",
      "Test loss (w/o reg) on all data: 0.41911048\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0033484614\n",
      "Norm of the params: 0.858398\n",
      "              Random: fixed  16 labels. Loss 0.41911. Accuracy 0.844.\n",
      "### Flips: 60, rs: 37, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [100] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.4576899\n",
      "Train loss (w/o reg) on all data: 0.4575532\n",
      "Test loss (w/o reg) on all data: 0.31716874\n",
      "Train acc on all data:  0.8113207547169812\n",
      "Test acc on all data:   0.9111111111111111\n",
      "Norm of the mean of gradients: 0.006346582\n",
      "Norm of the params: 1.6534948\n",
      "     Influence (LOO): fixed  30 labels. Loss 0.31717. Accuracy 0.911.\n",
      "Using normal model\n",
      "LBFGS training took [276] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3379052\n",
      "Train loss (w/o reg) on all data: 0.33765638\n",
      "Test loss (w/o reg) on all data: 0.31738317\n",
      "Train acc on all data:  0.8632075471698113\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.002744556\n",
      "Norm of the params: 2.2308104\n",
      "                Loss: fixed  38 labels. Loss 0.31738. Accuracy 0.867.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [356] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5626961\n",
      "Train loss (w/o reg) on all data: 0.56264734\n",
      "Test loss (w/o reg) on all data: 0.41251713\n",
      "Train acc on all data:  0.7452830188679245\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.00163732\n",
      "Norm of the params: 0.9875246\n",
      "              Random: fixed  23 labels. Loss 0.41252. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [320] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.61156756\n",
      "Train loss (w/o reg) on all data: 0.61153007\n",
      "Test loss (w/o reg) on all data: 0.4996124\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.004016077\n",
      "Norm of the params: 0.86587846\n",
      "Flipped loss: 0.49961. Accuracy: 0.822\n",
      "### Flips: 60, rs: 38, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [97] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5891552\n",
      "Train loss (w/o reg) on all data: 0.58910877\n",
      "Test loss (w/o reg) on all data: 0.45342547\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0044969684\n",
      "Norm of the params: 0.9637014\n",
      "     Influence (LOO): fixed   4 labels. Loss 0.45343. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [126] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.53550106\n",
      "Train loss (w/o reg) on all data: 0.53541684\n",
      "Test loss (w/o reg) on all data: 0.44939888\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0023651994\n",
      "Norm of the params: 1.297655\n",
      "                Loss: fixed   9 labels. Loss 0.44940. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [134] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58257514\n",
      "Train loss (w/o reg) on all data: 0.58252424\n",
      "Test loss (w/o reg) on all data: 0.44669944\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0043192846\n",
      "Norm of the params: 1.0091293\n",
      "              Random: fixed   8 labels. Loss 0.44670. Accuracy 0.822.\n",
      "### Flips: 60, rs: 38, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [146] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56095743\n",
      "Train loss (w/o reg) on all data: 0.5608946\n",
      "Test loss (w/o reg) on all data: 0.4102161\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0028534867\n",
      "Norm of the params: 1.120821\n",
      "     Influence (LOO): fixed   9 labels. Loss 0.41022. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [182] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.48462597\n",
      "Train loss (w/o reg) on all data: 0.48449007\n",
      "Test loss (w/o reg) on all data: 0.42307705\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00582862\n",
      "Norm of the params: 1.6485385\n",
      "                Loss: fixed  15 labels. Loss 0.42308. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [169] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58711904\n",
      "Train loss (w/o reg) on all data: 0.58706224\n",
      "Test loss (w/o reg) on all data: 0.4416976\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.002074397\n",
      "Norm of the params: 1.0661284\n",
      "              Random: fixed  11 labels. Loss 0.44170. Accuracy 0.867.\n",
      "### Flips: 60, rs: 38, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [260] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55989546\n",
      "Train loss (w/o reg) on all data: 0.55982727\n",
      "Test loss (w/o reg) on all data: 0.40719673\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.005423642\n",
      "Norm of the params: 1.1675968\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.40720. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [369] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.42927447\n",
      "Train loss (w/o reg) on all data: 0.42905578\n",
      "Test loss (w/o reg) on all data: 0.44417128\n",
      "Train acc on all data:  0.7924528301886793\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.001829554\n",
      "Norm of the params: 2.0913277\n",
      "                Loss: fixed  22 labels. Loss 0.44417. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [173] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58101374\n",
      "Train loss (w/o reg) on all data: 0.5809505\n",
      "Test loss (w/o reg) on all data: 0.4200831\n",
      "Train acc on all data:  0.6933962264150944\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.008073702\n",
      "Norm of the params: 1.1246991\n",
      "              Random: fixed  13 labels. Loss 0.42008. Accuracy 0.867.\n",
      "### Flips: 60, rs: 38, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [136] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5158278\n",
      "Train loss (w/o reg) on all data: 0.5157366\n",
      "Test loss (w/o reg) on all data: 0.39354512\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.010328945\n",
      "Norm of the params: 1.3503746\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.39355. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [239] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3948496\n",
      "Train loss (w/o reg) on all data: 0.39458027\n",
      "Test loss (w/o reg) on all data: 0.46284235\n",
      "Train acc on all data:  0.8018867924528302\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0059354594\n",
      "Norm of the params: 2.3208747\n",
      "                Loss: fixed  27 labels. Loss 0.46284. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [293] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5855222\n",
      "Train loss (w/o reg) on all data: 0.58546954\n",
      "Test loss (w/o reg) on all data: 0.42970166\n",
      "Train acc on all data:  0.6839622641509434\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0009048696\n",
      "Norm of the params: 1.0256896\n",
      "              Random: fixed  15 labels. Loss 0.42970. Accuracy 0.844.\n",
      "### Flips: 60, rs: 38, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [145] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.50852084\n",
      "Train loss (w/o reg) on all data: 0.5084089\n",
      "Test loss (w/o reg) on all data: 0.39853075\n",
      "Train acc on all data:  0.7547169811320755\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.00078346627\n",
      "Norm of the params: 1.4960827\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.39853. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.36134672\n",
      "Train loss (w/o reg) on all data: 0.3610033\n",
      "Test loss (w/o reg) on all data: 0.44722795\n",
      "Train acc on all data:  0.8254716981132075\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.007982069\n",
      "Norm of the params: 2.6207764\n",
      "                Loss: fixed  32 labels. Loss 0.44723. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [327] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58450246\n",
      "Train loss (w/o reg) on all data: 0.5844507\n",
      "Test loss (w/o reg) on all data: 0.42646298\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0012534048\n",
      "Norm of the params: 1.0171615\n",
      "              Random: fixed  16 labels. Loss 0.42646. Accuracy 0.844.\n",
      "### Flips: 60, rs: 38, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [124] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.47155288\n",
      "Train loss (w/o reg) on all data: 0.4713926\n",
      "Test loss (w/o reg) on all data: 0.35909936\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0023092933\n",
      "Norm of the params: 1.7903906\n",
      "     Influence (LOO): fixed  26 labels. Loss 0.35910. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [270] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.33601278\n",
      "Train loss (w/o reg) on all data: 0.33554348\n",
      "Test loss (w/o reg) on all data: 0.44940445\n",
      "Train acc on all data:  0.8301886792452831\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0015460794\n",
      "Norm of the params: 3.0636532\n",
      "                Loss: fixed  35 labels. Loss 0.44940. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [298] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5854646\n",
      "Train loss (w/o reg) on all data: 0.5854065\n",
      "Test loss (w/o reg) on all data: 0.42241526\n",
      "Train acc on all data:  0.7169811320754716\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.015136942\n",
      "Norm of the params: 1.0781316\n",
      "              Random: fixed  17 labels. Loss 0.42242. Accuracy 0.822.\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [261] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6357958\n",
      "Train loss (w/o reg) on all data: 0.63575006\n",
      "Test loss (w/o reg) on all data: 0.5060192\n",
      "Train acc on all data:  0.6132075471698113\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014364348\n",
      "Norm of the params: 0.95633566\n",
      "Flipped loss: 0.50602. Accuracy: 0.800\n",
      "### Flips: 60, rs: 39, checks: 10\n",
      "Using normal model\n",
      "LBFGS training took [212] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.58562905\n",
      "Train loss (w/o reg) on all data: 0.5855484\n",
      "Test loss (w/o reg) on all data: 0.44418457\n",
      "Train acc on all data:  0.6650943396226415\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0040562344\n",
      "Norm of the params: 1.2701288\n",
      "     Influence (LOO): fixed   6 labels. Loss 0.44418. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [109] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56933594\n",
      "Train loss (w/o reg) on all data: 0.5692363\n",
      "Test loss (w/o reg) on all data: 0.42675367\n",
      "Train acc on all data:  0.6745283018867925\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0091116065\n",
      "Norm of the params: 1.4117103\n",
      "                Loss: fixed   8 labels. Loss 0.42675. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [155] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6348791\n",
      "Train loss (w/o reg) on all data: 0.6348309\n",
      "Test loss (w/o reg) on all data: 0.49935117\n",
      "Train acc on all data:  0.6320754716981132\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.006666113\n",
      "Norm of the params: 0.9823129\n",
      "              Random: fixed   2 labels. Loss 0.49935. Accuracy 0.844.\n",
      "### Flips: 60, rs: 39, checks: 20\n",
      "Using normal model\n",
      "LBFGS training took [189] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.55250984\n",
      "Train loss (w/o reg) on all data: 0.55239284\n",
      "Test loss (w/o reg) on all data: 0.41822082\n",
      "Train acc on all data:  0.6792452830188679\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.005750434\n",
      "Norm of the params: 1.5297675\n",
      "     Influence (LOO): fixed  11 labels. Loss 0.41822. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [134] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5044516\n",
      "Train loss (w/o reg) on all data: 0.50429153\n",
      "Test loss (w/o reg) on all data: 0.38804534\n",
      "Train acc on all data:  0.7405660377358491\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004599921\n",
      "Norm of the params: 1.7891183\n",
      "                Loss: fixed  17 labels. Loss 0.38805. Accuracy 0.800.\n",
      "Using normal model\n",
      "LBFGS training took [156] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.6246734\n",
      "Train loss (w/o reg) on all data: 0.62460786\n",
      "Test loss (w/o reg) on all data: 0.49621242\n",
      "Train acc on all data:  0.6320754716981132\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.004561669\n",
      "Norm of the params: 1.145118\n",
      "              Random: fixed   4 labels. Loss 0.49621. Accuracy 0.800.\n",
      "### Flips: 60, rs: 39, checks: 30\n",
      "Using normal model\n",
      "LBFGS training took [102] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5429824\n",
      "Train loss (w/o reg) on all data: 0.5428697\n",
      "Test loss (w/o reg) on all data: 0.40723452\n",
      "Train acc on all data:  0.7216981132075472\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.0069342395\n",
      "Norm of the params: 1.501356\n",
      "     Influence (LOO): fixed  13 labels. Loss 0.40723. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [83] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.45266667\n",
      "Train loss (w/o reg) on all data: 0.45244223\n",
      "Test loss (w/o reg) on all data: 0.38376528\n",
      "Train acc on all data:  0.7641509433962265\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.002204086\n",
      "Norm of the params: 2.1186228\n",
      "                Loss: fixed  24 labels. Loss 0.38377. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [188] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.60813826\n",
      "Train loss (w/o reg) on all data: 0.60808116\n",
      "Test loss (w/o reg) on all data: 0.45953912\n",
      "Train acc on all data:  0.6698113207547169\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0072016753\n",
      "Norm of the params: 1.0687566\n",
      "              Random: fixed   8 labels. Loss 0.45954. Accuracy 0.822.\n",
      "### Flips: 60, rs: 39, checks: 40\n",
      "Using normal model\n",
      "LBFGS training took [158] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5212005\n",
      "Train loss (w/o reg) on all data: 0.52105904\n",
      "Test loss (w/o reg) on all data: 0.39385685\n",
      "Train acc on all data:  0.7594339622641509\n",
      "Test acc on all data:   0.8444444444444444\n",
      "Norm of the mean of gradients: 0.00090781157\n",
      "Norm of the params: 1.6820908\n",
      "     Influence (LOO): fixed  17 labels. Loss 0.39386. Accuracy 0.844.\n",
      "Using normal model\n",
      "LBFGS training took [163] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.41121125\n",
      "Train loss (w/o reg) on all data: 0.41094485\n",
      "Test loss (w/o reg) on all data: 0.3943407\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0016611888\n",
      "Norm of the params: 2.3082783\n",
      "                Loss: fixed  30 labels. Loss 0.39434. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [273] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5998253\n",
      "Train loss (w/o reg) on all data: 0.5997612\n",
      "Test loss (w/o reg) on all data: 0.4427403\n",
      "Train acc on all data:  0.6981132075471698\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0040446953\n",
      "Norm of the params: 1.132812\n",
      "              Random: fixed  10 labels. Loss 0.44274. Accuracy 0.822.\n",
      "### Flips: 60, rs: 39, checks: 50\n",
      "Using normal model\n",
      "LBFGS training took [254] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.499902\n",
      "Train loss (w/o reg) on all data: 0.49974248\n",
      "Test loss (w/o reg) on all data: 0.37238204\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0009870849\n",
      "Norm of the params: 1.7862806\n",
      "     Influence (LOO): fixed  19 labels. Loss 0.37238. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [308] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.38728604\n",
      "Train loss (w/o reg) on all data: 0.3869666\n",
      "Test loss (w/o reg) on all data: 0.3798544\n",
      "Train acc on all data:  0.8207547169811321\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.0018051193\n",
      "Norm of the params: 2.5276556\n",
      "                Loss: fixed  33 labels. Loss 0.37985. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [238] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.5953308\n",
      "Train loss (w/o reg) on all data: 0.5952827\n",
      "Test loss (w/o reg) on all data: 0.43312135\n",
      "Train acc on all data:  0.7028301886792453\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0014943302\n",
      "Norm of the params: 0.980832\n",
      "              Random: fixed  14 labels. Loss 0.43312. Accuracy 0.800.\n",
      "### Flips: 60, rs: 39, checks: 60\n",
      "Using normal model\n",
      "LBFGS training took [185] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.49097002\n",
      "Train loss (w/o reg) on all data: 0.49079952\n",
      "Test loss (w/o reg) on all data: 0.36686116\n",
      "Train acc on all data:  0.7688679245283019\n",
      "Test acc on all data:   0.8666666666666667\n",
      "Norm of the mean of gradients: 0.0015515256\n",
      "Norm of the params: 1.8465964\n",
      "     Influence (LOO): fixed  21 labels. Loss 0.36686. Accuracy 0.867.\n",
      "Using normal model\n",
      "LBFGS training took [220] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.3563165\n",
      "Train loss (w/o reg) on all data: 0.3559735\n",
      "Test loss (w/o reg) on all data: 0.40253466\n",
      "Train acc on all data:  0.8537735849056604\n",
      "Test acc on all data:   0.8222222222222222\n",
      "Norm of the mean of gradients: 0.011456952\n",
      "Norm of the params: 2.619128\n",
      "                Loss: fixed  38 labels. Loss 0.40253. Accuracy 0.822.\n",
      "Using normal model\n",
      "LBFGS training took [189] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.56577057\n",
      "Train loss (w/o reg) on all data: 0.5657033\n",
      "Test loss (w/o reg) on all data: 0.4224679\n",
      "Train acc on all data:  0.7311320754716981\n",
      "Test acc on all data:   0.8\n",
      "Norm of the mean of gradients: 0.0018278183\n",
      "Norm of the params: 1.1598889\n",
      "              Random: fixed  19 labels. Loss 0.42247. Accuracy 0.800.\n"
     ]
    }
   ],
   "source": [
    "tf_model.train()\n",
    "\n",
    "X_train = np.copy(tf_model.data_sets.train.x)\n",
    "Y_train = np.copy(tf_model.data_sets.train.labels)\n",
    "X_test = np.copy(tf_model.data_sets.test.x)\n",
    "Y_test = np.copy(tf_model.data_sets.test.labels) \n",
    "\n",
    "\n",
    "num_train_examples = Y_train.shape[0] \n",
    "num_flip_vals = 6\n",
    "num_check_vals = 6\n",
    "num_random_seeds = 40\n",
    "\n",
    "dims = (num_flip_vals, num_check_vals, num_random_seeds, 3)\n",
    "fixed_influence_loo_results = np.zeros(dims)\n",
    "fixed_loss_results = np.zeros(dims)\n",
    "fixed_random_results = np.zeros(dims)\n",
    "\n",
    "flipped_results = np.zeros((num_flip_vals, num_random_seeds, 3))\n",
    "\n",
    "orig_results = tf_model.sess.run(\n",
    "    [tf_model.loss_no_reg, tf_model.accuracy_op], \n",
    "    feed_dict=tf_model.all_test_feed_dict)\n",
    "            \n",
    "print('Orig loss: %.5f. Accuracy: %.3f' % (orig_results[0], orig_results[1]))\n",
    "\n",
    "for flips_idx in range(num_flip_vals):\n",
    "    for random_seed_idx in range(num_random_seeds):\n",
    "        \n",
    "        random_seed = flips_idx * (num_random_seeds * 3) + (random_seed_idx * 2)        \n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "        num_flips = int(num_train_examples / 20) * (flips_idx + 1)    \n",
    "        idx_to_flip = np.random.choice(num_train_examples, size=num_flips, replace=False)\n",
    "        Y_train_flipped = np.copy(Y_train)\n",
    "        Y_train_flipped[idx_to_flip] = 1 - Y_train[idx_to_flip] \n",
    "        \n",
    "        tf_model.update_train_x_y(X_train, Y_train_flipped)\n",
    "        tf_model.train()        \n",
    "        flipped_results[flips_idx, random_seed_idx, 1:] = tf_model.sess.run(\n",
    "            [tf_model.loss_no_reg, tf_model.accuracy_op], \n",
    "            feed_dict=tf_model.all_test_feed_dict)\n",
    "        print('Flipped loss: %.5f. Accuracy: %.3f' % (\n",
    "                flipped_results[flips_idx, random_seed_idx, 1], flipped_results[flips_idx, random_seed_idx, 2]))\n",
    "        \n",
    "        train_losses = tf_model.sess.run(tf_model.indiv_loss_no_reg, feed_dict=tf_model.all_train_feed_dict)\n",
    "        train_loo_influences = tf_model.get_loo_influences()\n",
    "\n",
    "        for checks_idx in range(num_check_vals):\n",
    "            np.random.seed(random_seed + 1)\n",
    "            num_checks = int(num_train_examples / 20) * (checks_idx + 1)\n",
    "\n",
    "            print('### Flips: %s, rs: %s, checks: %s' % (num_flips, random_seed_idx, num_checks))\n",
    "\n",
    "            fixed_influence_loo_results[flips_idx, checks_idx, random_seed_idx, :], \\\n",
    "              fixed_loss_results[flips_idx, checks_idx, random_seed_idx, :], \\\n",
    "              fixed_random_results[flips_idx, checks_idx, random_seed_idx, :] \\\n",
    "              = experiments.test_mislabeled_detection_batch(\n",
    "                tf_model, \n",
    "                X_train, Y_train,\n",
    "                Y_train_flipped,\n",
    "                X_test, Y_test, \n",
    "                train_losses, train_loo_influences,\n",
    "                num_flips, num_checks)\n",
    "\n",
    "\n",
    "np.savez(\n",
    "    'output/heart_results2', \n",
    "    orig_results=orig_results,\n",
    "    flipped_results=flipped_results,\n",
    "    fixed_influence_loo_results=fixed_influence_loo_results,\n",
    "    fixed_loss_results=fixed_loss_results,\n",
    "    fixed_random_results=fixed_random_results\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
